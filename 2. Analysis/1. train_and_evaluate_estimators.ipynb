{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5545a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_val_predict, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn import feature_selection, metrics, set_config, svm, utils\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,accuracy_score, balanced_accuracy_score,\n",
    "                             brier_score_loss, classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import (check_consistent_length, check_random_state, check_X_y)\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce15666",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split ratios\n",
    "n_jobs = 1\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.10\n",
    "validation_ratio = 0.15\n",
    "test_split = test_size = 1 - train_ratio\n",
    "validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "# Cross-validation\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "partition = True\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "class_weight = 'balanced'\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_sizes = [300, 100]\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "metrics_list = [\n",
    "    'Mean Validation Score',\n",
    "    'Explained Variance',\n",
    "    'Accuracy',\n",
    "    'Precision',\n",
    "    'Recall',\n",
    "    'F1-score',\n",
    "    'ROC',\n",
    "    'AUC',\n",
    "    'Matthews Correlation Coefficient',\n",
    "    f'{scoring.title()} Best Threshold',\n",
    "    f'{scoring.title()} Best Score',\n",
    "    'Log Loss/Cross Entropy',\n",
    "    'Classification Report',\n",
    "    'Confusion Matrix',\n",
    "]\n",
    "\n",
    "# Paths\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "plot_save_path = f'{data_dir}plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df119b8",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count_ = CountVectorizer()\n",
    "count_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'CountVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "count = [count_, count_params]\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf_ = TfidfVectorizer()\n",
    "tfidf_params = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "    'TfidfVectorizer___use_idf': [True, False],\n",
    "    'TfidfVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'TfidfVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "tfidf = [tfidf_, tfidf_params]\n",
    "\n",
    "## Vectorizers List\n",
    "vectorizers_list = [count, tfidf]\n",
    "\n",
    "### BOW FeatureUnion\n",
    "transformer_list = []\n",
    "bow_params = {}\n",
    "for vectorizer_and_params in vectorizers_list:\n",
    "    transformer_list.append(\n",
    "        tuple([vectorizer_and_params[0].__class__.__name__, vectorizer_and_params[0]])\n",
    "    )\n",
    "    bow_params.update(\n",
    "        vectorizer_and_params[1]\n",
    "    )\n",
    "\n",
    "bow_ = FeatureUnion(\n",
    "    transformer_list=[transformer_list]\n",
    ")\n",
    "bow = [bow_, bow_params]\n",
    "\n",
    "## Vectorizers List append bow\n",
    "vectorizers_list.append(bow)\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    vectorizer_and_params[0].__class__.__name__: vectorizer_and_params\n",
    "    for vectorizer_and_params in vectorizers_list\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaff38f",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SelectKBest\n",
    "selectkbest_ = SelectKBest()\n",
    "selectkbest_params = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "selectkbest = [selectkbest_, selectkbest_params]\n",
    "\n",
    "### SelectPercentile\n",
    "selectperc_ = SelectPercentile()\n",
    "selectperc_params = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectPercentile__percentile': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "selectperc = [selectperc_, selectperc_params]\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr_ = SelectFpr()\n",
    "selectfpr_params = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfpr = [selectfpr_, selectfpr_params]\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr_ = SelectFdr()\n",
    "selectfdr_params = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfdr = [selectfdr_, selectfdr_params]\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe_ = SelectFwe()\n",
    "selectfwe_params = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "selectfwe = [selectfwe_, selectfwe_params]\n",
    "\n",
    "## Selectors List\n",
    "selectors_list = [\n",
    "    selectkbest, selectperc, selectfpr, selectfdr, selectfwe\n",
    "]\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    selector_and_params[0].__class__.__name__: selector_and_params\n",
    "    for selector_and_params in selectors_list\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4da5a77",
   "metadata": {},
   "source": [
    "## Resamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed119d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resamplers\n",
    "### SMOTETomek Resampler\n",
    "smotetomek_ = SMOTETomek()\n",
    "smotetomek_params = {\n",
    "    'SMOTETomek__random_state': [random_state],\n",
    "    'SMOTETomek__tomek': [TomekLinks(sampling_strategy='majority')],\n",
    "    'SMOTETomek__n_jobs': [-1],\n",
    "}\n",
    "smotetomek = [smotetomek_, smotetomek_params]\n",
    "\n",
    "### SMOTENC Resampler\n",
    "smotenc_ = SMOTENC(categorical_features=[[df_manual.columns.get_loc('Warmth'), df_manual.columns.get_loc('Competence')]])\n",
    "smotenc_params = {\n",
    "    'SMOTENC__random_state': [random_state],\n",
    "    'SMOTENC__sampling_strategy': [.6],\n",
    "}\n",
    "smotenc = [smotenc_, smotenc_params]\n",
    "\n",
    "## Selectors List\n",
    "resamplers_list = [\n",
    "    # smotetomek,\n",
    "    smotenc\n",
    "]\n",
    "\n",
    "## Selectors Dict\n",
    "resamplers_pipe = {\n",
    "    resampler_and_params[0].__class__.__name__: resampler_and_params\n",
    "    for resampler_and_params in resamplers_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6d49",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy_ = DummyClassifier()\n",
    "dummy_params = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "dummy = [dummy_, dummy_params]\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "nb_ = MultinomialNB()\n",
    "nb_params = {\n",
    "    'MultinomialNB__fit_prior': [True],\n",
    "    'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "nb = [nb_, nb_params]\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb_ = BernoulliNB()\n",
    "bnb_params = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "bnb = [bnb_, bnb_params]\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb_ = GaussianNB()\n",
    "gnb_params = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "gnb = [gnb_, gnb_params]\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn_ = KNeighborsClassifier()\n",
    "knn_params = {\n",
    "    'KNeighborsClassifier__weights': ['uniform'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    'KNeighborsClassifier__metric': [\n",
    "        'minkowski',\n",
    "        'euclidean',\n",
    "        'cosine',\n",
    "        'correlation',\n",
    "    ],\n",
    "    'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "knn = [knn_, knn_params]\n",
    "\n",
    "### Logistic Regression\n",
    "lr_ = LogisticRegression()\n",
    "lr_params = {\n",
    "    'LogisticRegression__penalty': ['l2'],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LogisticRegression__multi_class': ['ovr', 'multinomial'],\n",
    "    'LogisticRegression__solver': ['newton-cg', 'liblinear'],\n",
    "    'LogisticRegression__C': [0.01, 1, 100],\n",
    "}\n",
    "\n",
    "lr = [lr_, lr_params]\n",
    "\n",
    "### Passive Aggressive\n",
    "pa_ = PassiveAggressiveClassifier()\n",
    "pa_params = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': [None, 'balanced'],\n",
    "    'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "pa = [pa_, pa_params]\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd_ = SGDClassifier()\n",
    "sgd_params = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': [None, 'balanced'],\n",
    "    'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "sgd = [sgd_, sgd_params]\n",
    "\n",
    "### SVM\n",
    "svm_ = LinearSVC()\n",
    "svm_params = {\n",
    "    'LinearSVC__penalty': ['l2'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': [None, 'balanced'],\n",
    "    'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "}\n",
    "\n",
    "svm = [svm_, svm_params]\n",
    "\n",
    "### Decision Tree\n",
    "dt_ = DecisionTreeClassifier()\n",
    "dt_params = {\n",
    "    'DecisionTreeClassifier__max_depth': [5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "dt = [dt_, dt_params]\n",
    "\n",
    "### Random Forest\n",
    "rf_ = RandomForestClassifier()\n",
    "rf_params = {\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__n_jobs': [-1],\n",
    "    'RandomForestClassifier__max_depth': [5, 10],\n",
    "    'RandomForestClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': [None, 'balanced'],\n",
    "    'RandomForestClassifier__oob_score': [True],\n",
    "}\n",
    "\n",
    "rf = [rf_, rf_params]\n",
    "\n",
    "### Extra Trees\n",
    "et_ = ExtraTreesClassifier()\n",
    "et_params = {\n",
    "    'ExtraTreesClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__n_jobs': [-1],\n",
    "    'ExtraTreesClassifier__max_depth': [5, 10],\n",
    "    'ExtraTreesClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'ExtraTreesClassifier__random_state': [random_state],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "et = [et_, et_params]\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc_ = GradientBoostingClassifier()\n",
    "gbc_params = {\n",
    "    'GradientBoostingClassifier__max_depth': [5, 10],\n",
    "    'GradientBoostingClassifier__criterion': ['gini', 'entropy'],\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__n_estimators': [10, 20],\n",
    "    'GradientBoostingClassifier__loss': ['deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__subsample': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'GradientBoostingClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "gbc = [gbc_, gbc_params]\n",
    "\n",
    "### AdaBoost\n",
    "ada_ = AdaBoostClassifier()\n",
    "ada_params = {\n",
    "    'AdaBoostClassifier__max_depth': [5, 10],\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(probability=True, kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "ada = [ada_, ada_params]\n",
    "\n",
    "### XGBoost\n",
    "xgb_ = XGBClassifier()\n",
    "xgb_params = {\n",
    "    'XGBClassifier__max_depth': [5, 10],\n",
    "    'XGBClassifier__learning_rate': [0.05],\n",
    "    'XGBClassifier__n_estimators': [1000],\n",
    "    'XGBClassifier__seed': [42],\n",
    "    'XGBClassifier__nthread': [1, 2, 3, 4],\n",
    "    'XGBClassifier__objective': ['binary:logitraw', 'binary:logistic', 'binary:hinge'],\n",
    "    'XGBClassifier__eval_metric': ['auc', 'rmse', 'rmsle', 'logloss'],\n",
    "    'XGBClassifier__sample_type': ['weighted', 'uniform'],\n",
    "}\n",
    "\n",
    "xgb = [xgb_, xgb_params]\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc_ = MLPClassifier()\n",
    "mlpc_params = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpc = [mlpc_, mlpc_params]\n",
    "\n",
    "### MLP Regressor\n",
    "mlpr_ = MLPRegressor()\n",
    "mlpr_params = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = [mlpr_, mlpr_params]\n",
    "\n",
    "# ## Stacking and Voting Classifiers\n",
    "# estimators = [\n",
    "#     ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "#     (\n",
    "#         'Logistic Regression',\n",
    "#         LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# ### Voting Classifier\n",
    "# voting = VotingClassifier(estimators=estimators)\n",
    "# voting_params = {\n",
    "#     'VotingClassifier__estimators': [\n",
    "#         ('dummy', dummy, params_dummy_freq),\n",
    "#         ('dummy', dummy, params_dummy_stratified),\n",
    "#         ('dummy', dummy, params_dummy_uniform),\n",
    "#         ('nb', nb, params_nb),\n",
    "#         ('bnb', bnb, params_bnb),\n",
    "#         ('gnb', gnb, params_gnb),\n",
    "#         ('knn', knn, params_knn),\n",
    "#         ('lr', lr, params_lr),\n",
    "#         ('pa', pa, params_pa),\n",
    "#         ('sgd', sgd, params_sgd),\n",
    "#         ('svm', svm, params_svm),\n",
    "#         ('dt', dt, params_dt),\n",
    "#         ('rf', rf, params_rf),\n",
    "#         ('gbc', gbc, params_gbc),\n",
    "#         ('ada', ada, params_ada),\n",
    "#         ('xgb', xgb, params_xgb),\n",
    "#         ('mlpc', mlpc, params_mlpc),\n",
    "#         ('mlpr', mlpr, params_mlpr),\n",
    "#     ],\n",
    "#     'VotingClassifier__voting': ['hard', 'soft'],\n",
    "#     'VotingClassifier__weights': [None, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]],\n",
    "# }\n",
    "\n",
    "# ### Stacking Classifier\n",
    "# stacking = StackingClassifier(estimators=estimators)\n",
    "# # final_estimator = LogisticRegression(random_state=random_state, class_weight=class_weight)\n",
    "# final_estimator = RandomForestClassifier(\n",
    "#     random_state=42, class_weight={0: 1, 1: 2}\n",
    "# )\n",
    "# stacking_params = {\n",
    "#     'StackingClassifier__estimator': [\n",
    "#         ('dummy', dummy),\n",
    "#         ('nb', nb),\n",
    "#         ('bnb', bnb),\n",
    "#         ('gnb', gnb),\n",
    "#         ('knn', knn),\n",
    "#         ('lr', lr),\n",
    "#         ('pa', pa),\n",
    "#         ('sgd', sgd),\n",
    "#         ('svm', svm),\n",
    "#         ('dt', dt),\n",
    "#         ('rf', rf),\n",
    "#         ('gbc', gbc),\n",
    "#         ('ada', ada),\n",
    "#         ('mlpc', mlpc),\n",
    "#     ],\n",
    "#     'StackingClassifier__cv': [3, 5, 7, 9, 11, 13, 15],\n",
    "#     'StackingClassifier__n_jobs': [-1],\n",
    "#     'StackingClassifier__stack_method': ['auto', 'predict_proba', 'decision_function', 'predict'],\n",
    "#     'StackingClassifier__passthrough': [True, False],\n",
    "# }\n",
    "\n",
    "## Classifiers Pipe list\n",
    "classifers_list = [\n",
    "    dummy, nb,\n",
    "#     bnb, gnb, knn, lr, pa, sgd, svm, dt, rf, gbc, ada, xgb, mlpc, mlpr\n",
    "]\n",
    "\n",
    "## Classifiers Pipe dict\n",
    "classifiers_pipe = {\n",
    "    classifier_and_params[0].__class__.__name__: classifier_and_params\n",
    "    for classifier_and_params in classifers_list\n",
    "}\n",
    "\n",
    "## Stacking and Voting Classifiers\n",
    "voting_stacking_params = {}\n",
    "for classifier_and_params in classifers_list:\n",
    "    voting_stacking_estimators = tuple([classifier_and_params[0].__class__.__name__, classifier_and_params[0]])\n",
    "    voting_stacking_params.update(classifier_and_params[1])\n",
    "\n",
    "### Voting Classifier\n",
    "voting_ = VotingClassifier(estimators = voting_stacking_estimators)\n",
    "voting_params = {\n",
    "#     'VotingClassifier__estimators': estimators,\n",
    "    'VotingClassifier__voting': ['soft', 'hard'],\n",
    "    'VotingClassifier__weights': [None],\n",
    "}\n",
    "\n",
    "voting = [voting_, voting_params]\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_ = StackingClassifier(estimators = voting_stacking_estimators)\n",
    "stacking_params = {\n",
    "#     'StackingClassifier__estimator': estimators,\n",
    "    'StackingClassifier__cv': cv,\n",
    "    'StackingClassifier__n_jobs': [-1],\n",
    "    'StackingClassifier__stack_method': ['auto', 'predict_proba', 'decision_function', 'predict'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "stacking = [stacking_, stacking_params]\n",
    "\n",
    "# Add stacking and voting classifiers to classifiers pipe dict\n",
    "voting_stacking_params.update(voting[1])\n",
    "classifiers_pipe[voting[0].__class__.__name__] = voting_stacking_params\n",
    "\n",
    "# voting_stacking_params.update(stacking[1])\n",
    "# classifiers_pipe[stacking[0].__class__.__name__] = voting_stacking_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_df(vectorizers_pipe, selectors_pipe, classifiers_pipe, analysis_columns, metrics_list):\n",
    "\n",
    "    # Table df\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            list(map(lambda selector: selector, selectors_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Selector', 'Measures'],\n",
    "    )\n",
    "    table_df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    return table_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col, analysis_columns):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # BOW Split\n",
    "    print(f'Splitting data into training and testing sets:\\nRatios: test size = {test_split}, train_size = {1-test_split}')\n",
    "    df_manual.dropna(subset=analysis_columns, how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "\n",
    "    X_test = np.array([x for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "\n",
    "    print(f'Done splitting data into training and testing sets.')\n",
    "    print('-'*10)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_percentage(\n",
    "    cm, col, vectorizer_name, selector_name, classifier_name\n",
    "):\n",
    "    plt.title(\n",
    "        f'Confusion Matrix Heatmap Plot {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}',\n",
    "        fontsize=14.0,\n",
    "    )\n",
    "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "    group_counts = [f'{value:0.0f}\\n' for value in cm.flatten()]\n",
    "    group_percentages = [f'{value:.2%}' for value in cm.flatten() / np.sum(cm)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(cm.shape[0], cm.shape[1])\n",
    "    cm_heatmap = sns.heatmap(\n",
    "        cm, cmap='PuBu', annot=labels, fmt='', annot_kws={'size': 12.0}\n",
    "    )\n",
    "    plt.xlabel('Predicted', fontsize=12.0)\n",
    "    plt.ylabel('Actual', fontsize=12.0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_heatmap.figure.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix Heatmap {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "        \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    return cm_heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(cm, cm_curve, roc_curve, pr_curve, calibration_curve, recall, precision, no_skill, y_validate):\n",
    "    # Plots\n",
    "    print('=' * 20)\n",
    "    print('Plotting:')\n",
    "    ## Confusion Matrix\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix:')\n",
    "    plt.title(\n",
    "        f'Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    cm_curve.plot(cmap=plt.cm.Blues)\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "\n",
    "    ## Confusion Matrix Heatmap\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Heatmap Plot:')\n",
    "    cm_heatmap = plot_confusion_matrix_percentage(cm, col, vectorizer_name, selector_name, classifier_name)\n",
    "    \n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print(f'ROC Curve:')      \n",
    "#     plt.title(\n",
    "#         f'ROC Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "#     )\n",
    "    roc_curve.plot()\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Precision Recall Curve:')\n",
    "#     plt.title(\n",
    "#         f'Precision-Recall Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "#     )\n",
    "#     plt.ylabel('Precision', fontsize=12.0)\n",
    "#     plt.xlabel('Recall', fontsize=12.0)\n",
    "    pr_curve.plot()\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "    \n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Calibration Curve:')\n",
    "#     plt.title(\n",
    "#         f'Calibration Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "#     )\n",
    "    calibration_curve.plot()\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Save Plots\n",
    "    print('Saving plots.')\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "#         cm_heatmap.figure.savefig(\n",
    "#             plot_save_path\n",
    "#             + f'Confusion Matrix Heatmap {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "#             format=image_save_format,\n",
    "#             dpi=3000, bbox_inches='tight'\n",
    "#         )\n",
    "\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Calibration Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(col, table_df, estimator, vectorizer_name, selector_name, classifier_name):\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    table_df.to_csv(table_save_path + 'Classifiers Table.csv')\n",
    "    table_df.to_pickle(table_save_path + 'Classifiers Table.pkl')\n",
    "    table_df.to_excel(table_save_path + 'Classifiers Table.xlsx')\n",
    "    table_df.to_latex(table_save_path + 'Classifiers Table.tex')\n",
    "    table_df.to_markdown(table_save_path + 'Classifiers Table.md')\n",
    "\n",
    "    with open(f'{models_save_path}Estimator {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "        joblib.dump(estimator, f)\n",
    "#     with open(f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "#         joblib.dump(vectorizer, f)\n",
    "#     with open(f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "#         joblib.dump(selector, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(estimator, col, vectorizer_name, selector_name, classifier_name, X_test, y_test, y_test_pred, y_test_pred_prob, best_score, scoring):\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    cross_validate_score = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    mean_validation_score = cross_validate_score_noscoring.get('test_score').mean()\n",
    "    explained_variance = cross_validate_score.get('test_explained_variance').mean()\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    loss = log_loss(y_test, y_test_pred_prob)\n",
    "    report = classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred,\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Mean Validation Score')\n",
    "    ] = float(mean_validation_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Explained Variance')\n",
    "    ] = float(explained_variance)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = str(threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Log Loss/Cross Entropy'),\n",
    "    ] = float(loss)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Classification Report')\n",
    "    ] = report\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Confusion Matrix')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Confusion Matrix')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Normalized Confusion Matrix')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, f'{scoring.title()} Normalized Confusion Matrix')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Normalized Confusion Matrix')\n",
    "    ] = str(cm_normalized)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Metrics for {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {report}')\n",
    "    print('-' * 20)\n",
    "    print(f'Mean Validation Score: {mean_validation_score}')\n",
    "    print('-' * 20)\n",
    "    print(f'Recall score: {recall}')\n",
    "    print('-' * 20)\n",
    "    print(f'Accuracy score: {accuracy}')\n",
    "    print('-' * 20)\n",
    "    print(f'Precision score: {precision}')\n",
    "    print('-' * 20)\n",
    "    print(f'F1 score: {f1}')\n",
    "    print('-' * 20)\n",
    "    print(f'Matthews correlation coefficient: {mcc}')\n",
    "    print('-' * 20)\n",
    "    print(f'{scores[0].title()} best score: {best_score}')\n",
    "    print('-' * 20)\n",
    "    print(f'{scores[0].title()} best threshold: {threshold}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix:\\n', cm)\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Normalized:\\n', cm_normalized)\n",
    "    print('-' * 20)\n",
    "    print('=' * 20)\n",
    "    \n",
    "    # Plot Metrics\n",
    "    plot_metrics(cm, cm_curve, roc_curve, pr_curve, calibration_curve, recall, precision, no_skill, y_test)\n",
    "\n",
    "    return table_df, mean_validation_score, explained_variance, accuracy, precision, recall, f1, mcc, loss, report, cm, cm_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "table_df = make_table_df(vectorizers_pipe, selectors_pipe, classifiers_pipe, analysis_columns, metrics_list)\n",
    "\n",
    "for col in analysis_columns:\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "\n",
    "    if (len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test = split_data(df_manual, col, text_col, analysis_columns)\n",
    "\n",
    "        for (vectorizer_name, vectorizer_and_params), (selector_name, selector_and_params), (resampler_name, resampler_and_params), (classifier_name, classifier_and_params) in itertools.product(vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()):\n",
    "\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "\n",
    "            # Pipeline\n",
    "            ## Steps\n",
    "            if col == 'Warmth':\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    ('SMOTENC', SMOTENC(\n",
    "                        categorical_features = [\n",
    "                            df_manual.columns.get_loc('Warmth'),\n",
    "                            df_manual.columns.get_loc('Competence')\n",
    "                        ], random_state=random_state, sampling_strategy=.6\n",
    "                    )),\n",
    "                    # (resampler_name, resampler),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "            else:\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "\n",
    "            ## Params\n",
    "            param_grid = {\n",
    "                **vectorizer_params,\n",
    "                **selector_params,\n",
    "                **classifier_params,\n",
    "            }\n",
    "\n",
    "            ## Pipeline\n",
    "            pipe = Pipeline(steps=steps)\n",
    "\n",
    "            # Search\n",
    "            print('-'*20)\n",
    "            print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "            print('-'*20)\n",
    "            print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "            print('+'*30)\n",
    "            search = GridSearchCV(\n",
    "                estimator=pipe,\n",
    "                param_grid=param_grid,\n",
    "                n_jobs=-1,\n",
    "                scoring=scores,\n",
    "                cv=cv,\n",
    "                refit=scores[0],\n",
    "                return_train_score=True,\n",
    "            )\n",
    "\n",
    "            # Fit SearchCV\n",
    "            searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "            # Best Parameters\n",
    "            best_index = searchcv.best_index_\n",
    "            cv_results = sorted(searchcv.cv_results_)\n",
    "            best_params = searchcv.best_params_\n",
    "            best_score = searchcv.best_score_\n",
    "            n_splits = searchcv.n_splits_\n",
    "            estimator = searchcv.best_estimator_\n",
    "            y_train_pred = estimator.predict(X_train)\n",
    "\n",
    "            # Identify and name steps in estimator\n",
    "            vectorizer = estimator[0]\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            selector = estimator[1]\n",
    "            selector_name = selector.__class__.__name__\n",
    "\n",
    "            if col == 'Warmth':\n",
    "                resampler = estimator[2]\n",
    "                resampler_name = resampler.__class__.__name__\n",
    "                classifier = estimator[3]\n",
    "                classifier_name = classifier.__class__.__name__\n",
    "            else:\n",
    "                classifier = estimator[2]\n",
    "                classifier_name = classifier.__class__.__name__\n",
    "\n",
    "            print('=' * 20)\n",
    "            print(f'Best index for {scores[0]}: {best_index}')\n",
    "            print(f'Best estimator for {scores[0]}: {estimator}')\n",
    "            print(f'Best params for {scores[0]}: {best_params}')\n",
    "            print(f'Best score for {scores[0]}: {best_score}')\n",
    "            print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "            print('-' * 20)\n",
    "            train_report = classification_report(y_train, y_train_pred)\n",
    "            print(f'Training Classification Report:\\n{train_report}')\n",
    "            print(f'Training Confusion Matrix:')\n",
    "            plt.title(\n",
    "                f'Training Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16\n",
    "            )\n",
    "            train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "                estimator, X_train, y_train\n",
    "            ).plot(xticks_rotation='vertical', cmap=plt.cm.Blues)\n",
    "            plt.clf()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            print('=' * 20)\n",
    "\n",
    "            # Make the predictions\n",
    "            if hasattr(searchcv, 'predict_proba'):\n",
    "                searchcv_predict_attr = searchcv.predict_proba\n",
    "            elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "            score = searchcv.score(X_test, y_test)\n",
    "            y_test_pred = searchcv.predict(X_test)\n",
    "            y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "#             y_validate_pred = searchcv.predict(X_validate)\n",
    "#             y_validate_pred_prob = searchcv_predict_attr(X_validate)[:, 1]\n",
    "#             y_validate_pred = y_validate_pred_prob\n",
    "\n",
    "            # Fit Best Model\n",
    "            print(f'Fitting {estimator}.')\n",
    "            estimator.set_params(**estimator.get_params())\n",
    "            estimator = estimator.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate Model\n",
    "            table_df, mean_validation_score, explained_variance, accuracy, precision, recall, f1, mcc, loss, report, cm, cm_normalized = evaluation(\n",
    "                estimator, col, vectorizer_name, selector_name, classifier_name, X_test, y_test, y_test_pred, y_test_pred_prob, best_score, scoring\n",
    "            )\n",
    "\n",
    "            # Save Vectorizer, Selector, and Classifier\n",
    "            saving_model_and_table(col, table_df, estimator, vectorizer_name, selector_name, classifier_name)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c7b91c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
