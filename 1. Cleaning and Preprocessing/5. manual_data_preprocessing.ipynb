{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a3d9bbd5",
            "metadata": {},
            "source": [
                "# ATTN: This script uses Google translate to detect job description language. Google translate will limit requests and take a very long time. Only run this script if redoing language detection."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ac2e2d85",
            "metadata": {},
            "source": [
                "## Read from scrapped data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be8129d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d2271ddb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a840ebcc",
            "metadata": {},
            "source": [
                "#### Read paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "999bbb1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "glob_paths = list(set(glob.glob(f'{scraped_data}Coding Material/*Folder/*/Job ID -*- Codebook (Automating Equity).xlsx')))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ad7c36f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 244 xlsx files\n",
                "len(glob_paths)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fbfdd650",
            "metadata": {},
            "source": [
                "#### Use paths to open files, fix keywords, and drop unneeded columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f2edb5d",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# columns\n",
                "cols=['Sector', \n",
                "      'Sector Code', \n",
                "      'Gender', \n",
                "      'Age', \n",
                "      'Language', \n",
                "      'Dutch Requirement', \n",
                "      'English Requirement', \n",
                "      'Gender_Female', \n",
                "      'Gender_Mixed', \n",
                "      'Gender_Male', \n",
                "      'Age_Older', \n",
                "      'Age_Mixed', \n",
                "      'Age_Younger', \n",
                "      'Gender_Num', \n",
                "      'Age_Num', \n",
                "      '% Female', \n",
                "      '% Male', \n",
                "      '% Older', \n",
                "      '% Younger']\n",
                "\n",
                "# Fix list catches all incorrect/faculty keyword search terms\n",
                "fix_list = []\n",
                "\n",
                "# Appended data catches all the fixed and cleaned dfs\n",
                "appended_data = []\n",
                "\n",
                "for glob_path in glob_paths:\n",
                "\n",
                "    try:\n",
                "        df_temp = pd.read_excel(glob_path)\n",
                "    except ValueError:\n",
                "        fix_list.append(glob_path)\n",
                "\n",
                "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
                "        df_temp = df_temp\n",
                "        df_temp = df_temp.drop(columns=cols, axis='columns', errors='ignore')\n",
                "        df_temp = df_temp.drop(\n",
                "        df_temp.columns[\n",
                "                df_temp.columns.str.contains(\n",
                "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
                "                )\n",
                "            ],\n",
                "            axis='columns',\n",
                "            errors='ignore',\n",
                "        )\n",
                "\n",
                "        appended_data.append(df_temp)\n",
                "\n",
                "# Concatonate list of dfs into one large df_manual\n",
                "df_manual = pd.concat(appended_data, axis='index')\n",
                "\n",
                "# Save df_manual to file\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5de12303",
            "metadata": {},
            "outputs": [],
            "source": [
                "# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
                "if len(fix_list) != 0:\n",
                "    print('Some keywords to fix!')\n",
                "    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
                "        json.dump(fix_list, f)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae1a9362",
            "metadata": {},
            "outputs": [],
            "source": [
                "# List of dfs, len = 244\n",
                "len(appended_data)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78c1c60f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Concatonate list of dfs into one large df_manual\n",
                "df_manual = pd.concat(appended_data, axis='index')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47b014ea",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 12400\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a4606db",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save df_manual to file\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3eb6e2dd",
            "metadata": {},
            "source": [
                "## Drop duplicated and missing data"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc049d41",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_RAW\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eda9efcf",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b7e42cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a080272c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "92e65cdc",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e92eeb12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 12400\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78c31c26",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b37b39d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean columns\n",
                "df_manual.columns = df_manual.columns.to_series().progress_apply(lambda x: str(x).strip())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8571e7f7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove columns 'Task_Mentioned', 'Task_Warmth', 'Task_Competence'\n",
                "df_manual = df_manual.drop(\n",
                "    columns=['Task_Mentioned', 'Task_Warmth', 'Task_Competence'],\n",
                "    axis='columns',\n",
                "    errors='ignore'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bfef72ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2044253e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missing values: Sentence = 4, Warmth = 2, Competence = 0\n",
                "df_manual.isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "46ab64d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop NA\n",
                "df_manual = df_manual.dropna(axis='index', how='all')\n",
                "df_manual = df_manual.dropna(axis='columns', how='all')\n",
                "df_manual = df_manual.dropna(\n",
                "    subset = ['Sentence', 'Warmth', 'Competence'],\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b539d479",
            "metadata": {},
            "outputs": [],
            "source": [
                "# No na values\n",
                "df_manual.isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ee50376",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82e2a312",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c5a7d46",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Warmth and Competence to int\n",
                "# Warmth 1 = 2826, Competence 1 = 5064\n",
                "int_cols = [\n",
                "    'Warmth',\n",
                "    'Competence',\n",
                "]\n",
                "\n",
                "for col in int_cols:\n",
                "    df_manual[col] = df_manual[col].astype(np.int64, errors='ignore')\n",
                "    print(f'{col} converted to int.' if all(df_manual[col].progress_apply(lambda x: isinstance(x, int))) else f'{col} NOT converted to int.')\n",
                "    print(f'{col} value counts:\\n{df_manual[col].value_counts()}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7666d35",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "793c8461",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Conver Job ID and Sentence to str\n",
                "str_cols = [\n",
                "    'Job ID',\n",
                "    'Sentence',\n",
                "]\n",
                "\n",
                "for col in str_cols:\n",
                "    df_manual[col] = df_manual[col].astype(str, errors='ignore').progress_apply(lambda x: x.strip().replace('[', '').replace(']', ''))\n",
                "    print(f'{col} converted to str.' if all(df_manual[col].progress_apply(lambda x: isinstance(x, str))) else f'{col} NOT converted to str.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a5caf8c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 12394\n",
                "len(df_manual)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d2cacdb",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e57a9f32",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Rename Sentence to 'Job Description spacy_sentencized'\n",
                "df_manual = df_manual.rename(\n",
                "    columns = {\n",
                "        'Sentence': 'Job Description spacy_sentencized'\n",
                "    },\n",
                "    errors='ignore'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a32f64c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3fb59e5d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop NA\n",
                "df_manual = df_manual.dropna(axis='index', how='all')\n",
                "df_manual = df_manual.dropna(axis='columns', how='all')\n",
                "df_manual = df_manual.dropna(\n",
                "    subset = ['Job Description spacy_sentencized', 'Warmth', 'Competence'],\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25218a8b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 12394\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "025e7b62",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 133\n",
                "len(df_manual.groupby(['Job ID'])['Job ID'].unique())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e3126f47",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop duplicates on subset of 'Job ID' and 'Sentence'\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)\n",
                "df_manual = df_manual.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'], keep='first')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67baaa73",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 6400\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ec0bc22",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove any rows with missing 'Job ID'\n",
                "df_manual = df_manual.drop(\n",
                "    df_manual[\n",
                "        (df_manual['Job ID'].isin(nan_list)) | \n",
                "        (df_manual['Job ID'].isnull()) | \n",
                "        (df_manual['Job ID'].isna())\n",
                "    ].index, \n",
                "    axis='index',\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "21a098a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 6400\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "705e11d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19e08303",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save df_manual to file\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_raw_dropped.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3bae0b99",
            "metadata": {},
            "source": [
                "## Add English and Dutch language requirement columns"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb838f06",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_RAW_DROPPED\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "489a38aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7cda531",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a13ef5b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl')#\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b3501aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3b5aa5a8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6400\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2e2a0c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8418d404",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Add language requirement column\n",
                "# Use regex to find language requirement\n",
                "dutch_requirement_pattern = r'[Ll]anguage: [Dd]utch|[Dd]utch [Pp]referred|[Dd]utch [Re]quired|[Dd]utch [Ll]anguage|[Pp]roficient in [Dd]utch|[Ss]peak [Dd]utch|[Kk]now [Dd]utch'\n",
                "english_requirement_pattern = r'[Ll]anguage: [Ee]nglish|[Ee]nglish [Pp]referred|[Ee]nglish [Re]quired|[Ee]nglish [Ll]anguage|[Pp]roficient in [Ee]nglish|[Ss]peak [Ee]nglish|[Kk]now [Ee]nglish'\n",
                "\n",
                "lang_requirements = {\n",
                "    'Dutch Requirement': dutch_requirement_pattern, 'English Requirement': english_requirement_pattern\n",
                "}\n",
                "\n",
                "for lang_req, lang_req_pattern in lang_requirements.items():\n",
                "    if lang_req in df_manual.columns:\n",
                "        df_manual = df_manual.drop(columns=[lang_req])\n",
                "    df_manual[lang_req] = np.where(\n",
                "        df_manual['Job Description spacy_sentencized'].str.contains(lang_req_pattern),\n",
                "        'Yes',\n",
                "        'No',\n",
                "    )\n",
                "\n",
                "    df_manual[lang_req] = df_manual[lang_req].astype('category').cat.reorder_categories(['No', 'Yes'], ordered=True)\n",
                "    df_manual[lang_req] = pd.Categorical(df_manual[lang_req], categories=['No', 'Yes'], ordered=True)\n",
                "\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_raw_english_requirement.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "33b1e721",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Yes = 7\n",
                "df_manual['Dutch Requirement'].value_counts()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6330c696",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Yes = 8\n",
                "df_manual['English Requirement'].value_counts()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c3ddedd",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_raw_language_requirement.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e79cea97",
            "metadata": {},
            "source": [
                "## Add data from Sectors dataframe (see CBS directory under scrapped_data directory) and Categorical data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "69513116",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_RAW_LANGUAGE_REQUIREMENT\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10b32849",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "875fd370",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6873d7c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_df_info(df, ivs_all=None):\n",
                "    if ivs_all is None:\n",
                "        ivs_all = [\n",
                "            'Gender',\n",
                "            'Gender_Num',\n",
                "            'Gender_Female',\n",
                "            'Gender_Mixed',\n",
                "            'Gender_Male',\n",
                "            'Age',\n",
                "            'Age_Num',\n",
                "            'Age_Older',\n",
                "            'Age_Mixed',\n",
                "            'Age_Younger',\n",
                "        ]\n",
                "    # Print Info\n",
                "    print('\\nDF INFO:\\n')\n",
                "    df.info()\n",
                "\n",
                "    for iv in ivs_all:\n",
                "        try:\n",
                "            print('='*20)\n",
                "            print(f'{iv}:')\n",
                "            print('-'*20)\n",
                "            if len(df[iv].value_counts()) > 5:\n",
                "                print(f'{iv} Counts:\\n{df[iv].value_counts()}')\n",
                "                print('-'*20)\n",
                "                print(f'{iv} Percentages:\\n{df[iv].value_counts(normalize=True).mul(100).round(1).astype(float)}')\n",
                "                print('-'*20)\n",
                "            min_val = df[iv].min()\n",
                "            max_val = df[iv].max()\n",
                "            if min_val not in [0, 1]:\n",
                "                print(f'Min {iv} value: {min_val}')\n",
                "            if max_val not in [1, 3]:\n",
                "                print(f'Max {iv} value: {max_val}')\n",
                "            with contextlib.suppress(Exception):\n",
                "                print('-'*20)\n",
                "                print(f'{iv} Mean: {df[iv].mean().round(2).astype(float)}')\n",
                "                print('-'*20)\n",
                "                print(f'{iv} Standard Deviation: {df[iv].std().round(2).astype(float)}')\n",
                "        except Exception:\n",
                "            print(f'{iv} not available.')\n",
                "\n",
                "    print('\\n')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79fe759f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8664a939",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40156303",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db503aad",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job ID'] = df_manual['Job ID'].progress_apply(lambda x: str(x).lower().strip())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98694f16",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_including_sector_genage_data.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d5116df",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49391453",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job ID'] = df_jobs['Job ID'].progress_apply(lambda x: str(x).lower().strip())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5aa6dcf",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82178d30",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.drop(\n",
                "    columns = [\n",
                "        'Job Description', 'Rating', 'Employment Type',\n",
                "        'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
                "        'Collection Date', 'Data Row', 'Tracking ID', 'Job Date',\n",
                "        'Type of ownership', 'Language', 'Dutch Requirement', 'English Requirement', \n",
                "    ],\n",
                "    errors='ignore'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9bba117b",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f11e0e01",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add sector and categorical data from df_jobs\n",
                "df_manual = df_manual.merge(df_jobs, on='Job ID', how='inner')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47f24060",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 5978\n",
                "len(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4fa2c4a7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40fa3bc0",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1d889ec9",
            "metadata": {},
            "source": [
                "#### Check if there is any missing sector data in the merged dataframe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43ddadb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Sector'].isna().sum()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb5b9064",
            "metadata": {},
            "outputs": [],
            "source": [
                "if df_manual['Sector'].isna().sum() != 0:\n",
                "    print('Some search keywords did not match a sector. Fixing')\n",
                "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
                "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n",
                "    df_manual = fix_keywords(df_manual)\n",
                "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
                "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d0d24ae2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual Job Ad info, len = 117\n",
                "get_df_info(df_manual.groupby(['Job ID']).first())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d4d65547",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual Job Sentence info\n",
                "get_df_info(df_manual)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d1d8a085",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f599e9cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "if df_manual['Sector'].isna().sum() == 0:\n",
                "    assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "    df_manual.to_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl')\n",
                "    df_manual.to_csv(f'{df_save_dir}df_manual_including_sector_genage_data.csv')\n",
                "else:\n",
                "    print(f\"MISSING SECTOR DATA: COUNT {df_manual['Sector'].isna().sum()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6373c116",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER spacy sentence splitting is completed.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "993064a6",
            "metadata": {},
            "source": [
                "## Use spacy to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c744a550",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_SENTENCIZED\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b997d96c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "390017aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c3b98f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_word_num_and_frequency(row, text_col):\n",
                "\n",
                "    with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "        custom_punct_chars = pickle.load(f)\n",
                "    row['Job Description num_words'] = len(str(row[text_col]).split())\n",
                "    row['Job Description num_unique_words'] = len(set(str(row[text_col]).split()))\n",
                "    row['Job Description num_chars'] = len(str(row[text_col]))\n",
                "    row['Job Description num_chars_no_whitespact_and_punt'] = len(\n",
                "        [\n",
                "            c.translate({ord(s): None for s in string.whitespace})\n",
                "            for c in str(row[text_col])\n",
                "            if c not in custom_punct_chars and c not in string.punctuation\n",
                "        ]\n",
                "    )\n",
                "    row['Job Description num_punctuations'] = len(\n",
                "        [\n",
                "            c\n",
                "            for c in str(row[text_col])\n",
                "            if c in custom_punct_chars and c in string.punctuation\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d31a583",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "130cdd3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bc21643d",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job Description spacy_sentencized_lower'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda job_sentence: job_sentence.strip().lower()\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b8dac3e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual[['Job Description spacy_sentencized', 'Job Description spacy_sentencized_lower']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1cf56055",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Spacy tokenize\n",
                "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "    custom_punct_chars = pickle.load(f)\n",
                "\n",
                "df_manual['Job Description spacy_tokenized'] = df_manual[\n",
                "    'Job Description spacy_sentencized'\n",
                "].progress_apply(\n",
                "    lambda job_sentence: [\n",
                "        str(token.text.strip().lower())\n",
                "        for token in nlp.tokenizer(job_sentence)\n",
                "        if len(token) != 0\n",
                "        and not token.is_space\n",
                "        and not token.is_stop\n",
                "        and not token.is_punct\n",
                "        and not token.is_bracket\n",
                "        and not token.like_email\n",
                "        and token.text not in custom_punct_chars\n",
                "    ]\n",
                ")\n",
                "\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd443d94",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job Description spacy_sentencized_cleaned'] = df_manual['Job Description spacy_tokenized'].str.join(' ')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65043f92",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Get sentence word frequencies\n",
                "df_manual = df_manual.progress_apply(\n",
                "    lambda row: get_word_num_and_frequency(\n",
                "        row=row, text_col='Job Description spacy_sentencized'\n",
                "    ), \n",
                "    axis='columns',\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "68abe4db",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual[\n",
                "    [\n",
                "        'Job Description spacy_sentencized',\n",
                "        'Job Description num_words', 'Job Description num_unique_words',\n",
                "        'Job Description num_chars', 'Job Description num_chars_no_whitespact_and_punt'\n",
                "    ]\n",
                "].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2acfe7f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e42686c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_manual)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5deddb9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e7f002d2",
            "metadata": {},
            "source": [
                "## Use NLTK to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92922872",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9fae46f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d4f11d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5011ea0a",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e40645d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c8a9f1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4475248f",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Tokenize with NLTK\n",
                "# stop_words = set(stopwords.words('english'))\n",
                "# punctuations = list(string.punctuation)\n",
                "# lemmatizer = WordNetLemmatizer()\n",
                "# stemmer = PorterStemmer()\n",
                "\n",
                "df_manual['Job Description nltk_tokenized'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda job_sentence: [\n",
                "        str(token.strip().lower()) \n",
                "        for token in word_tokenize(job_sentence) \n",
                "        if len(token) != 0 \n",
                "        and token != '...' \n",
                "        and not token.lower() in set(stopwords.words('english')) \n",
                "        and not token.lower() in list(string.punctuation) \n",
                "    ]\n",
                ")\n",
                "\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3bbd5d3f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job Description nltk_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86da6772",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69e6640b",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "81e54149",
            "metadata": {},
            "source": [
                "## Use gensim to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff307dba",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f89cb65",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac2ae121",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c13d88fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "58941640",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bc48f290",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e95fec77",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "df_manual['Job Description gensim_tokenized'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda sentence: preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
                ")\n",
                "\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1cc29198",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job Description gensim_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a69a8de",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a62be70e",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "eeb0131f",
            "metadata": {},
            "source": [
                "## Use BERT to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "efcf30fc",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c81e981f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6f6ae091",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec899945",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "316d9ace",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d42a3b60",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8bc6622",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "bert_model_name = 'bert-base-uncased'\n",
                "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
                "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
                "\n",
                "df_manual['Job Description bert_tokenized'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda sentence: bert_tokenizer.tokenize(str(sentence))\n",
                ")\n",
                "\n",
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ae92181",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual['Job Description bert_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ebbbdf54",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_for_trainning.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_for_trainning.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "853b13fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving df_manual length {len(df_manual)} to txt file.')\n",
                "with open(f'{data_dir}df_manual_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_manual)))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e4351f1b",
            "metadata": {},
            "source": [
                "### Specification Curve Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6bcdc7c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module import specification_curve_fork as specy # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "print(f'Running specification curve analysis with:\\nDEPENDENT VARIABLES = {dvs}\\nINDEPENDENT VARIABLES = {ivs_perc}\\nCONTROLS = {controls}')\n",
                "sc = specy.SpecificationCurve(df=df_manual, y_endog=dvs, x_exog=ivs_perc, controls=controls)\n",
                "sc.fit(estimator=sm.Logit)\n",
                "sc.plot(show_plot=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8936c818",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "72e4a4f3",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER all tokenization (spacy, nltk, gensim, and BERT) completed.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b376a87e",
            "metadata": {},
            "source": [
                "## Use spacy to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "67abb044",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM_BERT\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75edc613",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f3ecf3fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "108d5619",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c882af7d",
            "metadata": {},
            "outputs": [],
            "source": [
                "## Warmth 1 = 1595 (24.90%), Competence 1 = 2836 (44.30%)\n",
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2b826df2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8707126e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Load customer characters\n",
                "# with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "#     custom_punct_chars = pickle.load(f)\n",
                "\n",
                "# # POS tagging\n",
                "# df_manual['Job Description spacy_token_tags'] = df_manual[\n",
                "#     'Job Description spacy_sentencized'\n",
                "# ].progress_apply(\n",
                "#     lambda job_sentence: [\n",
                "#         (token.text.strip().lower(), token.tag_) for token in nlp(job_sentence)\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# # Lemmatization\n",
                "# df_manual['Job Description spacy_lemmas'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#     lambda job_sentence: [\n",
                "#         token.lemma_.strip().lower()\n",
                "#         for token in nlp(job_sentence)\n",
                "#         if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# # Stemming\n",
                "# df_manual['Job Description spacy_stems'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#     lambda job_sentence: [\n",
                "#         stemmer.stem(token.text.strip().lower())\n",
                "#         for token in nlp(job_sentence)\n",
                "#         if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aa791d2c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80ec3614",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual[\n",
                "#     [\n",
                "#         'Job Description spacy_token_tags',\n",
                "#         'Job Description spacy_lemmas',\n",
                "#         'Job Description spacy_stems'\n",
                "#     ]\n",
                "# ].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80467339",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9923c074",
            "metadata": {},
            "source": [
                "## Use NLTK to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9b5f6733",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d1e6c51e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b13a9c3f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c60ab001",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def get_wordnet_pos(token):\n",
                "#     \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
                "#     tag = nltk.pos_tag([token])[0][1][0].upper()\n",
                "#     tag_dict = {\"J\": wordnet.ADJ,\n",
                "#                 \"N\": wordnet.NOUN,\n",
                "#                 \"V\": wordnet.VERB,\n",
                "#                 \"R\": wordnet.ADV}\n",
                "\n",
                "#     return tag_dict.get(tag, wordnet.NOUN)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "451fd7e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88230e7f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f668190d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1710f8b5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # POS tagging\n",
                "# df_manual['Job Description nltk_token_tags'] = df_manual['Job Description spacy_tokenized'].progress_apply(\n",
                "#     lambda token: pos_tag(token)\n",
                "# )\n",
                "\n",
                "# # Lemmatization\n",
                "# df_manual['Job Description nltk_lemmas'] = df_manual['Job Description spacy_tokenized'].progress_apply(\n",
                "#     lambda tokens: [\n",
                "#         lemmatizer.lemmatize(\n",
                "#             token, get_wordnet_pos(\n",
                "#                 unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
                "#             )\n",
                "#         )\n",
                "#         for token in tokens\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# # Stemming\n",
                "# df_manual['Job Description nltk_stems'] = df_manual['Job Description spacy_tokenized'].progress_apply(\n",
                "#     lambda tokens: [\n",
                "#         stemmer.stem(\n",
                "#             unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
                "#         )\n",
                "#         for token in tokens\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b52e3777",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "afb9968b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual[['Job Description nltk_token_tags', 'Job Description nltk_lemmas', 'Job Description nltk_stems']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bbba0d31",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "56d7ecad",
            "metadata": {},
            "source": [
                "## Use BERT to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fb3049c9",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5c090eda",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ba8a95d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1a303b5f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "923053ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "889d5f89",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
                "# bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name).to(device)\n",
                "# bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer).to(device)\n",
                "\n",
                "# df_manual['Job Description bert_token_tags_with_scores'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#     lambda sentence: [\n",
                "#         (bert_pos_tag['word'], bert_pos_tag['entity'], bert_pos_tag['score'])\n",
                "#         for i in range(len(sentence.split()))\n",
                "#         for bert_pos_tag in bert_pos_tagger(sentence)\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "# df_manual['Job Description bert_token_tags'] = df_manual['Job Description bert_token_tags_with_scores'].progress_apply(\n",
                "#     lambda tag_list: [\n",
                "#         [(tag_list[i][0], tag_list[i][1])]\n",
                "#         for tag_tuple in tag_list\n",
                "#         for i in range(len(tag_list))\n",
                "#     ]\n",
                "# )\n",
                "\n",
                "\n",
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d77b1224",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual['Job Description bert_token_tags'].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43c03a2c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "51665e58",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy and nltk) completed. If BERT POS tagging was done, change pkl file loading\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc6e96cc",
            "metadata": {},
            "source": [
                "## Use spacy to create bi and trigrams\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e88b5858",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b6376abd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os\n",
                "# import sys\n",
                "# import importlib\n",
                "# from pathlib import Path\n",
                "# import numpy as np\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0760b42b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f0286d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def spacy_make_ngrams(sentence, matcher, gram_type):\n",
                "\n",
                "#     doc = nlp(sentence)\n",
                "#     matches = matcher(doc)\n",
                "#     matches_list = []\n",
                "\n",
                "#     for idx in range(len(matches)):\n",
                "#         for match_id, start, end in matches:\n",
                "#             if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
                "#                 match = doc[matches[idx][1]: matches[idx][2]].text\n",
                "#                 matches_list.append(match.lower())\n",
                "\n",
                "#     return list(set(matches_list))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a170297e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e9c2ed17",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52405b8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# df_manual['Job Description spacy_1grams_original_list'] = df_manual['Job Description spacy_tokenized']\n",
                "# df_manual['Job Description spacy_1grams'] = df_manual['Job Description spacy_tokenized'].progress_apply(\n",
                "#     lambda tokens: [\n",
                "#         tuple(token.split())\n",
                "#         for token in tokens\n",
                "#     ]\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53d8289c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Spacy bi and trigrams\n",
                "# matcher = Matcher(nlp.vocab)\n",
                "\n",
                "# bigram_rules = [\n",
                "#     ['NOUN', 'VERB'],\n",
                "#     ['VERB', 'NOUN'],\n",
                "#     ['ADJ', 'NOUN'],\n",
                "#     ['ADJ', 'PROPN'],\n",
                "#     # more rules here...\n",
                "# ]\n",
                "\n",
                "# trigram_rules = [\n",
                "#     ['VERB', 'ADJ', 'NOUN'],\n",
                "#     ['NOUN', 'VERB', 'ADV'],\n",
                "#     ['NOUN', 'ADP', 'NOUN'],\n",
                "#     # more rules here...\n",
                "# ]\n",
                "\n",
                "# patters_dict = {\n",
                "#     'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
                "#     'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
                "# }\n",
                "\n",
                "# ngram_dict = {\n",
                "#     'bigram': 2,\n",
                "#     'trigram': 3,\n",
                "# }\n",
                "\n",
                "# for ngram_name, ngram_num in tqdm.tqdm(ngram_dict.items()):\n",
                "\n",
                "#     matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
                "\n",
                "#     df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#         lambda sentence: \n",
                "#             [\n",
                "#                 '_'.join(ngram_.split())\n",
                "#                 for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
                "#             ]\n",
                "#     )\n",
                "\n",
                "#     df_manual[f'Job Description spacy_{str(ngram_num)}grams'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#         lambda sentence: \n",
                "#             [\n",
                "#                 tuple(ngram_.split())\n",
                "#                 for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
                "#             ]\n",
                "#     )\n",
                "\n",
                "#     df_manual[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
                "#         regex = {\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "\n",
                "#     if f'{ngram_name}_patterns' in matcher:\n",
                "#         matcher.remove(f'{ngram_name}_patterns')\n",
                "#     assert f'{ngram_name}_patterns' not in matcher\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd8bd6b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Spacy Allgrams\n",
                "# df_manual['Job Description spacy_123grams_original_list'] = df_manual['Job Description spacy_tokenized'] + df_manual['Job Description spacy_2grams_original_list'] + df_manual['Job Description spacy_3grams_original_list']\n",
                "# df_manual['Job Description spacy_123grams'] = df_manual['Job Description spacy_1grams'] + df_manual['Job Description spacy_2grams'] + df_manual['Job Description spacy_3grams']\n",
                "# df_manual['Job Description spacy_123grams_in_sent'] = (\n",
                "#     df_manual['Job Description spacy_sentencized']\n",
                "#     .str.lower()\n",
                "#     .replace(\n",
                "#         regex={\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[\n",
                "#                 'Job Description spacy_123grams_original_list'\n",
                "#             ]\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "023a05b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "83153ef0",
            "metadata": {},
            "source": [
                "## Use NLTK to create bi and trigrams\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6276c5ad",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "93b81c36",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "133c5578",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a73a21da",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3e9ef52e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "91d1a5b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# df_manual['Job Description nltk_1grams_original_list'] = df_manual['Job Description nltk_tokenized']\n",
                "# df_manual['Job Description nltk_1grams'] = df_manual['Job Description nltk_tokenized'].progress_apply(\n",
                "#     lambda tokens: [\n",
                "#         tuple(token.split())\n",
                "#         for token in tokens\n",
                "#     ]\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5fe5d598",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # NLTK bi and trigrams\n",
                "# ngram_dict = {\n",
                "#     'bigram': 2,\n",
                "#     'trigram': 3\n",
                "# }\n",
                "\n",
                "# for ngram_name, ngram_num in tqdm.tqdm(ngram_dict.items()):\n",
                "\n",
                "#     df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_manual['Job Description nltk_tokenized'].progress_apply(\n",
                "#         lambda tokens:\n",
                "#             list(\n",
                "#                 '_'.join(ngram_list)\n",
                "#                 for ngram_list in nltk.ngrams(tokens, ngram_num)\n",
                "#             )\n",
                "#     )\n",
                "\n",
                "#     df_manual[f'Job Description nltk_{str(ngram_num)}grams'] = df_manual['Job Description nltk_tokenized'].progress_apply(\n",
                "#         lambda tokens: list(nltk.ngrams(tokens, ngram_num))\n",
                "#     )\n",
                "\n",
                "#     df_manual[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
                "#         regex = {\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3226afac",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # NLTK Allgrams\n",
                "# df_manual['Job Description nltk_123grams_original_list'] = (\n",
                "#     df_manual['Job Description nltk_tokenized']\n",
                "#     + df_manual['Job Description nltk_2grams_original_list']\n",
                "#     + df_manual['Job Description nltk_3grams_original_list']\n",
                "# )\n",
                "# df_manual['Job Description nltk_123grams'] = (\n",
                "#     df_manual['Job Description nltk_1grams']\n",
                "#     + df_manual['Job Description nltk_2grams']\n",
                "#     + df_manual['Job Description nltk_3grams']\n",
                "# )\n",
                "# df_manual['Job Description nltk_123grams_in_sent'] = (\n",
                "#     df_manual['Job Description spacy_sentencized']\n",
                "#     .str.lower()\n",
                "#     .replace(\n",
                "#         regex={\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[\n",
                "#                 'Job Description nltk_123grams_original_list'\n",
                "#             ]\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0e1b66d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2663ad25",
            "metadata": {},
            "source": [
                "## Use Gensim to create bi and trigrams\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9e7a975e",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23ff67d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c6340feb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e27e12e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "111e7dd3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69b5c7e2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual['Job Description gensim_1grams_original_list'] = df_manual['Job Description gensim_tokenized']\n",
                "# df_manual['Job Description gensim_1grams'] = df_manual['Job Description gensim_tokenized'].progress_apply(\n",
                "#     lambda tokens: [\n",
                "#         tuple(token.split())\n",
                "#         for token in tokens\n",
                "#     ]\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8ae9d43",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Gensim bi and trigrams\n",
                "# pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
                "\n",
                "# # Gensim Bigrams\n",
                "# bigram = Phraser(Phrases(df_manual['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
                "# df_manual['Job Description gensim_2grams_original_list_all'] = bigram[df_manual['Job Description gensim_tokenized']]\n",
                "# df_manual['Job Description gensim_2grams_original_list'] = df_manual['Job Description gensim_2grams_original_list_all'].progress_apply(\n",
                "#     lambda ngrams_list: [\n",
                "#         ngram_\n",
                "#         for ngram_ in ngrams_list\n",
                "#         if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
                "#     ]\n",
                "# )\n",
                "# df_manual['Job Description gensim_2grams'] = df_manual['Job Description gensim_2grams_original_list'].progress_apply(\n",
                "#     lambda ngrams: [\n",
                "#         tuple(ngram.split('_'))\n",
                "#         for ngram in ngrams\n",
                "#         if '_' in ngram\n",
                "#     ]\n",
                "# )\n",
                "# df_manual['Job Description gensim_2grams_in_sent'] = (\n",
                "#     df_manual['Job Description spacy_sentencized']\n",
                "#     .str.lower()\n",
                "#     .progress_apply(\n",
                "#         lambda sentence: ' '.join(\n",
                "#             preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
                "#         )\n",
                "#     )\n",
                "#     .replace(\n",
                "#         regex={\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[\n",
                "#                 'Job Description gensim_2grams_original_list'\n",
                "#             ]\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "# )\n",
                "\n",
                "# # Gensim Trigrams\n",
                "# trigram = Phraser(Phrases(df_manual['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
                "# df_manual['Job Description gensim_3grams_original_list_all'] = trigram[df_manual['Job Description gensim_2grams_original_list_all']]\n",
                "# df_manual['Job Description gensim_3grams_original_list'] = df_manual['Job Description gensim_3grams_original_list_all'].progress_apply(\n",
                "#     lambda ngrams_list: [\n",
                "#         ngram_\n",
                "#         for ngram_ in ngrams_list\n",
                "#         if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
                "#     ]\n",
                "# )\n",
                "# df_manual['Job Description gensim_3grams'] = df_manual['Job Description gensim_3grams_original_list'].progress_apply(\n",
                "#     lambda ngrams: [\n",
                "#         tuple(ngram.split('_'))\n",
                "#         for ngram in ngrams\n",
                "#         if '_' in ngram\n",
                "#     ]\n",
                "# )\n",
                "# df_manual['Job Description gensim_3grams_in_sent'] = (\n",
                "#     df_manual['Job Description spacy_sentencized']\n",
                "#     .str.lower()\n",
                "#     .progress_apply(\n",
                "#         lambda sentence: ' '.join(\n",
                "#             preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
                "#         )\n",
                "#     )\n",
                "#     .replace(\n",
                "#         regex={\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[\n",
                "#                 'Job Description gensim_3grams_original_list'\n",
                "#             ]\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d9e51cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Gensim Allgrams\n",
                "# pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
                "\n",
                "# df_manual['Job Description gensim_123grams_original_list'] = (\n",
                "#     df_manual['Job Description gensim_tokenized']\n",
                "#     + df_manual['Job Description gensim_2grams_original_list']\n",
                "#     + df_manual['Job Description gensim_3grams_original_list']\n",
                "# )\n",
                "# df_manual['Job Description gensim_123grams'] = (\n",
                "#     df_manual['Job Description gensim_1grams']\n",
                "#     + df_manual['Job Description gensim_2grams']\n",
                "#     + df_manual['Job Description gensim_3grams']\n",
                "# )\n",
                "# df_manual['Job Description gensim_123grams_in_sent'] = (\n",
                "#     df_manual['Job Description spacy_sentencized']\n",
                "#     .str.lower()\n",
                "#     .progress_apply(\n",
                "#         lambda sentence: ' '.join(\n",
                "#             preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
                "#         )\n",
                "#     )\n",
                "#     .replace(\n",
                "#         regex={\n",
                "#             re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
                "#             for ngrams_list in df_manual[\n",
                "#                 'Job Description gensim_123grams_original_list'\n",
                "#             ]\n",
                "#             for ngram_ in ngrams_list\n",
                "#             if '_' in ngram_\n",
                "#         }\n",
                "#     )\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9bc6bb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2463a479",
            "metadata": {},
            "source": [
                "## Create word frequencies for uni, bi, and trigrams\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8a485ccc",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK_GENSIM\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "317c5260",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b80a110f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "da0c371b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def get_abs_frequency(row, text_col, ngram_num, embedding_library):\n",
                "\n",
                "#     abs_word_freq = defaultdict(int)\n",
                "#     for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
                "#         abs_word_freq[word] += 1\n",
                "\n",
                "#         abs_wtd_df = (\n",
                "#             pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
                "#             .rename(columns={0: 'abs_word_freq'})\n",
                "#             .sort_values(by=['abs_word_freq'], ascending=False)\n",
                "#             )\n",
                "#         abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
                "#         abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
                "\n",
                "#         row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
                "#         row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
                "#         row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
                "\n",
                "#     return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42065976",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "00d205ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56493c03",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# ngrams_list=[1, 2, 3, 123]\n",
                "# embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
                "\n",
                "# for embedding_library, ngram_num in tqdm_product(embedding_libraries_list, ngrams_list):\n",
                "#     df_manual[f'Job Description {embedding_library}_{ngram_num}grams_count'] = df_manual[f'Job Description {embedding_library}_{ngram_num}grams'].apply(lambda x: len(x))\n",
                "#     df_manual = df_manual.progress_apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62491c12",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_frequency.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b3db7e8c",
            "metadata": {},
            "source": [
                "## Create BoW dictionary, corpus, and tfidf matrix for uni, bi, and trigrams\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6ffb2e84",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_NGRAMS_FREQUENCY\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d323a33",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4bdd7c06",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad2d3083",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
                "\n",
                "#     ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
                "#     dictionary = Dictionary([ngrams_original_list])\n",
                "#     BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
                "#     tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
                "#     tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
                "\n",
                "#     row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
                "#     row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
                "#     row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
                "#     row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
                "\n",
                "#     return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "81a3d7d6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5b4a2467",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf89f753",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# ngrams_list=[1, 2, 3, 123]\n",
                "# embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
                "# for embedding_library, ngram_num in tqdm_product(embedding_libraries_list, ngrams_list):\n",
                "#     df_manual = df_manual.progress_apply(\n",
                "#         lambda row: get_corpus_and_dictionary(\n",
                "#             row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
                "#         ),\n",
                "#         axis='columns'\n",
                "#     )\n",
                "\n",
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "210fd7f2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "67230a0c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5828c6aa",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d9d3463",
            "metadata": {},
            "source": [
                "## Use spacy and nltk for sentiment scoring\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b5ae804",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_NGRAMS_BOW\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71af1593",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e8c1292",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b6c90353",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "26d2dbab",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43603e1f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Spacy sentiment\n",
                "# if 'spacytextblob' not in nlp.pipe_names:\n",
                "#     nlp.add_pipe('spacytextblob')\n",
                "\n",
                "# df_manual['Job Description spacy_sentiment'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#     lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
                "#     if isinstance(sentence, str) else np.nan\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b030259b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # NLTK sentiment\n",
                "# df_manual['Job Description nltk_sentiment'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
                "#     lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
                "#     if isinstance(sentence, str) else np.nan\n",
                "# )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d89a2ba1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_sentiment_spacy_nltk.csv')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1548cd86",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0ade7a1e",
            "metadata": {},
            "source": [
                "## Word2Vec and FastText embeddings\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9e73b044",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM df_manual_SENTIMENT_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a50cf828",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "# from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "# mod = sys.modules[__name__]\n",
                "\n",
                "# code_dir = None\n",
                "# code_dir_name = 'Code'\n",
                "# unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "# for _ in range(5):\n",
                "\n",
                "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "#         code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "#         if code_dir is not None:\n",
                "#             break\n",
                "\n",
                "# sys.path.append(code_dir)\n",
                "# # %load_ext autoreload\n",
                "# # %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14cfe726",
            "metadata": {},
            "outputs": [],
            "source": [
                "# from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0d24b55c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def build_train_word2vec(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
                "#     if words is None:\n",
                "#         words = [\n",
                "#             'she',\n",
                "#             'he',\n",
                "#             'support',\n",
                "#             'leader',\n",
                "#             'management',\n",
                "#             'team',\n",
                "#             'business',\n",
                "#             'customer',\n",
                "#             'risk',\n",
                "#             'build',\n",
                "#             'computer',\n",
                "#             'programmer',\n",
                "#         ]\n",
                "#     sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
                "\n",
                "#     w2v_model = Word2Vec(\n",
                "#         sentences=sentences,\n",
                "#         vector_size=size,\n",
                "#         min_count=0,\n",
                "#         window=2,\n",
                "#         sample=6e-5,\n",
                "#         alpha=0.03,\n",
                "#         min_alpha=0.0007,\n",
                "#         negative=20,\n",
                "#         workers=cores - 1,\n",
                "#         sg = 1,\n",
                "#     )\n",
                "\n",
                "#     w2v_model.build_vocab(sentences, progress_per=10000)\n",
                "#     print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
                "\n",
                "#     w2v_model.train(\n",
                "#         sentences,\n",
                "#         total_examples=w2v_model.corpus_count,\n",
                "#         epochs=30,\n",
                "#         report_delay=1,\n",
                "#     )\n",
                "\n",
                "#     print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
                "#     w2v_vocab = list(w2v_model.wv.index_to_key)\n",
                "\n",
                "#     print(f'Checking words form list of length {len(words)}')\n",
                "#     print(f'WORDS LIST: {words}')\n",
                "\n",
                "# #     for word in words:\n",
                "# #         print(f'Checking word:\\n{word.upper()}:')\n",
                "# #         try:\n",
                "# # #             print(f'Word2Vec {size}: {w2v_model.wv[word]}')\n",
                "# #             print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
                "# #             print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
                "# #             print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
                "\n",
                "# #         except KeyError as e:\n",
                "# #             print(e)\n",
                "\n",
                "#     return w2v_vocab, w2v_model\n",
                "\n",
                "# def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
                "\n",
                "#     sentences = [word for word in sentences if word in w2v_vocab]\n",
                "\n",
                "#     return (\n",
                "#         np.mean(w2v_model.wv[sentences], axis=0)\n",
                "#         if sentences\n",
                "#         else np.zeros(size)\n",
                "#     )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e882a3dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def build_train_fasttext(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
                "#     if words is None:\n",
                "#         words = [\n",
                "#             'she',\n",
                "#             'he',\n",
                "#             'support',\n",
                "#             'leader',\n",
                "#             'management',\n",
                "#             'team',\n",
                "#             'business',\n",
                "#             'customer',\n",
                "#             'risk',\n",
                "#             'build',\n",
                "#             'computer',\n",
                "#             'programmer',\n",
                "#         ]\n",
                "#     sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
                "\n",
                "#     ft_model = FastText(\n",
                "#         sentences=sentences,\n",
                "#         vector_size=size,\n",
                "#         min_count=0,\n",
                "#         window=2,\n",
                "#         sample=6e-5,\n",
                "#         alpha=0.03,\n",
                "#         min_alpha=0.0007,\n",
                "#         negative=20,\n",
                "#         workers=cores - 1,\n",
                "#         sg = 1,\n",
                "#     )\n",
                "\n",
                "#     ft_model.build_vocab(sentences, progress_per=10000)\n",
                "#     print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
                "\n",
                "#     ft_model.train(\n",
                "#         sentences,\n",
                "#         total_examples=ft_model.corpus_count,\n",
                "#         epochs=30,\n",
                "#         report_delay=1,\n",
                "#     )\n",
                "\n",
                "#     print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
                "#     ft_vocab = list(ft_model.wv.index_to_key)\n",
                "\n",
                "#     print(f'Checking words form list of length {len(words)}')\n",
                "#     print(f'WORDS LIST: {words}')\n",
                "\n",
                "# #     for word in words:\n",
                "# #         print(f'Checking word:\\n{word.upper()}:')\n",
                "# #         try:\n",
                "# # #             print(f'FastText {size}: {ft_model_300.wv[word]}')\n",
                "# #             print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
                "# #             print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
                "# #             print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
                "\n",
                "# #         except KeyError as e:\n",
                "# #             print(e)\n",
                "\n",
                "#     return ft_vocab, ft_model\n",
                "\n",
                "# def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
                "\n",
                "#     sentences = [word for word in sentences if word in ft_vocab]\n",
                "\n",
                "#     return np.mean(ft_model.wv[sentences], axis=0) if sentences else np.zeros(size)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65131d36",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
                "#     embeddings_index = {}\n",
                "#     with open(glove_file, 'r', encoding='utf8') as glove:\n",
                "\n",
                "#         for line in glove:\n",
                "#             values = line.split()\n",
                "#             word = values[0]\n",
                "\n",
                "#             with contextlib.suppress(ValueError):\n",
                "#                 coefs = np.asarray(values[1:], dtype='float32')\n",
                "#                 embeddings_index[word] = coefs\n",
                "#     print(f'Found {len(embeddings_index)} word vectors.')\n",
                "\n",
                "#     return embeddings_index\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28d3f00b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
                "\n",
                "#     if external_glove is False and embeddings_index is None:\n",
                "#         embeddings_index= get_glove()\n",
                "\n",
                "#     if extra_preprocessing_enabled is False:\n",
                "#         words = sentences\n",
                "\n",
                "#     elif extra_preprocessing_enabled is True:\n",
                "#         stop_words = set(sw.words('english'))\n",
                "#         words = str(sentences).lower()\n",
                "#         words = word_tokenize(words)\n",
                "#         words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
                "\n",
                "#     M = []\n",
                "\n",
                "#     try:\n",
                "#         for w in words:\n",
                "#             try:\n",
                "#                 M.append(embeddings_index[w])\n",
                "#             except Exception:\n",
                "#                 continue\n",
                "\n",
                "#         M = np.array(M)\n",
                "#         v = M.sum(axis='index')\n",
                "#         return np.zeros(300) if type(v) != np.ndarray else v / np.sqrt((v ** 2).sum())\n",
                "\n",
                "#     except Exception:\n",
                "#         return np.zeros(300)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c7e9593",
            "metadata": {},
            "outputs": [],
            "source": [
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0b71ec5d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get_df_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "88398f5f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# embedding_models_dict = {\n",
                "#     'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
                "#     'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
                "# }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abf4b3a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# %%time\n",
                "# # Make embeddings\n",
                "# ngrams_list=[1, 2, 3, 123]\n",
                "# embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
                "\n",
                "# for embedding_library, ngram_number in tqdm_product(embedding_libraries_list, ngrams_list):\n",
                "#     print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
                "\n",
                "#     for embed_model_name, embed_func_list in tqdm.tqdm(embedding_models_dict.items()):\n",
                "\n",
                "#         build_train_func, embed_func, model_loader = embed_func_list\n",
                "#         print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
                "\n",
                "#         vocab, model = build_train_func(\n",
                "#             df=df_manual,\n",
                "#             ngram_number=ngram_number,\n",
                "#             embedding_library=embedding_library,\n",
                "#         )\n",
                "\n",
                "#         print(f'Getting {embed_model_name} embeddings.')\n",
                "\n",
                "#         df_manual[\n",
                "#             f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
                "#         ] = df_manual[\n",
                "#             f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
                "#         ].progress_apply(\n",
                "#             lambda sentences: embed_func(sentences, vocab, model)\n",
                "#         )\n",
                "#         model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
                "\n",
                "#     # Sent2Vec\n",
                "#     print('Getting sent2vec embeddings.')\n",
                "#     embeddings_index = get_glove()\n",
                "#     df_manual[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_manual[f'Job Description {embedding_library}_{ngram_number}grams'].progress_apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
                "#     print('Done getting sent2vec embeddings.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3cde0242",
            "metadata": {},
            "outputs": [],
            "source": [
                "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "# df_manual.to_pickle(f'{df_save_dir}df_manual_for_trainning.pkl')\n",
                "# df_manual.to_csv(f'{df_save_dir}df_manual_for_trainning.csv')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2411df2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# print(f'Saving df_manual length {len(df_manual)} to txt file.')\n",
                "# with open(f'{data_dir}df_manual_len.txt', 'w') as f:\n",
                "#     f.write(str(len(df_manual)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a130971",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "study1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {},
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
