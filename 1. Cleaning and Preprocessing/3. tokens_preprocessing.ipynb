{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "6373c116",
            "metadata": {},
            "source": [
                "# ATTN: This script should be run AFTER spacy sentence splitting is completed.\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "993064a6",
            "metadata": {},
            "source": [
                "# Explode DF on sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c744a550",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_SENTENCIZED\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b997d96c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dc6c9e4c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3d31a583",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_sentencized.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1899a0d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=17599\n",
                "df_jobs.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ade74c2a",
            "metadata": {
                "scrolled": false
            },
            "outputs": [],
            "source": [
                "# Job Ad info\n",
                "get_df_info(df_jobs, ivs_all=ivs_all)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd7f42d8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ed9d0f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "256dcb71",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explode df so that every row is one sentence\n",
                "df_jobs = df_jobs.explode('Job Description spacy_sentencized', ignore_index=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "458b9f3b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 408599\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2224df52",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b92ee727",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c946c0b",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].tail()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02ec54cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d562a68",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get longest sentence, sentence does not have any punctuation\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description spacy_sentencized'].apply(len).idxmax()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d3938c8a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get length of longest sentence, len=2497\n",
                "df_jobs['Job Description spacy_sentencized'].apply(len).max()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3f79b59",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get length of shortest sentence, len=4\n",
                "df_jobs['Job Description spacy_sentencized'].apply(len).min()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bed13af6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get shortest sentence, its a punctuation (fullstop)\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description spacy_sentencized'].apply(len).idxmin()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "85d2b6cc",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda x: unicodedata.normalize('NFKD', x.encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "010e91c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_sentencized_exploded.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_sentencized_exploded.csv', index=False)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "175328e6",
            "metadata": {},
            "source": [
                "# Validate sentencization and get word frequencies\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3b9448d8",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_SENTENCIZED_EXPLODED\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "41d91301",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "feb7e1eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a745185",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_word_num_and_frequency(row, text_col):\n",
                "\n",
                "    with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "        custom_punct_chars = pickle.load(f)\n",
                "    row['Job Description num_words'] = len(str(row[text_col]).split())\n",
                "    row['Job Description num_unique_words'] = len(set(str(row[text_col]).split()))\n",
                "    row['Job Description num_chars'] = len(str(row[text_col]))\n",
                "    row['Job Description num_chars_no_whitespact_and_punt'] = len(\n",
                "        [\n",
                "            c\n",
                "            for c in str(row[text_col])\n",
                "            if c not in custom_punct_chars and c not in list(string.punctuation) and c in list(string.printable) and c not in list(string.whitespace) and c != ' '\n",
                "        ]\n",
                "    )\n",
                "    row['Job Description num_punctuations'] = len(\n",
                "        [\n",
                "            c\n",
                "            for c in str(row[text_col])\n",
                "            if c in custom_punct_chars and c in list(string.punctuation) and c in list(string.printable) and c not in list(string.whitespace) and c != ' '\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1786e5b8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_sentencized_exploded.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5545a60",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda x: unicodedata.normalize('NFKD', x.encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c2dc86e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs[\n",
                "    'Job Description spacy_sentencized'\n",
                "].apply(lambda x: x if isinstance(x, str) else ast.literal_eval(x))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0fc19a8c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda x: x if isinstance(x, str) else np.nan)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6aa352f0",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].astype(str)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce705aea",
            "metadata": {},
            "outputs": [],
            "source": [
                "#len=408599\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70db27d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs[\n",
                "    'Job Description spacy_sentencized'\n",
                "].apply(\n",
                "    lambda x: ' '.join(x.split()).strip()\n",
                "    if bool(x)\n",
                "    and isinstance(x, str)\n",
                "    and len(x.split()) > 0\n",
                "    else np.nan\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6dc34356",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs[\n",
                "    'Job Description spacy_sentencized'\n",
                "].apply(\n",
                "    lambda x: ' '.join(re.split(pattern, x)).strip()\n",
                "    if bool(x)\n",
                "    and isinstance(x, str)\n",
                "    and len(x.split()) > 0\n",
                "    else np.nan\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47800934",
            "metadata": {},
            "outputs": [],
            "source": [
                "#len=408599\n",
                "len(df_jobs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1517bf3",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.dropna(subset=['Job ID', 'Job Description spacy_sentencized'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cbb522c9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=408599\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7cd29095",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop na, empty strings, punctuations, and keep only sentences of length more than 0\n",
                "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "    custom_punct_chars = pickle.load(f)\n",
                "\n",
                "df_jobs = df_jobs.dropna(subset=['Job Description'])\n",
                "df_jobs = df_jobs.loc[\n",
                "    (df_jobs['Job Description spacy_sentencized'].isnull() == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].isna() == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].notnull() == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].notna() == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(bool) == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].apply(len) > 0)\n",
                "    & (pd.isna(df_jobs['Job Description spacy_sentencized']) == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(str).isin(custom_punct_chars) == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(str).isin(non_whitespace_nan_list) == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(str).isin(list(string.punctuation)) == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(str).isin(list(string.whitespace)) == False)\n",
                "]\n",
                "df_jobs = df_jobs.dropna(subset=['Job Description'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a7e681d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=408599\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32e9c3c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "41c40efc",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0503a4e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].dropna()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "19b1e42d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=408599\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d8bd1020",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b43c29f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# count=408599, unique=217622\n",
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "794d6f3c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "    custom_punct_chars = pickle.load(f)\n",
                "\n",
                "for idx, row in tqdm.tqdm(df_jobs['Job Description spacy_sentencized'].items()):\n",
                "    cleaned_job_sentence = row.split()\n",
                "    cleaned_job_sentence = list(\n",
                "        filter(\n",
                "            lambda items: (\n",
                "                item is not None\n",
                "                and isinstance(item, str)\n",
                "                and item not in nan_list\n",
                "                and item not in custom_punct_chars\n",
                "                and item not in list(string.punctuation)\n",
                "                and item not in list(string.whitespace)\n",
                "                and item in list(string.printable)\n",
                "                for item in items\n",
                "            ),\n",
                "            cleaned_job_sentence\n",
                "        )\n",
                "    )\n",
                "    cleaned_job_sentence = ' '.join(cleaned_job_sentence).strip()\n",
                "\n",
                "    if len(cleaned_job_sentence.split()) > 2:\n",
                "        df_jobs.loc[idx, 'Job Description spacy_sentencized'] = cleaned_job_sentence\n",
                "    else:\n",
                "        df_jobs = df_jobs.drop(idx)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ca6e0b77",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 347914\n",
                "len(df_jobs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "95891a51",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0d396b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# count=347914, unique=202343\n",
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ea0e092",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].dropna()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c639a80",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda x: unicodedata.normalize('NFKD', x.encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e34a03dc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 347914\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ae1dcc7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbc2f7ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "# count=347914, unique=202343\n",
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "61de05bb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Remove duplicate sentences from the same job ad\n",
                "df_jobs = df_jobs.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75385881",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len = 307464\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "004beb18",
            "metadata": {},
            "outputs": [],
            "source": [
                "# count=307464, unique=202343\n",
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "85d25bc1",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].dropna()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06530ffb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=307464\n",
                "len(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6479fa85",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f731381d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop na, empty strings, punctuations, and keep only sentences of length more than 0\n",
                "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "    custom_punct_chars = pickle.load(f)\n",
                "\n",
                "df_jobs = df_jobs.dropna(subset=['Job Description spacy_sentencized'])\n",
                "df_jobs = df_jobs.loc[\n",
                "    (df_jobs['Job Description spacy_sentencized'].isnull() == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].isna() == False)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].notnull() == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].notna() == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(bool) == True)\n",
                "    & (df_jobs['Job Description spacy_sentencized'].astype(str).apply(len) > 0)\n",
                "    & (pd.isna(df_jobs['Job Description spacy_sentencized']) == False)\n",
                "]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4afc36eb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=307464\n",
                "len(df_jobs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd546cec",
            "metadata": {},
            "outputs": [],
            "source": [
                "# count=307464, unique=202343\n",
                "df_jobs['Job Description spacy_sentencized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "488f6a5a",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda x: ' '.join(re.split(pattern, x)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ebd2583",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the longest sentence\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description spacy_sentencized'].apply(len).idxmax()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "01e9fc82",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=2496\n",
                "df_jobs['Job Description spacy_sentencized'].apply(len).max()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "994e6919",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the shortest sentence\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description spacy_sentencized'].apply(len).idxmin()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2c8092c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# len=6\n",
                "df_jobs['Job Description spacy_sentencized'].apply(len).min()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "94345b75",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Get sentence word frequencies\n",
                "df_jobs = df_jobs.progress_apply(\n",
                "    lambda row: get_word_num_and_frequency(\n",
                "        row=row, text_col='Job Description spacy_sentencized'\n",
                "    ), \n",
                "    axis='columns',\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aa853d04",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Job Description spacy_sentencized',\n",
                "        'Job Description num_words', 'Job Description num_unique_words',\n",
                "        'Job Description num_chars', 'Job Description num_chars_no_whitespact_and_punt'\n",
                "    ]\n",
                "].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "20c91bfe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the longest sentence words = 349\n",
                "df_jobs['Job Description num_words'].max()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "169963d6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the sentence with the max words\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description num_words'].idxmax()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ad25b505",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the longest sentence words = 3\n",
                "df_jobs['Job Description num_words'].min()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "731d94bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get the sentence with the min words\n",
                "df_jobs['Job Description spacy_sentencized'].loc[df_jobs['Job Description num_words'].idxmin()]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bc21643d",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized_lower'] = df_jobs['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda job_sentence: job_sentence.strip().lower()\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1b8dac3e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Job Description spacy_sentencized', 'Job Description spacy_sentencized_lower']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7133bfd6",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Add language requirement column\n",
                "# Use regex to find language requirement\n",
                "dutch_requirement_pattern = r'[Dd]utch [Pp]referred | [Dd]utch [Re]quired | [Dd]utch [Ll]anguage |[Pp]roficient in [Dd]utch |[Ss]peak [Dd]utch | [Kk]now [Dd]utch | [Ff]luent in [Dd]utch | [Dd]utch [Nn]ative | * [Dd]utch [Ll]evel'\n",
                "english_requirement_pattern = r'[Ee]nglish [Pp]referred | [Ee]nglish [Re]quired | [Ee]nglish [Ll]anguage |[Pp]roficient in [Ee]nglish |[Ss]peak [Ee]nglish | [Kk]now [Ee]nglish | [Ff]luent in [Ee]nglish | [Ee]nglish [Nn]ative | * [Ee]nglish [Ll]evel'\n",
                "\n",
                "lang_requirements = {\n",
                "    'Dutch Requirement in Sentence': dutch_requirement_pattern, 'English Requirement in Sentence': english_requirement_pattern\n",
                "}\n",
                "\n",
                "for lang_req, lang_req_pattern in lang_requirements.items():\n",
                "\n",
                "    if lang_req in df_jobs.columns:\n",
                "        df_jobs = df_jobs.drop(columns=[lang_req])\n",
                "    df_jobs[lang_req] = np.where(\n",
                "        df_jobs['Job Description'].str.contains(lang_req_pattern),\n",
                "        'Yes',\n",
                "        'No',\n",
                "    )\n",
                "    df_jobs[lang_req] = df_jobs[lang_req].astype('category').cat.reorder_categories(['No', 'Yes'], ordered=True)\n",
                "    df_jobs[lang_req] = pd.Categorical(df_jobs[lang_req], categories=['No', 'Yes'], ordered=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4744096",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Yes = 8809 job sentences\n",
                "df_jobs['Dutch Requirement in Job Ad'].value_counts()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aad0097b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Yes = 16008 job sentences\n",
                "df_jobs['English Requirement in Job Ad'].value_counts()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "25dad11d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_sentencized_validated.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_sentencized_validated.csv', index=False)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4e568b2c",
            "metadata": {},
            "source": [
                "# Use spacy to tokenize sentences"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "e4e1c487",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_SENTENCIZED_VALIDATED\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c288f82e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d09c526c",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8eeb47a5",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_sentencized_validated.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1cf56055",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Spacy tokenize\n",
                "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
                "    custom_punct_chars = pickle.load(f)\n",
                "\n",
                "df_jobs['Job Description spacy_tokenized'] = df_jobs['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda job_sentence: [\n",
                "        str(token.text.strip().lower())\n",
                "        for token in nlp.tokenizer(job_sentence)\n",
                "        if token.text is not None\n",
                "        and len(token) != 0\n",
                "        and len(token.text) != 0\n",
                "        and bool(token)\n",
                "        and bool(token.text)\n",
                "        and token.text != '...'\n",
                "        and not token.is_space\n",
                "        and not token.is_punct\n",
                "        and not token.is_quote\n",
                "        and not token.is_bracket\n",
                "        and not token.like_email\n",
                "    ]\n",
                ")\n",
                "\n",
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd443d94",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_sentencized_cleaned'] = df_jobs['Job Description spacy_tokenized'].str.join(' ')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dbaea2bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b24c9976",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.columns\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ba33d712",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Job Sentence info\n",
                "get_df_info(df_jobs, ivs_all=ivs_all)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5deddb9d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e7f002d2",
            "metadata": {},
            "source": [
                "# Use NLTK to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "92922872",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_TOKENIZED_SPACY\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9fae46f4",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e7cbbc2a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5011ea0a",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_tokenized_spacy.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9c8a9f1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4475248f",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Tokenize with NLTK\n",
                "stop_words = set(stopwords.words('english'))\n",
                "punctuations = list(string.punctuation)\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "df_jobs['Job Description nltk_tokenized'] = df_jobs['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda job_sentence: [\n",
                "        str(token.strip().lower()) \n",
                "        for token in word_tokenize(job_sentence) \n",
                "        if len(token) != 0 \n",
                "        and token != '...' \n",
                "        and not token.lower() in nan_list\n",
                "        and not token.lower() in custom_punct_chars\n",
                "        and not token.lower() in set(stopwords.words('english')) \n",
                "        and not token.lower() in list(string.punctuation)\n",
                "        and not token.lower() in list(string.whitespace)\n",
                "        and (t.lower() in list(string.printable) for t in token.lower())\n",
                "    ]\n",
                ")\n",
                "\n",
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy_nltk.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3bbd5d3f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description nltk_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "99aef0ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description nltk_tokenized'].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "43044b19",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Keep the some of the [] rows because they contain 'Who are we', 'What we do', etc. and remove '1. ' and '2. ' etc. abd 'T)'\n",
                "df_jobs = df_jobs.loc[~df_jobs['Job Description spacy_sentencized'].str.match('^\\d\\.|.\\)')]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "86da6772",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "69e6640b",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy_nltk.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "81e54149",
            "metadata": {},
            "source": [
                "# Use gensim to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ff307dba",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_TOKENIZED_SPACY_NLTK\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f89cb65",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ddc01de",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c13d88fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bc48f290",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e95fec77",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "df_jobs['Job Description gensim_tokenized'] = df_jobs['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda sentence: preprocess_string(re.sub(pattern, '.', sentence.strip().lower()))\n",
                ")\n",
                "\n",
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1cc29198",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a69a8de",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a62be70e",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eeb0131f",
            "metadata": {},
            "source": [
                "# Use BERT to tokenize sentences\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "efcf30fc",
            "metadata": {},
            "source": [
                "### START HERE IF SOURCING FROM DF_JOBS_TOKENIZED_SPACY_NLTK_GENSIM\n",
                "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c81e981f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df5d59b2",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec899945",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim.pkl')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d42a3b60",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f8bc6622",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "bert_model_name = 'bert-base-uncased'\n",
                "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
                "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
                "\n",
                "df_jobs['Job Description bert_tokenized'] = df_jobs['Job Description spacy_sentencized'].progress_apply(\n",
                "    lambda sentence: bert_tokenizer.tokenize(str(sentence))\n",
                ")\n",
                "\n",
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim_bert.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ae92181",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description bert_tokenized'].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8936c818",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_classification.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "45c900fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving df_jobs length {len(df_jobs)} to txt file.')\n",
                "with open(f'{data_dir}df_jobs_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_jobs)))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e13d801e",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "study1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
