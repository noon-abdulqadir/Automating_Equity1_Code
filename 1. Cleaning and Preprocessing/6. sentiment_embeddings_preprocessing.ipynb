{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976d079",
   "metadata": {},
   "source": [
    "# Use spacy and nltk for sentiment scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentiment\n",
    "if 'spacytextblob' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "\n",
    "df_jobs['Job Description spacy_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK sentiment\n",
    "df_jobs['Job Description nltk_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e65256",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26883380",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_SENTIMENT_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75534c8",
   "metadata": {},
   "source": [
    "# Word2Vec and FastText embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b37021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import googletrans\n",
    "import torch\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, Trainer, TrainingArguments\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification, BertForPreTraining, BertConfig, BertModel\n",
    ")\n",
    "device_name = 'cuda'\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_sentiment_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d145aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_word2vec(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    w2v_model.train(\n",
    "        sentences,\n",
    "        total_examples=w2v_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    w2v_vocab = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "            # print(f'{sector} 300: {w2v_model_300.wv[word]}')\n",
    "            # print(f'{sector} 100: {w2v_model_100.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return w2v_vocab, w2v_model\n",
    "\n",
    "def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in w2v_vocab]\n",
    "\n",
    "    return np.mean(w2v_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60af39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_fasttext(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    ft_model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    ft_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    ft_model.train(\n",
    "        sentences,\n",
    "        total_examples=ft_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    ft_vocab = list(ft_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "            # print(f'{sector} 300: {ft_model_300.wv[word]}')\n",
    "            # print(f'{sector} 100: {ft_model_100.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return ft_vocab, ft_model\n",
    "\n",
    "def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in ft_vocab]\n",
    "\n",
    "    return np.mean(ft_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as glove:\n",
    "\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec52618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "    if external_glove is False and embeddings_index is None:\n",
    "        embeddings_index= get_glove()\n",
    "\n",
    "    if extra_preprocessing_enabled is False:\n",
    "        words = sentences\n",
    "\n",
    "    elif extra_preprocessing_enabled is True:\n",
    "        stop_words = set(sw.words('english'))\n",
    "        words = str(sentences).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "    M = []\n",
    "\n",
    "    try:\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis='index')\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(300)\n",
    "\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc0e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_dict = {\n",
    "    'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
    "    'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
    "\n",
    "    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "\n",
    "        build_train_func, embed_func, model_loader = embed_func_list\n",
    "        print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
    "\n",
    "        vocab, model = build_train_func(\n",
    "            df=df_jobs,\n",
    "            ngram_number=ngram_number,\n",
    "            embedding_library=embedding_library,\n",
    "        )\n",
    "\n",
    "        print(f'Getting {embed_model_name} embeddings.')\n",
    "\n",
    "        df_jobs[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
    "        ] = df_jobs[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
    "        ].apply(\n",
    "            lambda sentences: embed_func(sentences, vocab, model)\n",
    "        )\n",
    "        model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
    "\n",
    "    # Sent2Vec\n",
    "    print('Getting sent2vec embeddings.')\n",
    "    embeddings_index = get_glove()\n",
    "    df_jobs[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_jobs[f'Job Description {embedding_library}_{ngram_number}grams'].apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
    "    print('Done getting sent2vec embeddings.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75806adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_trainning.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{df_save_dir}df_jobs_for_trainning.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb13b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
