{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d9bbd5",
   "metadata": {},
   "source": [
    "# ATTN: This script uses Google translate to detect job description language. Google translate will limit requests and take a very long time. Only run this script if redoing language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e2d85",
   "metadata": {},
   "source": [
    "# Read from scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8129d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a74773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840ebcc",
   "metadata": {},
   "source": [
    "#### Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_paths = list(set(glob.glob(f'{scraped_data}Coding Material/*Folder/*/Job ID -*- Codebook (Automating Equity).xlsx')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 244 xlsx files\n",
    "len(glob_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdd650",
   "metadata": {},
   "source": [
    "#### Use paths to open files, fix keywords, and drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2edb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix list catches all incorrect/faculty keyword search terms\n",
    "\n",
    "fix_list = []\n",
    "\n",
    "# Appended data catches all the fixed and cleaned dfs\n",
    "appended_data = []\n",
    "\n",
    "for glob_path in glob_paths:\n",
    "\n",
    "    try:\n",
    "        df_temp = pd.read_excel(glob_path).reset_index(drop=True)\n",
    "    except ValueError:\n",
    "        fix_list.append(glob_path)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
    "        df_temp.drop(\n",
    "        df_temp.columns[\n",
    "                df_temp.columns.str.contains(\n",
    "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
    "                )\n",
    "            ],\n",
    "            axis='columns',\n",
    "            inplace=True,\n",
    "            errors='ignore',\n",
    "        )\n",
    "\n",
    "        appended_data.append(df_temp.reset_index(drop=True))\n",
    "\n",
    "# Concatonate list of dfs into one large df_manual\n",
    "df_manual = pd.concat(appended_data).reset_index(drop=True)\n",
    "\n",
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de12303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
    "if len(fix_list) != 0:\n",
    "    print('Some keywords to fix!')\n",
    "    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
    "        json.dump(fix_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dfs, len = 244\n",
    "len(appended_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate list of dfs into one large df_manual\n",
    "df_manual = pd.concat(appended_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b014ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 12400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4606db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6e2dd",
   "metadata": {},
   "source": [
    "# Drop duplicated and missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc049d41",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda9efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92eeb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 12400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c31c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e2a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d837cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns\n",
    "df_manual.columns = df_manual.columns.to_series().apply(lambda x: str(x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns 'Task_Mentioned', 'Task_Warmth', 'Task_Competence'\n",
    "df_manual.drop(\n",
    "    columns=['Task_Mentioned', 'Task_Warmth', 'Task_Competence'],\n",
    "    axis='columns',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Warmth'] = df_manual['Warmth'].astype(np.float64)\n",
    "df_manual['Competence'] = df_manual['Competence'].astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7666d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Sentence to 'Job Description spacy_sentencized'\n",
    "df_manual.rename(\n",
    "    columns = {\n",
    "        'Sentence': 'Job Description spacy_sentencized'\n",
    "    },\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb59e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df_manual.dropna(axis='index', how='all', inplace=True)\n",
    "df_manual.dropna(axis='columns', how='all', inplace=True)\n",
    "df_manual.dropna(\n",
    "    subset = ['Job Description spacy_sentencized', 'Warmth', 'Competence'],\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25218a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 12394\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 133\n",
    "df_manual.groupby(['Job ID'])['Job ID'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3126f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates on subset of 'Job ID' and 'Sentence'\n",
    "df_manual.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'], keep='first', ignore_index=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67baaa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing 'Job ID'\n",
    "df_manual.drop(\n",
    "    df_manual[\n",
    "        (df_manual['Job ID'].isin(nan_list)) | \n",
    "        (df_manual['Job ID'].isnull()) | \n",
    "        (df_manual['Job ID'].isna())\n",
    "    ].index, \n",
    "    axis='index',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a098a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e08303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_dropped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0b99",
   "metadata": {},
   "source": [
    "# Add English and Dutch language requirement columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb838f06",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW_DROPPED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aa5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77db8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add language requirement column\n",
    "# Use regex to find language requirement\n",
    "dutch_requirement_pattern = r'[Ll]anguage: [Dd]utch|[Dd]utch [Pp]referred|[Dd]utch [Re]quired|[Dd]utch [Ll]anguage|[Pp]roficient in [Dd]utch|[Ss]peak [Dd]utch|[Kk]now [Dd]utch'\n",
    "english_requirement_pattern = r'[Ll]anguage: [Ee]nglish|[Ee]nglish [Pp]referred|[Ee]nglish [Re]quired|[Ee]nglish [Ll]anguage|[Pp]roficient in [Ee]nglish|[Ss]peak [Ee]nglish|[Kk]now [Ee]nglish'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language requirements\n",
    "# Dutch\n",
    "if 'Dutch Requirement' in df_manual.columns:\n",
    "    df_manual.drop(columns=['Dutch Requirement'], inplace=True)\n",
    "df_manual['Dutch Requirement'] = np.where(\n",
    "    df_manual['Job Description spacy_sentencized'].str.contains(dutch_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")\n",
    "\n",
    "# English\n",
    "if 'English Requirement' in df_manual.columns:\n",
    "    df_manual.drop(columns=['English Requirement'], inplace=True)\n",
    "df_manual['English Requirement'] = np.where(\n",
    "    df_manual['Job Description spacy_sentencized'].str.contains(english_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_english_requirement.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes = 235\n",
    "df_manual['Dutch Requirement'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes = 526\n",
    "df_manual['English Requirement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ddedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_language_requirement.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cea97",
   "metadata": {},
   "source": [
    "# Add data from Sectors dataframe (see CBS directory under scrapped_data directory) and Categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69513116",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW_LANGUAGE_REQUIREMENT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b32849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff55233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to print df gender and age info\n",
    "def df_gender_age_info(\n",
    "    df,\n",
    "    ivs_all = [\n",
    "        'Gender',\n",
    "        'Gender_Num',\n",
    "        'Gender_Female',\n",
    "        'Gender_Mixed',\n",
    "        'Gender_Male',\n",
    "        'Age',\n",
    "        'Age_Num',\n",
    "        'Age_Older',\n",
    "        'Age_Mixed',\n",
    "        'Age_Younger',\n",
    "    ],\n",
    "):\n",
    "    # Print Info\n",
    "    print('\\nDF INFO:\\n')\n",
    "    df.info()\n",
    "\n",
    "    for iv in ivs_all:\n",
    "        try:\n",
    "            counts = df[f\"{iv}\"].value_counts()\n",
    "            percentages = df[f\"{iv}\"].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
    "            print('='*20)\n",
    "            print(f'{iv}:')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Counts:\\n{counts}')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Percentages:\\n{percentages}')\n",
    "\n",
    "            try:\n",
    "                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
    "                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Mean: {mean}')\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Standard Deviation: {sd}')\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            print(f'{iv} not available.')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40156303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db503aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job ID'] = df_manual['Job ID'].apply(lambda x: str(x).lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98694f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_including_sector_genage_data.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5116df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49391453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job ID'] = df_jobs['Job ID'].apply(lambda x: str(x).lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82178d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.drop(\n",
    "    columns = [\n",
    "        'Job Description', 'Rating', 'Employment Type',\n",
    "        'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
    "        'Collection Date', 'Data Row', 'Tracking ID', 'Job Date',\n",
    "        'Type of ownership', 'Language', 'Dutch Requirement', 'English Requirement', \n",
    "    ],\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sector and categorical data from df_jobs\n",
    "df_manual = df_manual.merge(df_jobs, on='Job ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f24060",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa3bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d889ec9",
   "metadata": {},
   "source": [
    "#### Check if there is any missing sector data in the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Sector'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_manual['Sector'].isna().sum() != 0:\n",
    "    print('Some search keywords did not match a sector. Fixing')\n",
    "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
    "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n",
    "    df_manual = fix_keywords(df_manual)\n",
    "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
    "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d24ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Job Ad info\n",
    "df_gender_age_info(df_manual.groupby(['Job ID']).first())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d65547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Job Sentence info\n",
    "df_gender_age_info(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Job Sentence info\n",
    "df_gender_age_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_manual['Sector'].isna().sum() == 0:\n",
    "    assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "    df_manual.to_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl')\n",
    "    df_manual.to_csv(f'{df_save_dir}df_manual_including_sector_genage_data.csv', index=False)\n",
    "else:\n",
    "    print(f\"MISSING SECTOR DATA: COUNT {df_manual['Sector'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c116",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER spacy sentence splitting is completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993064a6",
   "metadata": {},
   "source": [
    "# Use spacy to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744a550",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_SENTENCIZED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb270b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a63dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_num_and_frequency(row, text_col):\n",
    "\n",
    "    row['Job Description num_words'] = len(str(row[f'{text_col}']).split())\n",
    "    row['Job Description num_unique_words'] = len(set(str(row[f'{text_col}']).split()))\n",
    "    row['Job Description num_chars'] = len(str(row[f'{text_col}']))\n",
    "    row['Job Description num_punctuations'] = len([c for c in str(row[f'{text_col}']) if c in string.punctuation])\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc21643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_sentencized_lower'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: job_sentence.strip().lower()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual[['Job Description spacy_sentencized', 'Job Description spacy_sentencized_lower']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf56055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenize\n",
    "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
    "    custom_punct_chars = pickle.load(f)\n",
    "\n",
    "df_manual['Job Description spacy_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.text.strip().lower())\n",
    "        for token in nlp.tokenizer(job_sentence)\n",
    "        if len(token) != 0\n",
    "        and not token.is_space\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "        and not token.is_bracket\n",
    "        and not token.like_email\n",
    "        and not token.text in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd443d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_sentencized_cleaned'] = df_manual['Job Description spacy_tokenized'].str.join(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65043f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence word frequencies\n",
    "df_manual = df_manual.apply(\n",
    "    lambda row: get_word_num_and_frequency(\n",
    "        row=row, text_col='Job Description spacy_sentencized'\n",
    "    ), \n",
    "    axis='columns',\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2acfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deddb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f002d2",
   "metadata": {},
   "source": [
    "# Use NLTK to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92922872",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93afe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4475248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize with NLTK\n",
    "df_manual['Job Description nltk_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.strip().lower()) \n",
    "        for token in word_tokenize(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and token != '...' \n",
    "        and not token.lower() in set(stopwords.words('english')) \n",
    "        and not token.lower() in list(string.punctuation) \n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbd5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description nltk_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e6640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e54149",
   "metadata": {},
   "source": [
    "# Use gensim to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff307dba",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74624e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb91e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description gensim_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc29198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description gensim_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62be70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0131f",
   "metadata": {},
   "source": [
    "# Use BERT to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf30fc",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7958db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff83773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec899945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description bert_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: bert_tokenizer.tokenize(str(sentence))\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae92181",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description bert_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bfad5",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all tokenization (spacy, nltk, gensim, and BERT) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a62460",
   "metadata": {},
   "source": [
    "# Use spacy to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d2178",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM_BERT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10424739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e45b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer characters\n",
    "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
    "    custom_punct_chars = pickle.load(f)\n",
    "\n",
    "# POS tagging\n",
    "df_manual['Job Description spacy_token_tags'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        tuple([token.text.strip().lower(), token.tag_])\n",
    "        for token in nlp(job_sentence)\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Lemmatization\n",
    "df_manual['Job Description spacy_lemmas'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        token.lemma_.strip().lower()\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Stemming\n",
    "df_manual['Job Description spacy_stems'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        stemmer.stem(token.text.strip().lower())\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8332f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual[\n",
    "    [\n",
    "        'Job Description spacy_token_tags',\n",
    "        'Job Description spacy_lemmas',\n",
    "        'Job Description spacy_stems'\n",
    "    ]\n",
    "].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7acdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74665a",
   "metadata": {},
   "source": [
    "# Use NLTK to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe6cb2",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c765e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac327ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(token):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS stagging\n",
    "df_manual['Job Description nltk_token_tags'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda token: pos_tag(token)\n",
    ")\n",
    "\n",
    "# Lemmatization\n",
    "df_manual['Job Description nltk_lemmas'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        lemmatizer.lemmatize(\n",
    "            token, get_wordnet_pos(\n",
    "                unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            )\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Stemming\n",
    "df_manual['Job Description nltk_stems'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        stemmer.stem(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2af7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual[['Job Description nltk_token_tags', 'Job Description nltk_lemmas', 'Job Description nltk_stems']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102509c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f98098",
   "metadata": {},
   "source": [
    "# Use BERT to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd84df",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# mod = sys.modules[__name__]\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# # %load_ext autoreload\n",
    "# # %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c26cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAIN DIR\n",
    "# main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# # code_dir\n",
    "# code_dir = f'{code_dir}/'\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# # scraping dir\n",
    "# scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# # data dir\n",
    "# data_dir = f'{code_dir}data/'\n",
    "\n",
    "# # df save sir\n",
    "# df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# # lang models dir\n",
    "# llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# # sites\n",
    "# site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# # columns\n",
    "# cols=['Sector', \n",
    "#       'Sector Code', \n",
    "#       'Gender', \n",
    "#       'Age', \n",
    "#       'Language', \n",
    "#       'Dutch Requirement', \n",
    "#       'English Requirement', \n",
    "#       'Gender_Female', \n",
    "#       'Gender_Mixed', \n",
    "#       'Gender_Male', \n",
    "#       'Age_Older', \n",
    "#       'Age_Mixed', \n",
    "#       'Age_Younger', \n",
    "#       'Gender_Num', \n",
    "#       'Age_Num', \n",
    "#       '% Female', \n",
    "#       '% Male', \n",
    "#       '% Older', \n",
    "#       '% Younger']\n",
    "\n",
    "# int_variable: str = 'Job ID'\n",
    "# str_variable: str = 'Job Description'\n",
    "# gender: str = 'Gender'\n",
    "# age: str = 'Age'\n",
    "# language: str = 'en'\n",
    "# languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "# str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "# nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "# pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d35cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import re\n",
    "# import time\n",
    "# import json\n",
    "# import csv\n",
    "# import glob\n",
    "# import pickle\n",
    "# import random\n",
    "# import unicodedata\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import googletrans\n",
    "# from googletrans import Translator\n",
    "# random.seed(42)\n",
    "\n",
    "# # Set up Spacy\n",
    "# import spacy\n",
    "# from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Set up NLK\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "# from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "# nltk_path = f'{llm_path}/nltk'\n",
    "# nltk.data.path.append(nltk_path)\n",
    "\n",
    "# nltk.download('words', download_dir = nltk_path)\n",
    "# nltk.download('stopwords', download_dir = nltk_path)\n",
    "# nltk.download('punkt', download_dir = nltk_path)\n",
    "# nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# punctuations = list(string.punctuation)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Set up Gensim\n",
    "# from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "# from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# # Set up Bert\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "# bert_model_name = 'bert-base-uncased'\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "# bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81972d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a48ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "# bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "# bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "# df_manual['Job Description bert_token_tags_with_scores'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "#     lambda sentence: [\n",
    "#         (bert_pos_tag['word'], bert_pos_tag['entity'], bert_pos_tag['score'])\n",
    "#         for i in range(len(sentence.split()))\n",
    "#         for bert_pos_tag in bert_pos_tagger(sentence)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df_manual['Job Description bert_token_tags'] = df_manual['Job Description bert_token_tags_with_scores'].apply(\n",
    "#     lambda tag_list: [\n",
    "#         [(tag_list[i][0], tag_list[i][1])]\n",
    "#         for tag_tuple in tag_list\n",
    "#         for i in range(len(tag_list))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
    "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e99e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_manual['Job Description bert_token_tags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
    "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cb736",
   "metadata": {},
   "source": [
    "# Use spacy to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_make_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    matches_list = []\n",
    "\n",
    "    for idx in range(len(matches)):\n",
    "        for match_id, start, end in matches:\n",
    "            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "                match = doc[matches[idx][1]: matches[idx][2]].text\n",
    "                matches_list.append(match.lower())\n",
    "    \n",
    "    return list(set(matches_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_1grams_original_list'] = df_manual['Job Description spacy_tokenized']\n",
    "df_manual['Job Description spacy_1grams'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "patters_dict = {\n",
    "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
    "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
    "}\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3,\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
    "\n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                '_'.join(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                tuple(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if f'{ngram_name}_patterns' in matcher:\n",
    "        matcher.remove(f'{ngram_name}_patterns')\n",
    "    assert f'{ngram_name}_patterns' not in matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Allgrams\n",
    "df_manual['Job Description spacy_123grams_original_list'] = df_manual['Job Description spacy_tokenized'] + df_manual['Job Description spacy_2grams_original_list'] + df_manual['Job Description spacy_3grams_original_list']\n",
    "df_manual['Job Description spacy_123grams'] = df_manual['Job Description spacy_1grams'] + df_manual['Job Description spacy_2grams'] + df_manual['Job Description spacy_3grams']\n",
    "df_manual['Job Description spacy_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description spacy_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258e491",
   "metadata": {},
   "source": [
    "# Use NLTK to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767712",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3991b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61db031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description nltk_1grams_original_list'] = df_manual['Job Description nltk_tokenized']\n",
    "df_manual['Job Description nltk_1grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens:\n",
    "            list(\n",
    "                '_'.join(ngram_list)\n",
    "                for ngram_list in ngrams(tokens, ngram_num)\n",
    "            )\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Allgrams\n",
    "df_manual[f'Job Description nltk_123grams_original_list'] = df_manual['Job Description nltk_tokenized'] + df_manual['Job Description nltk_2grams_original_list'] + df_manual['Job Description nltk_3grams_original_list']\n",
    "df_manual[f'Job Description nltk_123grams'] = df_manual['Job Description nltk_1grams'] + df_manual['Job Description nltk_2grams'] + df_manual['Job Description nltk_3grams']\n",
    "df_manual['Job Description nltk_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description nltk_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17610cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440bd34",
   "metadata": {},
   "source": [
    "# Use Gensim to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494636f",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad74074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d29aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description gensim_1grams_original_list'] = df_manual['Job Description gensim_tokenized']\n",
    "df_manual['Job Description gensim_1grams'] = df_manual['Job Description gensim_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_manual['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_manual['Job Description gensim_2grams_original_list_all'] = bigram[df_manual['Job Description gensim_tokenized']]\n",
    "df_manual['Job Description gensim_2grams_original_list'] = df_manual['Job Description gensim_2grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_manual['Job Description gensim_2grams'] = df_manual['Job Description gensim_2grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_manual[f'Job Description gensim_2grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_2grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_manual['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_manual['Job Description gensim_3grams_original_list_all'] = trigram[df_manual['Job Description gensim_2grams_original_list_all']]\n",
    "df_manual['Job Description gensim_3grams_original_list'] = df_manual['Job Description gensim_3grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_manual['Job Description gensim_3grams'] = df_manual['Job Description gensim_3grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_manual[f'Job Description gensim_3grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_3grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d89c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Allgrams\n",
    "df_manual[f'Job Description gensim_123grams_original_list'] = df_manual['Job Description gensim_tokenized'] + df_manual['Job Description gensim_2grams_original_list'] + df_manual['Job Description gensim_3grams_original_list']\n",
    "df_manual[f'Job Description gensim_123grams'] = df_manual['Job Description gensim_1grams'] + df_manual['Job Description gensim_2grams'] + df_manual['Job Description gensim_3grams']\n",
    "df_manual['Job Description gensim_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac58f92",
   "metadata": {},
   "source": [
    "# Create word frequencies for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33d9ab",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK_GENSIM\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb934586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_frequency(row, text_col, ngram_num, embedding_library):\n",
    "\n",
    "    abs_word_freq = defaultdict(int)\n",
    "    for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
    "        abs_word_freq[word] += 1\n",
    "\n",
    "        abs_wtd_df = (\n",
    "            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
    "            .rename(columns={0: 'abs_word_freq'})\n",
    "            .sort_values(by=['abs_word_freq'], ascending=False)\n",
    "            )\n",
    "        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
    "        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
    "\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43206134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_manual = df_manual.apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30affd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_frequency.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670f255",
   "metadata": {},
   "source": [
    "# Create BoW dictionary, corpus, and tfidf matrix for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989aa0",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_FREQUENCY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051454b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f57c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57359916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
    "    \n",
    "    ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
    "    dictionary = Dictionary([ngrams_original_list])\n",
    "    BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
    "    tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "    tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
    "\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b00a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_manual = df_manual.apply(\n",
    "        lambda row: get_corpus_and_dictionary(\n",
    "            row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51eae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31f120",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976d079",
   "metadata": {},
   "source": [
    "# Use spacy and nltk for sentiment scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61138e8",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_BOW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cf19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444550f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a35056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentiment\n",
    "if 'spacytextblob' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "\n",
    "df_manual['Job Description spacy_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58173ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK sentiment\n",
    "df_manual['Job Description nltk_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_sentiment_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5cf4e",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad73af",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_SENTIMENT_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48122d98",
   "metadata": {},
   "source": [
    "# Word2Vec and FastText embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57105477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd263006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91117606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_word2vec(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    w2v_model.train(\n",
    "        sentences,\n",
    "        total_examples=w2v_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    w2v_vocab = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "#             print(f'Word2Vec {size}: {w2v_model.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return w2v_vocab, w2v_model\n",
    "\n",
    "def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in w2v_vocab]\n",
    "\n",
    "    return np.mean(w2v_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_fasttext(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    ft_model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    ft_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    ft_model.train(\n",
    "        sentences,\n",
    "        total_examples=ft_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    ft_vocab = list(ft_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "#             print(f'FastText {size}: {ft_model_300.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return ft_vocab, ft_model\n",
    "\n",
    "def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in ft_vocab]\n",
    "\n",
    "    return np.mean(ft_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d43d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as glove:\n",
    "\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "    if external_glove is False and embeddings_index is None:\n",
    "        embeddings_index= get_glove()\n",
    "\n",
    "    if extra_preprocessing_enabled is False:\n",
    "        words = sentences\n",
    "\n",
    "    elif extra_preprocessing_enabled is True:\n",
    "        stop_words = set(sw.words('english'))\n",
    "        words = str(sentences).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "    M = []\n",
    "\n",
    "    try:\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis='index')\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(300)\n",
    "\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0675fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_dict = {\n",
    "    'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
    "    'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f55d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
    "\n",
    "    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "\n",
    "        build_train_func, embed_func, model_loader = embed_func_list\n",
    "        print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
    "\n",
    "        vocab, model = build_train_func(\n",
    "            df=df_manual,\n",
    "            ngram_number=ngram_number,\n",
    "            embedding_library=embedding_library,\n",
    "        )\n",
    "\n",
    "        print(f'Getting {embed_model_name} embeddings.')\n",
    "\n",
    "        df_manual[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
    "        ] = df_manual[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
    "        ].apply(\n",
    "            lambda sentences: embed_func(sentences, vocab, model)\n",
    "        )\n",
    "        model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
    "\n",
    "    # Sent2Vec\n",
    "    print('Getting sent2vec embeddings.')\n",
    "    embeddings_index = get_glove()\n",
    "    df_manual[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_manual[f'Job Description {embedding_library}_{ngram_number}grams'].apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
    "    print('Done getting sent2vec embeddings.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_for_trainning.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_for_trainning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f8888",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all embeddings are completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795b68f",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_FOR_TRAINNING\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79862f16",
   "metadata": {},
   "source": [
    "# Descriptives and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1245d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb3c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59dec7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb174c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to print df gender and age info\n",
    "def df_gender_age_info(\n",
    "    df,\n",
    "    ivs_all = [\n",
    "        'Gender',\n",
    "        'Gender_Num',\n",
    "        'Gender_Female',\n",
    "        'Gender_Mixed',\n",
    "        'Gender_Male',\n",
    "        'Age',\n",
    "        'Age_Num',\n",
    "        'Age_Older',\n",
    "        'Age_Mixed',\n",
    "        'Age_Younger',\n",
    "    ],\n",
    "):\n",
    "    # Print Info\n",
    "    print('\\nDF INFO:\\n')\n",
    "    df.info()\n",
    "\n",
    "    for iv in ivs_all:\n",
    "        try:\n",
    "            counts = df[f\"{iv}\"].value_counts()\n",
    "            percentages = df[f\"{iv}\"].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
    "            print('='*20)\n",
    "            print(f'{iv}:')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Counts:\\n{counts}')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Percentages:\\n{percentages}')\n",
    "\n",
    "            try:\n",
    "                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
    "                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Mean: {mean}')\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Standard Deviation: {sd}')\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            print(f'{iv} not available.')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1862fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Columns: 219 entries, % Sector per Workforce to Job Description gensim_123grams_sent2vec_embeddings\n",
      "dtypes: float64(34), int64(4), object(181)\n",
      "memory usage: 10.0+ MB\n",
      "\n",
      "DF INFO:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Columns: 219 entries, % Sector per Workforce to Job Description gensim_123grams_sent2vec_embeddings\n",
      "dtypes: float64(34), int64(4), object(181)\n",
      "memory usage: 10.0+ MB\n",
      "====================\n",
      "Warmth:\n",
      "--------------------\n",
      "Warmth Counts:\n",
      "0.000    4463\n",
      "1.000    1515\n",
      "Name: Warmth, dtype: int64\n",
      "--------------------\n",
      "Warmth Percentages:\n",
      "0.000   74.700\n",
      "1.000   25.300\n",
      "Name: Warmth, dtype: float64\n",
      "--------------------\n",
      "Warmth Mean: 0.25\n",
      "--------------------\n",
      "Warmth Standard Deviation: 0.44\n",
      "====================\n",
      "Competence:\n",
      "--------------------\n",
      "Competence Counts:\n",
      "0.000    3346\n",
      "1.000    2632\n",
      "Name: Competence, dtype: int64\n",
      "--------------------\n",
      "Competence Percentages:\n",
      "0.000   56.000\n",
      "1.000   44.000\n",
      "Name: Competence, dtype: float64\n",
      "--------------------\n",
      "Competence Mean: 0.44\n",
      "--------------------\n",
      "Competence Standard Deviation: 0.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data balance\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n",
    "cb_fr_hex = [\n",
    "    '#377eb8', '#ff7f00', '#4daf4a',\n",
    "    '#f781bf', '#a65628', '#984ea3',\n",
    "    '#999999', '#e41a1c', '#dede00'\n",
    "]\n",
    "color1 = cb_fr_hex[7]\n",
    "color2 = cb_fr_hex[0]\n",
    "color3 = cb_fr_hex[1]\n",
    "\n",
    "analysis_columns = [\n",
    "    'Warmth',\n",
    "    'Competence'\n",
    "]\n",
    "\n",
    "df_manual.info()\n",
    "\n",
    "df_gender_age_info(df_manual, ivs_all = analysis_columns)\n",
    "\n",
    "warm_and_comp_transposed = pd.concat(\n",
    "    [\n",
    "        df_manual['Warmth'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
    "        df_manual['Competence'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Training Dataset: Warmth and Competence Sentence Percentages')\n",
    "ax.set_xlabel('Manually Annotated Sentencence Warmth and Competence Percentage from Total')\n",
    "\n",
    "warm_and_comp_transposed.plot(\n",
    "    kind='barh', legend=True, stacked=True, ax=ax, color=[color1, color2],\n",
    ")\n",
    "ax.legend(['Absent', 'Present'])\n",
    "\n",
    "for i, c in enumerate(ax.containers):\n",
    "    labels = [f'{w:.1f}%' for v in c if (w := v.get_width())]\n",
    "    ax.bar_label(c, labels=labels, label_type='center', color= 'white')\n",
    "\n",
    "for save_format in ['eps', 'png']:\n",
    "    fig.savefig(f'{data_dir}/plots/Manual Warmth and Competence Sentences.{save_format}', format=save_format, dpi=3000, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f39dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
