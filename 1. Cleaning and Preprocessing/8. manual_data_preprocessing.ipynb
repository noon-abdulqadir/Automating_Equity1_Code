{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d9bbd5",
   "metadata": {},
   "source": [
    "# ATTN: This script uses Google translate to detect job description language. Google translate will limit requests and take a very long time. Only run this script if redoing language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e2d85",
   "metadata": {},
   "source": [
    "# Read from scrapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8129d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a74773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e9bad34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840ebcc",
   "metadata": {},
   "source": [
    "#### Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999bbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_paths = list(set(glob.glob(f'{scraped_data}Coding Material/*Folder/*/Job ID -*- Codebook (Automating Equity).xlsx')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ad7c36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 244 xlsx files\n",
    "len(glob_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdd650",
   "metadata": {},
   "source": [
    "#### Use paths to open files, fix keywords, and drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2edb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 s, sys: 161 ms, total: 5.66 s\n",
      "Wall time: 6.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fix list catches all incorrect/faculty keyword search terms\n",
    "fix_list = []\n",
    "\n",
    "# Appended data catches all the fixed and cleaned dfs\n",
    "appended_data = []\n",
    "\n",
    "for glob_path in glob_paths:\n",
    "\n",
    "    try:\n",
    "        df_temp = pd.read_excel(glob_path).reset_index(drop=True)\n",
    "    except ValueError:\n",
    "        fix_list.append(glob_path)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
    "        df_temp.drop(\n",
    "        df_temp.columns[\n",
    "                df_temp.columns.str.contains(\n",
    "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
    "                )\n",
    "            ],\n",
    "            axis='columns',\n",
    "            inplace=True,\n",
    "            errors='ignore',\n",
    "        )\n",
    "\n",
    "        appended_data.append(df_temp.reset_index(drop=True))\n",
    "\n",
    "# Concatonate list of dfs into one large df_manual\n",
    "df_manual = pd.concat(appended_data).reset_index(drop=True)\n",
    "\n",
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5de12303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
    "if len(fix_list) != 0:\n",
    "    print('Some keywords to fix!')\n",
    "    with open(f'{df_save_dir}fix_list.txt', 'w') as f:\n",
    "        json.dump(fix_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1a9362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of dfs, len = 244\n",
    "len(appended_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c1c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate list of dfs into one large df_manual\n",
    "df_manual = pd.concat(appended_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b014ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 12400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4606db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6e2dd",
   "metadata": {},
   "source": [
    "# Drop duplicated and missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc049d41",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda9efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46cdcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "477d7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a080272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e92eeb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 12400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c31c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12400 entries, 0 to 12399\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Job ID           12400 non-null  object \n",
      " 1   Sentence         12396 non-null  object \n",
      " 2   Warmth           12398 non-null  float64\n",
      " 3   Competence       12400 non-null  int64  \n",
      " 4   Task_Mentioned   12398 non-null  float64\n",
      " 5   Task_Warmth      12398 non-null  float64\n",
      " 6   Task_Competence  12398 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 678.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82e2a312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job ID', 'Sentence', 'Warmth', 'Competence', 'Task_Mentioned',\n",
       "       'Task_Warmth', 'Task_Competence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28d837cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns\n",
    "df_manual.columns = df_manual.columns.to_series().apply(lambda x: str(x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93c1a0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns 'Task_Mentioned', 'Task_Warmth', 'Task_Competence'\n",
    "df_manual.drop(\n",
    "    columns=['Task_Mentioned', 'Task_Warmth', 'Task_Competence'],\n",
    "    axis='columns',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c5a7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Warmth'] = df_manual['Warmth'].astype(np.float64)\n",
    "df_manual['Competence'] = df_manual['Competence'].astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7666d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12400 entries, 0 to 12399\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Job ID      12400 non-null  object \n",
      " 1   Sentence    12396 non-null  object \n",
      " 2   Warmth      12398 non-null  float64\n",
      " 3   Competence  12400 non-null  float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 387.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e57a9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Sentence to 'Job Description spacy_sentencized'\n",
    "df_manual.rename(\n",
    "    columns = {\n",
    "        'Sentence': 'Job Description spacy_sentencized'\n",
    "    },\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a32f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job ID', 'Job Description spacy_sentencized', 'Warmth', 'Competence'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fb59e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df_manual.dropna(axis='index', how='all', inplace=True)\n",
    "df_manual.dropna(axis='columns', how='all', inplace=True)\n",
    "df_manual.dropna(\n",
    "    subset = ['Job Description spacy_sentencized', 'Warmth', 'Competence'],\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25218a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 12394\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "025e7b62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job ID\n",
       "2466455525                      [2466455525]\n",
       "3768944208                      [3768944208]\n",
       "4023920432                      [4023920432]\n",
       "4039450758                      [4039450758]\n",
       "4040119601                      [4040119601]\n",
       "                               ...          \n",
       "pj_68fdfd2b272e0e56    [pj_68fdfd2b272e0e56]\n",
       "pj_8a0cc9d327b77ff2    [pj_8a0cc9d327b77ff2]\n",
       "pj_a4ac3e531abef752    [pj_a4ac3e531abef752]\n",
       "pj_b62dd960c26ce093    [pj_b62dd960c26ce093]\n",
       "pj_e2cc3db57891f5d9    [pj_e2cc3db57891f5d9]\n",
       "Name: Job ID, Length: 133, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 133\n",
    "df_manual.groupby(['Job ID'])['Job ID'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3126f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates on subset of 'Job ID' and 'Sentence'\n",
    "df_manual.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'], keep='first', ignore_index=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67baaa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ec0bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing 'Job ID'\n",
    "df_manual.drop(\n",
    "    df_manual[\n",
    "        (df_manual['Job ID'].isin(nan_list)) | \n",
    "        (df_manual['Job ID'].isnull()) | \n",
    "        (df_manual['Job ID'].isna())\n",
    "    ].index, \n",
    "    axis='index',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a098a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19e08303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_manual to file\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_dropped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae0b99",
   "metadata": {},
   "source": [
    "# Add English and Dutch language requirement columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb838f06",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW_DROPPED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "489a38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72ff9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bef85d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a13ef5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b5aa5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6400\n",
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2e2a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6400 entries, 0 to 6399\n",
      "Data columns (total 4 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Job ID                             6400 non-null   object \n",
      " 1   Job Description spacy_sentencized  6400 non-null   object \n",
      " 2   Warmth                             6400 non-null   float64\n",
      " 3   Competence                         6400 non-null   float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 200.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a77db8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add language requirement column\n",
    "# Use regex to find language requirement\n",
    "dutch_requirement_pattern = r'[Ll]anguage: [Dd]utch|[Dd]utch [Pp]referred|[Dd]utch [Re]quired|[Dd]utch [Ll]anguage|[Pp]roficient in [Dd]utch|[Ss]peak [Dd]utch|[Kk]now [Dd]utch'\n",
    "english_requirement_pattern = r'[Ll]anguage: [Ee]nglish|[Ee]nglish [Pp]referred|[Ee]nglish [Re]quired|[Ee]nglish [Ll]anguage|[Pp]roficient in [Ee]nglish|[Ss]peak [Ee]nglish|[Kk]now [Ee]nglish'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8418d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language requirements\n",
    "# Dutch\n",
    "if 'Dutch Requirement' in df_manual.columns:\n",
    "    df_manual.drop(columns=['Dutch Requirement'], inplace=True)\n",
    "df_manual['Dutch Requirement'] = np.where(\n",
    "    df_manual['Job Description spacy_sentencized'].str.contains(dutch_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")\n",
    "\n",
    "# English\n",
    "if 'English Requirement' in df_manual.columns:\n",
    "    df_manual.drop(columns=['English Requirement'], inplace=True)\n",
    "df_manual['English Requirement'] = np.where(\n",
    "    df_manual['Job Description spacy_sentencized'].str.contains(english_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_english_requirement.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33b1e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     6393\n",
       "Yes       7\n",
       "Name: Dutch Requirement, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yes = 235\n",
    "df_manual['Dutch Requirement'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6330c696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     6392\n",
       "Yes       8\n",
       "Name: English Requirement, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yes = 526\n",
    "df_manual['English Requirement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c3ddedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_raw_language_requirement.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cea97",
   "metadata": {},
   "source": [
    "# Add data from Sectors dataframe (see CBS directory under scrapped_data directory) and Categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69513116",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_RAW_LANGUAGE_REQUIREMENT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10b32849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2b9a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ff55233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6873d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to print df gender and age info\n",
    "def df_gender_age_info(\n",
    "    df,\n",
    "    ivs_all = [\n",
    "        'Gender',\n",
    "        'Gender_Num',\n",
    "        'Gender_Female',\n",
    "        'Gender_Mixed',\n",
    "        'Gender_Male',\n",
    "        'Age',\n",
    "        'Age_Num',\n",
    "        'Age_Older',\n",
    "        'Age_Mixed',\n",
    "        'Age_Younger',\n",
    "    ],\n",
    "):\n",
    "    # Print Info\n",
    "    print('\\nDF INFO:\\n')\n",
    "    df.info()\n",
    "\n",
    "    for iv in ivs_all:\n",
    "        try:\n",
    "            counts = df[f\"{iv}\"].value_counts()\n",
    "            percentages = df[f\"{iv}\"].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
    "            print('='*20)\n",
    "            print(f'{iv}:')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Counts:\\n{counts}')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Percentages:\\n{percentages}')\n",
    "\n",
    "            try:\n",
    "                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
    "                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Mean: {mean}')\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Standard Deviation: {sd}')\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            print(f'{iv} not available.')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79fe759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40156303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6400 entries, 0 to 6399\n",
      "Data columns (total 6 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Job ID                             6400 non-null   object \n",
      " 1   Job Description spacy_sentencized  6400 non-null   object \n",
      " 2   Warmth                             6400 non-null   float64\n",
      " 3   Competence                         6400 non-null   float64\n",
      " 4   Dutch Requirement                  6400 non-null   object \n",
      " 5   English Requirement                6400 non-null   object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 300.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db503aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job ID'] = df_manual['Job ID'].apply(lambda x: str(x).lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98694f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_including_sector_genage_data.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d5116df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17599 entries, 0 to 17598\n",
      "Data columns (total 56 columns):\n",
      " #   Column                                          Non-Null Count  Dtype   \n",
      "---  ------                                          --------------  -----   \n",
      " 0   Search Keyword                                  17599 non-null  object  \n",
      " 1   Platform                                        17599 non-null  object  \n",
      " 2   Job ID                                          17599 non-null  object  \n",
      " 3   Job Title                                       17599 non-null  object  \n",
      " 4   Company Name                                    17597 non-null  object  \n",
      " 5   Location                                        17599 non-null  object  \n",
      " 6   Job Description                                 17599 non-null  object  \n",
      " 7   Rating                                          3780 non-null   float64 \n",
      " 8   Employment Type                                 17017 non-null  object  \n",
      " 9   Company URL                                     15959 non-null  object  \n",
      " 10  Job URL                                         17599 non-null  object  \n",
      " 11  Job Age                                         17599 non-null  object  \n",
      " 12  Job Age Number                                  17599 non-null  object  \n",
      " 13  Collection Date                                 17599 non-null  object  \n",
      " 14  Data Row                                        13816 non-null  float64 \n",
      " 15  Tracking ID                                     13816 non-null  object  \n",
      " 16  Industry                                        14401 non-null  object  \n",
      " 17  Job Date                                        13819 non-null  object  \n",
      " 18  Type of ownership                               582 non-null    object  \n",
      " 19  Language                                        17599 non-null  object  \n",
      " 20  Dutch Requirement                               17599 non-null  object  \n",
      " 21  English Requirement                             17599 non-null  object  \n",
      " 22  Sector Code                                     17599 non-null  object  \n",
      " 23  Sector                                          17599 non-null  object  \n",
      " 24  Keywords Count                                  17599 non-null  float64 \n",
      " 25  % per Sector                                    17599 non-null  float64 \n",
      " 26  % per Social Category                           17599 non-null  float64 \n",
      " 27  % per Workforce                                 17599 non-null  float64 \n",
      " 28  Female Count (x 1000)                           17599 non-null  float64 \n",
      " 29  Gender_Female_% per Sector                      17599 non-null  float64 \n",
      " 30  Gender_Female_% per Social Category             17599 non-null  float64 \n",
      " 31  Gender_Female_% per Workforce                   17599 non-null  float64 \n",
      " 32  Male Count (x 1000)                             17599 non-null  float64 \n",
      " 33  Gender_Male_% per Sector                        17599 non-null  float64 \n",
      " 34  Gender_Male_% per Social Category               17599 non-null  float64 \n",
      " 35  Gender_Male_% per Workforce                     17599 non-null  float64 \n",
      " 36  Gender                                          17599 non-null  category\n",
      " 37  Age_Older (>= 45 years)_n                       17599 non-null  float64 \n",
      " 38  Age_Older (>= 45 years)_% per Sector            17599 non-null  float64 \n",
      " 39  Age_Older (>= 45 years)_% per Social Category   17599 non-null  float64 \n",
      " 40  Age_Older (>= 45 years)_% per Workforce         17599 non-null  float64 \n",
      " 41  Age_Younger (< 45 years)_n                      17599 non-null  float64 \n",
      " 42  Age_Younger (< 45 years)_% per Sector           17599 non-null  float64 \n",
      " 43  Age_Younger (< 45 years)_% per Social Category  17599 non-null  float64 \n",
      " 44  Age_Younger (< 45 years)_% per Workforce        17599 non-null  float64 \n",
      " 45  Age                                             17599 non-null  category\n",
      " 46  Sector Count (x 1000)                           17599 non-null  float64 \n",
      " 47  % Sector per Workforce                          17599 non-null  float64 \n",
      " 48  Gender_Female                                   17599 non-null  float64 \n",
      " 49  Gender_Mixed                                    17599 non-null  float64 \n",
      " 50  Gender_Male                                     17599 non-null  float64 \n",
      " 51  Age_Older                                       17599 non-null  float64 \n",
      " 52  Age_Mixed                                       17599 non-null  float64 \n",
      " 53  Age_Younger                                     17599 non-null  float64 \n",
      " 54  Gender_Num                                      17599 non-null  float64 \n",
      " 55  Age_Num                                         17599 non-null  float64 \n",
      "dtypes: category(2), float64(32), object(22)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49391453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job ID'] = df_jobs['Job ID'].apply(lambda x: str(x).lower().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5aa6dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name',\n",
       "       'Location', 'Job Description', 'Rating', 'Employment Type',\n",
       "       'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
       "       'Collection Date', 'Data Row', 'Tracking ID', 'Industry', 'Job Date',\n",
       "       'Type of ownership', 'Language', 'Dutch Requirement',\n",
       "       'English Requirement', 'Sector Code', 'Sector', 'Keywords Count',\n",
       "       '% per Sector', '% per Social Category', '% per Workforce',\n",
       "       'Female Count (x 1000)', 'Gender_Female_% per Sector',\n",
       "       'Gender_Female_% per Social Category', 'Gender_Female_% per Workforce',\n",
       "       'Male Count (x 1000)', 'Gender_Male_% per Sector',\n",
       "       'Gender_Male_% per Social Category', 'Gender_Male_% per Workforce',\n",
       "       'Gender', 'Age_Older (>= 45 years)_n',\n",
       "       'Age_Older (>= 45 years)_% per Sector',\n",
       "       'Age_Older (>= 45 years)_% per Social Category',\n",
       "       'Age_Older (>= 45 years)_% per Workforce', 'Age_Younger (< 45 years)_n',\n",
       "       'Age_Younger (< 45 years)_% per Sector',\n",
       "       'Age_Younger (< 45 years)_% per Social Category',\n",
       "       'Age_Younger (< 45 years)_% per Workforce', 'Age',\n",
       "       'Sector Count (x 1000)', '% Sector per Workforce', 'Gender_Female',\n",
       "       'Gender_Mixed', 'Gender_Male', 'Age_Older', 'Age_Mixed', 'Age_Younger',\n",
       "       'Gender_Num', 'Age_Num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "82178d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.drop(\n",
    "    columns = [\n",
    "        'Job Description', 'Rating', 'Employment Type',\n",
    "        'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
    "        'Collection Date', 'Data Row', 'Tracking ID', 'Job Date',\n",
    "        'Type of ownership', 'Language', 'Dutch Requirement', 'English Requirement', \n",
    "    ],\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9bba117b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name',\n",
       "       'Location', 'Industry', 'Sector Code', 'Sector', 'Keywords Count',\n",
       "       '% per Sector', '% per Social Category', '% per Workforce',\n",
       "       'Female Count (x 1000)', 'Gender_Female_% per Sector',\n",
       "       'Gender_Female_% per Social Category', 'Gender_Female_% per Workforce',\n",
       "       'Male Count (x 1000)', 'Gender_Male_% per Sector',\n",
       "       'Gender_Male_% per Social Category', 'Gender_Male_% per Workforce',\n",
       "       'Gender', 'Age_Older (>= 45 years)_n',\n",
       "       'Age_Older (>= 45 years)_% per Sector',\n",
       "       'Age_Older (>= 45 years)_% per Social Category',\n",
       "       'Age_Older (>= 45 years)_% per Workforce', 'Age_Younger (< 45 years)_n',\n",
       "       'Age_Younger (< 45 years)_% per Sector',\n",
       "       'Age_Younger (< 45 years)_% per Social Category',\n",
       "       'Age_Younger (< 45 years)_% per Workforce', 'Age',\n",
       "       'Sector Count (x 1000)', '% Sector per Workforce', 'Gender_Female',\n",
       "       'Gender_Mixed', 'Gender_Male', 'Age_Older', 'Age_Mixed', 'Age_Younger',\n",
       "       'Gender_Num', 'Age_Num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f11e0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sector and categorical data from df_jobs\n",
    "df_manual = df_manual.merge(df_jobs, on='Job ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47f24060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5978"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4fa2c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5978 entries, 0 to 5977\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                          Non-Null Count  Dtype   \n",
      "---  ------                                          --------------  -----   \n",
      " 0   Job ID                                          5978 non-null   object  \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object  \n",
      " 2   Warmth                                          5978 non-null   float64 \n",
      " 3   Competence                                      5978 non-null   float64 \n",
      " 4   Dutch Requirement                               5978 non-null   object  \n",
      " 5   English Requirement                             5978 non-null   object  \n",
      " 6   Search Keyword                                  5978 non-null   object  \n",
      " 7   Platform                                        5978 non-null   object  \n",
      " 8   Job Title                                       5978 non-null   object  \n",
      " 9   Company Name                                    5978 non-null   object  \n",
      " 10  Location                                        5978 non-null   object  \n",
      " 11  Industry                                        291 non-null    object  \n",
      " 12  Sector Code                                     5978 non-null   object  \n",
      " 13  Sector                                          5978 non-null   object  \n",
      " 14  Keywords Count                                  5978 non-null   float64 \n",
      " 15  % per Sector                                    5978 non-null   float64 \n",
      " 16  % per Social Category                           5978 non-null   float64 \n",
      " 17  % per Workforce                                 5978 non-null   float64 \n",
      " 18  Female Count (x 1000)                           5978 non-null   float64 \n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64 \n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64 \n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64 \n",
      " 22  Male Count (x 1000)                             5978 non-null   float64 \n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64 \n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64 \n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64 \n",
      " 26  Gender                                          5978 non-null   category\n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64 \n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64 \n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64 \n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64 \n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64 \n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64 \n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64 \n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64 \n",
      " 35  Age                                             5978 non-null   category\n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64 \n",
      " 37  % Sector per Workforce                          5978 non-null   float64 \n",
      " 38  Gender_Female                                   5978 non-null   float64 \n",
      " 39  Gender_Mixed                                    5978 non-null   float64 \n",
      " 40  Gender_Male                                     5978 non-null   float64 \n",
      " 41  Age_Older                                       5978 non-null   float64 \n",
      " 42  Age_Mixed                                       5978 non-null   float64 \n",
      " 43  Age_Younger                                     5978 non-null   float64 \n",
      " 44  Gender_Num                                      5978 non-null   float64 \n",
      " 45  Age_Num                                         5978 non-null   float64 \n",
      "dtypes: category(2), float64(32), object(12)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40fa3bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job ID</th>\n",
       "      <th>Job Description spacy_sentencized</th>\n",
       "      <th>Warmth</th>\n",
       "      <th>Competence</th>\n",
       "      <th>Dutch Requirement</th>\n",
       "      <th>English Requirement</th>\n",
       "      <th>Search Keyword</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Sector Count (x 1000)</th>\n",
       "      <th>% Sector per Workforce</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Mixed</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Age_Older</th>\n",
       "      <th>Age_Mixed</th>\n",
       "      <th>Age_Younger</th>\n",
       "      <th>Gender_Num</th>\n",
       "      <th>Age_Num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pj_4538c73030ddf6e8</td>\n",
       "      <td>Do you want to start your HR career at one of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>electricity</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Associate HR administrator</td>\n",
       "      <td>Adecco</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pj_4538c73030ddf6e8</td>\n",
       "      <td>Are you the one that likes to work in a challe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>electricity</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Associate HR administrator</td>\n",
       "      <td>Adecco</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pj_4538c73030ddf6e8</td>\n",
       "      <td>Than Tesla is looking for you!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>electricity</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Associate HR administrator</td>\n",
       "      <td>Adecco</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pj_4538c73030ddf6e8</td>\n",
       "      <td>We are looking for an associate HR administrator.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>electricity</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Associate HR administrator</td>\n",
       "      <td>Adecco</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pj_4538c73030ddf6e8</td>\n",
       "      <td>What are you going to do?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>electricity</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Associate HR administrator</td>\n",
       "      <td>Adecco</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Job ID                  Job Description spacy_sentencized  \\\n",
       "0  pj_4538c73030ddf6e8  Do you want to start your HR career at one of ...   \n",
       "1  pj_4538c73030ddf6e8  Are you the one that likes to work in a challe...   \n",
       "2  pj_4538c73030ddf6e8                     Than Tesla is looking for you!   \n",
       "3  pj_4538c73030ddf6e8  We are looking for an associate HR administrator.   \n",
       "4  pj_4538c73030ddf6e8                          What are you going to do?   \n",
       "\n",
       "   Warmth  Competence Dutch Requirement English Requirement Search Keyword  \\\n",
       "0     0.0         1.0                No                  No    electricity   \n",
       "1     0.0         1.0                No                  No    electricity   \n",
       "2     0.0         0.0                No                  No    electricity   \n",
       "3     0.0         0.0                No                  No    electricity   \n",
       "4     0.0         0.0                No                  No    electricity   \n",
       "\n",
       "  Platform                   Job Title Company Name  ...  \\\n",
       "0   Indeed  Associate HR administrator       Adecco  ...   \n",
       "1   Indeed  Associate HR administrator       Adecco  ...   \n",
       "2   Indeed  Associate HR administrator       Adecco  ...   \n",
       "3   Indeed  Associate HR administrator       Adecco  ...   \n",
       "4   Indeed  Associate HR administrator       Adecco  ...   \n",
       "\n",
       "  Sector Count (x 1000) % Sector per Workforce Gender_Female Gender_Mixed  \\\n",
       "0                  29.0               0.001148           0.0          1.0   \n",
       "1                  29.0               0.001148           0.0          1.0   \n",
       "2                  29.0               0.001148           0.0          1.0   \n",
       "3                  29.0               0.001148           0.0          1.0   \n",
       "4                  29.0               0.001148           0.0          1.0   \n",
       "\n",
       "   Gender_Male  Age_Older  Age_Mixed  Age_Younger  Gender_Num  Age_Num  \n",
       "0          0.0        0.0        1.0          0.0         2.0      2.0  \n",
       "1          0.0        0.0        1.0          0.0         2.0      2.0  \n",
       "2          0.0        0.0        1.0          0.0         2.0      2.0  \n",
       "3          0.0        0.0        1.0          0.0         2.0      2.0  \n",
       "4          0.0        0.0        1.0          0.0         2.0      2.0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d889ec9",
   "metadata": {},
   "source": [
    "#### Check if there is any missing sector data in the merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "43ddadb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Sector'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb5b9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_manual['Sector'].isna().sum() != 0:\n",
    "    print('Some search keywords did not match a sector. Fixing')\n",
    "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
    "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n",
    "    df_manual = fix_keywords(df_manual)\n",
    "    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
    "    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0d24ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DF INFO:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 117 entries, 3768944208 to pj_a4ac3e531abef752\n",
      "Data columns (total 45 columns):\n",
      " #   Column                                          Non-Null Count  Dtype   \n",
      "---  ------                                          --------------  -----   \n",
      " 0   Job Description spacy_sentencized               117 non-null    object  \n",
      " 1   Warmth                                          117 non-null    float64 \n",
      " 2   Competence                                      117 non-null    float64 \n",
      " 3   Dutch Requirement                               117 non-null    object  \n",
      " 4   English Requirement                             117 non-null    object  \n",
      " 5   Search Keyword                                  117 non-null    object  \n",
      " 6   Platform                                        117 non-null    object  \n",
      " 7   Job Title                                       117 non-null    object  \n",
      " 8   Company Name                                    117 non-null    object  \n",
      " 9   Location                                        117 non-null    object  \n",
      " 10  Industry                                        5 non-null      object  \n",
      " 11  Sector Code                                     117 non-null    object  \n",
      " 12  Sector                                          117 non-null    object  \n",
      " 13  Keywords Count                                  117 non-null    float64 \n",
      " 14  % per Sector                                    117 non-null    float64 \n",
      " 15  % per Social Category                           117 non-null    float64 \n",
      " 16  % per Workforce                                 117 non-null    float64 \n",
      " 17  Female Count (x 1000)                           117 non-null    float64 \n",
      " 18  Gender_Female_% per Sector                      117 non-null    float64 \n",
      " 19  Gender_Female_% per Social Category             117 non-null    float64 \n",
      " 20  Gender_Female_% per Workforce                   117 non-null    float64 \n",
      " 21  Male Count (x 1000)                             117 non-null    float64 \n",
      " 22  Gender_Male_% per Sector                        117 non-null    float64 \n",
      " 23  Gender_Male_% per Social Category               117 non-null    float64 \n",
      " 24  Gender_Male_% per Workforce                     117 non-null    float64 \n",
      " 25  Gender                                          117 non-null    category\n",
      " 26  Age_Older (>= 45 years)_n                       117 non-null    float64 \n",
      " 27  Age_Older (>= 45 years)_% per Sector            117 non-null    float64 \n",
      " 28  Age_Older (>= 45 years)_% per Social Category   117 non-null    float64 \n",
      " 29  Age_Older (>= 45 years)_% per Workforce         117 non-null    float64 \n",
      " 30  Age_Younger (< 45 years)_n                      117 non-null    float64 \n",
      " 31  Age_Younger (< 45 years)_% per Sector           117 non-null    float64 \n",
      " 32  Age_Younger (< 45 years)_% per Social Category  117 non-null    float64 \n",
      " 33  Age_Younger (< 45 years)_% per Workforce        117 non-null    float64 \n",
      " 34  Age                                             117 non-null    category\n",
      " 35  Sector Count (x 1000)                           117 non-null    float64 \n",
      " 36  % Sector per Workforce                          117 non-null    float64 \n",
      " 37  Gender_Female                                   117 non-null    float64 \n",
      " 38  Gender_Mixed                                    117 non-null    float64 \n",
      " 39  Gender_Male                                     117 non-null    float64 \n",
      " 40  Age_Older                                       117 non-null    float64 \n",
      " 41  Age_Mixed                                       117 non-null    float64 \n",
      " 42  Age_Younger                                     117 non-null    float64 \n",
      " 43  Gender_Num                                      117 non-null    float64 \n",
      " 44  Age_Num                                         117 non-null    float64 \n",
      "dtypes: category(2), float64(32), object(11)\n",
      "memory usage: 40.7+ KB\n",
      "====================\n",
      "Gender:\n",
      "--------------------\n",
      "Gender Counts:\n",
      "Mixed Gender    89\n",
      "Male            16\n",
      "Female          12\n",
      "Name: Gender, dtype: int64\n",
      "--------------------\n",
      "Gender Percentages:\n",
      "Mixed Gender    76.1\n",
      "Male            13.7\n",
      "Female          10.3\n",
      "Name: Gender, dtype: float64\n",
      "====================\n",
      "Gender_Num:\n",
      "--------------------\n",
      "Gender_Num Counts:\n",
      "2.0    89\n",
      "3.0    16\n",
      "1.0    12\n",
      "Name: Gender_Num, dtype: int64\n",
      "--------------------\n",
      "Gender_Num Percentages:\n",
      "2.0    76.1\n",
      "3.0    13.7\n",
      "1.0    10.3\n",
      "Name: Gender_Num, dtype: float64\n",
      "--------------------\n",
      "Gender_Num Mean: 2.03\n",
      "--------------------\n",
      "Gender_Num Standard Deviation: 0.49\n",
      "====================\n",
      "Gender_Female:\n",
      "--------------------\n",
      "Gender_Female Counts:\n",
      "0.0    105\n",
      "1.0     12\n",
      "Name: Gender_Female, dtype: int64\n",
      "--------------------\n",
      "Gender_Female Percentages:\n",
      "0.0    89.7\n",
      "1.0    10.3\n",
      "Name: Gender_Female, dtype: float64\n",
      "--------------------\n",
      "Gender_Female Mean: 0.1\n",
      "--------------------\n",
      "Gender_Female Standard Deviation: 0.3\n",
      "====================\n",
      "Gender_Mixed:\n",
      "--------------------\n",
      "Gender_Mixed Counts:\n",
      "1.0    89\n",
      "0.0    28\n",
      "Name: Gender_Mixed, dtype: int64\n",
      "--------------------\n",
      "Gender_Mixed Percentages:\n",
      "1.0    76.1\n",
      "0.0    23.9\n",
      "Name: Gender_Mixed, dtype: float64\n",
      "--------------------\n",
      "Gender_Mixed Mean: 0.76\n",
      "--------------------\n",
      "Gender_Mixed Standard Deviation: 0.43\n",
      "====================\n",
      "Gender_Male:\n",
      "--------------------\n",
      "Gender_Male Counts:\n",
      "0.0    101\n",
      "1.0     16\n",
      "Name: Gender_Male, dtype: int64\n",
      "--------------------\n",
      "Gender_Male Percentages:\n",
      "0.0    86.3\n",
      "1.0    13.7\n",
      "Name: Gender_Male, dtype: float64\n",
      "--------------------\n",
      "Gender_Male Mean: 0.14\n",
      "--------------------\n",
      "Gender_Male Standard Deviation: 0.35\n",
      "====================\n",
      "Age:\n",
      "--------------------\n",
      "Age Counts:\n",
      "Younger      60\n",
      "Mixed Age    42\n",
      "Older        15\n",
      "Name: Age, dtype: int64\n",
      "--------------------\n",
      "Age Percentages:\n",
      "Younger      51.3\n",
      "Mixed Age    35.9\n",
      "Older        12.8\n",
      "Name: Age, dtype: float64\n",
      "====================\n",
      "Age_Num:\n",
      "--------------------\n",
      "Age_Num Counts:\n",
      "3.0    60\n",
      "2.0    42\n",
      "1.0    15\n",
      "Name: Age_Num, dtype: int64\n",
      "--------------------\n",
      "Age_Num Percentages:\n",
      "3.0    51.3\n",
      "2.0    35.9\n",
      "1.0    12.8\n",
      "Name: Age_Num, dtype: float64\n",
      "--------------------\n",
      "Age_Num Mean: 2.38\n",
      "--------------------\n",
      "Age_Num Standard Deviation: 0.71\n",
      "====================\n",
      "Age_Older:\n",
      "--------------------\n",
      "Age_Older Counts:\n",
      "0.0    102\n",
      "1.0     15\n",
      "Name: Age_Older, dtype: int64\n",
      "--------------------\n",
      "Age_Older Percentages:\n",
      "0.0    87.2\n",
      "1.0    12.8\n",
      "Name: Age_Older, dtype: float64\n",
      "--------------------\n",
      "Age_Older Mean: 0.13\n",
      "--------------------\n",
      "Age_Older Standard Deviation: 0.34\n",
      "====================\n",
      "Age_Mixed:\n",
      "--------------------\n",
      "Age_Mixed Counts:\n",
      "0.0    75\n",
      "1.0    42\n",
      "Name: Age_Mixed, dtype: int64\n",
      "--------------------\n",
      "Age_Mixed Percentages:\n",
      "0.0    64.1\n",
      "1.0    35.9\n",
      "Name: Age_Mixed, dtype: float64\n",
      "--------------------\n",
      "Age_Mixed Mean: 0.36\n",
      "--------------------\n",
      "Age_Mixed Standard Deviation: 0.48\n",
      "====================\n",
      "Age_Younger:\n",
      "--------------------\n",
      "Age_Younger Counts:\n",
      "1.0    60\n",
      "0.0    57\n",
      "Name: Age_Younger, dtype: int64\n",
      "--------------------\n",
      "Age_Younger Percentages:\n",
      "1.0    51.3\n",
      "0.0    48.7\n",
      "Name: Age_Younger, dtype: float64\n",
      "--------------------\n",
      "Age_Younger Mean: 0.51\n",
      "--------------------\n",
      "Age_Younger Standard Deviation: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Job Ad info\n",
    "df_gender_age_info(df_manual.groupby(['Job ID']).first())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4d65547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DF INFO:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5978 entries, 0 to 5977\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                          Non-Null Count  Dtype   \n",
      "---  ------                                          --------------  -----   \n",
      " 0   Job ID                                          5978 non-null   object  \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object  \n",
      " 2   Warmth                                          5978 non-null   float64 \n",
      " 3   Competence                                      5978 non-null   float64 \n",
      " 4   Dutch Requirement                               5978 non-null   object  \n",
      " 5   English Requirement                             5978 non-null   object  \n",
      " 6   Search Keyword                                  5978 non-null   object  \n",
      " 7   Platform                                        5978 non-null   object  \n",
      " 8   Job Title                                       5978 non-null   object  \n",
      " 9   Company Name                                    5978 non-null   object  \n",
      " 10  Location                                        5978 non-null   object  \n",
      " 11  Industry                                        291 non-null    object  \n",
      " 12  Sector Code                                     5978 non-null   object  \n",
      " 13  Sector                                          5978 non-null   object  \n",
      " 14  Keywords Count                                  5978 non-null   float64 \n",
      " 15  % per Sector                                    5978 non-null   float64 \n",
      " 16  % per Social Category                           5978 non-null   float64 \n",
      " 17  % per Workforce                                 5978 non-null   float64 \n",
      " 18  Female Count (x 1000)                           5978 non-null   float64 \n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64 \n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64 \n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64 \n",
      " 22  Male Count (x 1000)                             5978 non-null   float64 \n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64 \n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64 \n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64 \n",
      " 26  Gender                                          5978 non-null   category\n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64 \n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64 \n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64 \n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64 \n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64 \n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64 \n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64 \n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64 \n",
      " 35  Age                                             5978 non-null   category\n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64 \n",
      " 37  % Sector per Workforce                          5978 non-null   float64 \n",
      " 38  Gender_Female                                   5978 non-null   float64 \n",
      " 39  Gender_Mixed                                    5978 non-null   float64 \n",
      " 40  Gender_Male                                     5978 non-null   float64 \n",
      " 41  Age_Older                                       5978 non-null   float64 \n",
      " 42  Age_Mixed                                       5978 non-null   float64 \n",
      " 43  Age_Younger                                     5978 non-null   float64 \n",
      " 44  Gender_Num                                      5978 non-null   float64 \n",
      " 45  Age_Num                                         5978 non-null   float64 \n",
      "dtypes: category(2), float64(32), object(12)\n",
      "memory usage: 2.1+ MB\n",
      "====================\n",
      "Gender:\n",
      "--------------------\n",
      "Gender Counts:\n",
      "Mixed Gender    4682\n",
      "Male             664\n",
      "Female           632\n",
      "Name: Gender, dtype: int64\n",
      "--------------------\n",
      "Gender Percentages:\n",
      "Mixed Gender    78.3\n",
      "Male            11.1\n",
      "Female          10.6\n",
      "Name: Gender, dtype: float64\n",
      "====================\n",
      "Gender_Num:\n",
      "--------------------\n",
      "Gender_Num Counts:\n",
      "2.0    4682\n",
      "3.0     664\n",
      "1.0     632\n",
      "Name: Gender_Num, dtype: int64\n",
      "--------------------\n",
      "Gender_Num Percentages:\n",
      "2.0    78.3\n",
      "3.0    11.1\n",
      "1.0    10.6\n",
      "Name: Gender_Num, dtype: float64\n",
      "--------------------\n",
      "Gender_Num Mean: 2.01\n",
      "--------------------\n",
      "Gender_Num Standard Deviation: 0.47\n",
      "====================\n",
      "Gender_Female:\n",
      "--------------------\n",
      "Gender_Female Counts:\n",
      "0.0    5346\n",
      "1.0     632\n",
      "Name: Gender_Female, dtype: int64\n",
      "--------------------\n",
      "Gender_Female Percentages:\n",
      "0.0    89.4\n",
      "1.0    10.6\n",
      "Name: Gender_Female, dtype: float64\n",
      "--------------------\n",
      "Gender_Female Mean: 0.11\n",
      "--------------------\n",
      "Gender_Female Standard Deviation: 0.31\n",
      "====================\n",
      "Gender_Mixed:\n",
      "--------------------\n",
      "Gender_Mixed Counts:\n",
      "1.0    4682\n",
      "0.0    1296\n",
      "Name: Gender_Mixed, dtype: int64\n",
      "--------------------\n",
      "Gender_Mixed Percentages:\n",
      "1.0    78.3\n",
      "0.0    21.7\n",
      "Name: Gender_Mixed, dtype: float64\n",
      "--------------------\n",
      "Gender_Mixed Mean: 0.78\n",
      "--------------------\n",
      "Gender_Mixed Standard Deviation: 0.41\n",
      "====================\n",
      "Gender_Male:\n",
      "--------------------\n",
      "Gender_Male Counts:\n",
      "0.0    5314\n",
      "1.0     664\n",
      "Name: Gender_Male, dtype: int64\n",
      "--------------------\n",
      "Gender_Male Percentages:\n",
      "0.0    88.9\n",
      "1.0    11.1\n",
      "Name: Gender_Male, dtype: float64\n",
      "--------------------\n",
      "Gender_Male Mean: 0.11\n",
      "--------------------\n",
      "Gender_Male Standard Deviation: 0.31\n",
      "====================\n",
      "Age:\n",
      "--------------------\n",
      "Age Counts:\n",
      "Younger      3237\n",
      "Mixed Age    2059\n",
      "Older         682\n",
      "Name: Age, dtype: int64\n",
      "--------------------\n",
      "Age Percentages:\n",
      "Younger      54.1\n",
      "Mixed Age    34.4\n",
      "Older        11.4\n",
      "Name: Age, dtype: float64\n",
      "====================\n",
      "Age_Num:\n",
      "--------------------\n",
      "Age_Num Counts:\n",
      "3.0    3237\n",
      "2.0    2059\n",
      "1.0     682\n",
      "Name: Age_Num, dtype: int64\n",
      "--------------------\n",
      "Age_Num Percentages:\n",
      "3.0    54.1\n",
      "2.0    34.4\n",
      "1.0    11.4\n",
      "Name: Age_Num, dtype: float64\n",
      "--------------------\n",
      "Age_Num Mean: 2.43\n",
      "--------------------\n",
      "Age_Num Standard Deviation: 0.69\n",
      "====================\n",
      "Age_Older:\n",
      "--------------------\n",
      "Age_Older Counts:\n",
      "0.0    5296\n",
      "1.0     682\n",
      "Name: Age_Older, dtype: int64\n",
      "--------------------\n",
      "Age_Older Percentages:\n",
      "0.0    88.6\n",
      "1.0    11.4\n",
      "Name: Age_Older, dtype: float64\n",
      "--------------------\n",
      "Age_Older Mean: 0.11\n",
      "--------------------\n",
      "Age_Older Standard Deviation: 0.32\n",
      "====================\n",
      "Age_Mixed:\n",
      "--------------------\n",
      "Age_Mixed Counts:\n",
      "0.0    3919\n",
      "1.0    2059\n",
      "Name: Age_Mixed, dtype: int64\n",
      "--------------------\n",
      "Age_Mixed Percentages:\n",
      "0.0    65.6\n",
      "1.0    34.4\n",
      "Name: Age_Mixed, dtype: float64\n",
      "--------------------\n",
      "Age_Mixed Mean: 0.34\n",
      "--------------------\n",
      "Age_Mixed Standard Deviation: 0.48\n",
      "====================\n",
      "Age_Younger:\n",
      "--------------------\n",
      "Age_Younger Counts:\n",
      "1.0    3237\n",
      "0.0    2741\n",
      "Name: Age_Younger, dtype: int64\n",
      "--------------------\n",
      "Age_Younger Percentages:\n",
      "1.0    54.1\n",
      "0.0    45.9\n",
      "Name: Age_Younger, dtype: float64\n",
      "--------------------\n",
      "Age_Younger Mean: 0.54\n",
      "--------------------\n",
      "Age_Younger Standard Deviation: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Job Sentence info\n",
    "df_gender_age_info(df_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1d8a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DF INFO:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5978 entries, 0 to 5977\n",
      "Data columns (total 46 columns):\n",
      " #   Column                                          Non-Null Count  Dtype   \n",
      "---  ------                                          --------------  -----   \n",
      " 0   Job ID                                          5978 non-null   object  \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object  \n",
      " 2   Warmth                                          5978 non-null   float64 \n",
      " 3   Competence                                      5978 non-null   float64 \n",
      " 4   Dutch Requirement                               5978 non-null   object  \n",
      " 5   English Requirement                             5978 non-null   object  \n",
      " 6   Search Keyword                                  5978 non-null   object  \n",
      " 7   Platform                                        5978 non-null   object  \n",
      " 8   Job Title                                       5978 non-null   object  \n",
      " 9   Company Name                                    5978 non-null   object  \n",
      " 10  Location                                        5978 non-null   object  \n",
      " 11  Industry                                        291 non-null    object  \n",
      " 12  Sector Code                                     5978 non-null   object  \n",
      " 13  Sector                                          5978 non-null   object  \n",
      " 14  Keywords Count                                  5978 non-null   float64 \n",
      " 15  % per Sector                                    5978 non-null   float64 \n",
      " 16  % per Social Category                           5978 non-null   float64 \n",
      " 17  % per Workforce                                 5978 non-null   float64 \n",
      " 18  Female Count (x 1000)                           5978 non-null   float64 \n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64 \n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64 \n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64 \n",
      " 22  Male Count (x 1000)                             5978 non-null   float64 \n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64 \n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64 \n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64 \n",
      " 26  Gender                                          5978 non-null   category\n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64 \n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64 \n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64 \n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64 \n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64 \n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64 \n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64 \n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64 \n",
      " 35  Age                                             5978 non-null   category\n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64 \n",
      " 37  % Sector per Workforce                          5978 non-null   float64 \n",
      " 38  Gender_Female                                   5978 non-null   float64 \n",
      " 39  Gender_Mixed                                    5978 non-null   float64 \n",
      " 40  Gender_Male                                     5978 non-null   float64 \n",
      " 41  Age_Older                                       5978 non-null   float64 \n",
      " 42  Age_Mixed                                       5978 non-null   float64 \n",
      " 43  Age_Younger                                     5978 non-null   float64 \n",
      " 44  Gender_Num                                      5978 non-null   float64 \n",
      " 45  Age_Num                                         5978 non-null   float64 \n",
      "dtypes: category(2), float64(32), object(12)\n",
      "memory usage: 2.1+ MB\n",
      "====================\n",
      "Warmth:\n",
      "--------------------\n",
      "Warmth Counts:\n",
      "0.0    4458\n",
      "1.0    1520\n",
      "Name: Warmth, dtype: int64\n",
      "--------------------\n",
      "Warmth Percentages:\n",
      "0.0    74.6\n",
      "1.0    25.4\n",
      "Name: Warmth, dtype: float64\n",
      "--------------------\n",
      "Warmth Mean: 0.25\n",
      "--------------------\n",
      "Warmth Standard Deviation: 0.44\n",
      "====================\n",
      "Competence:\n",
      "--------------------\n",
      "Competence Counts:\n",
      "0.0    3282\n",
      "1.0    2696\n",
      "Name: Competence, dtype: int64\n",
      "--------------------\n",
      "Competence Percentages:\n",
      "0.0    54.9\n",
      "1.0    45.1\n",
      "Name: Competence, dtype: float64\n",
      "--------------------\n",
      "Competence Mean: 0.45\n",
      "--------------------\n",
      "Competence Standard Deviation: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Job Sentence info\n",
    "df_gender_age_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f599e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_manual['Sector'].isna().sum() == 0:\n",
    "    assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "    df_manual.to_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl')\n",
    "    df_manual.to_csv(f'{df_save_dir}df_manual_including_sector_genage_data.csv', index=False)\n",
    "else:\n",
    "    print(f\"MISSING SECTOR DATA: COUNT {df_manual['Sector'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373c116",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER spacy sentence splitting is completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993064a6",
   "metadata": {},
   "source": [
    "# Use spacy to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744a550",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_SENTENCIZED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b997d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccb270b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17a63dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c3b98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_num_and_frequency(row, text_col):\n",
    "\n",
    "    row['Job Description num_words'] = len(str(row[f'{text_col}']).split())\n",
    "    row['Job Description num_unique_words'] = len(set(str(row[f'{text_col}']).split()))\n",
    "    row['Job Description num_chars'] = len(str(row[f'{text_col}']))\n",
    "    row['Job Description num_punctuations'] = len([c for c in str(row[f'{text_col}']) if c in string.punctuation])\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3d31a583",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc21643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_sentencized_lower'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: job_sentence.strip().lower()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b8dac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Description spacy_sentencized</th>\n",
       "      <th>Job Description spacy_sentencized_lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you want to start your HR career at one of ...</td>\n",
       "      <td>do you want to start your hr career at one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you the one that likes to work in a challe...</td>\n",
       "      <td>are you the one that likes to work in a challe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Than Tesla is looking for you!</td>\n",
       "      <td>than tesla is looking for you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We are looking for an associate HR administrator.</td>\n",
       "      <td>we are looking for an associate hr administrator.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are you going to do?</td>\n",
       "      <td>what are you going to do?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Job Description spacy_sentencized  \\\n",
       "0  Do you want to start your HR career at one of ...   \n",
       "1  Are you the one that likes to work in a challe...   \n",
       "2                     Than Tesla is looking for you!   \n",
       "3  We are looking for an associate HR administrator.   \n",
       "4                          What are you going to do?   \n",
       "\n",
       "             Job Description spacy_sentencized_lower  \n",
       "0  do you want to start your hr career at one of ...  \n",
       "1  are you the one that likes to work in a challe...  \n",
       "2                     than tesla is looking for you!  \n",
       "3  we are looking for an associate hr administrator.  \n",
       "4                          what are you going to do?  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[['Job Description spacy_sentencized', 'Job Description spacy_sentencized_lower']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cf56055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenize\n",
    "with open(f'{df_save_dir}punctuations.txt', 'rb') as f:\n",
    "    custom_punct_chars = pickle.load(f)\n",
    "\n",
    "df_manual['Job Description spacy_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.text.strip().lower())\n",
    "        for token in nlp.tokenizer(job_sentence)\n",
    "        if len(token) != 0\n",
    "        and not token.is_space\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "        and not token.is_bracket\n",
    "        and not token.like_email\n",
    "        and not token.text in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd443d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_sentencized_cleaned'] = df_manual['Job Description spacy_tokenized'].str.join(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "65043f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence word frequencies\n",
    "df_manual = df_manual.apply(\n",
    "    lambda row: get_word_num_and_frequency(\n",
    "        row=row, text_col='Job Description spacy_sentencized'\n",
    "    ), \n",
    "    axis='columns',\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2acfe7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job ID', 'Job Description spacy_sentencized', 'Warmth', 'Competence',\n",
       "       'Dutch Requirement', 'English Requirement', 'Search Keyword',\n",
       "       'Platform', 'Job Title', 'Company Name', 'Location', 'Industry',\n",
       "       'Sector Code', 'Sector', 'Keywords Count', '% per Sector',\n",
       "       '% per Social Category', '% per Workforce', 'Female Count (x 1000)',\n",
       "       'Gender_Female_% per Sector', 'Gender_Female_% per Social Category',\n",
       "       'Gender_Female_% per Workforce', 'Male Count (x 1000)',\n",
       "       'Gender_Male_% per Sector', 'Gender_Male_% per Social Category',\n",
       "       'Gender_Male_% per Workforce', 'Gender', 'Age_Older (>= 45 years)_n',\n",
       "       'Age_Older (>= 45 years)_% per Sector',\n",
       "       'Age_Older (>= 45 years)_% per Social Category',\n",
       "       'Age_Older (>= 45 years)_% per Workforce', 'Age_Younger (< 45 years)_n',\n",
       "       'Age_Younger (< 45 years)_% per Sector',\n",
       "       'Age_Younger (< 45 years)_% per Social Category',\n",
       "       'Age_Younger (< 45 years)_% per Workforce', 'Age',\n",
       "       'Sector Count (x 1000)', '% Sector per Workforce', 'Gender_Female',\n",
       "       'Gender_Mixed', 'Gender_Male', 'Age_Older', 'Age_Mixed', 'Age_Younger',\n",
       "       'Gender_Num', 'Age_Num', 'Job Description spacy_sentencized_lower',\n",
       "       'Job Description spacy_tokenized',\n",
       "       'Job Description spacy_sentencized_cleaned',\n",
       "       'Job Description num_words', 'Job Description num_unique_words',\n",
       "       'Job Description num_chars', 'Job Description num_punctuations'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5deddb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f002d2",
   "metadata": {},
   "source": [
    "# Use NLTK to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92922872",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fae46f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d49d6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a93afe1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5011ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c8a9f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 53 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      "dtypes: float64(32), int64(4), object(17)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4475248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.21 s, sys: 1.82 s, total: 7.02 s\n",
      "Wall time: 7.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize with NLTK\n",
    "df_manual['Job Description nltk_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.strip().lower()) \n",
    "        for token in word_tokenize(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and token != '...' \n",
    "        and not token.lower() in set(stopwords.words('english')) \n",
    "        and not token.lower() in list(string.punctuation) \n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3bbd5d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [want, start, hr, career, one, innovative, bra...\n",
       "1    [one, likes, work, challenging, fast, moving, ...\n",
       "2                                     [tesla, looking]\n",
       "3              [looking, associate, hr, administrator]\n",
       "4                                              [going]\n",
       "Name: Job Description nltk_tokenized, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Job Description nltk_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86da6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 54 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      "dtypes: float64(32), int64(4), object(18)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "69e6640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e54149",
   "metadata": {},
   "source": [
    "# Use gensim to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff307dba",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f89cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b74624e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dcb91e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c13d88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc48f290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 54 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      "dtypes: float64(32), int64(4), object(18)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e95fec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 534 ms, sys: 19.1 ms, total: 554 ms\n",
      "Wall time: 583 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_manual['Job Description gensim_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1cc29198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [want, start, career, innov, brand, world]\n",
       "1     [like, work, challeng, fast, move, organ]\n",
       "2                                 [tesla, look]\n",
       "3                     [look, associ, administr]\n",
       "4                                          [go]\n",
       "Name: Job Description gensim_tokenized, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Job Description gensim_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0a69a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 55 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      "dtypes: float64(32), int64(4), object(19)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a62be70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0131f",
   "metadata": {},
   "source": [
    "# Use BERT to tokenize sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf30fc",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c81e981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7958db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fff83773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec899945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d42a3b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 55 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      "dtypes: float64(32), int64(4), object(19)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8bc6622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 87 ms, total: 1.28 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_manual['Job Description bert_encodings'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: bert_tokenizer(str(sentence), truncation=True, padding=True)\n",
    ")\n",
    "\n",
    "df_manual['Job Description bert_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: bert_tokenizer.tokenize(str(sentence))\n",
    ")\n",
    "\n",
    "df_manual['Job Description bert_tokenized_to_id'] = df_manual['Job Description bert_tokenized'].apply(\n",
    "    lambda sentence: bert_tokenizer.convert_tokens_to_ids(str(sentence))\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1d996bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [input_ids, token_type_ids, attention_mask]\n",
       "1    [input_ids, token_type_ids, attention_mask]\n",
       "2    [input_ids, token_type_ids, attention_mask]\n",
       "3    [input_ids, token_type_ids, attention_mask]\n",
       "4    [input_ids, token_type_ids, attention_mask]\n",
       "Name: Job Description bert_encodings, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Job Description bert_encodings'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9ae92181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [do, you, want, to, start, your, hr, career, a...\n",
       "1    [are, you, the, one, that, likes, to, work, in...\n",
       "2              [than, tesla, is, looking, for, you, !]\n",
       "3    [we, are, looking, for, an, associate, hr, adm...\n",
       "4                   [what, are, you, going, to, do, ?]\n",
       "Name: Job Description bert_tokenized, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Job Description bert_tokenized'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5f7a4ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100\n",
       "1    100\n",
       "2    100\n",
       "3    100\n",
       "4    100\n",
       "Name: Job Description bert_tokenized_to_id, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual['Job Description bert_tokenized_to_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8936c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bfad5",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all tokenization (spacy, nltk, gensim, and BERT) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a62460",
   "metadata": {},
   "source": [
    "# Use spacy to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d2178",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM_BERT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c74f96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a94928eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "628c5e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "10424739",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "08c5b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 58 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      " 55  Job Description bert_encodings                  5978 non-null   object \n",
      " 56  Job Description bert_tokenized                  5978 non-null   object \n",
      " 57  Job Description bert_tokenized_to_id            5978 non-null   int64  \n",
      "dtypes: float64(32), int64(5), object(21)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "45e45b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer characters\n",
    "with open(f'{df_save_dir}punctuations.txt', 'rb') as f:\n",
    "    custom_punct_chars = pickle.load(f)\n",
    "\n",
    "# POS tagging\n",
    "df_manual['Job Description spacy_token_tags'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        tuple([token.text.strip().lower(), token.tag_])\n",
    "        for token in nlp(job_sentence)\n",
    "        \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Lemmatization\n",
    "df_manual['Job Description spacy_lemmas'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        token.lemma_.strip().lower()\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Stemming\n",
    "df_manual['Job Description spacy_stems'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        stemmer.stem(token.text.strip().lower())\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9d8332f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 61 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      " 55  Job Description bert_encodings                  5978 non-null   object \n",
      " 56  Job Description bert_tokenized                  5978 non-null   object \n",
      " 57  Job Description bert_tokenized_to_id            5978 non-null   int64  \n",
      " 58  Job Description spacy_token_tags                5978 non-null   object \n",
      " 59  Job Description spacy_lemmas                    5978 non-null   object \n",
      " 60  Job Description spacy_stems                     5978 non-null   object \n",
      "dtypes: float64(32), int64(5), object(24)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5d8d37b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Description spacy_token_tags</th>\n",
       "      <th>Job Description spacy_lemmas</th>\n",
       "      <th>Job Description spacy_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(do, VBP), (you, PRP), (want, VB), (to, TO), ...</td>\n",
       "      <td>[want, start, hr, career, innovative, brand, w...</td>\n",
       "      <td>[want, start, hr, career, innov, brand, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(are, VBP), (you, PRP), (the, DT), (one, NN),...</td>\n",
       "      <td>[like, work, challenging, fast, move, organiza...</td>\n",
       "      <td>[like, work, challeng, fast, move, organ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(than, IN), (tesla, NNP), (is, VBZ), (looking...</td>\n",
       "      <td>[tesla, look]</td>\n",
       "      <td>[tesla, look]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(we, PRP), (are, VBP), (looking, VBG), (for, ...</td>\n",
       "      <td>[look, associate, hr, administrator]</td>\n",
       "      <td>[look, associ, hr, administr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(what, WP), (are, VBP), (you, PRP), (going, V...</td>\n",
       "      <td>[go]</td>\n",
       "      <td>[go]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Job Description spacy_token_tags  \\\n",
       "0  [(do, VBP), (you, PRP), (want, VB), (to, TO), ...   \n",
       "1  [(are, VBP), (you, PRP), (the, DT), (one, NN),...   \n",
       "2  [(than, IN), (tesla, NNP), (is, VBZ), (looking...   \n",
       "3  [(we, PRP), (are, VBP), (looking, VBG), (for, ...   \n",
       "4  [(what, WP), (are, VBP), (you, PRP), (going, V...   \n",
       "\n",
       "                        Job Description spacy_lemmas  \\\n",
       "0  [want, start, hr, career, innovative, brand, w...   \n",
       "1  [like, work, challenging, fast, move, organiza...   \n",
       "2                                      [tesla, look]   \n",
       "3               [look, associate, hr, administrator]   \n",
       "4                                               [go]   \n",
       "\n",
       "                      Job Description spacy_stems  \n",
       "0  [want, start, hr, career, innov, brand, world]  \n",
       "1       [like, work, challeng, fast, move, organ]  \n",
       "2                                   [tesla, look]  \n",
       "3                   [look, associ, hr, administr]  \n",
       "4                                            [go]  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[\n",
    "    [\n",
    "        'Job Description spacy_token_tags',\n",
    "        'Job Description spacy_lemmas',\n",
    "        'Job Description spacy_stems'\n",
    "    ]\n",
    "].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fa7acdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74665a",
   "metadata": {},
   "source": [
    "# Use NLTK to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe6cb2",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7c765e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dedd3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4f17167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ac327ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(token):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b5e476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6f5c1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 61 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      " 55  Job Description bert_encodings                  5978 non-null   object \n",
      " 56  Job Description bert_tokenized                  5978 non-null   object \n",
      " 57  Job Description bert_tokenized_to_id            5978 non-null   int64  \n",
      " 58  Job Description spacy_token_tags                5978 non-null   object \n",
      " 59  Job Description spacy_lemmas                    5978 non-null   object \n",
      " 60  Job Description spacy_stems                     5978 non-null   object \n",
      "dtypes: float64(32), int64(5), object(24)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92fb117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS stagging\n",
    "df_manual['Job Description nltk_token_tags'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda token: pos_tag(token)\n",
    ")\n",
    "\n",
    "# Lemmatization\n",
    "df_manual['Job Description nltk_lemmas'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        lemmatizer.lemmatize(\n",
    "            token, get_wordnet_pos(\n",
    "                unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            )\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Stemming\n",
    "df_manual['Job Description nltk_stems'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        stemmer.stem(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2893388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5978 entries, 0 to 5977\n",
      "Data columns (total 64 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Job ID                                          5978 non-null   object \n",
      " 1   Job Description spacy_sentencized               5978 non-null   object \n",
      " 2   Warmth                                          5978 non-null   float64\n",
      " 3   Competence                                      5978 non-null   float64\n",
      " 4   Dutch Requirement                               5978 non-null   object \n",
      " 5   English Requirement                             5978 non-null   object \n",
      " 6   Search Keyword                                  5978 non-null   object \n",
      " 7   Platform                                        5978 non-null   object \n",
      " 8   Job Title                                       5978 non-null   object \n",
      " 9   Company Name                                    5978 non-null   object \n",
      " 10  Location                                        5978 non-null   object \n",
      " 11  Industry                                        291 non-null    object \n",
      " 12  Sector Code                                     5978 non-null   object \n",
      " 13  Sector                                          5978 non-null   object \n",
      " 14  Keywords Count                                  5978 non-null   float64\n",
      " 15  % per Sector                                    5978 non-null   float64\n",
      " 16  % per Social Category                           5978 non-null   float64\n",
      " 17  % per Workforce                                 5978 non-null   float64\n",
      " 18  Female Count (x 1000)                           5978 non-null   float64\n",
      " 19  Gender_Female_% per Sector                      5978 non-null   float64\n",
      " 20  Gender_Female_% per Social Category             5978 non-null   float64\n",
      " 21  Gender_Female_% per Workforce                   5978 non-null   float64\n",
      " 22  Male Count (x 1000)                             5978 non-null   float64\n",
      " 23  Gender_Male_% per Sector                        5978 non-null   float64\n",
      " 24  Gender_Male_% per Social Category               5978 non-null   float64\n",
      " 25  Gender_Male_% per Workforce                     5978 non-null   float64\n",
      " 26  Gender                                          5978 non-null   object \n",
      " 27  Age_Older (>= 45 years)_n                       5978 non-null   float64\n",
      " 28  Age_Older (>= 45 years)_% per Sector            5978 non-null   float64\n",
      " 29  Age_Older (>= 45 years)_% per Social Category   5978 non-null   float64\n",
      " 30  Age_Older (>= 45 years)_% per Workforce         5978 non-null   float64\n",
      " 31  Age_Younger (< 45 years)_n                      5978 non-null   float64\n",
      " 32  Age_Younger (< 45 years)_% per Sector           5978 non-null   float64\n",
      " 33  Age_Younger (< 45 years)_% per Social Category  5978 non-null   float64\n",
      " 34  Age_Younger (< 45 years)_% per Workforce        5978 non-null   float64\n",
      " 35  Age                                             5978 non-null   object \n",
      " 36  Sector Count (x 1000)                           5978 non-null   float64\n",
      " 37  % Sector per Workforce                          5978 non-null   float64\n",
      " 38  Gender_Female                                   5978 non-null   float64\n",
      " 39  Gender_Mixed                                    5978 non-null   float64\n",
      " 40  Gender_Male                                     5978 non-null   float64\n",
      " 41  Age_Older                                       5978 non-null   float64\n",
      " 42  Age_Mixed                                       5978 non-null   float64\n",
      " 43  Age_Younger                                     5978 non-null   float64\n",
      " 44  Gender_Num                                      5978 non-null   float64\n",
      " 45  Age_Num                                         5978 non-null   float64\n",
      " 46  Job Description spacy_sentencized_lower         5978 non-null   object \n",
      " 47  Job Description spacy_tokenized                 5978 non-null   object \n",
      " 48  Job Description spacy_sentencized_cleaned       5978 non-null   object \n",
      " 49  Job Description num_words                       5978 non-null   int64  \n",
      " 50  Job Description num_unique_words                5978 non-null   int64  \n",
      " 51  Job Description num_chars                       5978 non-null   int64  \n",
      " 52  Job Description num_punctuations                5978 non-null   int64  \n",
      " 53  Job Description nltk_tokenized                  5978 non-null   object \n",
      " 54  Job Description gensim_tokenized                5978 non-null   object \n",
      " 55  Job Description bert_encodings                  5978 non-null   object \n",
      " 56  Job Description bert_tokenized                  5978 non-null   object \n",
      " 57  Job Description bert_tokenized_to_id            5978 non-null   int64  \n",
      " 58  Job Description spacy_token_tags                5978 non-null   object \n",
      " 59  Job Description spacy_lemmas                    5978 non-null   object \n",
      " 60  Job Description spacy_stems                     5978 non-null   object \n",
      " 61  Job Description nltk_token_tags                 5978 non-null   object \n",
      " 62  Job Description nltk_lemmas                     5978 non-null   object \n",
      " 63  Job Description nltk_stems                      5978 non-null   object \n",
      "dtypes: float64(32), int64(5), object(27)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_manual.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "84e2af7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Description nltk_token_tags</th>\n",
       "      <th>Job Description nltk_lemmas</th>\n",
       "      <th>Job Description nltk_stems</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(want, JJ), (start, NN), (hr, VB), (career, N...</td>\n",
       "      <td>[want, start, hr, career, innovative, brand, w...</td>\n",
       "      <td>[want, start, hr, career, innov, brand, world]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(likes, NNS), (work, VBP), (challenging, VBG)...</td>\n",
       "      <td>[like, work, challenge, fast, move, organization]</td>\n",
       "      <td>[like, work, challeng, fast, move, organ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(tesla, NN), (looking, VBG)]</td>\n",
       "      <td>[tesla, look]</td>\n",
       "      <td>[tesla, look]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(looking, VBG), (associate, JJ), (hr, NNS), (...</td>\n",
       "      <td>[look, associate, hr, administrator]</td>\n",
       "      <td>[look, associ, hr, administr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(going, VBG)]</td>\n",
       "      <td>[go]</td>\n",
       "      <td>[go]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Job Description nltk_token_tags  \\\n",
       "0  [(want, JJ), (start, NN), (hr, VB), (career, N...   \n",
       "1  [(likes, NNS), (work, VBP), (challenging, VBG)...   \n",
       "2                      [(tesla, NN), (looking, VBG)]   \n",
       "3  [(looking, VBG), (associate, JJ), (hr, NNS), (...   \n",
       "4                                     [(going, VBG)]   \n",
       "\n",
       "                         Job Description nltk_lemmas  \\\n",
       "0  [want, start, hr, career, innovative, brand, w...   \n",
       "1  [like, work, challenge, fast, move, organization]   \n",
       "2                                      [tesla, look]   \n",
       "3               [look, associate, hr, administrator]   \n",
       "4                                               [go]   \n",
       "\n",
       "                       Job Description nltk_stems  \n",
       "0  [want, start, hr, career, innov, brand, world]  \n",
       "1       [like, work, challeng, fast, move, organ]  \n",
       "2                                   [tesla, look]  \n",
       "3                   [look, associ, hr, administr]  \n",
       "4                                            [go]  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual[['Job Description nltk_token_tags', 'Job Description nltk_lemmas', 'Job Description nltk_stems']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "102509c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f98098",
   "metadata": {},
   "source": [
    "# Use BERT to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dd84df",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cfc8511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "\n",
    "# mod = sys.modules[__name__]\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# # %load_ext autoreload\n",
    "# # %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b3c26cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MAIN DIR\n",
    "# main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# # code_dir\n",
    "# code_dir = f'{code_dir}/'\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# # scraping dir\n",
    "# scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# # data dir\n",
    "# data_dir = f'{code_dir}data/'\n",
    "\n",
    "# # df save sir\n",
    "# df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# # lang models dir\n",
    "# llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# # sites\n",
    "# site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# # columns\n",
    "# cols=['Sector', \n",
    "#       'Sector Code', \n",
    "#       'Gender', \n",
    "#       'Age', \n",
    "#       'Language', \n",
    "#       'Dutch Requirement', \n",
    "#       'English Requirement', \n",
    "#       'Gender_Female', \n",
    "#       'Gender_Mixed', \n",
    "#       'Gender_Male', \n",
    "#       'Age_Older', \n",
    "#       'Age_Mixed', \n",
    "#       'Age_Younger', \n",
    "#       'Gender_Num', \n",
    "#       'Age_Num', \n",
    "#       '% Female', \n",
    "#       '% Male', \n",
    "#       '% Older', \n",
    "#       '% Younger']\n",
    "\n",
    "# int_variable: str = 'Job ID'\n",
    "# str_variable: str = 'Job Description'\n",
    "# gender: str = 'Gender'\n",
    "# age: str = 'Age'\n",
    "# language: str = 'en'\n",
    "# languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "# str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "# nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "# pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9d35cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import re\n",
    "# import time\n",
    "# import json\n",
    "# import csv\n",
    "# import glob\n",
    "# import pickle\n",
    "# import random\n",
    "# import unicodedata\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import googletrans\n",
    "# from googletrans import Translator\n",
    "# random.seed(42)\n",
    "\n",
    "# # Set up Spacy\n",
    "# import spacy\n",
    "# from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Set up NLK\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "# from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "# nltk_path = f'{llm_path}/nltk'\n",
    "# nltk.data.path.append(nltk_path)\n",
    "\n",
    "# nltk.download('words', download_dir = nltk_path)\n",
    "# nltk.download('stopwords', download_dir = nltk_path)\n",
    "# nltk.download('punkt', download_dir = nltk_path)\n",
    "# nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# punctuations = list(string.punctuation)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Set up Gensim\n",
    "# from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "# from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# # Set up Bert\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "# bert_model_name = 'bert-base-uncased'\n",
    "# bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "# bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "# device_name = 'cuda'\n",
    "# random.seed(42)\n",
    "# np.random.seed(42)\n",
    "# random_state = 42\n",
    "# max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81972d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0a48ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "# bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "# bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "# df_manual['Job Description bert_token_tags_with_scores'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "#     lambda sentence: [\n",
    "#         (bert_pos_tag['word'], bert_pos_tag['entity'], bert_pos_tag['score'])\n",
    "#         for i in range(len(sentence.split()))\n",
    "#         for bert_pos_tag in bert_pos_tagger(sentence)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# df_manual['Job Description bert_token_tags'] = df_manual['Job Description bert_token_tags_with_scores'].apply(\n",
    "#     lambda tag_list: [\n",
    "#         [(tag_list[i][0], tag_list[i][1])]\n",
    "#         for tag_tuple in tag_list\n",
    "#         for i in range(len(tag_list))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
    "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f4e99e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_manual['Job Description bert_token_tags'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0f7e50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
    "# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cb736",
   "metadata": {},
   "source": [
    "# Use spacy to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8a93bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ce59f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "405cfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_make_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    matches_list = []\n",
    "\n",
    "    for idx in range(len(matches)):\n",
    "        for match_id, start, end in matches:\n",
    "            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "                match = doc[matches[idx][1]: matches[idx][2]].text\n",
    "                matches_list.append(match.lower())\n",
    "    \n",
    "    return list(set(matches_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae54c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description spacy_1grams_original_list'] = df_manual['Job Description spacy_tokenized']\n",
    "df_manual['Job Description spacy_1grams'] = df_manual['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "patters_dict = {\n",
    "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
    "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
    "}\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3,\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
    "\n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                '_'.join(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence: \n",
    "            [\n",
    "                tuple(ngram_.split())\n",
    "                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if f'{ngram_name}_patterns' in matcher:\n",
    "        matcher.remove(f'{ngram_name}_patterns')\n",
    "    assert f'{ngram_name}_patterns' not in matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9fc3ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Allgrams\n",
    "df_manual['Job Description spacy_123grams_original_list'] = df_manual['Job Description spacy_tokenized'] + df_manual['Job Description spacy_2grams_original_list'] + df_manual['Job Description spacy_3grams_original_list']\n",
    "df_manual['Job Description spacy_123grams'] = df_manual['Job Description spacy_1grams'] + df_manual['Job Description spacy_2grams'] + df_manual['Job Description spacy_3grams']\n",
    "df_manual['Job Description spacy_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description spacy_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d68848b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258e491",
   "metadata": {},
   "source": [
    "# Use NLTK to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e767712",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45f3b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "221f97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3991b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a61db031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2afc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description nltk_1grams_original_list'] = df_manual['Job Description nltk_tokenized']\n",
    "df_manual['Job Description nltk_1grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens:\n",
    "            list(\n",
    "                '_'.join(ngram_list)\n",
    "                for ngram_list in ngrams(tokens, ngram_num)\n",
    "            )\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
    "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
    "    )\n",
    "\n",
    "    df_manual[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "            for ngrams_list in df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
    "            for ngram_ in ngrams_list\n",
    "            if '_' in ngram_\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "32f3ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Allgrams\n",
    "df_manual[f'Job Description nltk_123grams_original_list'] = df_manual['Job Description nltk_tokenized'] + df_manual['Job Description nltk_2grams_original_list'] + df_manual['Job Description nltk_3grams_original_list']\n",
    "df_manual[f'Job Description nltk_123grams'] = df_manual['Job Description nltk_1grams'] + df_manual['Job Description nltk_2grams'] + df_manual['Job Description nltk_3grams']\n",
    "df_manual['Job Description nltk_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description nltk_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "17610cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440bd34",
   "metadata": {},
   "source": [
    "# Use Gensim to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494636f",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dad74074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "51a2dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c63a9219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8c9c33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "59d29aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Job Description gensim_1grams_original_list'] = df_manual['Job Description gensim_tokenized']\n",
    "df_manual['Job Description gensim_1grams'] = df_manual['Job Description gensim_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        tuple(token.split())\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "becdbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_manual['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_manual['Job Description gensim_2grams_original_list_all'] = bigram[df_manual['Job Description gensim_tokenized']]\n",
    "df_manual['Job Description gensim_2grams_original_list'] = df_manual['Job Description gensim_2grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_manual['Job Description gensim_2grams'] = df_manual['Job Description gensim_2grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_manual[f'Job Description gensim_2grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_2grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_manual['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_manual['Job Description gensim_3grams_original_list_all'] = trigram[df_manual['Job Description gensim_2grams_original_list_all']]\n",
    "df_manual['Job Description gensim_3grams_original_list'] = df_manual['Job Description gensim_3grams_original_list_all'].apply(\n",
    "    lambda ngrams_list: [\n",
    "        ngram_\n",
    "        for ngram_ in ngrams_list\n",
    "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
    "    ]\n",
    ")\n",
    "df_manual['Job Description gensim_3grams'] = df_manual['Job Description gensim_3grams_original_list'].apply(\n",
    "    lambda ngrams: [\n",
    "        tuple(ngram.split('_'))\n",
    "        for ngram in ngrams\n",
    "        if '_' in ngram\n",
    "    ]\n",
    ")\n",
    "df_manual[f'Job Description gensim_3grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_3grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "84d89c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim Allgrams\n",
    "df_manual[f'Job Description gensim_123grams_original_list'] = df_manual['Job Description gensim_tokenized'] + df_manual['Job Description gensim_2grams_original_list'] + df_manual['Job Description gensim_3grams_original_list']\n",
    "df_manual[f'Job Description gensim_123grams'] = df_manual['Job Description gensim_1grams'] + df_manual['Job Description gensim_2grams'] + df_manual['Job Description gensim_3grams']\n",
    "df_manual['Job Description gensim_123grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().apply(\n",
    "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
    ").replace(\n",
    "    regex = {\n",
    "        re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
    "        for ngrams_list in df_manual[f'Job Description gensim_123grams_original_list']\n",
    "        for ngram_ in ngrams_list\n",
    "        if '_' in ngram_\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac58f92",
   "metadata": {},
   "source": [
    "# Create word frequencies for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33d9ab",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK_GENSIM\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "735bed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "57ad6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "bb934586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8731dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_frequency(row, text_col, ngram_num, embedding_library):\n",
    "\n",
    "    abs_word_freq = defaultdict(int)\n",
    "    for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
    "        abs_word_freq[word] += 1\n",
    "\n",
    "        abs_wtd_df = (\n",
    "            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
    "            .rename(columns={0: 'abs_word_freq'})\n",
    "            .sort_values(by=['abs_word_freq'], ascending=False)\n",
    "            )\n",
    "        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
    "        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
    "\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8e4a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "43206134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_manual = df_manual.apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "30affd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_frequency.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1670f255",
   "metadata": {},
   "source": [
    "# Create BoW dictionary, corpus, and tfidf matrix for uni, bi, and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989aa0",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_FREQUENCY\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "051454b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "26bb7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "35f57c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "57359916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
    "    \n",
    "    ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
    "    dictionary = Dictionary([ngrams_original_list])\n",
    "    BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
    "    tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "    tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
    "\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "30b00a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ba6a2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_manual = df_manual.apply(\n",
    "        lambda row: get_corpus_and_dictionary(\n",
    "            row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
    "        ),\n",
    "        axis='columns'\n",
    "    )\n",
    "\n",
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b51eae79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['% Sector per Workforce', '% per Sector', '% per Social Category',\n",
       "       '% per Workforce', 'Age', 'Age_Mixed', 'Age_Num', 'Age_Older',\n",
       "       'Age_Older (>= 45 years)_% per Sector',\n",
       "       'Age_Older (>= 45 years)_% per Social Category',\n",
       "       ...\n",
       "       'Job Description gensim_2grams_tfidf',\n",
       "       'Job Description gensim_2grams_tfidf_matrix',\n",
       "       'Job Description gensim_3grams_dictionary',\n",
       "       'Job Description gensim_3grams_BoW_corpus',\n",
       "       'Job Description gensim_3grams_tfidf',\n",
       "       'Job Description gensim_3grams_tfidf_matrix',\n",
       "       'Job Description gensim_123grams_dictionary',\n",
       "       'Job Description gensim_123grams_BoW_corpus',\n",
       "       'Job Description gensim_123grams_tfidf',\n",
       "       'Job Description gensim_123grams_tfidf_matrix'],\n",
       "      dtype='object', length=183)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manual.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a03d8769",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31f120",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976d079",
   "metadata": {},
   "source": [
    "# Use spacy and nltk for sentiment scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61138e8",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_NGRAMS_BOW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "47bdc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4c0cf19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "444550f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "25a35056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "795034b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentiment\n",
    "if 'spacytextblob' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('spacytextblob')\n",
    "\n",
    "df_manual['Job Description spacy_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "58173ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK sentiment\n",
    "df_manual['Job Description nltk_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ec80fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_sentiment_spacy_nltk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5cf4e",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad73af",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_SENTIMENT_SPACY_NLTK\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48122d98",
   "metadata": {},
   "source": [
    "# Word2Vec and FastText embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "57105477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "dd263006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "64c7bdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertTokenizerFast, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "device_name = 'cuda'\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "max_length = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "91117606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_word2vec(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    w2v_model.train(\n",
    "        sentences,\n",
    "        total_examples=w2v_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    w2v_vocab = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "#             print(f'Word2Vec {size}: {w2v_model.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return w2v_vocab, w2v_model\n",
    "\n",
    "def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in w2v_vocab]\n",
    "\n",
    "    return np.mean(w2v_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "69dc70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_fasttext(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    ft_model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    ft_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    ft_model.train(\n",
    "        sentences,\n",
    "        total_examples=ft_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    ft_vocab = list(ft_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "#             print(f'FastText {size}: {ft_model_300.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return ft_vocab, ft_model\n",
    "\n",
    "def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in ft_vocab]\n",
    "\n",
    "    return np.mean(ft_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b6d43d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as glove:\n",
    "\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1022f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "    if external_glove is False and embeddings_index is None:\n",
    "        embeddings_index= get_glove()\n",
    "\n",
    "    if extra_preprocessing_enabled is False:\n",
    "        words = sentences\n",
    "\n",
    "    elif extra_preprocessing_enabled is True:\n",
    "        stop_words = set(sw.words('english'))\n",
    "        words = str(sentences).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "    M = []\n",
    "\n",
    "    try:\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis='index')\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(300)\n",
    "\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a512f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5c0675fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_dict = {\n",
    "    'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
    "    'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f55d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building spacy_1grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 0.06 mins\n",
      "Time to build w2v_vocab for 300: 0.11 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 6550\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 6550\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to support: [('sales', 0.9977598190307617), ('account', 0.9975799322128296), ('design', 0.9975284934043884), ('plans', 0.9974655508995056), ('strategic', 0.9974338412284851)]\n",
      "300 - Negative most similar to support: [('consideration', -0.6192004084587097), ('political', -0.6207641363143921), ('sexual', -0.623869776725769), ('veteran', -0.6240389347076416), ('color', -0.624832272529602)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to leader: [('win', 0.9978167414665222), ('solid', 0.9978152513504028), ('desired', 0.9977708458900452), ('impactful', 0.9977309107780457), ('drug', 0.9977274537086487)]\n",
      "300 - Negative most similar to leader: [('consideration', -0.664385974407196), ('political', -0.667663037776947), ('veteran', -0.6721065640449524), ('sexual', -0.6725314855575562), ('color', -0.6732675433158875)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to management: [('proposals', 0.9968798160552979), ('account', 0.9966575503349304), ('technical', 0.9964849352836609), ('sales', 0.996146023273468), ('treasury', 0.9955900311470032)]\n",
      "300 - Negative most similar to management: [('consideration', -0.6068530678749084), ('color', -0.6179603934288025), ('political', -0.6194185614585876), ('sexual', -0.620063841342926), ('veteran', -0.6206887364387512)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to team: [('working', 0.9941924214363098), ('development', 0.9941498041152954), ('product', 0.993996262550354), ('strategy', 0.9929997324943542), ('services', 0.9928638935089111)]\n",
      "300 - Negative most similar to team: [('political', -0.5425224304199219), ('sexual', -0.5444073677062988), ('consideration', -0.544745922088623), ('veteran', -0.5450306534767151), ('orientation', -0.5452637672424316)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to business: [('develop', 0.9965496063232422), ('development', 0.9965161085128784), ('strategy', 0.9964480996131897), ('sales', 0.9963400363922119), ('services', 0.9963262677192688)]\n",
      "300 - Negative most similar to business: [('consideration', -0.5901721119880676), ('political', -0.5905550718307495), ('sexual', -0.5922479629516602), ('veteran', -0.5926471948623657), ('orientation', -0.5933412313461304)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to customer: [('satisfaction', 0.9982549548149109), ('success', 0.9979627132415771), ('functions', 0.9977558255195618), ('ensure', 0.9975598454475403), ('building', 0.9974820017814636)]\n",
      "300 - Negative most similar to customer: [('consideration', -0.6479716897010803), ('political', -0.6559920907020569), ('sexual', -0.6578159332275391), ('color', -0.6583794355392456), ('veteran', -0.6586500406265259)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to risk: [('credit', 0.9967972636222839), ('mitigation', 0.9967049360275269), ('modelling', 0.9965967535972595), ('mitigating', 0.9965471029281616), ('advisors', 0.9965348243713379)]\n",
      "300 - Negative most similar to risk: [('consideration', -0.6854389309883118), ('political', -0.697237491607666), ('color', -0.6989119052886963), ('sexual', -0.6993592977523804), ('veteran', -0.6997637748718262)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to build: [('relationships', 0.9984561800956726), ('trusted', 0.9981193542480469), ('customers', 0.9974057078361511), ('relationship', 0.9973958134651184), ('develop', 0.9973155856132507)]\n",
      "300 - Negative most similar to build: [('consideration', -0.6147662997245789), ('political', -0.6184592247009277), ('sexual', -0.6204224228858948), ('color', -0.6210080981254578), ('veteran', -0.6211206912994385)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to computer: [('applied', 0.9985792636871338), ('econometrics', 0.9983772039413452), ('discipline', 0.9983745217323303), ('undergraduate', 0.9983119368553162), ('exceeds', 0.998274564743042)]\n",
      "300 - Negative most similar to computer: [('consideration', -0.6904715299606323), ('political', -0.6954604387283325), ('veteran', -0.6998783349990845), ('sexual', -0.7007349729537964), ('color', -0.7013011574745178)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 6550\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 0.22 mins\n",
      "Time to build vocab for 300: 0.35 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to she: [('shown', 0.9995778799057007), ('breakroom', 0.9993847608566284), ('break', 0.9993384480476379), ('took', 0.9991793632507324), ('pix', 0.9991410374641418)]\n",
      "300 - Negative most similar to she: [('\\u200b', 0.02846001833677292), ('y.', -0.015891747549176216), ('nations', -0.4164723753929138), ('station', -0.44081512093544006), ('relation', -0.4411025643348694)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to he: [('luxe', 0.9989113211631775), ('3.600', 0.9986169934272766), ('nu', 0.9985879063606262), ('10.00', 0.9984660744667053), ('werk', 0.9982972145080566)]\n",
      "300 - Negative most similar to he: [('\\u200b', 0.023479802533984184), ('y.', -0.0052836923860013485), ('action', -0.3519500494003296), ('ring', -0.3609115183353424), ('relation', -0.37244707345962524)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to support: [('supports', 0.9986106753349304), ('indirect', 0.9954384565353394), ('objects', 0.9951075315475464), ('ims', 0.9950118064880371), ('direct', 0.9949071407318115)]\n",
      "300 - Negative most similar to support: [('\\u200b', 0.038811031728982925), ('y.', -0.002103671431541443), ('een', -0.5935314297676086), ('ring', -0.6210586428642273), ('nations', -0.6295527815818787)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to leader: [('lead', 0.9960399866104126), ('leads', 0.9947817921638489), ('order', 0.9946645498275757), ('broader', 0.9939616322517395), ('lead:-', 0.9938080310821533)]\n",
      "300 - Negative most similar to leader: [('\\u200b', 0.019699854776263237), ('y.', -0.0008126337779685855), ('nations', -0.5561145544052124), ('ring', -0.5612426996231079), ('action', -0.5711383819580078)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to management: [('improvement', 0.9967821836471558), ('procurement', 0.9958961606025696), ('supplement', 0.995572566986084), ('engagement', 0.9953457117080688), ('enablement', 0.99420565366745)]\n",
      "300 - Negative most similar to management: [('\\u200b', 0.04905282333493233), ('y.', 0.030223611742258072), ('een', -0.3018607199192047), ('star', -0.3881603479385376), ('start', -0.4029442369937897)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to team: [('creators', 0.9963429570198059), ('dead', 0.99544757604599), ('ops', 0.9952265620231628), ('superpowers', 0.9951386451721191), ('teams', 0.9951117038726807)]\n",
      "300 - Negative most similar to team: [('\\u200b', 0.04346692934632301), ('y.', -0.015560480765998363), ('nations', -0.4759923815727234), ('station', -0.4960784912109375), ('rotation', -0.51429682970047)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to business: [('a?business', 0.9977061748504639), ('business/', 0.9973620772361755), ('businesses', 0.9966586232185364), ('businesses/', 0.9961854815483093), ('pocess', 0.9952342510223389)]\n",
      "300 - Negative most similar to business: [('\\u200b', 0.04428974166512489), ('y.', -0.0033719989005476236), ('een', -0.5735790729522705), ('nations', -0.610988438129425), ('station', -0.6242872476577759)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to customer: [('custom', 0.9986254572868347), ('-customer', 0.9978464245796204), ('super', 0.9963662624359131), ('infrastructure', 0.995868444442749), ('inner', 0.9958338141441345)]\n",
      "300 - Negative most similar to customer: [('\\u200b', 0.04228927567601204), ('y.', -0.01828972063958645), ('nations', -0.45815351605415344), ('station', -0.4864582419395447), ('rotation', -0.5054758191108704)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to risk: [('risks', 0.9981024861335754), ('meta', 0.9980553388595581), ('phase', 0.9978917241096497), ('methane', 0.9976460337638855), ('pics', 0.9974731802940369)]\n",
      "300 - Negative most similar to risk: [('\\u200b', 0.02070557326078415), ('y.', -0.005542442202568054), ('nations', -0.5728268623352051), ('ring', -0.5846548676490784), ('action', -0.5903387069702148)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to build: [('maximize', 0.9984011650085449), ('roll', 0.9983508586883545), ('sun', 0.9981873035430908), ('surf', 0.9980764389038086), ('funnel', 0.9980711936950684)]\n",
      "300 - Negative most similar to build: [('\\u200b', 0.03353751450777054), ('y.', -0.0107315918430686), ('nations', -0.531475841999054), ('station', -0.5499585866928101), ('relation', -0.556115984916687)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to computer: [('comprised', 0.9938632845878601), ('company;act', 0.9921491146087646), ('company', 0.9909948110580444), ('competitors', 0.9909536838531494), ('coffee', 0.9904283881187439)]\n",
      "300 - Negative most similar to computer: [('\\u200b', 0.03817164897918701), ('y.', -0.009272576309740543), ('ring', -0.4507957696914673), ('ping', -0.4766795337200165), ('bing', -0.508959174156189)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 6550\n",
      "300 - Positive most similar to programmer: [('programme', 0.9972044825553894), ('productaanbod', 0.9963045716285706), ('maximise', 0.9944008588790894), ('timeline', 0.9942589998245239), ('businesses;perform', 0.9940980076789856)]\n",
      "300 - Negative most similar to programmer: [('\\u200b', 0.03708234801888466), ('y.', -0.011019956320524216), ('nations', -0.535548985004425), ('station', -0.5548913478851318), ('relation', -0.5653573870658875)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building spacy_2grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 1.65 mins\n",
      "Time to build w2v_vocab for 300: 1.67 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'support' not present in vocabulary\"\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'leader' not present in vocabulary\"\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'management' not present in vocabulary\"\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'team' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'business' not present in vocabulary\"\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'customer' not present in vocabulary\"\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'risk' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'build' not present in vocabulary\"\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'computer' not present in vocabulary\"\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 5588\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 1.76 mins\n",
      "Time to build vocab for 300: 1.86 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to she: [('short_notice', 0.9999539256095886), ('short_test', 0.9999536275863647), ('story_starts', 0.999953031539917), ('prominent_start', 0.9999529123306274), ('period_starts', 0.9999527931213379)]\n",
      "300 - Negative most similar to she: [('vocational_education', -0.9998323917388916), ('11,66_bruto', -0.9998367428779602), ('international_education', -0.9998395442962646), ('international_cooperation', -0.999854564666748), ('international_law', -0.9998551607131958)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to he: [('healthy_lifestyle', 0.9999849796295166), ('health_takes', 0.9999845027923584), ('assemble_hardware', 0.9999844431877136), ('maximize_sales', 0.9999843239784241), ('help_hundreds', 0.9999842643737793)]\n",
      "300 - Negative most similar to he: [('vocational_education', -0.9998701214790344), ('11,66_bruto', -0.9998736381530762), ('international_education', -0.9998756051063538), ('international_law', -0.9998893141746521), ('international_cooperation', -0.9998907446861267)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to support: [('hoc_support', 0.9999983906745911), ('support_thought', 0.9999970197677612), ('support_sales', 0.9999967217445374), ('support_handover', 0.9999961853027344), ('advisory_support', 0.9999961256980896)]\n",
      "300 - Negative most similar to support: [('vocational_education', -0.9998568892478943), ('international_education', -0.9998644590377808), ('international_cooperation', -0.9998813271522522), ('international_law', -0.9998819231987), ('11,66_bruto', -0.9998875856399536)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to leader: [('thought_leader', 0.9999983310699463), ('find_leaders', 0.9999982118606567), ('thought_leadership', 0.999998152256012), ('thought_leaders', 0.9999980926513672), ('core_leadership', 0.9999980330467224)]\n",
      "300 - Negative most similar to leader: [('vocational_education', -0.9998740553855896), ('international_education', -0.9998795986175537), ('international_law', -0.999894380569458), ('international_cooperation', -0.9998950958251953), ('11,66_bruto', -0.999896764755249)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to management: [('senior_management', 0.9999988079071045), ('overall_management', 0.9999982714653015), ('sound_judgement', 0.9999980330467224), ('live_engagement', 0.9999978542327881), ('industrial_management', 0.9999977946281433)]\n",
      "300 - Negative most similar to management: [('vocational_education', -0.9998680353164673), ('international_education', -0.9998741745948792), ('11,66_bruto', -0.9998867511749268), ('international_law', -0.9998892545700073), ('international_cooperation', -0.9998907446861267)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to team: [('team_aims', 0.9999954700469971), ('key_team', 0.9999954700469971), ('fun_team', 0.9999954700469971), ('true_team', 0.999995231628418), ('small_team', 0.9999949336051941)]\n",
      "300 - Negative most similar to team: [('vocational_education', -0.9998648762702942), ('international_education', -0.9998705983161926), ('11,66_bruto', -0.9998753666877747), ('international_law', -0.9998858571052551), ('international_cooperation', -0.9998859167098999)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to business: [('warm_business', 0.9999988675117493), ('does_business', 0.9999988079071045), ('meet_business', 0.9999985098838806), ('good_business', 0.9999979138374329), ('steer_business', 0.9999974370002747)]\n",
      "300 - Negative most similar to business: [('vocational_education', -0.9998273849487305), ('international_education', -0.999833881855011), ('international_law', -0.9998512268066406), ('international_cooperation', -0.999852180480957), ('international_relations', -0.9998589158058167)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to customer: [('debug_customer', 0.9999989867210388), ('tricky_customer', 0.9999980926513672), ('own_customer', 0.9999978542327881), ('key_customer', 0.9999975562095642), ('customers_see', 0.9999969005584717)]\n",
      "300 - Negative most similar to customer: [('vocational_education', -0.9998070597648621), ('international_education', -0.9998171329498291), ('international_cooperation', -0.9998341202735901), ('international_law', -0.9998371005058289), ('international_relations', -0.9998449683189392)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to risk: [('minimize_risk', 0.9999935626983643), ('minimize_risks', 0.999992847442627), ('own_risk', 0.9999927878379822), ('manages_risk', 0.9999919533729553), ('take_risks', 0.9999918341636658)]\n",
      "300 - Negative most similar to risk: [('vocational_education', -0.9998685717582703), ('international_education', -0.999874472618103), ('international_law', -0.9998895525932312), ('international_cooperation', -0.999890148639679), ('11,66_bruto', -0.999890923500061)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to build: [('build_today', 0.9999961853027344), ('build_tools', 0.9999960064888), ('build_rapport', 0.9999958276748657), ('largest_shipbuilders', 0.9999955296516418), ('trust_built', 0.9999954104423523)]\n",
      "300 - Negative most similar to build: [('vocational_education', -0.9998726844787598), ('international_education', -0.9998788237571716), ('11,66_bruto', -0.9998878240585327), ('international_law', -0.9998937845230103), ('international_cooperation', -0.9998942613601685)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to computer: [('computers_shipped', 0.9999987483024597), ('claimed_computer', 0.9999985098838806), ('mobile_computers', 0.9999984502792358), ('rugged_computer', 0.9999982714653015), ('industrial_computers', 0.9999977946281433)]\n",
      "300 - Negative most similar to computer: [('vocational_education', -0.9998858571052551), ('international_education', -0.9998924136161804), ('11,66_bruto', -0.9998928308486938), ('international_law', -0.9999069571495056), ('international_cooperation', -0.9999074339866638)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 5588\n",
      "300 - Positive most similar to programmer: [('programmatic_media', 0.9999975562095642), ('impactful_program', 0.9999973773956299), ('progressive_areas', 0.9999973773956299), ('novel_programs', 0.9999972581863403), ('measure_progress', 0.9999969005584717)]\n",
      "300 - Negative most similar to programmer: [('vocational_education', -0.9998735189437866), ('international_education', -0.9998798370361328), ('11,66_bruto', -0.9998936653137207), ('international_law', -0.9998957514762878), ('international_cooperation', -0.9998959302902222)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building spacy_3grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 3.39 mins\n",
      "Time to build w2v_vocab for 300: 3.39 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'support' not present in vocabulary\"\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'leader' not present in vocabulary\"\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'management' not present in vocabulary\"\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'team' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'business' not present in vocabulary\"\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'customer' not present in vocabulary\"\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'risk' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'build' not present in vocabulary\"\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'computer' not present in vocabulary\"\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 1699\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 3.45 mins\n",
      "Time to build vocab for 300: 3.46 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to she: [('_per_year', 0.11942638456821442), ('think_full_stack', 0.11886631697416306), ('voices_like_yours', 0.11580928415060043), ('heart_beat_faster', 0.11461971700191498), ('optimism_in_others', 0.11420521885156631)]\n",
      "300 - Negative most similar to she: [('journey_towards_fossil', 0.06597571820020676), ('drive_rapid_system', 0.06330697238445282), ('roles_at_net', 0.04229024425148964), ('learn_new_systems', 0.03561015427112579), ('team_works_closely', 0.02804785780608654)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to he: [('piece_by_piece', 0.07405979931354523), ('days_per_week', 0.0736914724111557), ('hotels_en_resorts', 0.06401563435792923), ('hours_per_week', 0.05703789368271828), ('futurist_at_heart', 0.04952634498476982)]\n",
      "300 - Negative most similar to he: [('analyze_relevant_territory', 0.13537493348121643), ('host_strategic_review', 0.13409695029258728), ('roles_at_net', 0.11289793998003006), ('willen_dat_onze', 0.11211197823286057), ('standards_against_industry', 0.11126360297203064)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to support: [('support_new_business', 0.9454192519187927), ('strategy_in_support', 0.944229245185852), ('support_of_product', 0.9423998594284058), ('communities_in_support', 0.9416419863700867), ('point_of_support', 0.9414519667625427)]\n",
      "300 - Negative most similar to support: [('piece_by_piece', -0.27723509073257446), ('achieve_most_3d', -0.4180601239204407), ('think_full_stack', -0.4365427494049072), ('last_just_minutes', -0.4574441611766815), ('chat_via_video', -0.4603937864303589)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to leader: [('leader_within_healthcare', 0.7381646037101746), ('leader_for_customers', 0.7364145517349243), ('leader_in_customer', 0.7359606623649597), ('leader_with_experience', 0.7332162857055664), ('leader_in_identity', 0.7327609658241272)]\n",
      "300 - Negative most similar to leader: [('piece_by_piece', -0.25928133726119995), ('achieve_most_3d', -0.27220144867897034), ('think_full_stack', -0.29201194643974304), ('chat_via_video', -0.33281663060188293), ('see_specific_task', -0.3701450526714325)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to management: [('management_of_hr', 0.9671421051025391), ('management_of_proposals', 0.9634547233581543), ('amount_of_management', 0.9606738090515137), ('management_of_functions', 0.9597982168197632), ('topics_of_engagement', 0.9471566081047058)]\n",
      "300 - Negative most similar to management: [('piece_by_piece', -0.288833886384964), ('achieve_most_3d', -0.3974694609642029), ('chat_via_video', -0.4594244956970215), ('think_full_stack', -0.5000909566879272), ('see_specific_task', -0.5031731724739075)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to team: [('teams_in_order', 0.6967304348945618), ('team_in_regard', 0.6947540044784546), ('teams_on_project', 0.6914151310920715), ('team_in_order', 0.690259575843811), ('team_comes_in', 0.6865090727806091)]\n",
      "300 - Negative most similar to team: [('piece_by_piece', -0.26965680718421936), ('achieve_most_3d', -0.277965784072876), ('chat_via_video', -0.31841138005256653), ('see_specific_task', -0.3206194043159485), ('work_flexible_hours', -0.3301216959953308)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to business: [('business_of_pw', 0.9658698439598083), ('terms_of_business', 0.9605500102043152), ('lines_of_business', 0.9595699310302734), ('field_of_business', 0.9580748081207275), ('years_of_business', 0.9578402042388916)]\n",
      "300 - Negative most similar to business: [('piece_by_piece', -0.2705962061882019), ('achieve_most_3d', -0.3913056552410126), ('think_full_stack', -0.4579430818557739), ('chat_via_video', -0.4716872274875641), ('see_specific_task', -0.49017074704170227)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to customer: [('role_of_customer', 0.9538787603378296), ('teams_on_customer', 0.9522252678871155), ('aim_of_customer', 0.9505574107170105), ('team_in_customer', 0.9498192071914673), ('parts_for_customer', 0.9487467408180237)]\n",
      "300 - Negative most similar to customer: [('piece_by_piece', -0.2752719521522522), ('achieve_most_3d', -0.3972579538822174), ('think_full_stack', -0.47440305352211), ('chat_via_video', -0.48820844292640686), ('futurist_at_heart', -0.4907415807247162)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to risk: [('risk_new_ways', 0.3607495129108429), ('interest_in_risk', 0.30906417965888977), ('explore_new_technologies', 0.2932994067668915), ('host_strategic_review', 0.2862618565559387), ('apply_new_technologies', 0.2796855866909027)]\n",
      "300 - Negative most similar to risk: [('chat_via_video', -0.03871423378586769), ('piece_by_piece', -0.07044166326522827), ('dagen_per_week', -0.08352133631706238), ('tools_as_part', -0.09231947362422943), ('generate_own_leads', -0.095045305788517)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to build: [('building_new_concepts', 0.4647810757160187), ('build_new_products', 0.44949594140052795), ('build_new_skills', 0.4425384998321533), ('have_regular_team(building', 0.43909701704978943), ('build_impactful_relationships', 0.4256390929222107)]\n",
      "300 - Negative most similar to build: [('piece_by_piece', -0.04009571671485901), ('team_works_closely', -0.12728653848171234), ('achieve_most_3d', -0.14319373667240143), ('add_new_items', -0.14790350198745728), ('see_specific_task', -0.1552143096923828)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to computer: [('excitement_for_computer', 0.8254035711288452), ('provide_complete_solutions', 0.782653272151947), ('experience_in_word', 0.7814285755157471), ('amount_of_experience', 0.7812591195106506), ('compliance_with_law', 0.7807794213294983)]\n",
      "300 - Negative most similar to computer: [('piece_by_piece', -0.2623499631881714), ('chat_via_video', -0.34557804465293884), ('achieve_most_3d', -0.3554876744747162), ('futurist_at_heart', -0.3713044226169586), ('think_full_stack', -0.3895987272262573)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 1699\n",
      "300 - Positive most similar to programmer: [('includes_statistical_programming', 0.8260553479194641), ('areas_of_responsibility', 0.8132105469703674), ('data_within_program', 0.8130853176116943), ('programs_from_discovery', 0.8110385537147522), ('optimizing_operational_efficiency', 0.8100782036781311)]\n",
      "300 - Negative most similar to programmer: [('piece_by_piece', -0.2373601198196411), ('think_full_stack', -0.34928202629089355), ('last_just_minutes', -0.3709735870361328), ('achieve_most_3d', -0.37453004717826843), ('chat_via_video', -0.3913431763648987)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building spacy_123grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 4.92 mins\n",
      "Time to build w2v_vocab for 300: 5.04 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 13837\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 13837\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to support: [('align', 0.9913126826286316), ('hr', 0.9911817312240601), ('closely', 0.9903498291969299), ('lead', 0.9900276064872742), ('owners', 0.9899851679801941)]\n",
      "300 - Negative most similar to support: [('national_origin', -0.52632075548172), ('sexual_orientation', -0.5328196883201599), ('characteristics', -0.5377027988433838), ('marital_status', -0.5407543182373047), ('receive_consideration', -0.5411784648895264)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to leader: [('rollout', 0.9941366910934448), ('iqvia', 0.9939428567886353), ('drug', 0.9938486218452454), ('trade', 0.9938341379165649), ('hundreds', 0.9937878847122192)]\n",
      "300 - Negative most similar to leader: [('national_origin', -0.6171835064888), ('applicants', -0.617493212223053), ('marital_status', -0.6235681176185608), ('sexual_orientation', -0.6265891790390015), ('employment_without_regard', -0.6288323402404785)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to management: [('project', 0.9912647604942322), ('technical', 0.9896228909492493), ('proposals', 0.9889891147613525), ('communicating', 0.9872016906738281), ('account', 0.9870620965957642)]\n",
      "300 - Negative most similar to management: [('national_origin', -0.5379797220230103), ('applicants', -0.5451991558074951), ('marital_status', -0.5491047501564026), ('sexual_orientation', -0.5499274730682373), ('receive_consideration', -0.553192138671875)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to team: [('development', 0.9855098724365234), ('digital', 0.9825592637062073), ('strategy', 0.9811801910400391), ('working', 0.9808938503265381), ('teams', 0.9801507592201233)]\n",
      "300 - Negative most similar to team: [('disability', -0.3761023283004761), ('characteristics', -0.3771134614944458), ('marital', -0.37723100185394287), ('veteran', -0.37778440117836), ('sexual_orientation', -0.3792368471622467)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to business: [('sales', 0.9911701083183289), ('bu', 0.9903130531311035), ('account', 0.990241527557373), ('design', 0.9901553988456726), ('develop', 0.9898819923400879)]\n",
      "300 - Negative most similar to business: [('sexual_orientation', -0.45908474922180176), ('characteristics', -0.4610605239868164), ('disability', -0.4630621373653412), ('marital', -0.46500009298324585), ('veteran', -0.46507611870765686)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to customer: [('develop', 0.9954540133476257), ('deliver', 0.9935818910598755), ('implement', 0.9932273030281067), ('build', 0.9931348562240601), ('initiatives', 0.9930599331855774)]\n",
      "300 - Negative most similar to customer: [('sexual_orientation', -0.4884391129016876), ('characteristics', -0.49012133479118347), ('veteran', -0.4917502999305725), ('disability', -0.4918202757835388), ('marital', -0.49209901690483093)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to risk: [('modelling', 0.9892926812171936), ('credit', 0.98896723985672), ('mitigation', 0.9873872995376587), ('sbts', 0.9868646264076233), ('understanding_of_cyber', 0.9863215684890747)]\n",
      "300 - Negative most similar to risk: [('written', -0.60834139585495), ('verbal', -0.615932822227478), ('english', -0.6197605133056641), ('national_origin', -0.6315441131591797), ('marital_status', -0.6324148178100586)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to build: [('relationships', 0.9956682324409485), ('trusted', 0.9950180649757385), ('develop', 0.9934521317481995), ('customer', 0.9931347966194153), ('organization', 0.9917915463447571)]\n",
      "300 - Negative most similar to build: [('marital', -0.4738367199897766), ('characteristics', -0.473864883184433), ('disability', -0.4741819500923157), ('sexual_orientation', -0.4742709696292877), ('veteran', -0.4750303030014038)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to computer: [('equivalent_experience', 0.9954110383987427), ('university', 0.995405375957489), ('education', 0.9944684505462646), ('studies', 0.9944248199462891), ('econometrics', 0.9943233728408813)]\n",
      "300 - Negative most similar to computer: [('applicants', -0.6569126844406128), ('national_origin', -0.6620455384254456), ('marital_status', -0.6633355617523193), ('employment_without_regard', -0.6658220887184143), ('consideration_for_employment', -0.6705471873283386)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 13837\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 5.18 mins\n",
      "Time to build vocab for 300: 5.38 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to she: [('shown', 0.9956234097480774), ('blocks', 0.9911584854125977), ('type', 0.9911284446716309), ('airbnb', 0.9904860258102417), ('town', 0.9904189109802246)]\n",
      "300 - Negative most similar to she: [('\\u200b', 0.015188997611403465), ('y.', -0.002680022269487381), ('nations', -0.23123517632484436), ('national', -0.23886609077453613), ('stations', -0.2499736100435257)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to he: [('heartbeat', 0.9840573668479919), ('heijn', 0.9820214509963989), ('bit', 0.9809163212776184), ('heart_beat', 0.9797865748405457), ('albeit', 0.978558361530304)]\n",
      "300 - Negative most similar to he: [('\\u200b', 0.03419681638479233), ('y.', 0.012476100586354733), ('relations', -0.14454321563243866), ('implementations', -0.16014935076236725), ('relation', -0.16240300238132477)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to support: [('supports', 0.980775773525238), ('hoc_support', 0.967913031578064), ('support_sales', 0.9466238021850586), ('supported', 0.9397406578063965), ('port', 0.9385554790496826)]\n",
      "300 - Negative most similar to support: [('y.', 0.055915288627147675), ('\\u200b', -0.03813713788986206), ('en', -0.10118630528450012), ('een', -0.18546919524669647), ('start', -0.20071899890899658)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to leader: [('order', 0.9502159953117371), ('border', 0.9489392638206482), ('thought_leader', 0.9433189630508423), ('gartner', 0.9416579008102417), ('thought_leadership', 0.9384176135063171)]\n",
      "300 - Negative most similar to leader: [('y.', 0.04053916782140732), ('\\u200b', -0.046950165182352066), ('environment', -0.17052938044071198), ('environment/', -0.19161522388458252), ('environments', -0.19930770993232727)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to management: [('engagement', 0.9749566316604614), ('supplement', 0.9716840982437134), ('overall_management', 0.9697557687759399), ('senior_management', 0.969415545463562), ('judgement', 0.9675833582878113)]\n",
      "300 - Negative most similar to management: [('start', -0.0058378675021231174), ('star', -0.009582757949829102), ('y.', -0.012153825722634792), ('million', -0.03858817741274834), ('starter', -0.041449662297964096)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to team: [('teams', 0.9573348760604858), ('small_teams', 0.9529633522033691), ('tea', 0.9469935297966003), ('ciso_teams', 0.9448224902153015), ('key_team', 0.942358672618866)]\n",
      "300 - Negative most similar to team: [('\\u200b', 0.010874001309275627), ('y.', -0.011923594400286674), ('non', -0.027238646522164345), ('xion', -0.0628097727894783), ('pension', -0.09735172986984253)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to business: [('a?business', 0.9979530572891235), ('business/', 0.9934738874435425), ('businesses/', 0.9831551313400269), ('meet_business', 0.9824439287185669), ('does_business', 0.9775010347366333)]\n",
      "300 - Negative most similar to business: [('y.', 0.02115531824529171), ('\\u200b', -0.03580744192004204), ('non', -0.08622398972511292), ('xion', -0.09926530718803406), ('motion', -0.13572625815868378)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to customer: [('-customer', 0.9986153244972229), ('debug_customer', 0.9856071472167969), ('own_customer', 0.9849861264228821), ('custom', 0.9829201698303223), ('key_customer', 0.9826242923736572)]\n",
      "300 - Negative most similar to customer: [('\\u200b', -0.03243699669837952), ('y.', -0.040701065212488174), ('condition', -0.11332781612873077), ('non', -0.11834035813808441), ('position', -0.11999297142028809)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to risk: [('risks', 0.9732717871665955), ('liaise', 0.9679446816444397), ('rise', 0.9669394493103027), ('ghg', 0.9666197299957275), ('docs', 0.9661492109298706)]\n",
      "300 - Negative most similar to risk: [('\\u200b', -0.007083097007125616), ('y.', -0.020696593448519707), ('ties', -0.28922203183174133), ('ers', -0.301540344953537), ('locations', -0.30536171793937683)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to build: [('builds', 0.9884710907936096), ('built', 0.9626655578613281), ('trust_built', 0.9580866694450378), ('grow_partnerships', 0.9562233090400696), ('ips', 0.954731285572052)]\n",
      "300 - Negative most similar to build: [('y.', 0.009298879653215408), ('\\u200b', 0.0007849520188756287), ('ring', -0.2899581491947174), ('including', -0.3029458224773407), ('cities', -0.3210494816303253)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to computer: [('comparison', 0.9322169423103333), ('compass', 0.9306018352508545), ('computers_shipped', 0.9278534054756165), ('computers', 0.9197219610214233), ('company_has', 0.9194884300231934)]\n",
      "300 - Negative most similar to computer: [('\\u200b', -0.0024358059745281935), ('y.', -0.051136743277311325), ('improvement', -0.17956972122192383), ('city', -0.184501051902771), ('mentality', -0.18536444008350372)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 13837\n",
      "300 - Positive most similar to programmer: [('programme', 0.9784565567970276), ('program', 0.9673203229904175), ('programs', 0.960983157157898), ('programmatic', 0.9420437216758728), ('profound', 0.9365396499633789)]\n",
      "300 - Negative most similar to programmer: [('y.', -0.04542388394474983), ('\\u200b', -0.05907311290502548), ('ties', -0.2996392548084259), ('come', -0.31444934010505676), ('location', -0.3147241175174713)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building nltk_1grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 6.86 mins\n",
      "Time to build w2v_vocab for 300: 6.92 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 7138\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 7138\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to support: [('teams', 0.9966263175010681), ('business', 0.9962443113327026), ('managers', 0.9961097836494446), ('closely', 0.9959682822227478), ('design', 0.9959470629692078)]\n",
      "300 - Negative most similar to support: [('veteran', -0.5851545333862305), ('disability', -0.5869570970535278), ('age', -0.5920048356056213), ('national', -0.5921079516410828), ('political', -0.5930197834968567)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to leader: [('neurologist', 0.9977079033851624), ('bg', 0.9976856112480164), ('launch', 0.9974000453948975), ('troubleshoot', 0.9973200559616089), ('collaborators', 0.9972949624061584)]\n",
      "300 - Negative most similar to leader: [('consideration', -0.6790013909339905), ('regard', -0.6839751601219177), ('receive', -0.6846819519996643), ('veteran', -0.6873020529747009), ('disability', -0.6891657710075378)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to management: [('project', 0.995113730430603), ('topics', 0.9950056076049805), ('sector', 0.9949431419372559), ('treasury', 0.9947840571403503), ('account', 0.9943142533302307)]\n",
      "300 - Negative most similar to management: [('consideration', -0.6276516914367676), ('regard', -0.6336818337440491), ('receive', -0.6340561509132385), ('veteran', -0.641815185546875), ('age', -0.6420796513557434)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to team: [('product', 0.9941210746765137), ('development', 0.9918274879455566), ('digital', 0.991754949092865), ('strategy', 0.9915102124214172), ('across', 0.9913178086280823)]\n",
      "300 - Negative most similar to team: [('veteran', -0.5171828866004944), ('disability', -0.5194581747055054), ('age', -0.520138680934906), ('color', -0.5215840339660645), ('national', -0.5243464112281799)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to business: [('support', 0.9962444305419922), ('closely', 0.995945930480957), ('teams', 0.9954586029052734), ('managers', 0.9952024817466736), ('collaboration', 0.9950334429740906)]\n",
      "300 - Negative most similar to business: [('veteran', -0.5798734426498413), ('disability', -0.5822985768318176), ('age', -0.5861799120903015), ('color', -0.5879876017570496), ('national', -0.588091254234314)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to customer: [('needs', 0.9975435137748718), ('success', 0.9973888397216797), ('implement', 0.9970691204071045), ('customers', 0.99676114320755), ('help', 0.9967426061630249)]\n",
      "300 - Negative most similar to customer: [('consideration', -0.6204623579978943), ('veteran', -0.6221932768821716), ('regard', -0.6227061152458191), ('disability', -0.6237363815307617), ('age', -0.6246161460876465)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to risk: [('scenario', 0.9966294169425964), ('mitigation', 0.9960508346557617), ('modelling', 0.9959343671798706), ('analyzing', 0.9956238865852356), ('architecture', 0.9955194592475891)]\n",
      "300 - Negative most similar to risk: [('consideration', -0.6623621582984924), ('regard', -0.6663980484008789), ('veteran', -0.6675841212272644), ('receive', -0.6696156859397888), ('disability', -0.6702154874801636)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to build: [('relationships', 0.9976391792297363), ('trusted', 0.9973415732383728), ('long-term', 0.9972967505455017), ('clients', 0.9969278573989868), ('develop', 0.996764600276947)]\n",
      "300 - Negative most similar to build: [('veteran', -0.6057562828063965), ('disability', -0.6075429916381836), ('age', -0.6096721291542053), ('color', -0.6107387542724609), ('regard', -0.6113278269767761)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to computer: [('econometrics', 0.998081624507904), ('applied', 0.9979419708251953), ('studies', 0.9979339241981506), ('biology', 0.9978442192077637), ('education', 0.9975931644439697)]\n",
      "300 - Negative most similar to computer: [('consideration', -0.6910009980201721), ('receive', -0.6958352327346802), ('regard', -0.6970585584640503), ('veteran', -0.701859176158905), ('disability', -0.7036256790161133)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 7138\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 7.03 mins\n",
      "Time to build vocab for 300: 7.18 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to she: [(\"val-d'isre\", 0.9996820688247681), ('sheet', 0.9996238350868225), ('tomorrow', 0.9995734095573425), ('breakdown', 0.9995065331459045), ('hundreds', 0.9994910359382629)]\n",
      "300 - Negative most similar to she: [('\\u200b', -0.14107073843479156), ('nations', -0.464294970035553), ('relation', -0.4705035090446472), ('action', -0.4790537357330322), ('station', -0.48427098989486694)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to he: [(\"n't\", 0.9987449049949646), ('kom', 0.9986750483512878), ('2.300,00', 0.9986278414726257), ('sofitel.com', 0.9986032843589783), ('2,149.00', 0.9985000491142273)]\n",
      "300 - Negative most similar to he: [('\\u200b', -0.1347217708826065), ('ring', -0.3507744371891022), ('acting', -0.39241501688957214), ('action', -0.4058876037597656), ('actions', -0.41012659668922424)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to support: [('supports', 0.9976009726524353), ('indirect', 0.9939212799072266), ('objects', 0.9937699437141418), ('ims', 0.9937243461608887), ('facts', 0.9935736656188965)]\n",
      "300 - Negative most similar to support: [('\\u200b', -0.10143765062093735), ('een', -0.5481928586959839), ('nat', -0.5876470804214478), ('0', -0.6307380199432373), ('20', -0.6402121186256409)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to leader: [('leads', 0.9949191212654114), ('order', 0.9947023391723633), ('lead', 0.9942566752433777), ('mac', 0.9923962950706482), ('market-proven', 0.9920345544815063)]\n",
      "300 - Negative most similar to leader: [('\\u200b', -0.12094379216432571), ('ring', -0.5108836889266968), ('ping', -0.5543574690818787), ('thing', -0.5661177635192871), ('bing', -0.5760387182235718)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to management: [('engagement', 0.9967712759971619), ('supplement', 0.9960404634475708), ('procurement', 0.9946755766868591), ('improvement', 0.9943726658821106), ('enablement', 0.994341254234314)]\n",
      "300 - Negative most similar to management: [('\\u200b', -0.057257991284132004), ('een', -0.2613137364387512), ('start', -0.33995819091796875), ('20', -0.35716888308525085), ('0', -0.3742198646068573)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to team: [('crm', 0.9942901134490967), ('creators', 0.9939208030700684), ('customer', 0.9935438632965088), ('superpowers', 0.992846667766571), ('involved', 0.9925797581672668)]\n",
      "300 - Negative most similar to team: [('\\u200b', -0.1364009827375412), ('nations', -0.4711929261684418), ('station', -0.48706313967704773), ('nat', -0.4909801781177521), ('relation', -0.5087159872055054)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to business: [('business/project', 0.9956933856010437), ('businesses', 0.99531090259552), ('business/', 0.9946091175079346), ('pocess', 0.9935263395309448), ('renewables', 0.9934881329536438)]\n",
      "300 - Negative most similar to business: [('\\u200b', -0.08306096494197845), ('een', -0.5150219798088074), ('0', -0.5930949449539185), ('start', -0.6012341380119324), ('20', -0.6080674529075623)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to customer: [('impactful', 0.997218132019043), ('crm', 0.9971673488616943), ('-customer', 0.9970995187759399), ('infrastructure', 0.9967374205589294), ('needs', 0.9958792924880981)]\n",
      "300 - Negative most similar to customer: [('\\u200b', -0.13346505165100098), ('nations', -0.48272112011909485), ('station', -0.5025040507316589), ('relation', -0.5182754397392273), ('rotation', -0.5298768281936646)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to risk: [('risks', 0.9984399080276489), ('meta', 0.9979749321937561), ('warehousing/logistics', 0.9977476000785828), ('addendums', 0.9976174235343933), ('tcfd', 0.9975709319114685)]\n",
      "300 - Negative most similar to risk: [('\\u200b', -0.12269439548254013), ('ring', -0.5466415286064148), ('ping', -0.5868480801582336), ('thing', -0.5968189835548401), ('nations', -0.6056178212165833)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to build: [('ships', 0.9981351494789124), ('built', 0.9975283145904541), ('builds', 0.9974656105041504), ('dreams', 0.9969978928565979), ('incredibly', 0.996927797794342)]\n",
      "300 - Negative most similar to build: [('\\u200b', -0.13024811446666718), ('nations', -0.5205471515655518), ('station', -0.5351865291595459), ('relation', -0.5407042503356934), ('rotation', -0.5585697889328003)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to computer: [('comprised', 0.9895641803741455), ('company', 0.9894921183586121), ('company-wide', 0.988601803779602), ('comparison', 0.9880586862564087), ('company-sponsored', 0.9876033663749695)]\n",
      "300 - Negative most similar to computer: [('\\u200b', -0.10590389370918274), ('ring', -0.3942441940307617), ('ping', -0.4426840841770172), ('thing', -0.4475429654121399), ('bing', -0.46156105399131775)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 7138\n",
      "300 - Positive most similar to programmer: [('programme', 0.9973134994506836), ('profound', 0.9953601360321045), ('proof-of-concept', 0.994627833366394), ('proof-of-concepts', 0.993860125541687), ('expertise', 0.9938161969184875)]\n",
      "300 - Negative most similar to programmer: [('\\u200b', -0.122191421687603), ('nations', -0.6116235852241516), ('een', -0.6233034133911133), ('ring', -0.6292102336883545), ('station', -0.6319020986557007)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building nltk_2grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 8.78 mins\n",
      "Time to build w2v_vocab for 300: 8.92 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'support' not present in vocabulary\"\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'leader' not present in vocabulary\"\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'management' not present in vocabulary\"\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'team' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'business' not present in vocabulary\"\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'customer' not present in vocabulary\"\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'risk' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'build' not present in vocabulary\"\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'computer' not present in vocabulary\"\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 32102\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 9.19 mins\n",
      "Time to build vocab for 300: 9.52 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to she: [('sharp_dohme', 0.9848225116729736), ('8:00_6:00', 0.9829040169715881), ('n_3.600', 0.9815523028373718), ('1-4-2021job_type', 0.9814321994781494), ('job_type', 0.9795567989349365)]\n",
      "300 - Negative most similar to she: [('1_client', -0.27043020725250244), ('clients_46', -0.27984485030174255), ('content_', -0.2924959659576416), ('implement_improvements', -0.29491010308265686), ('bti_client', -0.2973218560218811)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to he: [('help_scope', 0.9767352342605591), ('help_100', 0.9702633619308472), ('buddy_help', 0.9627490639686584), ('help_shape', 0.9623358845710754), ('keen_help', 0.9608964323997498)]\n",
      "300 - Negative most similar to he: [('managing_client', -0.08295831829309464), ('management_talent', -0.09461729973554611), ('1_client', -0.09500572085380554), ('managing_content', -0.09801878780126572), ('excellent_client', -0.10099969059228897)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to support: [('r_support', 0.9992365837097168), ('support_ii', 0.9991598129272461), ('_support', 0.9988707304000854), ('ui_support', 0.9986589550971985), ('support_rfi', 0.9986457228660583)]\n",
      "300 - Negative most similar to support: [('relevant_knowledge', 0.03716543689370155), ('establish_industry', 0.028918037191033363), ('established_highly', 0.02552187442779541), ('relevant_industry', 0.024350320920348167), ('relevant_field', 0.021553458645939827)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to leader: [('also_leader', 0.9825568795204163), ('leader_solid', 0.9704989790916443), ('leader_dev', 0.9671175479888916), ('leader_emea', 0.9646827578544617), ('thought_leader', 0.9618828892707825)]\n",
      "300 - Negative most similar to leader: [('recruiting_using', 0.05578991398215294), ('compliance_updating', 0.055493202060461044), ('compliance_existing', 0.05220004916191101), ('games_recruiting', 0.050263360142707825), ('cycle_recruiting', 0.04324742406606674)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to management: [('ghg_management', 0.9989938735961914), ('management_hr', 0.9989888072013855), ('_management', 0.998927891254425), ('management_saa', 0.9988823533058167), (\"management_'s\", 0.9986622929573059)]\n",
      "300 - Negative most similar to management: [('individual_motivated', 0.12464241683483124), ('cz_health', 0.10125412791967392), ('innovative_health', 0.08534914255142212), ('motivated_mindset', 0.0827728882431984), ('motivated_innovative', 0.08237843960523605)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to team: [('team_5', 0.9989500045776367), ('team_', 0.9986855387687683), ('team_1', 0.998658299446106), ('team_ux', 0.9983155131340027), ('team_41+', 0.997866153717041)]\n",
      "300 - Negative most similar to team: [('requires_candidate', 0.10854851454496384), ('press_red', 0.09540312737226486), ('possess_required', 0.09028252959251404), ('require_assistance', 0.07989419996738434), ('recovery_data', 0.07520643621683121)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to business: [('_business', 0.9997813105583191), ('ui_business', 0.9996341466903687), ('business_eb', 0.9996224641799927), ('business_', 0.9996084570884705), ('sr._business', 0.9995736479759216)]\n",
      "300 - Negative most similar to business: [('security_office', 0.05396850407123566), ('_security', 0.044339604675769806), ('office_every', 0.04002644121646881), ('security_insights', 0.036560043692588806), ('courage_every', 0.03518490120768547)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to customer: [('run_customer', 0.9991937875747681), ('aim_customer', 0.9990248680114746), (\"'s_customer\", 0.9990143775939941), ('.in_customer', 0.9990097284317017), ('``_customer', 0.9990062117576599)]\n",
      "300 - Negative most similar to customer: [('4_within', 0.028982367366552353), ('_within', 0.02120012603700161), ('ideally_within', 0.013320396654307842), ('within_dbc', 0.011214673519134521), ('within_two', 0.01061242911964655)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to risk: [('tax_risks', 0.9709204435348511), ('hr_risks', 0.9674243330955505), ('rigour_risk', 0.9625492691993713), ('sbts_risk', 0.9394470453262329), ('canada_msd', 0.9298561215400696)]\n",
      "300 - Negative most similar to risk: [('\\uf0b7_develop', -0.15470373630523682), ('develop_design', -0.17738664150238037), ('level_4', -0.17864170670509338), ('level_1', -0.18610121309757233), ('creative_innovative', -0.18767376244068146)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to build: [('build_', 0.9900661706924438), ('_build', 0.9836061000823975), ('also_build', 0.966978132724762), ('way_build', 0.9645060896873474), ('help_build', 0.9544777274131775)]\n",
      "300 - Negative most similar to build: [('chance_convince', 0.09501201659440994), ('reference_check', 0.08336422592401505), ('human_science', 0.07750439643859863), ('annual_conference', 0.0664098709821701), ('_presence', 0.06574315577745438)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to computer: [('_computer', 0.9986099600791931), ('rugged_computer', 0.984609067440033), ('computer_nerd', 0.9834522008895874), ('computer_games', 0.9741055369377136), ('software_computer', 0.948469877243042)]\n",
      "300 - Negative most similar to computer: [('1_digital', 0.05202962085604668), ('gso_digital', 0.04056708514690399), ('make_digital', 0.03932442143559456), ('new_digital', 0.037779755890369415), ('responsible_digital', 0.03636251389980316)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 32102\n",
      "300 - Positive most similar to programmer: [('itp_programme', 0.9808133840560913), ('program_means', 0.979228138923645), ('programme_also', 0.9779433012008667), ('bonus_programs', 0.9750261902809143), ('_programs', 0.973189651966095)]\n",
      "300 - Negative most similar to programmer: [('every_3', -0.007425197400152683), ('discoveries_every', -0.014846045523881912), ('every_24', -0.024412862956523895), ('every_day', -0.025148434564471245), ('one_innovative', -0.03268793225288391)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building nltk_3grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 11.04 mins\n",
      "Time to build w2v_vocab for 300: 11.2 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'support' not present in vocabulary\"\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'leader' not present in vocabulary\"\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'management' not present in vocabulary\"\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'team' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'business' not present in vocabulary\"\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'customer' not present in vocabulary\"\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'risk' not present in vocabulary\"\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'build' not present in vocabulary\"\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'computer' not present in vocabulary\"\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 32912\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 11.56 mins\n",
      "Time to build vocab for 300: 12.07 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to she: [('``_circa_10', 0.9746938347816467), ('17_februari_2021', 0.9673198461532593), ('psd2_sca/mfa_amld', 0.9671436548233032), ('_3000_', 0.9667121171951294), ('_n_3.600', 0.9665808081626892)]\n",
      "300 - Negative most similar to she: [('improvements_improvements_erp', -0.2585858106613159), ('clients_focus_client', -0.26918017864227295), ('client_engagements_mentoring', -0.2737482786178589), ('implement_improvements_manage', -0.2832496464252472), ('documents_per_client', -0.2859902083873749)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to he: [('help__afraid', 0.9791154265403748), ('help_scope_seize', 0.9746864438056946), ('also_help_hundreds', 0.9624524116516113), ('help_us_get', 0.9595704078674316), (\"tree_tomorrow_''\", 0.9578380584716797)]\n",
      "300 - Negative most similar to he: [('client_engagements_mentoring', -0.15430696308612823), ('identifying_analysing_documenting', -0.18305565416812897), ('managing_implementing_sales', -0.18815384805202484), ('management_path_management', -0.19119830429553986), ('management_risk_management', -0.1916171759366989)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to support: [('support_rfi_rfp', 0.9950869083404541), ('bid_rfp_support', 0.9932070374488831), (\"'ll_get_support\", 0.9871622920036316), ('r_support_important', 0.9865192174911499), (\"support_n't_easy\", 0.9864785671234131)]\n",
      "300 - Negative most similar to support: [('knowledge_industry_standards', 0.09603356570005417), ('knowledge_logistics_industry', 0.07989824563264847), ('relevant_logistics_1', 0.07416466623544693), ('relevant_field_e.g', 0.07241654396057129), ('and/or_retail_industry', 0.06979915499687195)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to leader: [('_thought_leadership', 0.9530677199363708), (\"'ll_find_leaders\", 0.9477471113204956), ('cyber_leadership_law', 0.9443894028663635), ('_leadership_bias', 0.939900815486908), ('also_leader_acritas', 0.9387367367744446)]\n",
      "300 - Negative most similar to leader: [('impacting_compliance_existing', 0.12038803100585938), ('avmsd_compliance_updating', 0.11511852592229843), ('love_writing_excellent', 0.11035294830799103), ('initiative_gaining_experience', 0.10245049744844437), ('writing_report_presenting', 0.10220082849264145)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to management: [('management_path_management', 0.99844890832901), ('management_risk_management', 0.9983513355255127), ('management_treasury_management', 0.9978547096252441), ('management_change_management', 0.9953547120094299), ('volume_management_path', 0.9927991032600403)]\n",
      "300 - Negative most similar to management: [('like-minded_innovators_best', 0.11160936951637268), ('like-minded_individuals_people', 0.10055101662874222), ('motivated_innovative_dedicated', 0.08064717799425125), ('journey_think_growth', 0.06915317475795746), ('individuals_179_offices', 0.06845242530107498)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to team: [('team_hr_team', 0.9966009259223938), ('fs__team', 0.9906092882156372), ('join_team_eu', 0.9827106595039368), ('teams_ecom_team', 0.9823352694511414), ('join_team_1', 0.9821119904518127)]\n",
      "300 - Negative most similar to team: [('high_quality_research', 0.1500035971403122), ('position_requires_candidate', 0.13647238910198212), (\"'s_capability_resilience\", 0.13262830674648285), ('address_sustainability_challenges', 0.12696442008018494), ('finance_sustainable_finance', 0.12270743399858475)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to business: [('cmaas__business', 0.9958626627922058), ('us__business', 0.9958058595657349), ('business_similar_business', 0.9954996109008789), ('us_amd_business', 0.9940301179885864), ('business__seat', 0.9933916926383972)]\n",
      "300 - Negative most similar to business: [('office_every_day', 0.07619195431470871), ('security_office_.your', 0.048653677105903625), ('security_challenges_infrastructure', 0.04806787893176079), ('every_day_', 0.04240773618221283), ('better_version_everyday', 0.039363157004117966)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to customer: [('1_``_customer', 0.9987115263938904), ('way_aim_customer', 0.9940913319587708), (\"customer_'s_key\", 0.9925715923309326), ('austria_.in_customer', 0.9925102591514587), (\"uber_'s_customer\", 0.9920714497566223)]\n",
      "300 - Negative most similar to customer: [('reports_4_within', 0.03565256670117378), ('clinical_non-clinical_cmc', 0.032974973320961), ('teams_10_technical', 0.02631433680653572), ('clinical_study_report', 0.025761472061276436), ('clinical_study_reports', 0.02424881048500538)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to risk: [('firewalls/utms_ids/ips_malware', 0.9293259382247925), ('eu_fx_scheme', 0.9243532419204712), ('wat_wij_bieden', 0.9185949563980103), ('dt_bij_abn', 0.9178677201271057), ('vormen_van_voortgezet', 0.9169678092002869)]\n",
      "300 - Negative most similar to risk: [('co_develop_design', -0.23038452863693237), ('develop_innovative_ml', -0.23747847974300385), ('drive_innovative_solutions', -0.24686729907989502), ('ml_ai_solutions', -0.24811707437038422), ('\\uf0b7_develop_lead', -0.2500632107257843)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to build: [('need_build_new', 0.9111889004707336), ('way_also_build', 0.8940237760543823), ('help_build_future', 0.8918374180793762), ('build__next', 0.8842121958732605), ('cisco__build', 0.8834018111228943)]\n",
      "300 - Negative most similar to build: [('human_science_data', 0.09685947000980377), ('science_data_science', 0.09548495709896088), ('human_data_science', 0.08969152718782425), ('chance_convince_us', 0.08841075003147125), ('experience_l_performance', 0.07939168065786362)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to computer: [(\"telecom_'s_company\", 0.9382615089416504), ('company_listed_1', 0.9336415529251099), ('subsidiary_u.s_company', 0.9330714344978333), ('company_12_months', 0.9325176477432251), ('day_parcel_company', 0.9311222434043884)]\n",
      "300 - Negative most similar to computer: [('c__digital', 0.12857015430927277), ('currently_1_digital', 0.10626935958862305), ('topic_design_digital', 0.08585651218891144), ('proven_record_digital', 0.08309997618198395), ('nike_direct_digital', 0.07430949807167053)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 32912\n",
      "300 - Positive most similar to programmer: [('find_itp_programme', 0.9764554500579834), ('health_well_program', 0.9588459134101868), ('care_program_means', 0.9580522179603577), ('ib_diploma_programme', 0.9570850729942322), ('benefits_bonus_programs', 0.9524100422859192)]\n",
      "300 - Negative most similar to programmer: [('discoveries_every_day', 0.11806150525808334), ('leave_every_year', 0.09923192113637924), ('every_day_', 0.0960666835308075), ('every_year_almost', 0.0789499580860138), ('48_days_year', 0.07859383523464203)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n",
      "Found 2195885 word vectors.\n",
      "Done getting sent2vec embeddings.\n",
      "Building nltk_123grams model and vocabulary.\n",
      "Building w2v from word2vec_embeddings function.\n",
      "Time to train the model for 300: 13.83 mins\n",
      "Time to build w2v_vocab for 300: 14.26 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 72151\n",
      "\"Key 'she' not present in vocabulary\"\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 72151\n",
      "\"Key 'he' not present in vocabulary\"\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to support: [('aspects', 0.966793954372406), ('creation', 0.9594851732254028), ('interactions', 0.9592812657356262), ('administrative', 0.9591491222381592), ('\\uf0b7', 0.9587143063545227)]\n",
      "300 - Negative most similar to support: [('internal_external', -0.09085749834775925), ('per_week', -0.09119410067796707), ('best_practices', -0.1058676540851593), ('legal_public_affairs', -0.11051323264837265), ('around_world', -0.11194801330566406)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to leader: [('transmit', 0.9426091909408569), ('iqvia', 0.9415968656539917), ('sport', 0.9405696988105774), ('distribution', 0.9402828216552734), ('end-to-end', 0.9398147463798523)]\n",
      "300 - Negative most similar to leader: [('legal_public_affairs', -0.0713280662894249), ('best_practices', -0.09155845642089844), ('internal_external', -0.10102714598178864), ('veteran_status', -0.10793338716030121), ('national_origin', -0.1093035489320755)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to management: [('stakeholder', 0.947608470916748), ('land', 0.9330899715423584), ('qualitative', 0.9253230094909668), ('governance', 0.9227283000946045), ('report_findings', 0.9226076006889343)]\n",
      "300 - Negative most similar to management: [('legal_public_affairs', -0.07009822130203247), ('personal_development', -0.100565105676651), ('sr_managing_counsel', -0.12542645633220673), ('per_week', -0.1281909942626953), ('around_world', -0.1297096461057663)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to team: [('specialists', 0.9163527488708496), ('transformational', 0.912517249584198), ('requires', 0.9114009141921997), ('shape', 0.9104070067405701), ('actively', 0.9103130102157593)]\n",
      "300 - Negative most similar to team: [('best_practices', -0.0988079309463501), ('around_world', -0.10799208283424377), ('internal_external', -0.12042897194623947), ('per_week', -0.12288948893547058), ('national_origin', -0.1364215463399887)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to business: [('student', 0.9589509963989258), ('executives', 0.9500901103019714), ('strength', 0.9474044442176819), ('therapy', 0.946940541267395), ('translating', 0.9451651573181152)]\n",
      "300 - Negative most similar to business: [('best_practices', -0.0972977876663208), ('around_world', -0.10724984109401703), ('internal_external', -0.11576856672763824), ('legal_public_affairs', -0.12225495278835297), ('per_week', -0.1235787495970726)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to customer: [('satisfaction', 0.9674606323242188), ('ensuring', 0.9582335352897644), ('propositions', 0.956307590007782), ('enabling', 0.9553244113922119), ('voice', 0.9551644325256348)]\n",
      "300 - Negative most similar to customer: [('best_practices', -0.09589515626430511), ('internal_external', -0.09759477525949478), ('around_world', -0.10913994163274765), ('per_week', -0.1152358129620552), ('legal_public_affairs', -0.11689674109220505)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to risk: [('modelling', 0.9674999117851257), ('credit', 0.9542757868766785), ('sbts', 0.9457046985626221), ('tcfd', 0.9337485432624817), ('controls', 0.9319288730621338)]\n",
      "300 - Negative most similar to risk: [('legal_public_affairs', -0.08402347564697266), ('internal_external', -0.09015783667564392), ('around_world', -0.09096957743167877), ('best_practices', -0.10598760843276978), ('per_week', -0.10954980552196503)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to build: [('long-term', 0.9790450930595398), ('motivate', 0.9730985164642334), ('investments', 0.9715469479560852), ('maintain', 0.9679171442985535), ('prospects', 0.9671472907066345)]\n",
      "300 - Negative most similar to build: [('best_practices', -0.08763445913791656), ('internal_external', -0.10348928719758987), ('legal_public_affairs', -0.10722073167562485), ('around_world', -0.10729574412107468), ('per_week', -0.12712426483631134)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to computer: [('bachelor', 0.9682207107543945), ('science', 0.96506267786026), ('equivalent', 0.9643222689628601), ('studies', 0.9613596796989441), ('university', 0.9587193131446838)]\n",
      "300 - Negative most similar to computer: [('ahold_delhaize', -0.16808927059173584), ('best_practices', -0.17590242624282837), ('national_origin', -0.18815085291862488), ('around_world', -0.1894121915102005), ('legal_public_affairs', -0.18974167108535767)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 72151\n",
      "\"Key 'programmer' not present in vocabulary\"\n",
      "Getting w2v embeddings.\n",
      "Building ft from fasttext_embeddings function.\n",
      "Time to train the model for 300: 15.31 mins\n",
      "Time to build vocab for 300: 16.11 mins\n",
      "Checking words form list of length 12\n",
      "WORDS LIST: ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer']\n",
      "Checking word:\n",
      "SHE:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to she: [('she/he', 0.9306459426879883), ('24/7', 0.9189874529838562), ('pc', 0.9185811877250671), ('kid', 0.9181790351867676), ('19', 0.9164568781852722)]\n",
      "300 - Negative most similar to she: [('\\u200b', -0.030365539714694023), ('building_expanding_client', -0.041720226407051086), (\"gain_client_'s\", -0.042003657668828964), ('manufacturing_facility_c', -0.04421614482998848), ('helping_clients_drive', -0.046608325093984604)]\n",
      "Checking word:\n",
      "HE:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to he: [('heeft', 0.9302965402603149), ('he/she', 0.9032946228981018), ('het', 0.9011730551719666), ('held', 0.8962103724479675), ('heavily', 0.889647364616394)]\n",
      "300 - Negative most similar to he: [('timely_managing_key', 0.115892693400383), ('agents_providing_ria', 0.10915763676166534), ('existing_agents_according', 0.10146481543779373), ('manufacturing_facility_c', 0.09545520693063736), ('branding_content_assisting', 0.09252288192510605)]\n",
      "Checking word:\n",
      "SUPPORT:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to support: [('supports', 0.9940914511680603), ('ad-hoc_support', 0.9881439805030823), ('support_lms', 0.9845830798149109), ('ui_support', 0.9834433794021606), (\"support_n't\", 0.9829039573669434)]\n",
      "300 - Negative most similar to support: [('\\u200b', 0.09394653141498566), ('must_deep', 0.0741562768816948), ('utmost_find', 0.061266787350177765), ('achieve_highest', 0.060665201395750046), ('find_way', 0.05593559890985489)]\n",
      "Checking word:\n",
      "LEADER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to leader: [('also_leader', 0.9428580403327942), ('thought_leader', 0.9311094284057617), ('leads', 0.9230740070343018), ('lead', 0.9152109622955322), ('csm_lead', 0.912834107875824)]\n",
      "300 - Negative most similar to leader: [('welcoming_innovative_environment', 0.09666053205728531), ('automotive_environment', 0.0882183089852333), ('innovative_environment', 0.08465608209371567), ('dynamic_innovative_environment', 0.08296076953411102), ('updates_investigative_site', 0.08287648111581802)]\n",
      "Checking word:\n",
      "MANAGEMENT:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to management: [('bid-management', 0.9946234226226807), ('ghg_management', 0.9847885966300964), ('_management', 0.9824771285057068), ('path_management', 0.9820769429206848), ('farm_management', 0.9790844917297363)]\n",
      "300 - Negative most similar to management: [('travel_believe', 0.1282944232225418), (\"'s_believe_``\", 0.1229938343167305), ('healthy_fresh', 0.12139765918254852), ('believe_add', 0.11973848193883896), ('believe_``', 0.1184084489941597)]\n",
      "Checking word:\n",
      "TEAM:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to team: [('tea', 0.9846524596214294), ('sec_team', 0.9614787697792053), ('ai_team', 0.9589864611625671), ('miro_team', 0.9568663239479065), ('fs__team', 0.955211877822876)]\n",
      "300 - Negative most similar to team: [('agency_referral', 0.11111383885145187), ('restaurant_retail_reception', 0.10326643288135529), ('retail_reception', 0.09632117301225662), ('gsk_vaccines_standard', 0.09273908287286758), ('referring_candidates_gsk', 0.09170665591955185)]\n",
      "Checking word:\n",
      "BUSINESS:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to business: [('business/', 0.9928125143051147), ('businesses/', 0.9849362969398499), ('24/7_business', 0.9848355650901794), ('amd_business', 0.9817491173744202), ('bu_business', 0.9811369776725769)]\n",
      "300 - Negative most similar to business: [('office_every_day', 0.07940833270549774), ('week_every_week', 0.07232606410980225), ('office_every', 0.06750079244375229), ('daily_cover_end', 0.06364774703979492), ('cover_end', 0.061629295349121094)]\n",
      "Checking word:\n",
      "CUSTOMER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to customer: [('-customer', 0.9945932030677795), ('type_-customer', 0.9908358454704285), ('custom', 0.9859842658042908), ('aim_customer', 0.9819266200065613), ('debug_customer', 0.9810150265693665)]\n",
      "300 - Negative most similar to customer: [('yearly_staff', 0.02837389148771763), ('european_time', 0.02380228042602539), ('virtually_work', 0.022469688206911087), ('european_electricity', 0.02166425995528698), ('projects_european_electricity', 0.020712217316031456)]\n",
      "Checking word:\n",
      "RISK:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to risk: [('risks', 0.890273928642273), ('tax_risks', 0.8601782321929932), ('rigour_risk', 0.8203403353691101), ('rise', 0.8150020241737366), ('hr_risks', 0.8052680492401123)]\n",
      "300 - Negative most similar to risk: [('help_develop_grow', 0.1140049546957016), ('help_develop', 0.10858810693025589), ('help_develop_deep', 0.10746397078037262), ('develop_innovative_ml', 0.10487408936023712), ('executive_level', 0.10483124852180481)]\n",
      "Checking word:\n",
      "BUILD:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to build: [('builds', 0.9927195906639099), ('_build', 0.9603458642959595), ('build_', 0.9597188830375671), ('built', 0.9597141146659851), ('built-in', 0.9377787113189697)]\n",
      "300 - Negative most similar to build: [('chance_convince_us', 0.1291329264640808), ('reference_check', 0.12854227423667908), ('title_human_resources', 0.11287502199411392), ('human_resources_us', 0.1088266372680664), ('chance_convince', 0.10695219039916992)]\n",
      "Checking word:\n",
      "COMPUTER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to computer: [('_computer', 0.9545605778694153), ('computer_games', 0.9416751265525818), ('computer_nerd', 0.9315212965011597), ('computers', 0.9067498445510864), ('rugged_computer', 0.9041690230369568)]\n",
      "300 - Negative most similar to computer: [('towards_new', 0.04965774714946747), ('identify_new', 0.049274493008852005), ('new_emea_hr', 0.04678456112742424), ('identifying_new', 0.03877606987953186), ('changes_new', 0.03674496337771416)]\n",
      "Checking word:\n",
      "PROGRAMMER:\n",
      "Length of 300 model vobal: 72151\n",
      "300 - Positive most similar to programmer: [('programme', 0.9790380597114563), ('program', 0.9647250771522522), ('programs', 0.9583001732826233), ('programme_also', 0.9494993090629578), ('itp_programme', 0.9377206563949585)]\n",
      "300 - Negative most similar to programmer: [('identify_potential', 0.055260710418224335), ('\\uf0b7_identify', 0.05342840403318405), ('games_essential', 0.04875725507736206), ('values_essential', 0.0475136898458004), ('centres_24', 0.045631587505340576)]\n",
      "Getting ft embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting sent2vec embeddings.\n"
     ]
    }
   ],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
    "\n",
    "    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "\n",
    "        build_train_func, embed_func, model_loader = embed_func_list\n",
    "        print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
    "\n",
    "        vocab, model = build_train_func(\n",
    "            df=df_manual,\n",
    "            ngram_number=ngram_number,\n",
    "            embedding_library=embedding_library,\n",
    "        )\n",
    "\n",
    "        print(f'Getting {embed_model_name} embeddings.')\n",
    "\n",
    "        df_manual[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
    "        ] = df_manual[\n",
    "            f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
    "        ].apply(\n",
    "            lambda sentences: embed_func(sentences, vocab, model)\n",
    "        )\n",
    "        model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
    "\n",
    "    # Sent2Vec\n",
    "    print('Getting sent2vec embeddings.')\n",
    "    embeddings_index = get_glove()\n",
    "    df_manual[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_manual[f'Job Description {embedding_library}_{ngram_number}grams'].apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
    "    print('Done getting sent2vec embeddings.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
    "df_manual.to_pickle(f'{df_save_dir}df_manual_for_trainning.pkl')\n",
    "df_manual.to_csv(f'{df_save_dir}df_manual_for_trainning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f8888",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all embeddings are completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795b68f",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_FOR_TRAINNING\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79862f16",
   "metadata": {},
   "source": [
    "# Descriptives and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1245d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb3c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# models dir\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "\n",
    "# output tables dir\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "\n",
    "# plots dir\n",
    "plot_save_path = f'{data_dir}plots/'\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dec7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb174c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to print df gender and age info\n",
    "def df_gender_age_info(\n",
    "    df,\n",
    "    ivs_all = [\n",
    "        'Gender',\n",
    "        'Gender_Num',\n",
    "        'Gender_Female',\n",
    "        'Gender_Mixed',\n",
    "        'Gender_Male',\n",
    "        'Age',\n",
    "        'Age_Num',\n",
    "        'Age_Older',\n",
    "        'Age_Mixed',\n",
    "        'Age_Younger',\n",
    "    ],\n",
    "):\n",
    "    # Print Info\n",
    "    print('\\nDF INFO:\\n')\n",
    "    df.info()\n",
    "\n",
    "    for iv in ivs_all:\n",
    "        try:\n",
    "            counts = df[f'{iv}'].value_counts()\n",
    "            percentages = df[f'{iv}'].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
    "            print('='*20)\n",
    "            print(f'{iv}:')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Counts:\\n{counts}')\n",
    "            print('-'*20)\n",
    "            print(f'{iv} Percentages:\\n{percentages}')\n",
    "\n",
    "            try:\n",
    "                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
    "                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Mean: {mean}')\n",
    "                print('-'*20)\n",
    "                print(f'{iv} Standard Deviation: {sd}')\n",
    "\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            print(f'{iv} not available.')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245dd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to order categories\n",
    "def categorize_df_gender_age(\n",
    "    df,\n",
    "):\n",
    "    gender_order = ['Female', 'Male', 'Mixed Gender']\n",
    "    age_order = ['Older', 'Younger', 'Mixed Age']\n",
    "    ivs_dict = {'Gender': gender_order, 'Age': age_order}\n",
    "\n",
    "    # Arrange Categories\n",
    "    try:\n",
    "        df['Gender'] = df['Gender'].astype('category').cat.reorder_categories(gender_order, ordered=True)\n",
    "\n",
    "        df['Gender'] = pd.Categorical(\n",
    "            df['Gender'], categories=gender_order, ordered=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        df['Age'] = df['Age'].astype('category').cat.reorder_categories(age_order, ordered=True)\n",
    "\n",
    "        df['Age'] = pd.Categorical(\n",
    "            df['Age'], categories=age_order, ordered=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1862fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data balance\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "plt.style.use('tableau-colorblind10')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681db815-a531-4ace-a8a3-cb957945bde1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All info\n",
    "analysis_columns = [\n",
    "    'Warmth',\n",
    "    'Competence'\n",
    "]\n",
    "\n",
    "df_manual = categorize_df_gender_age(df_manual)\n",
    "\n",
    "df_manual.info()\n",
    "\n",
    "# Gender and Age info by job ad\n",
    "print('='*30)\n",
    "print('Gender and Age info at Sentence Level')\n",
    "print('-'*30)\n",
    "df_gender_age_info(df_manual)\n",
    "\n",
    "# Gender and Age info by job ad\n",
    "print('='*30)\n",
    "print('Gender and Age info at Job Advertisement Level')\n",
    "print('-'*30)\n",
    "df_gender_age_info(df_manual.groupby(['Job ID']).first())\n",
    "\n",
    "# Warmth and Competence info by job ad\n",
    "print('='*30)\n",
    "print('Warmth and Competence info at Sentence Level')\n",
    "print('-'*30)\n",
    "df_gender_age_info(df_manual, ivs_all = analysis_columns)\n",
    "\n",
    "# Warmth and Competence info by job ad\n",
    "print('='*30)\n",
    "print('Warmth and Competence info at Job Advertisement Level')\n",
    "print('-'*30)\n",
    "df_gender_age_info(df_manual.groupby(['Job ID']).first(), ivs_all = analysis_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd6de-970d-43c0-b29f-5e61cae2bdb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imbalance Ratio\n",
    "warmth_imbalance_ratio = df_manual['Warmth'].loc[\n",
    "    df_manual['Warmth'] == 1].count()/df_manual['Warmth'].loc[df_manual['Warmth'] == 0\n",
    "].count()\n",
    "competence_imbalance_ratio = df_manual['Competence'].loc[\n",
    "    df_manual['Competence'] == 1].count()/df_manual['Competence'].loc[df_manual['Competence'] == 0\n",
    "].count()\n",
    "\n",
    "all_imbalance_ratio_dict = {\n",
    "    'Warmth': warmth_imbalance_ratio,\n",
    "    'Competence': competence_imbalance_ratio\n",
    "}\n",
    "\n",
    "print('='*20)\n",
    "print('Imabalance Ratios')\n",
    "print('-'*10)\n",
    "print(f'Warmth IR: {warmth_imbalance_ratio:.2f}')\n",
    "print(f'Competence IR: {competence_imbalance_ratio:.2f}')\n",
    "print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e139202-9f4b-4a83-8164-d13ca74dff98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ploting Warmth and Competence\n",
    "df_warm_comp_transposed = pd.concat(\n",
    "    [\n",
    "        df_manual['Warmth'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
    "        df_manual['Competence'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Training Dataset: Warmth and Competence Sentence Percentages')\n",
    "ax.set_xlabel('Manually Annotated Sentencence Warmth and Competence Percentage from Total')\n",
    "\n",
    "df_warm_comp_transposed.plot(\n",
    "    kind='barh', legend=True, stacked=True, ax=ax, color=['C5', 'C0'],\n",
    ")\n",
    "ax.legend(['Absent', 'Present'])\n",
    "\n",
    "for container in ax.containers:\n",
    "    labels = [f'{width:.1f}%' for v in container if float(width:= v.get_width())]\n",
    "    ax.bar_label(container, labels=labels, label_type='center', color='white')\n",
    "    ax.set_xlabel('% Job Ad Sentences')\n",
    "\n",
    "for i, tick_label in enumerate(ax.get_ymajorticklabels()):\n",
    "    ax.annotate(\n",
    "        f'IR for {tick_label.get_text()} = {all_imbalance_ratio_dict[tick_label.get_text()]:.2f}',\n",
    "        xy=(48, 0.3+i), ha='center', va='center'\n",
    "    )\n",
    "\n",
    "for save_format in ['eps', 'png']:\n",
    "    fig.savefig(\n",
    "        f'{plot_save_path}Manually Annotated Warmth and Competence Sentences.{save_format}',\n",
    "        format=save_format, dpi=3000, bbox_inches='tight'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c70b2b-8d54-40b1-930c-62b853a3219e",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ploting Gender and Age\n",
    "df_gender_transposed = df_manual['Gender'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T\n",
    "df_age_transposed = df_manual['Age'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Training Dataset: Gender and Age Sentence Percentages')\n",
    "\n",
    "df_gender_transposed.plot(\n",
    "    kind='bar', legend=True, stacked=True, ax=axs[0], color=['C5', 'C2', 'C0']\n",
    ")\n",
    "df_age_transposed.plot(\n",
    "    kind='bar', legend=True, stacked=True, ax=axs[1], color=['C5', 'C2', 'C0']\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    for container in ax.containers:\n",
    "        labels = [f'{height:.1f}%' for v in container if float(height:= v.get_height())]\n",
    "        ax.bar_label(container, labels=labels, label_type='center', color='white')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "        ax.set_ylabel('% Job Ad Sentences')\n",
    "\n",
    "for save_format in ['eps', 'png']:\n",
    "    fig.savefig(\n",
    "        f'{plot_save_path}Manually Annotated Gender and Age Sentences.{save_format}',\n",
    "        format=save_format, dpi=3000, bbox_inches='tight'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplot(df, iv, dvs_list):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.suptitle(f'Training Dataset: Warmth and Competence Sentence Percentages per {iv}')\n",
    "\n",
    "    vars_list = [iv]\n",
    "    vars_list.extend(dvs_list)\n",
    "\n",
    "    df_pivot = df[\n",
    "        vars_list\n",
    "    ].pivot_table(\n",
    "        index=iv, values=dvs_list, fill_value=0, aggfunc=lambda x: (100*x.sum())/len(df)\n",
    "    )\n",
    "\n",
    "    df_pivot.sort_values(by=iv, ascending=False).plot(kind='barh', legend=True, stacked=True, ax=ax, color=['C0', 'C5'])\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.set_xlabel('% Job Ad Sentences')\n",
    "        ax.set_ylabel(iv)\n",
    "\n",
    "    for save_format in ['eps', 'png']:\n",
    "        fig.savefig(\n",
    "            f'{plot_save_path}Barplot - Manually Annotated {iv} x {dvs_list[0]} and {dvs_list[1]} Sentences.{save_format}',\n",
    "            format=save_format, dpi=3000, bbox_inches='tight'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stacked barplots\n",
    "ivs_list = ['Gender', 'Age', 'Sector']\n",
    "\n",
    "for iv in ivs_list:\n",
    "    make_barplot(df_manual, iv=iv, dvs_list=['Warmth', 'Competence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc39735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lineplot(df, iv, dv):\n",
    "\n",
    "    df = categorize_df_gender_age(df)\n",
    "\n",
    "    title = f'Means of {dv}-related frames in job ads from {iv} segregated sectors'\n",
    "    data = df.groupby(iv)[dv].agg('mean')\n",
    "\n",
    "    line_plot = sns.lineplot(\n",
    "        data=data, marker='o', legend='full'\n",
    "    )\n",
    "    line_plot.set(\n",
    "        title=title,\n",
    "        xlabel=f'{iv}-dominated Sector',\n",
    "        ylabel=f'{dv} Mean',\n",
    "    )\n",
    "    fig = line_plot.get_figure()\n",
    "\n",
    "    for save_format in ['eps', 'png']:\n",
    "        fig.savefig(\n",
    "            f'{plot_save_path}Line Plot - Manually Annotated {iv} x {dv} Sentences.{save_format}',\n",
    "            format=save_format, dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b62d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make line plots\n",
    "for iv, dv in itertools.product(['Gender', 'Age'], ['Warmth', 'Competence']):\n",
    "    make_lineplot(df_manual, iv=iv, dv=dv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae28ed1-d6af-4f68-b871-b2ddb3844ab3",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all visualizations are completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26485ee2-7c93-4dfb-9a57-2a5c73b83e17",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM df_manual_FOR_TRAINNING\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e43151-4bcf-424c-9230-44b7db260f21",
   "metadata": {},
   "source": [
    "# Make descriptive tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd45b6-d549-47cf-aadf-cee03947eda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b382d-4670-4155-b3d7-2338748b6214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# models dir\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "\n",
    "# output tables dir\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "\n",
    "# plots dir\n",
    "plot_save_path = f'{data_dir}plots/'\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621df2b7-b174-427c-8d3b-0b792ea966e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to order categories\n",
    "def categorize_df_gender_age(\n",
    "    df,\n",
    "):\n",
    "    gender_order = ['Female', 'Male', 'Mixed Gender']\n",
    "    age_order = ['Older', 'Younger', 'Mixed Age']\n",
    "    ivs_dict = {'Gender': gender_order, 'Age': age_order}\n",
    "\n",
    "    # Arrange Categories\n",
    "    try:\n",
    "        df['Gender'] = df['Gender'].astype('category').cat.reorder_categories(gender_order, ordered=True)\n",
    "\n",
    "        df['Gender'] = pd.Categorical(\n",
    "            df['Gender'], categories=gender_order, ordered=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    try:\n",
    "        df['Age'] = df['Age'].astype('category').cat.reorder_categories(age_order, ordered=True)\n",
    "\n",
    "        df['Age'] = pd.Categorical(\n",
    "            df['Age'], categories=age_order, ordered=True\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d1b15-886f-4891-a749-a3fd218dedf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to make descriptives tables\n",
    "def make_descriptives_table(df, v, level):\n",
    "\n",
    "    df = categorize_df_gender_age(df)\n",
    "\n",
    "    gender_order = ['Female', 'Male', 'Mixed Gender']\n",
    "    age_order = ['Older', 'Younger', 'Mixed Age']\n",
    "    ivs_dict = {'Gender': gender_order, 'Age': age_order}\n",
    "\n",
    "    if level.title() == 'Job Advertisement':\n",
    "        level_df = df.groupby(['Job ID']).first()\n",
    "    elif level.title() == 'Sentence':\n",
    "        level_df = df\n",
    "    else:\n",
    "        raise Exception(f'Specified level {level} not in data.')\n",
    "\n",
    "    if v in list(ivs_dict.keys()):\n",
    "        cat_dict = ivs_dict\n",
    "        index = [\n",
    "            f'{v_cat}-dominated'\n",
    "            if 'Mixed' not in v_cat\n",
    "            else\n",
    "            f'{\"-\".join(v_cat.split())}'\n",
    "            for v_cat in cat_dict[v]\n",
    "        ]\n",
    "        caption = [\n",
    "            f'{v}_{v_cat.split()[0]}'\n",
    "            for v_cat in cat_dict[v]\n",
    "        ]\n",
    "\n",
    "    desc_dict = {\n",
    "        'Sectors': index,\n",
    "        'n': [\n",
    "            level_df[v].value_counts()[v_cat]\n",
    "            for v_cat in cat_dict[v]\n",
    "        ],\n",
    "        '%': [\n",
    "            level_df[v].value_counts(normalize=True).mul(100).round(2).astype(float)[v_cat]\n",
    "            for v_cat in cat_dict[v]\n",
    "        ],\n",
    "        'M': [\n",
    "            level_df[caption].mean().round(2).astype(float)[i]\n",
    "            for i in range(len(cat_dict[v]))\n",
    "        ],\n",
    "        'S.D.': [\n",
    "            level_df[caption].std().round(2).astype(float)[i]\n",
    "            for i in range(len(cat_dict[v]))\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Make DF from dict\n",
    "    df_desc = pd.DataFrame(desc_dict)\n",
    "    df_desc.set_index('Sectors', inplace=True)\n",
    "\n",
    "    return df_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a423cb0-71d4-4202-b10a-faacdb62b75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_multiindex_cols(v, level, data_type):\n",
    "\n",
    "    if v.title() == 'Gender':\n",
    "        cols = [\n",
    "            (f'{data_type.title()} Job Advertisements', v, level.title(), 'n'),\n",
    "            (f'{data_type.title()} Job Advertisements', v, level.title(), '%'),\n",
    "            (f'{data_type.title()} Job Advertisements', v, level.title(), 'M'),\n",
    "            (f'{data_type.title()} Job Advertisements', v, level.title(), 'S.D.'),\n",
    "        ]\n",
    "    elif v.title() == 'Age':\n",
    "        cols = [\n",
    "            (f'{data_type.title()} Job Advertisements', level.title(), 'n'),\n",
    "            (f'{data_type.title()} Job Advertisements', level.title(), '%'),\n",
    "            (f'{data_type.title()} Job Advertisements', level.title(), 'M'),\n",
    "            (f'{data_type.title()} Job Advertisements', level.title(), 'S.D.'),\n",
    "        ]\n",
    "\n",
    "    return cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4be3a-5317-48b4-b0c6-5546a8a6f37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc26566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_gender_dict = defaultdict(list)\n",
    "df_desc_age_dict = defaultdict(list)\n",
    "\n",
    "for v, level, data_type in itertools.product(\n",
    "    ['Gender', 'Age'],\n",
    "    ['Job Advertisement', 'Sentence'],\n",
    "    ['Manually Annotated', 'Collected']\n",
    "):\n",
    "    df_desc = make_descriptives_table(df=df_manual, v=v, level=level)\n",
    "\n",
    "    df_desc.columns = pd.MultiIndex.from_tuples(\n",
    "        make_multiindex_cols(\n",
    "            v=v, level=level, data_type=data_type\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if v == 'Gender':\n",
    "        df_desc_gender.join(df_desc, how='outer', lsuffix='_')\n",
    "        df_desc_gender_dict[data_type].append(df_desc)\n",
    "    elif v == 'Age':\n",
    "        df_desc_age.join(df_desc, how='outer', lsuffix='_')\n",
    "        df_desc_age_dict[data_type].append(df_desc, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61517c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in df_desc_gender_dict.items():\n",
    "    if v[0].columns[0][0] == v[1].columns[0][0] and v[0].columns[0][0]:\n",
    "        df_desc_gender = pd.merge(v[0], v[1], left_index=True, right_index=True)\n",
    "\n",
    "for k, v in df_desc_age_dict.items():\n",
    "    df_desc_age = pd.concat(v, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb76a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c72035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge DFs\n",
    "# df_desc_gender = pd.merge(df_desc_gender, df_desc_gender_sent, left_index=True, right_index=True)\n",
    "# df_desc_age = pd.merge(df_desc_age_job, df_desc_age_sent, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_gender_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3ca69-759e-4d9e-9506-4e3cb2e4ae18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make dfs for Warmth and Competence\n",
    "df_desc_warmth_job = make_descriptives_table(df_manual, 'Warmth', 'Job')\n",
    "df_desc_comp_job = make_descriptives_table(df_manual, 'Competence', 'Job')\n",
    "df_desc_warmth_sent = make_descriptives_table(df_manual, 'Warmth', 'Sentence')\n",
    "df_desc_comp_sent = make_descriptives_table(df_manual, 'Competence', 'Sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd4b7f-0500-42d9-a3f1-81bad474c02f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make dfs for Gender and Age\n",
    "df_desc_gender_job = make_descriptives_table(df_manual, 'Gender', 'Job')\n",
    "df_desc_age_job = make_descriptives_table(df_manual, 'Age', 'Job')\n",
    "df_desc_gender_sent = make_descriptives_table(df_manual, 'Gender', 'Sentence')\n",
    "df_desc_age_sent = make_descriptives_table(df_manual, 'Age', 'Sentence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c27dd-0d3c-43bc-90c6-02b4f6368c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make dfs into multiindex dfs\n",
    "df_desc_gender_job.columns = pd.MultiIndex.from_tuples(\n",
    "    make_multiindex_cols(\n",
    "        data_type='Manually Annotated', iv='Gender', data_structure='Job Advertisements'\n",
    "    )\n",
    ")\n",
    "df_desc_gender_sent.columns = pd.MultiIndex.from_tuples(\n",
    "    make_multiindex_cols(\n",
    "        data_type='Manually Annotated', iv='Gender', data_structure='Sentences'\n",
    "    )\n",
    ")\n",
    "df_desc_age_job.columns = pd.MultiIndex.from_tuples(\n",
    "    make_multiindex_cols(\n",
    "        data_type='Manually Annotated', iv='Age', data_structure='Job Advertisements'\n",
    "    )\n",
    ")\n",
    "df_desc_age_sent.columns = pd.MultiIndex.from_tuples(\n",
    "    make_multiindex_cols(\n",
    "        data_type='Manually Annotated', iv='Age', data_structure='Sentences'\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f93ed3-9c28-4eae-8b61-12d47d110757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DFs\n",
    "df_desc_gender = pd.merge(df_desc_gender_job, df_desc_gender_sent, left_index=True, right_index=True)\n",
    "df_desc_age = pd.merge(df_desc_age_job, df_desc_age_sent, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c735b-e185-4410-be49-a776ec311452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Tables\n",
    "# Gender\n",
    "df_desc_gender.to_csv(f'{table_save_path}Gender - Manually Annotated Job Advertisement Descriptives.csv', index=False)\n",
    "df_desc_gender.to_pickle(f'{table_save_path}Gender - Manually Annotated Job Advertisement Descriptives.pkl')\n",
    "with pd.option_context('max_colwidth', 10000000000):\n",
    "    df_desc_gender.to_latex(f'{table_save_path}Gender - Manually Annotated Job Advertisement Descriptives.tex', index=False, longtable=True, escape=True, multicolumn=True, multicolumn_format='c', position='H', caption='Sectoral Gender and Age Composition and Segregation, Keywords, Counts, and Percentages', label='Jobs Count per Sector (x 1000)')\n",
    "df_desc_gender.to_markdown(f'{table_save_path}Gender - Manually Annotated Job Advertisement Descriptives.md', index=True)\n",
    "# save_sector_excel(df_sectors_all, data_save_dir)\n",
    "\n",
    "# Age\n",
    "df_desc_age.to_csv(f'{table_save_path}Age - Manually Annotated Job Advertisement Descriptives.csv', index=False)\n",
    "df_desc_age.to_pickle(f'{table_save_path}Age - Manually Annotated Job Advertisement Descriptives.pkl')\n",
    "with pd.option_context('max_colwidth', 10000000000):\n",
    "    df_desc_age.to_latex(f'{table_save_path}Age - Manually Annotated Job Advertisement Descriptives.tex', index=False, longtable=True, escape=True, multicolumn=True, multicolumn_format='c', position='H', caption='Sectoral Gender and Age Composition and Segregation, Keywords, Counts, and Percentages', label='Jobs Count per Sector (x 1000)')\n",
    "df_desc_age.to_markdown(f'{table_save_path}Age - Manually Annotated Job Advertisement Descriptives.md', index=True)\n",
    "# save_sector_excel(df_sectors_all, data_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0bd55-1c3e-43bc-b3c7-6991aa595942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_ngrams_spacy.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bec2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0707c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcb6bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
