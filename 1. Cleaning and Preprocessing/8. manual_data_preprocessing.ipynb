{
	"cells": [
		{
			"cell_type": "markdown",
			"id": "a3d9bbd5",
			"metadata": {},
			"source": [
				"# ATTN: This script uses Google translate to detect job description language. Google translate will limit requests and take a very long time. Only run this script if redoing language detection."
			]
		},
		{
			"cell_type": "markdown",
			"id": "ac2e2d85",
			"metadata": {},
			"source": [
				"# Read from scrapped data"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 14,
			"id": "be8129d5",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 15,
			"id": "d2271ddb",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "a840ebcc",
			"metadata": {},
			"source": [
				"#### Read paths"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 16,
			"id": "999bbb1d",
			"metadata": {},
			"outputs": [],
			"source": [
				"glob_paths = list(set(glob.glob(f'{scraped_data}Coding Material/*Folder/*/Job ID -*- Codebook (Automating Equity).xlsx')))\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 17,
			"id": "4ad7c36f",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"244"
						]
					},
					"execution_count": 17,
					"metadata": {},
					"output_type": "execute_result"
				},
				{
					"ename": "",
					"evalue": "",
					"output_type": "error",
					"traceback": [
						"\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
					]
				}
			],
			"source": [
				"# 244 xlsx files\n",
				"len(glob_paths)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "fbfdd650",
			"metadata": {},
			"source": [
				"#### Use paths to open files, fix keywords, and drop unneeded columns"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 6,
			"id": "9f2edb5d",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"CPU times: user 5.63 s, sys: 153 ms, total: 5.78 s\n",
						"Wall time: 6.12 s\n"
					]
				}
			],
			"source": [
				"%%time\n",
				"# columns\n",
				"cols=['Sector', \n",
				"      'Sector Code', \n",
				"      'Gender', \n",
				"      'Age', \n",
				"      'Language', \n",
				"      'Dutch Requirement', \n",
				"      'English Requirement', \n",
				"      'Gender_Female', \n",
				"      'Gender_Mixed', \n",
				"      'Gender_Male', \n",
				"      'Age_Older', \n",
				"      'Age_Mixed', \n",
				"      'Age_Younger', \n",
				"      'Gender_Num', \n",
				"      'Age_Num', \n",
				"      '% Female', \n",
				"      '% Male', \n",
				"      '% Older', \n",
				"      '% Younger']\n",
				"\n",
				"# Fix list catches all incorrect/faculty keyword search terms\n",
				"fix_list = []\n",
				"\n",
				"# Appended data catches all the fixed and cleaned dfs\n",
				"appended_data = []\n",
				"\n",
				"for glob_path in glob_paths:\n",
				"\n",
				"    try:\n",
				"        df_temp = pd.read_excel(glob_path).reset_index(drop=True)\n",
				"    except ValueError:\n",
				"        fix_list.append(glob_path)\n",
				"\n",
				"    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
				"        df_temp.reset_index(drop=True, inplace=True)\n",
				"        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
				"        df_temp.drop(\n",
				"        df_temp.columns[\n",
				"                df_temp.columns.str.contains(\n",
				"                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
				"                )\n",
				"            ],\n",
				"            axis='columns',\n",
				"            inplace=True,\n",
				"            errors='ignore',\n",
				"        )\n",
				"\n",
				"        appended_data.append(df_temp.reset_index(drop=True))\n",
				"\n",
				"# Concatonate list of dfs into one large df_manual\n",
				"df_manual = pd.concat(appended_data, axis='index').reset_index(drop=True)\n",
				"\n",
				"# Save df_manual to file\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 7,
			"id": "5de12303",
			"metadata": {},
			"outputs": [],
			"source": [
				"# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
				"if len(fix_list) != 0:\n",
				"    print('Some keywords to fix!')\n",
				"    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
				"        json.dump(fix_list, f)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 8,
			"id": "ae1a9362",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"244"
						]
					},
					"execution_count": 8,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# List of dfs, len = 244\n",
				"len(appended_data)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 9,
			"id": "78c1c60f",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Concatonate list of dfs into one large df_manual\n",
				"df_manual = pd.concat(appended_data, axis='index').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 10,
			"id": "47b014ea",
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"12400"
						]
					},
					"execution_count": 10,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"# len = 12400\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0a4606db",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Save df_manual to file\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_raw.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_raw.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "3eb6e2dd",
			"metadata": {},
			"source": [
				"# Drop duplicated and missing data"
			]
		},
		{
			"cell_type": "markdown",
			"id": "bc049d41",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_RAW\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "eda9efcf",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4b7e42cc",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a080272c",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e92eeb12",
			"metadata": {},
			"outputs": [],
			"source": [
				"# len = 12400\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "78c31c26",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "82e2a312",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.columns"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "28d837cb",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Clean columns\n",
				"df_manual.columns = df_manual.columns.to_series().apply(lambda x: str(x).strip())\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "93c1a0a8",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Remove columns 'Task_Mentioned', 'Task_Warmth', 'Task_Competence'\n",
				"df_manual.drop(\n",
				"    columns=['Task_Mentioned', 'Task_Warmth', 'Task_Competence'],\n",
				"    axis='columns',\n",
				"    inplace=True,\n",
				"    errors='ignore'\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1c5a7d46",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Warmth'] = df_manual['Warmth'].astype(np.float64)\n",
				"df_manual['Competence'] = df_manual['Competence'].astype(np.float64)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a7666d35",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e57a9f32",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Rename Sentence to 'Job Description spacy_sentencized'\n",
				"df_manual.rename(\n",
				"    columns = {\n",
				"        'Sentence': 'Job Description spacy_sentencized'\n",
				"    },\n",
				"    inplace=True,\n",
				"    errors='ignore'\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1a32f64c",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.columns\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3fb59e5d",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Drop NA\n",
				"df_manual.dropna(axis='index', how='all', inplace=True)\n",
				"df_manual.dropna(axis='columns', how='all', inplace=True)\n",
				"df_manual.dropna(\n",
				"    subset = ['Job Description spacy_sentencized', 'Warmth', 'Competence'],\n",
				"    inplace=True\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "25218a8b",
			"metadata": {},
			"outputs": [],
			"source": [
				"# len = 12394\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "025e7b62",
			"metadata": {},
			"outputs": [],
			"source": [
				"# len = 133\n",
				"df_manual.groupby(['Job ID'])['Job ID'].unique()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e3126f47",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Drop duplicates on subset of 'Job ID' and 'Sentence'\n",
				"df_manual.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'], keep='first', ignore_index=True, inplace=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "67baaa73",
			"metadata": {},
			"outputs": [],
			"source": [
				"# len = 6400\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3ec0bc22",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Remove any rows with missing 'Job ID'\n",
				"df_manual.drop(\n",
				"    df_manual[\n",
				"        (df_manual['Job ID'].isin(nan_list)) | \n",
				"        (df_manual['Job ID'].isnull()) | \n",
				"        (df_manual['Job ID'].isna())\n",
				"    ].index, \n",
				"    axis='index',\n",
				"    inplace=True,\n",
				"    errors='ignore'\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "21a098a0",
			"metadata": {},
			"outputs": [],
			"source": [
				"# len = 6400\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "19e08303",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Save df_manual to file\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_raw_dropped.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "3bae0b99",
			"metadata": {},
			"source": [
				"# Add English and Dutch language requirement columns"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eb838f06",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_RAW_DROPPED\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "489a38aa",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f7cda531",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a13ef5b9",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_dropped.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3b5aa5a8",
			"metadata": {},
			"outputs": [],
			"source": [
				"# 6400\n",
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e2e2a0c6",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8418d404",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Add language requirement column\n",
				"# Use regex to find language requirement\n",
				"dutch_requirement_pattern = r'[Ll]anguage: [Dd]utch|[Dd]utch [Pp]referred|[Dd]utch [Re]quired|[Dd]utch [Ll]anguage|[Pp]roficient in [Dd]utch|[Ss]peak [Dd]utch|[Kk]now [Dd]utch'\n",
				"english_requirement_pattern = r'[Ll]anguage: [Ee]nglish|[Ee]nglish [Pp]referred|[Ee]nglish [Re]quired|[Ee]nglish [Ll]anguage|[Pp]roficient in [Ee]nglish|[Ss]peak [Ee]nglish|[Kk]now [Ee]nglish'\n",
				"\n",
				"lang_requirements = {\n",
				"    'Dutch Requirement': dutch_requirement_pattern, 'English Requirement': english_requirement_pattern\n",
				"}\n",
				"\n",
				"for lang_req, lang_req_pattern in lang_requirements.items():\n",
				"    \n",
				"    if lang_req in df_manual.columns:\n",
				"        df_manual.drop(columns=[lang_req], inplace=True)\n",
				"    df_manual[lang_req] = np.where(\n",
				"        df_manual['Job Description'].str.contains(lang_req_pattern),\n",
				"        'Yes',\n",
				"        'No',\n",
				"    )\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_raw_english_requirement.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "33b1e721",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Yes = 235\n",
				"df_manual['Dutch Requirement'].value_counts()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6330c696",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Yes = 526\n",
				"df_manual['English Requirement'].value_counts()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5c3ddedd",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_raw_language_requirement.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e79cea97",
			"metadata": {},
			"source": [
				"# Add data from Sectors dataframe (see CBS directory under scrapped_data directory) and Categorical data\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "69513116",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_RAW_LANGUAGE_REQUIREMENT\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "10b32849",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "875fd370",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6873d7c6",
			"metadata": {},
			"outputs": [],
			"source": [
				"def df_gender_age_info(df, ivs_all=None):\n",
				"    if ivs_all is None:\n",
				"        ivs_all = [\n",
				"            'Gender',\n",
				"            'Gender_Num',\n",
				"            'Gender_Female',\n",
				"            'Gender_Mixed',\n",
				"            'Gender_Male',\n",
				"            'Age',\n",
				"            'Age_Num',\n",
				"            'Age_Older',\n",
				"            'Age_Mixed',\n",
				"            'Age_Younger',\n",
				"        ]\n",
				"    # Print Info\n",
				"    print('\\nDF INFO:\\n')\n",
				"    df.info()\n",
				"\n",
				"    for iv in ivs_all:\n",
				"        try:\n",
				"            counts = df[f\"{iv}\"].value_counts()\n",
				"            percentages = df[f\"{iv}\"].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
				"            print('='*20)\n",
				"            print(f'{iv}:')\n",
				"            print('-'*20)\n",
				"            print(f'{iv} Counts:\\n{counts}')\n",
				"            print('-'*20)\n",
				"            print(f'{iv} Percentages:\\n{percentages}')\n",
				"\n",
				"            with contextlib.suppress(Exception):\n",
				"                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
				"                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
				"                print('-'*20)\n",
				"                print(f'{iv} Mean: {mean}')\n",
				"                print('-'*20)\n",
				"                print(f'{iv} Standard Deviation: {sd}')\n",
				"\n",
				"        except Exception:\n",
				"            print(f'{iv} not available.')\n",
				"\n",
				"    print('\\n')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "79fe759f",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_raw_language_requirement.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "40156303",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "db503aad",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job ID'] = df_manual['Job ID'].apply(lambda x: str(x).lower().strip())\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "98694f16",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_including_sector_genage_data.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8d5116df",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs.info()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "49391453",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs['Job ID'] = df_jobs['Job ID'].apply(lambda x: str(x).lower().strip())\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f5aa6dcf",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs.columns\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "82178d30",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs.drop(\n",
				"    columns = [\n",
				"        'Job Description', 'Rating', 'Employment Type',\n",
				"        'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
				"        'Collection Date', 'Data Row', 'Tracking ID', 'Job Date',\n",
				"        'Type of ownership', 'Language', 'Dutch Requirement', 'English Requirement', \n",
				"    ],\n",
				"    inplace=True,\n",
				"    errors='ignore'\n",
				")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9bba117b",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_jobs.columns\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f11e0e01",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Add sector and categorical data from df_jobs\n",
				"df_manual = df_manual.merge(df_jobs, on='Job ID', how='inner')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "47f24060",
			"metadata": {},
			"outputs": [],
			"source": [
				"len(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4fa2c4a7",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "40fa3bc0",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.head()"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1d889ec9",
			"metadata": {},
			"source": [
				"#### Check if there is any missing sector data in the merged dataframe"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "43ddadb7",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Sector'].isna().sum()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "cb5b9064",
			"metadata": {},
			"outputs": [],
			"source": [
				"if df_manual['Sector'].isna().sum() != 0:\n",
				"    print('Some search keywords did not match a sector. Fixing')\n",
				"    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
				"    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n",
				"    df_manual = fix_keywords(df_manual)\n",
				"    print(set(df_manual['Search Keyword'].loc[df_manual['Sector'].isna()].to_list()))\n",
				"    print(len(df_manual['Search Keyword'].loc[df_manual['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d0d24ae2",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Manual Job Ad info\n",
				"df_gender_age_info(df_manual.groupby(['Job ID']).first())\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d4d65547",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Manual Job Sentence info\n",
				"df_gender_age_info(df_manual)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d1d8a085",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Manual Job Sentence info\n",
				"df_gender_age_info(df_manual, ivs_all=['Warmth', 'Competence'])\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f599e9cd",
			"metadata": {},
			"outputs": [],
			"source": [
				"if df_manual['Sector'].isna().sum() == 0:\n",
				"    assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"    df_manual.to_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl')\n",
				"    df_manual.to_csv(f'{df_save_dir}df_manual_including_sector_genage_data.csv', index=False)\n",
				"else:\n",
				"    print(f\"MISSING SECTOR DATA: COUNT {df_manual['Sector'].isna().sum()}\")"
			]
		},
		{
			"cell_type": "markdown",
			"id": "6373c116",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER spacy sentence splitting is completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "993064a6",
			"metadata": {},
			"source": [
				"# Use spacy to tokenize sentences\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "c744a550",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_SENTENCIZED\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b997d96c",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "390017aa",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4c3b98f1",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_word_num_and_frequency(row, text_col):\n",
				"\n",
				"    row['Job Description num_words'] = len(str(row[f'{text_col}']).split())\n",
				"    row['Job Description num_unique_words'] = len(set(str(row[f'{text_col}']).split()))\n",
				"    row['Job Description num_chars'] = len(str(row[f'{text_col}']))\n",
				"    row['Job Description num_punctuations'] = len([c for c in str(row[f'{text_col}']) if c in string.punctuation])\n",
				"\n",
				"    return row\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3d31a583",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_including_sector_genage_data.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bc21643d",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description spacy_sentencized_lower'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda job_sentence: job_sentence.strip().lower()\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1b8dac3e",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual[['Job Description spacy_sentencized', 'Job Description spacy_sentencized_lower']].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1cf56055",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Spacy tokenize\n",
				"with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
				"    custom_punct_chars = pickle.load(f)\n",
				"\n",
				"df_manual['Job Description spacy_tokenized'] = df_manual[\n",
				"    'Job Description spacy_sentencized'\n",
				"].apply(\n",
				"    lambda job_sentence: [\n",
				"        str(token.text.strip().lower())\n",
				"        for token in nlp.tokenizer(job_sentence)\n",
				"        if len(token) != 0\n",
				"        and not token.is_space\n",
				"        and not token.is_stop\n",
				"        and not token.is_punct\n",
				"        and not token.is_bracket\n",
				"        and not token.like_email\n",
				"        and token.text not in custom_punct_chars\n",
				"    ]\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bd443d94",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description spacy_sentencized_cleaned'] = df_manual['Job Description spacy_tokenized'].str.join(' ')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "65043f92",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Get sentence word frequencies\n",
				"df_manual = df_manual.apply(\n",
				"    lambda row: get_word_num_and_frequency(\n",
				"        row=row, text_col='Job Description spacy_sentencized'\n",
				"    ), \n",
				"    axis='columns',\n",
				"    \n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f2acfe7f",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.columns\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5deddb9d",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e7f002d2",
			"metadata": {},
			"source": [
				"# Use NLTK to tokenize sentences\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "92922872",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9fae46f4",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3d4f11d1",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5011ea0a",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9c8a9f1e",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4475248f",
			"metadata": {},
			"outputs": [],
			"source": [
				"%%time\n",
				"# Tokenize with NLTK\n",
				"df_manual['Job Description nltk_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda job_sentence: [\n",
				"        str(token.strip().lower()) \n",
				"        for token in word_tokenize(job_sentence) \n",
				"        if len(token) != 0 \n",
				"        and token != '...' \n",
				"        and not token.lower() in set(stopwords.words('english')) \n",
				"        and not token.lower() in list(string.punctuation) \n",
				"    ]\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3bbd5d3f",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description nltk_tokenized'].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "86da6772",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "69e6640b",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "81e54149",
			"metadata": {},
			"source": [
				"# Use gensim to tokenize sentences\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "ff307dba",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "3f89cb65",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ac2ae121",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c13d88fb",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bc48f290",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e95fec77",
			"metadata": {},
			"outputs": [],
			"source": [
				"%%time\n",
				"df_manual['Job Description gensim_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda sentence: preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1cc29198",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description gensim_tokenized'].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0a69a8de",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a62be70e",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "eeb0131f",
			"metadata": {},
			"source": [
				"# Use BERT to tokenize sentences\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "efcf30fc",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c81e981f",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6f6ae091",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ec899945",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d42a3b60",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f8bc6622",
			"metadata": {},
			"outputs": [],
			"source": [
				"%%time\n",
				"df_manual['Job Description bert_encodings'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda sentence: bert_tokenizer(str(sentence), truncation=True, padding=True)\n",
				")\n",
				"\n",
				"df_manual['Job Description bert_tokenized'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda sentence: bert_tokenizer.tokenize(str(sentence))\n",
				")\n",
				"\n",
				"df_manual['Job Description bert_tokenized_to_id'] = df_manual['Job Description bert_tokenized'].apply(\n",
				"    lambda sentence: bert_tokenizer.convert_tokens_to_ids(str(sentence))\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1d996bc4",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description bert_encodings'].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9ae92181",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description bert_tokenized'].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5f7a4ae3",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description bert_tokenized_to_id'].head()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8936c818",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "6c0bfad5",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all tokenization (spacy, nltk, gensim, and BERT) completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e4a62460",
			"metadata": {},
			"source": [
				"# Use spacy to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "374d2178",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TOKENIZED_SPACY_NLTK_GENSIM_BERT\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "c74f96a0",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0053cd25",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "10424739",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tokenized_spacy_nltk_gensim_bert.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "08c5b9fb",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "45e45b2c",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Load customer characters\n",
				"with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
				"    custom_punct_chars = pickle.load(f)\n",
				"\n",
				"# POS tagging\n",
				"df_manual['Job Description spacy_token_tags'] = df_manual[\n",
				"    'Job Description spacy_sentencized'\n",
				"].apply(\n",
				"    lambda job_sentence: [\n",
				"        (token.text.strip().lower(), token.tag_) for token in nlp(job_sentence)\n",
				"    ]\n",
				")\n",
				"\n",
				"# Lemmatization\n",
				"df_manual['Job Description spacy_lemmas'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda job_sentence: [\n",
				"        token.lemma_.strip().lower()\n",
				"        for token in nlp(job_sentence)\n",
				"        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
				"    ]\n",
				")\n",
				"\n",
				"# Stemming\n",
				"df_manual['Job Description spacy_stems'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda job_sentence: [\n",
				"        stemmer.stem(token.text.strip().lower())\n",
				"        for token in nlp(job_sentence)\n",
				"        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
				"    ]\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9d8332f9",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5d8d37b3",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual[\n",
				"    [\n",
				"        'Job Description spacy_token_tags',\n",
				"        'Job Description spacy_lemmas',\n",
				"        'Job Description spacy_stems'\n",
				"    ]\n",
				"].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "fa7acdc1",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "8c74665a",
			"metadata": {},
			"source": [
				"# Use NLTK to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "12fe6cb2",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7c765e8d",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d114a298",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ac327ec6",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_wordnet_pos(token):\n",
				"    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
				"    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
				"    tag_dict = {\"J\": wordnet.ADJ,\n",
				"                \"N\": wordnet.NOUN,\n",
				"                \"V\": wordnet.VERB,\n",
				"                \"R\": wordnet.ADV}\n",
				"\n",
				"    return tag_dict.get(tag, wordnet.NOUN)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0b5e476b",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6f5c1c44",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "92fb117a",
			"metadata": {},
			"outputs": [],
			"source": [
				"# POS stagging\n",
				"df_manual['Job Description nltk_token_tags'] = df_manual['Job Description spacy_tokenized'].apply(\n",
				"    lambda token: pos_tag(token)\n",
				")\n",
				"\n",
				"# Lemmatization\n",
				"df_manual['Job Description nltk_lemmas'] = df_manual['Job Description spacy_tokenized'].apply(\n",
				"    lambda tokens: [\n",
				"        lemmatizer.lemmatize(\n",
				"            token, get_wordnet_pos(\n",
				"                unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
				"            )\n",
				"        )\n",
				"        for token in tokens\n",
				"    ]\n",
				")\n",
				"\n",
				"# Stemming\n",
				"df_manual['Job Description nltk_stems'] = df_manual['Job Description spacy_tokenized'].apply(\n",
				"    lambda tokens: [\n",
				"        stemmer.stem(\n",
				"            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
				"        )\n",
				"        for token in tokens\n",
				"    ]\n",
				")\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "2893388c",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.info()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "84e2af7a",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual[['Job Description nltk_token_tags', 'Job Description nltk_lemmas', 'Job Description nltk_stems']].head()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "102509c1",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e5f98098",
			"metadata": {},
			"source": [
				"# Use BERT to create Parts-Of-Speech (POS) tags, lemmas, and stems\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "03dd84df",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "cfc8511b",
			"metadata": {},
			"outputs": [],
			"source": [
				"# import os\n",
				"# import sys\n",
				"# import importlib\n",
				"# from pathlib import Path\n",
				"# import numpy as np\n",
				"\n",
				"# mod = sys.modules[__name__]\n",
				"\n",
				"# code_dir = None\n",
				"# code_dir_name = 'Code'\n",
				"# unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"# for _ in range(5):\n",
				"\n",
				"#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"#         code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"#         if code_dir is not None:\n",
				"#             break\n",
				"\n",
				"# sys.path.append(code_dir)\n",
				"# # %load_ext autoreload\n",
				"# # %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ec559046",
			"metadata": {},
			"outputs": [],
			"source": [
				"# from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "81972d25",
			"metadata": {},
			"outputs": [],
			"source": [
				"# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e0a48ebd",
			"metadata": {},
			"outputs": [],
			"source": [
				"# %%time\n",
				"# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
				"# bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
				"# bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n",
				"\n",
				"# df_manual['Job Description bert_token_tags_with_scores'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"#     lambda sentence: [\n",
				"#         (bert_pos_tag['word'], bert_pos_tag['entity'], bert_pos_tag['score'])\n",
				"#         for i in range(len(sentence.split()))\n",
				"#         for bert_pos_tag in bert_pos_tagger(sentence)\n",
				"#     ]\n",
				"# )\n",
				"\n",
				"# df_manual['Job Description bert_token_tags'] = df_manual['Job Description bert_token_tags_with_scores'].apply(\n",
				"#     lambda tag_list: [\n",
				"#         [(tag_list[i][0], tag_list[i][1])]\n",
				"#         for tag_tuple in tag_list\n",
				"#         for i in range(len(tag_list))\n",
				"#     ]\n",
				"# )\n",
				"\n",
				"\n",
				"# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
				"# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f4e99e0d",
			"metadata": {},
			"outputs": [],
			"source": [
				"# df_manual['Job Description bert_token_tags'].head()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "0f7e50a4",
			"metadata": {},
			"outputs": [],
			"source": [
				"# assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"# df_manual.to_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.pkl')\n",
				"# df_manual.to_csv(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk_bert.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1122d883",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy and nltk) completed.\n",
				"# If BERT POS tagging was done, change pkl file loading\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "206a80dd",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK\n",
				"### IF BERT POS TAGGING WAS DONE, SOURCING FROM df_manual_TAGS_LEMMAS_STEMS_SPACY_NLTK_BERT\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e54cb736",
			"metadata": {},
			"source": [
				"# Use spacy to create bi and trigrams\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "980b3608",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "520614c1",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "405cfdc3",
			"metadata": {},
			"outputs": [],
			"source": [
				"def spacy_make_ngrams(sentence, matcher, gram_type):\n",
				"\n",
				"    doc = nlp(sentence)\n",
				"    matches = matcher(doc)\n",
				"    matches_list = []\n",
				"\n",
				"    for idx in range(len(matches)):\n",
				"        for match_id, start, end in matches:\n",
				"            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
				"                match = doc[matches[idx][1]: matches[idx][2]].text\n",
				"                matches_list.append(match.lower())\n",
				"    \n",
				"    return list(set(matches_list))\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b6d7a1b4",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ae54c422",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description spacy_1grams_original_list'] = df_manual['Job Description spacy_tokenized']\n",
				"df_manual['Job Description spacy_1grams'] = df_manual['Job Description spacy_tokenized'].apply(\n",
				"    lambda tokens: [\n",
				"        tuple(token.split())\n",
				"        for token in tokens\n",
				"    ]\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1a0c8ffe",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Spacy bi and trigrams\n",
				"matcher = Matcher(nlp.vocab)\n",
				"\n",
				"bigram_rules = [\n",
				"    ['NOUN', 'VERB'],\n",
				"    ['VERB', 'NOUN'],\n",
				"    ['ADJ', 'NOUN'],\n",
				"    ['ADJ', 'PROPN'],\n",
				"    # more rules here...\n",
				"]\n",
				"\n",
				"trigram_rules = [\n",
				"    ['VERB', 'ADJ', 'NOUN'],\n",
				"    ['NOUN', 'VERB', 'ADV'],\n",
				"    ['NOUN', 'ADP', 'NOUN'],\n",
				"    # more rules here...\n",
				"]\n",
				"\n",
				"patters_dict = {\n",
				"    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
				"    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
				"}\n",
				"\n",
				"ngram_dict = {\n",
				"    'bigram': 2,\n",
				"    'trigram': 3,\n",
				"}\n",
				"\n",
				"for ngram_name, ngram_num in ngram_dict.items():\n",
				"    \n",
				"    \n",
				"    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
				"\n",
				"    df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"        lambda sentence: \n",
				"            [\n",
				"                '_'.join(ngram_.split())\n",
				"                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
				"            ]\n",
				"    )\n",
				"    \n",
				"    df_manual[f'Job Description spacy_{str(ngram_num)}grams'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"        lambda sentence: \n",
				"            [\n",
				"                tuple(ngram_.split())\n",
				"                for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
				"            ]\n",
				"    )\n",
				"\n",
				"    df_manual[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
				"        regex = {\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				"    \n",
				"    if f'{ngram_name}_patterns' in matcher:\n",
				"        matcher.remove(f'{ngram_name}_patterns')\n",
				"    assert f'{ngram_name}_patterns' not in matcher\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "9fc3ad40",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Spacy Allgrams\n",
				"df_manual['Job Description spacy_123grams_original_list'] = df_manual['Job Description spacy_tokenized'] + df_manual['Job Description spacy_2grams_original_list'] + df_manual['Job Description spacy_3grams_original_list']\n",
				"df_manual['Job Description spacy_123grams'] = df_manual['Job Description spacy_1grams'] + df_manual['Job Description spacy_2grams'] + df_manual['Job Description spacy_3grams']\n",
				"df_manual['Job Description spacy_123grams_in_sent'] = (\n",
				"    df_manual['Job Description spacy_sentencized']\n",
				"    .str.lower()\n",
				"    .replace(\n",
				"        regex={\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[\n",
				"                'Job Description spacy_123grams_original_list'\n",
				"            ]\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d68848b6",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "d258e491",
			"metadata": {},
			"source": [
				"# Use NLTK to create bi and trigrams\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "0e767712",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "45f3b2f6",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "10cde291",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a61db031",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "2afc29c7",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description nltk_1grams_original_list'] = df_manual['Job Description nltk_tokenized']\n",
				"df_manual['Job Description nltk_1grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
				"    lambda tokens: [\n",
				"        tuple(token.split())\n",
				"        for token in tokens\n",
				"    ]\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "6c5e1032",
			"metadata": {},
			"outputs": [],
			"source": [
				"# NLTK bi and trigrams\n",
				"ngram_dict = {\n",
				"    'bigram': 2,\n",
				"    'trigram': 3\n",
				"}\n",
				"\n",
				"for ngram_name, ngram_num in ngram_dict.items():\n",
				"\n",
				"    df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_manual['Job Description nltk_tokenized'].apply(\n",
				"        lambda tokens:\n",
				"            list(\n",
				"                '_'.join(ngram_list)\n",
				"                for ngram_list in ngrams(tokens, ngram_num)\n",
				"            )\n",
				"    )\n",
				"\n",
				"    df_manual[f'Job Description nltk_{str(ngram_num)}grams'] = df_manual['Job Description nltk_tokenized'].apply(\n",
				"        lambda tokens: list(ngrams(tokens, ngram_num))\n",
				"    )\n",
				"\n",
				"    df_manual[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_manual['Job Description spacy_sentencized'].str.lower().replace(\n",
				"        regex = {\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "32f3ae0a",
			"metadata": {},
			"outputs": [],
			"source": [
				"# NLTK Allgrams\n",
				"df_manual['Job Description nltk_123grams_original_list'] = (\n",
				"    df_manual['Job Description nltk_tokenized']\n",
				"    + df_manual['Job Description nltk_2grams_original_list']\n",
				"    + df_manual['Job Description nltk_3grams_original_list']\n",
				")\n",
				"df_manual['Job Description nltk_123grams'] = (\n",
				"    df_manual['Job Description nltk_1grams']\n",
				"    + df_manual['Job Description nltk_2grams']\n",
				"    + df_manual['Job Description nltk_3grams']\n",
				")\n",
				"df_manual['Job Description nltk_123grams_in_sent'] = (\n",
				"    df_manual['Job Description spacy_sentencized']\n",
				"    .str.lower()\n",
				"    .replace(\n",
				"        regex={\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[\n",
				"                'Job Description nltk_123grams_original_list'\n",
				"            ]\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "17610cd5",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1440bd34",
			"metadata": {},
			"source": [
				"# Use Gensim to create bi and trigrams\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "0494636f",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "dad74074",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "067dda8f",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8c9c33c2",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "59d29aaf",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual['Job Description gensim_1grams_original_list'] = df_manual['Job Description gensim_tokenized']\n",
				"df_manual['Job Description gensim_1grams'] = df_manual['Job Description gensim_tokenized'].apply(\n",
				"    lambda tokens: [\n",
				"        tuple(token.split())\n",
				"        for token in tokens\n",
				"    ]\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "becdbf23",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Gensim bi and trigrams\n",
				"pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
				"\n",
				"# Gensim Bigrams\n",
				"bigram = Phraser(Phrases(df_manual['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
				"df_manual['Job Description gensim_2grams_original_list_all'] = bigram[df_manual['Job Description gensim_tokenized']]\n",
				"df_manual['Job Description gensim_2grams_original_list'] = df_manual['Job Description gensim_2grams_original_list_all'].apply(\n",
				"    lambda ngrams_list: [\n",
				"        ngram_\n",
				"        for ngram_ in ngrams_list\n",
				"        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
				"    ]\n",
				")\n",
				"df_manual['Job Description gensim_2grams'] = df_manual['Job Description gensim_2grams_original_list'].apply(\n",
				"    lambda ngrams: [\n",
				"        tuple(ngram.split('_'))\n",
				"        for ngram in ngrams\n",
				"        if '_' in ngram\n",
				"    ]\n",
				")\n",
				"df_manual['Job Description gensim_2grams_in_sent'] = (\n",
				"    df_manual['Job Description spacy_sentencized']\n",
				"    .str.lower()\n",
				"    .apply(\n",
				"        lambda sentence: ' '.join(\n",
				"            preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
				"        )\n",
				"    )\n",
				"    .replace(\n",
				"        regex={\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[\n",
				"                'Job Description gensim_2grams_original_list'\n",
				"            ]\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				")\n",
				"\n",
				"# Gensim Trigrams\n",
				"trigram = Phraser(Phrases(df_manual['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
				"df_manual['Job Description gensim_3grams_original_list_all'] = trigram[df_manual['Job Description gensim_2grams_original_list_all']]\n",
				"df_manual['Job Description gensim_3grams_original_list'] = df_manual['Job Description gensim_3grams_original_list_all'].apply(\n",
				"    lambda ngrams_list: [\n",
				"        ngram_\n",
				"        for ngram_ in ngrams_list\n",
				"        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
				"    ]\n",
				")\n",
				"df_manual['Job Description gensim_3grams'] = df_manual['Job Description gensim_3grams_original_list'].apply(\n",
				"    lambda ngrams: [\n",
				"        tuple(ngram.split('_'))\n",
				"        for ngram in ngrams\n",
				"        if '_' in ngram\n",
				"    ]\n",
				")\n",
				"df_manual['Job Description gensim_3grams_in_sent'] = (\n",
				"    df_manual['Job Description spacy_sentencized']\n",
				"    .str.lower()\n",
				"    .apply(\n",
				"        lambda sentence: ' '.join(\n",
				"            preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
				"        )\n",
				"    )\n",
				"    .replace(\n",
				"        regex={\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[\n",
				"                'Job Description gensim_3grams_original_list'\n",
				"            ]\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "84d89c0a",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Gensim Allgrams\n",
				"pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
				"\n",
				"df_manual['Job Description gensim_123grams_original_list'] = (\n",
				"    df_manual['Job Description gensim_tokenized']\n",
				"    + df_manual['Job Description gensim_2grams_original_list']\n",
				"    + df_manual['Job Description gensim_3grams_original_list']\n",
				")\n",
				"df_manual['Job Description gensim_123grams'] = (\n",
				"    df_manual['Job Description gensim_1grams']\n",
				"    + df_manual['Job Description gensim_2grams']\n",
				"    + df_manual['Job Description gensim_3grams']\n",
				")\n",
				"df_manual['Job Description gensim_123grams_in_sent'] = (\n",
				"    df_manual['Job Description spacy_sentencized']\n",
				"    .str.lower()\n",
				"    .apply(\n",
				"        lambda sentence: ' '.join(\n",
				"            preprocess_string(re.sub(pattern, ' ', sentence.strip().lower()))\n",
				"        )\n",
				"    )\n",
				"    .replace(\n",
				"        regex={\n",
				"            re.escape(' '.join(ngram_.split('_'))): re.escape(ngram_)\n",
				"            for ngrams_list in df_manual[\n",
				"                'Job Description gensim_123grams_original_list'\n",
				"            ]\n",
				"            for ngram_ in ngrams_list\n",
				"            if '_' in ngram_\n",
				"        }\n",
				"    )\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "17e46331",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "6ac58f92",
			"metadata": {},
			"source": [
				"# Create word frequencies for uni, bi, and trigrams\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "7c33d9ab",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_NGRAMS_SPACY_NLTK_GENSIM\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "735bed8e",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "024dc14f",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8731dee4",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_abs_frequency(row, text_col, ngram_num, embedding_library):\n",
				"\n",
				"    abs_word_freq = defaultdict(int)\n",
				"    for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
				"        abs_word_freq[word] += 1\n",
				"\n",
				"        abs_wtd_df = (\n",
				"            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
				"            .rename(columns={0: 'abs_word_freq'})\n",
				"            .sort_values(by=['abs_word_freq'], ascending=False)\n",
				"            )\n",
				"        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
				"        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
				"\n",
				"        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
				"        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
				"        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
				"\n",
				"    return row\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8e4a92a0",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_spacy_nltk_gensim.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "43206134",
			"metadata": {},
			"outputs": [],
			"source": [
				"ngrams_list=[1, 2, 3, 123]\n",
				"embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
				"\n",
				"for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
				"    df_manual = df_manual.apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "30affd16",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_frequency.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1670f255",
			"metadata": {},
			"source": [
				"# Create BoW dictionary, corpus, and tfidf matrix for uni, bi, and trigrams\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "16989aa0",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_NGRAMS_FREQUENCY\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "051454b4",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e09d527a",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "57359916",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
				"    \n",
				"    ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
				"    dictionary = Dictionary([ngrams_original_list])\n",
				"    BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
				"    tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
				"    tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
				"\n",
				"    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
				"    row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
				"    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
				"    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
				"    \n",
				"    return row\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "30b00a00",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ba6a2c99",
			"metadata": {},
			"outputs": [],
			"source": [
				"ngrams_list=[1, 2, 3, 123]\n",
				"embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
				"\n",
				"for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
				"    df_manual = df_manual.apply(\n",
				"        lambda row: get_corpus_and_dictionary(\n",
				"            row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
				"        ),\n",
				"        axis='columns'\n",
				"    )\n",
				"\n",
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_frequency.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b51eae79",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual.columns\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a03d8769",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_ngrams_BoW.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "ec31f120",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all bi and trigrams (spacy, nltk, and gensim) completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "c976d079",
			"metadata": {},
			"source": [
				"# Use spacy and nltk for sentiment scoring\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "d61138e8",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_NGRAMS_BOW\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "47bdc9a2",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d1bc37fc",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "25a35056",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_ngrams_BoW.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "795034b1",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Spacy sentiment\n",
				"if 'spacytextblob' not in nlp.pipe_names:\n",
				"    nlp.add_pipe('spacytextblob')\n",
				"\n",
				"df_manual['Job Description spacy_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
				"    if isinstance(sentence, str) else np.nan\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "58173ce5",
			"metadata": {},
			"outputs": [],
			"source": [
				"# NLTK sentiment\n",
				"df_manual['Job Description nltk_sentiment'] = df_manual['Job Description spacy_sentencized'].apply(\n",
				"    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
				"    if isinstance(sentence, str) else np.nan\n",
				")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "ec80fe6a",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_sentiment_spacy_nltk.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "9db5cf4e",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all sentiment scoring (spacy and nltk) completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "baad73af",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_SENTIMENT_SPACY_NLTK\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "48122d98",
			"metadata": {},
			"source": [
				"# Word2Vec and FastText embeddings\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "57105477",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "fb13e58e",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "91117606",
			"metadata": {},
			"outputs": [],
			"source": [
				"def build_train_word2vec(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
				"    if words is None:\n",
				"        words = [\n",
				"            'she',\n",
				"            'he',\n",
				"            'support',\n",
				"            'leader',\n",
				"            'management',\n",
				"            'team',\n",
				"            'business',\n",
				"            'customer',\n",
				"            'risk',\n",
				"            'build',\n",
				"            'computer',\n",
				"            'programmer',\n",
				"        ]\n",
				"    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
				"\n",
				"    w2v_model = Word2Vec(\n",
				"        sentences=sentences,\n",
				"        vector_size=size,\n",
				"        min_count=0,\n",
				"        window=2,\n",
				"        sample=6e-5,\n",
				"        alpha=0.03,\n",
				"        min_alpha=0.0007,\n",
				"        negative=20,\n",
				"        workers=cores - 1,\n",
				"        sg = 1,\n",
				"    )\n",
				"\n",
				"    w2v_model.build_vocab(sentences, progress_per=10000)\n",
				"    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
				"\n",
				"    w2v_model.train(\n",
				"        sentences,\n",
				"        total_examples=w2v_model.corpus_count,\n",
				"        epochs=30,\n",
				"        report_delay=1,\n",
				"    )\n",
				"\n",
				"    print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
				"    w2v_vocab = list(w2v_model.wv.index_to_key)\n",
				"\n",
				"    print(f'Checking words form list of length {len(words)}')\n",
				"    print(f'WORDS LIST: {words}')\n",
				"\n",
				"    for word in words:\n",
				"        print(f'Checking word:\\n{word.upper()}:')\n",
				"        try:\n",
				"#             print(f'Word2Vec {size}: {w2v_model.wv[word]}')\n",
				"            print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
				"            print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
				"            print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
				"\n",
				"        except KeyError as e:\n",
				"            print(e)\n",
				"\n",
				"    return w2v_vocab, w2v_model\n",
				"\n",
				"def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
				"\n",
				"    sentences = [word for word in sentences if word in w2v_vocab]\n",
				"\n",
				"    return (\n",
				"        np.mean(w2v_model.wv[sentences], axis=0)\n",
				"        if sentences\n",
				"        else np.zeros(size)\n",
				"    )\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "69dc70f3",
			"metadata": {},
			"outputs": [],
			"source": [
				"def build_train_fasttext(df, ngram_number, embedding_library, size = 300, words=None, t = time.time(), cores = multiprocessing.cpu_count()):\n",
				"    if words is None:\n",
				"        words = [\n",
				"            'she',\n",
				"            'he',\n",
				"            'support',\n",
				"            'leader',\n",
				"            'management',\n",
				"            'team',\n",
				"            'business',\n",
				"            'customer',\n",
				"            'risk',\n",
				"            'build',\n",
				"            'computer',\n",
				"            'programmer',\n",
				"        ]\n",
				"    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
				"\n",
				"    ft_model = FastText(\n",
				"        sentences=sentences,\n",
				"        vector_size=size,\n",
				"        min_count=0,\n",
				"        window=2,\n",
				"        sample=6e-5,\n",
				"        alpha=0.03,\n",
				"        min_alpha=0.0007,\n",
				"        negative=20,\n",
				"        workers=cores - 1,\n",
				"        sg = 1,\n",
				"    )\n",
				"\n",
				"    ft_model.build_vocab(sentences, progress_per=10000)\n",
				"    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
				"\n",
				"    ft_model.train(\n",
				"        sentences,\n",
				"        total_examples=ft_model.corpus_count,\n",
				"        epochs=30,\n",
				"        report_delay=1,\n",
				"    )\n",
				"\n",
				"    print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
				"    ft_vocab = list(ft_model.wv.index_to_key)\n",
				"\n",
				"    print(f'Checking words form list of length {len(words)}')\n",
				"    print(f'WORDS LIST: {words}')\n",
				"\n",
				"    for word in words:\n",
				"        print(f'Checking word:\\n{word.upper()}:')\n",
				"        try:\n",
				"#             print(f'FastText {size}: {ft_model_300.wv[word]}')\n",
				"            print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
				"            print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
				"            print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
				"\n",
				"        except KeyError as e:\n",
				"            print(e)\n",
				"\n",
				"    return ft_vocab, ft_model\n",
				"\n",
				"def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
				"\n",
				"    sentences = [word for word in sentences if word in ft_vocab]\n",
				"\n",
				"    return np.mean(ft_model.wv[sentences], axis=0) if sentences else np.zeros(size)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "b6d43d45",
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
				"    embeddings_index = {}\n",
				"    with open(glove_file, 'r', encoding='utf8') as glove:\n",
				"\n",
				"        for line in glove:\n",
				"            values = line.split()\n",
				"            word = values[0]\n",
				"\n",
				"            with contextlib.suppress(ValueError):\n",
				"                coefs = np.asarray(values[1:], dtype='float32')\n",
				"                embeddings_index[word] = coefs\n",
				"    print(f'Found {len(embeddings_index)} word vectors.')\n",
				"\n",
				"    return embeddings_index\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1022f815",
			"metadata": {},
			"outputs": [],
			"source": [
				"def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
				"\n",
				"    if external_glove is False and embeddings_index is None:\n",
				"        embeddings_index= get_glove()\n",
				"\n",
				"    if extra_preprocessing_enabled is False:\n",
				"        words = sentences\n",
				"\n",
				"    elif extra_preprocessing_enabled is True:\n",
				"        stop_words = set(sw.words('english'))\n",
				"        words = str(sentences).lower()\n",
				"        words = word_tokenize(words)\n",
				"        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
				"\n",
				"    M = []\n",
				"\n",
				"    try:\n",
				"        for w in words:\n",
				"            try:\n",
				"                M.append(embeddings_index[w])\n",
				"            except Exception:\n",
				"                continue\n",
				"\n",
				"        M = np.array(M)\n",
				"        v = M.sum(axis='index')\n",
				"        return np.zeros(300) if type(v) != np.ndarray else v / np.sqrt((v ** 2).sum())\n",
				"\n",
				"    except Exception:\n",
				"        return np.zeros(300)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "a512f2b4",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_sentiment_spacy_nltk.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5c0675fd",
			"metadata": {},
			"outputs": [],
			"source": [
				"embedding_models_dict = {\n",
				"    'w2v': [build_train_word2vec, word2vec_embeddings, Word2Vec],\n",
				"    'ft': [build_train_fasttext, fasttext_embeddings, FastText],\n",
				"}\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bd6f55d3",
			"metadata": {},
			"outputs": [],
			"source": [
				"ngrams_list=[1, 2, 3, 123]\n",
				"embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
				"\n",
				"for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
				"    print(f'Building {embedding_library}_{ngram_number}grams model and vocabulary.')\n",
				"\n",
				"    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
				"\n",
				"        build_train_func, embed_func, model_loader = embed_func_list\n",
				"        print(f'Building {embed_model_name} from {embed_func.__name__} function.')\n",
				"\n",
				"        vocab, model = build_train_func(\n",
				"            df=df_manual,\n",
				"            ngram_number=ngram_number,\n",
				"            embedding_library=embedding_library,\n",
				"        )\n",
				"\n",
				"        print(f'Getting {embed_model_name} embeddings.')\n",
				"\n",
				"        df_manual[\n",
				"            f'Job Description {embedding_library}_{ngram_number}grams_mean_{embed_model_name}_embeddings'\n",
				"        ] = df_manual[\n",
				"            f'Job Description {embedding_library}_{ngram_number}grams_original_list'\n",
				"        ].apply(\n",
				"            lambda sentences: embed_func(sentences, vocab, model)\n",
				"        )\n",
				"        model.save(f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model')\n",
				"\n",
				"    # Sent2Vec\n",
				"    print('Getting sent2vec embeddings.')\n",
				"    embeddings_index = get_glove()\n",
				"    df_manual[f'Job Description {embedding_library}_{ngram_number}grams_sent2vec_embeddings'] = df_manual[f'Job Description {embedding_library}_{ngram_number}grams'].apply(lambda sentences: sent2vec(sentences, embeddings_index=embeddings_index, external_glove=True, extra_preprocessing_enabled=False))\n",
				"    print('Done getting sent2vec embeddings.')\n",
				"        \n",
				"    "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f1cd59ac",
			"metadata": {},
			"outputs": [],
			"source": [
				"assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame)\n",
				"df_manual.to_pickle(f'{df_save_dir}df_manual_for_trainning.pkl')\n",
				"df_manual.to_csv(f'{df_save_dir}df_manual_for_trainning.csv', index=False)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "923f8888",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all embeddings are completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "1795b68f",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_FOR_TRAINNING\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "79862f16",
			"metadata": {},
			"source": [
				"# Descriptives and visualization\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1245d864",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e1bee517",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7fb174c3",
			"metadata": {},
			"outputs": [],
			"source": [
				"def df_gender_age_info(df, ivs_all=None):\n",
				"    if ivs_all is None:\n",
				"        ivs_all = [\n",
				"            'Gender',\n",
				"            'Gender_Num',\n",
				"            'Gender_Female',\n",
				"            'Gender_Mixed',\n",
				"            'Gender_Male',\n",
				"            'Age',\n",
				"            'Age_Num',\n",
				"            'Age_Older',\n",
				"            'Age_Mixed',\n",
				"            'Age_Younger',\n",
				"        ]\n",
				"    # Print Info\n",
				"    print('\\nDF INFO:\\n')\n",
				"    df.info()\n",
				"\n",
				"    for iv in ivs_all:\n",
				"        try:\n",
				"            counts = df[f'{iv}'].value_counts()\n",
				"            percentages = df[f'{iv}'].value_counts(normalize=True).mul(100).round(1).astype(float)\n",
				"            print('='*20)\n",
				"            print(f'{iv}:')\n",
				"            print('-'*20)\n",
				"            print(f'{iv} Counts:\\n{counts}')\n",
				"            print('-'*20)\n",
				"            print(f'{iv} Percentages:\\n{percentages}')\n",
				"\n",
				"            with contextlib.suppress(Exception):\n",
				"                mean = df[f\"{iv}\"].mean().round(2).astype(float)\n",
				"                sd = df[f\"{iv}\"].std().round(2).astype(float)\n",
				"                print('-'*20)\n",
				"                print(f'{iv} Mean: {mean}')\n",
				"                print('-'*20)\n",
				"                print(f'{iv} Standard Deviation: {sd}')\n",
				"\n",
				"        except Exception:\n",
				"            print(f'{iv} not available.')\n",
				"\n",
				"    print('\\n')\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "245dd780",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to order categories\n",
				"def categorize_df_gender_age(df, gender_order=None, age_order=None, ivs=None):\n",
				"    if gender_order is None:\n",
				"        gender_order = ['Female', 'Mixed Gender', 'Male']\n",
				"    if age_order is None:\n",
				"        age_order = ['Older', 'Mixed Age', 'Younger']\n",
				"    if ivs is None:\n",
				"        ivs = ['Gender', 'Age']\n",
				"    # Arrange Categories\n",
				"    for iv in ivs:\n",
				"        if iv == 'Gender':\n",
				"            order = gender_order\n",
				"        elif iv == 'Age':\n",
				"            order = age_order\n",
				"        try:\n",
				"            df[iv] = df[iv].astype('category').cat.reorder_categories(order, ordered=True)\n",
				"\n",
				"            df[iv] = pd.Categorical(\n",
				"                df[iv], categories=order, ordered=True\n",
				"            )\n",
				"            df[f'{iv}_Num'] = pd.to_numeric(df[iv].cat.codes).astype('int64')\n",
				"        except ValueError as e:\n",
				"            print(e)\n",
				"\n",
				"    return df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5ff3d20a",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "681db815-a531-4ace-a8a3-cb957945bde1",
			"metadata": {
				"tags": []
			},
			"outputs": [],
			"source": [
				"# All info\n",
				"analysis_columns = [\n",
				"    'Warmth',\n",
				"    'Competence'\n",
				"]\n",
				"\n",
				"df_manual = categorize_df_gender_age(df_manual)\n",
				"\n",
				"df_manual.info()\n",
				"\n",
				"# Gender and Age info by job ad\n",
				"print('='*30)\n",
				"print('Gender and Age info at Sentence Level')\n",
				"print('-'*30)\n",
				"df_gender_age_info(df_manual)\n",
				"\n",
				"# Gender and Age info by job ad\n",
				"print('='*30)\n",
				"print('Gender and Age info at Job Advertisement Level')\n",
				"print('-'*30)\n",
				"df_gender_age_info(df_manual.groupby(['Job ID']).first())\n",
				"\n",
				"# Warmth and Competence info by job ad\n",
				"print('='*30)\n",
				"print('Warmth and Competence info at Sentence Level')\n",
				"print('-'*30)\n",
				"df_gender_age_info(df_manual, ivs_all = analysis_columns)\n",
				"\n",
				"# Warmth and Competence info by job ad\n",
				"print('='*30)\n",
				"print('Warmth and Competence info at Job Advertisement Level')\n",
				"print('-'*30)\n",
				"df_gender_age_info(df_manual.groupby(['Job ID']).first(), ivs_all = analysis_columns)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "7dcfd6de-970d-43c0-b29f-5e61cae2bdb8",
			"metadata": {
				"tags": []
			},
			"outputs": [],
			"source": [
				"# Imbalance Ratio\n",
				"warmth_imbalance_ratio = df_manual['Warmth'].loc[\n",
				"    df_manual['Warmth'] == 1].count()/df_manual['Warmth'].loc[df_manual['Warmth'] == 0\n",
				"].count()\n",
				"competence_imbalance_ratio = df_manual['Competence'].loc[\n",
				"    df_manual['Competence'] == 1].count()/df_manual['Competence'].loc[df_manual['Competence'] == 0\n",
				"].count()\n",
				"\n",
				"all_imbalance_ratio_dict = {\n",
				"    'Warmth': warmth_imbalance_ratio,\n",
				"    'Competence': competence_imbalance_ratio\n",
				"}\n",
				"\n",
				"print('='*20)\n",
				"print('Imabalance Ratios')\n",
				"print('-'*10)\n",
				"print(f'Warmth IR: {warmth_imbalance_ratio:.2f}')\n",
				"print(f'Competence IR: {competence_imbalance_ratio:.2f}')\n",
				"print('='*20)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1e139202-9f4b-4a83-8164-d13ca74dff98",
			"metadata": {
				"tags": []
			},
			"outputs": [],
			"source": [
				"# Ploting Warmth and Competence\n",
				"df_warm_comp_transposed = pd.concat(\n",
				"    [\n",
				"        df_manual['Warmth'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
				"        df_manual['Competence'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T,\n",
				"    ]\n",
				")\n",
				"\n",
				"fig, ax = plt.subplots()\n",
				"ax.set_title('Training Dataset: Warmth and Competence Sentence Percentages')\n",
				"ax.set_xlabel('Manually Annotated Sentencence Warmth and Competence Percentage from Total')\n",
				"\n",
				"df_warm_comp_transposed.plot(\n",
				"    kind='barh', legend=True, stacked=True, ax=ax, color=['C5', 'C0'],\n",
				")\n",
				"ax.legend(['Absent', 'Present'])\n",
				"\n",
				"for container in ax.containers:\n",
				"    labels = [f'{width:.1f}%' for v in container if float(width:= v.get_width())]\n",
				"    ax.bar_label(container, labels=labels, label_type='center', color='white')\n",
				"    ax.set_xlabel('% Job Ad Sentences')\n",
				"\n",
				"for i, tick_label in enumerate(ax.get_ymajorticklabels()):\n",
				"    ax.annotate(\n",
				"        f'IR for {tick_label.get_text()} = {all_imbalance_ratio_dict[tick_label.get_text()]:.2f}',\n",
				"        xy=(48, 0.3+i), ha='center', va='center'\n",
				"    )\n",
				"\n",
				"for save_format in ['eps', 'png']:\n",
				"    fig.savefig(\n",
				"        f'{plot_save_path}Manually Annotated Warmth and Competence Sentences.{save_format}',\n",
				"        format=save_format, dpi=3000, bbox_inches='tight'\n",
				"    )\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "42c70b2b-8d54-40b1-930c-62b853a3219e",
			"metadata": {
				"scrolled": false,
				"tags": []
			},
			"outputs": [],
			"source": [
				"# Ploting Gender and Age\n",
				"df_gender_transposed = df_manual['Gender'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T\n",
				"df_age_transposed = df_manual['Age'].value_counts(normalize=True).mul(100).round(2).astype(float).to_frame().T\n",
				"\n",
				"fig, axs = plt.subplots(1, 2)\n",
				"fig.suptitle('Training Dataset: Gender and Age Sentence Percentages')\n",
				"\n",
				"df_gender_transposed.plot(\n",
				"    kind='bar', legend=True, stacked=True, ax=axs[0], color=['C5', 'C2', 'C0']\n",
				")\n",
				"df_age_transposed.plot(\n",
				"    kind='bar', legend=True, stacked=True, ax=axs[1], color=['C5', 'C2', 'C0']\n",
				")\n",
				"\n",
				"for ax in axs:\n",
				"    for container in ax.containers:\n",
				"        labels = [f'{height:.1f}%' for v in container if float(height:= v.get_height())]\n",
				"        ax.bar_label(container, labels=labels, label_type='center', color='white')\n",
				"        ax.legend(loc='upper right', fontsize=8)\n",
				"        ax.set_ylabel('% Job Ad Sentences')\n",
				"\n",
				"for save_format in ['eps', 'png']:\n",
				"    fig.savefig(\n",
				"        f'{plot_save_path}Manually Annotated Gender and Age Sentences.{save_format}',\n",
				"        format=save_format, dpi=3000, bbox_inches='tight'\n",
				"    )\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1f7d3db3",
			"metadata": {},
			"outputs": [],
			"source": [
				"def make_barplot(df, iv, dvs_list):\n",
				"    fig, ax = plt.subplots()\n",
				"    fig.suptitle(f'Training Dataset: Warmth and Competence Sentence Percentages per {iv}')\n",
				"\n",
				"    vars_list = [iv]\n",
				"    vars_list.extend(dvs_list)\n",
				"\n",
				"    df_pivot = df[\n",
				"        vars_list\n",
				"    ].pivot_table(\n",
				"        index=iv, values=dvs_list, fill_value=0, aggfunc=lambda x: (100*x.sum())/len(df)\n",
				"    )\n",
				"\n",
				"    df_pivot.sort_values(by=iv, ascending=False).plot(kind='barh', legend=True, stacked=True, ax=ax, color=['C0', 'C5'])\n",
				"\n",
				"    for _ in ax.containers:\n",
				"        ax.set_xlabel('% Job Ad Sentences')\n",
				"        ax.set_ylabel(iv)\n",
				"\n",
				"    for save_format in ['eps', 'png']:\n",
				"        fig.savefig(\n",
				"            f'{plot_save_path}Barplot - Manually Annotated {iv} x {dvs_list[0]} and {dvs_list[1]} Sentences.{save_format}',\n",
				"            format=save_format, dpi=3000, bbox_inches='tight'\n",
				"        )"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8392efd5",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Make stacked barplots\n",
				"ivs_list = ['Gender', 'Age', 'Sector']\n",
				"\n",
				"for iv in ivs_list:\n",
				"    make_barplot(df_manual, iv=iv, dvs_list=['Warmth', 'Competence'])\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "8bc39735",
			"metadata": {},
			"outputs": [],
			"source": [
				"def make_lineplot(df, iv, dv):\n",
				"\n",
				"    df = categorize_df_gender_age(df)\n",
				"\n",
				"    title = f'Means of {dv}-related frames in job ads from {iv} segregated sectors'\n",
				"    data = df.groupby(iv)[dv].agg('mean')\n",
				"\n",
				"    line_plot = sns.lineplot(\n",
				"        data=data, marker='o', legend='full'\n",
				"    )\n",
				"    line_plot.set(\n",
				"        title=title,\n",
				"        xlabel=f'{iv}-dominated Sector',\n",
				"        ylabel=f'{dv} Mean',\n",
				"    )\n",
				"    fig = line_plot.get_figure()\n",
				"\n",
				"    for save_format in ['eps', 'png']:\n",
				"        fig.savefig(\n",
				"            f'{plot_save_path}Line Plot - Manually Annotated {iv} x {dv} Sentences.{save_format}',\n",
				"            format=save_format, dpi=3000, bbox_inches='tight'\n",
				"        )\n",
				"    plt.show()\n",
				"    plt.clf()\n",
				"    plt.cla()\n",
				"    plt.close()\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "cf9b62d5",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Make line plots\n",
				"for iv, dv in itertools.product(['Gender', 'Age'], ['Warmth', 'Competence']):\n",
				"    make_lineplot(df_manual, iv=iv, dv=dv)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "5ae28ed1-d6af-4f68-b871-b2ddb3844ab3",
			"metadata": {},
			"source": [
				"# ATTN: This script should be run AFTER all visualizations are completed.\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "26485ee2-7c93-4dfb-9a57-2a5c73b83e17",
			"metadata": {},
			"source": [
				"### START HERE IF SOURCING FROM df_manual_FOR_TRAINNING\n",
				"### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "a4e43151-4bcf-424c-9230-44b7db260f21",
			"metadata": {},
			"source": [
				"# Make descriptive tables\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "eabd45b6-d549-47cf-aadf-cee03947eda7",
			"metadata": {},
			"outputs": [],
			"source": [
				"import os\n",
				"import sys\n",
				"import importlib\n",
				"from pathlib import Path\n",
				"import numpy as np\n",
				"\n",
				"mod = sys.modules[__name__]\n",
				"\n",
				"code_dir = None\n",
				"code_dir_name = 'Code'\n",
				"unwanted_subdir_name = 'Analysis'\n",
				"\n",
				"for _ in range(5):\n",
				"\n",
				"    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
				"\n",
				"    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
				"\n",
				"        code_dir = str(Path.cwd().parents[_])\n",
				"\n",
				"        if code_dir is not None:\n",
				"            break\n",
				"\n",
				"sys.path.append(code_dir)\n",
				"# %load_ext autoreload\n",
				"# %autoreload 2\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "4c2a4ac8",
			"metadata": {},
			"outputs": [],
			"source": [
				"from setup_module.imports import *\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "f77c49f9",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Function to order categories\n",
				"def categorize_df_gender_age(df, gender_order=None, age_order=None, ivs=None):\n",
				"    if gender_order is None:\n",
				"        gender_order = ['Female', 'Mixed Gender', 'Male']\n",
				"    if age_order is None:\n",
				"        age_order = ['Older', 'Mixed Age', 'Younger']\n",
				"    if ivs is None:\n",
				"        ivs = ['Gender', 'Age']\n",
				"\n",
				"    # Arrange Categories\n",
				"    for iv in ivs:\n",
				"        if iv == 'Gender':\n",
				"            order = gender_order\n",
				"        elif iv == 'Age':\n",
				"            order = age_order\n",
				"        try:\n",
				"            df[iv] = df[iv].astype('category').cat.reorder_categories(order, ordered=True)\n",
				"\n",
				"            df[iv] = pd.Categorical(\n",
				"                df[iv], categories=order, ordered=True\n",
				"            )\n",
				"            df[f'{iv}_Num'] = pd.to_numeric(df[iv].cat.codes).astype('int64')\n",
				"        except ValueError as e:\n",
				"            print(e)\n",
				"\n",
				"    return df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "e34e7473",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "e5f5f69c",
			"metadata": {},
			"source": [
				"### Gender and Age tables"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "aa6d1b15-886f-4891-a749-a3fd218dedf3",
			"metadata": {
				"tags": []
			},
			"outputs": [],
			"source": [
				"# Function to make descriptives tables\n",
				"def genage_make_descriptives_table(df, v, level, gender_order=None, age_order=None):\n",
				"\n",
				"    if gender_order is None:\n",
				"        gender_order = ['Female', 'Male', 'Mixed Gender']\n",
				"    if age_order is None:\n",
				"        age_order = ['Older', 'Younger', 'Mixed Age']\n",
				"    \n",
				"    ivs_dict = {'Gender': gender_order, 'Age': age_order}\n",
				"\n",
				"    df = categorize_df_gender_age(df)\n",
				"\n",
				"    if level.title() == 'Job Advertisement':\n",
				"        level_df = df.groupby(['Job ID']).first()\n",
				"    elif level.title() == 'Sentence':\n",
				"        level_df = df\n",
				"    else:\n",
				"        raise Exception(f'Specified level {level} not in data.')\n",
				"\n",
				"    if v in list(ivs_dict.keys()):\n",
				"        cat_dict = ivs_dict\n",
				"        index = [\n",
				"            f'{v_cat}-dominated'\n",
				"            if 'Mixed' not in v_cat\n",
				"            else\n",
				"            f'{\"-\".join(v_cat.split())}'\n",
				"            for v_cat in cat_dict[v]\n",
				"        ]\n",
				"        caption = [\n",
				"            f'{v}_{v_cat.split()[0]}'\n",
				"            for v_cat in cat_dict[v]\n",
				"        ]\n",
				"\n",
				"    desc_dict = {\n",
				"        'Sectors': index,\n",
				"        'n': [\n",
				"            level_df[v].value_counts()[v_cat]\n",
				"            for v_cat in cat_dict[v]\n",
				"        ],\n",
				"        '%': [\n",
				"            level_df[v].value_counts(normalize=True).mul(100).round(2).astype(float)[v_cat]\n",
				"            for v_cat in cat_dict[v]\n",
				"        ],\n",
				"        'M': [\n",
				"            level_df[caption].mean().round(2).astype(float)[i]\n",
				"            for i in range(len(cat_dict[v]))\n",
				"        ],\n",
				"        'S.D.': [\n",
				"            level_df[caption].std().round(2).astype(float)[i]\n",
				"            for i in range(len(cat_dict[v]))\n",
				"        ]\n",
				"    }\n",
				"\n",
				"    # Make DF from dict\n",
				"    df_desc = pd.DataFrame(desc_dict)\n",
				"    df_desc.set_index('Sectors', inplace=True)\n",
				"\n",
				"    return df_desc\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "bbc26566",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_desc_gender_dict = defaultdict(list)\n",
				"df_desc_age_dict = defaultdict(list)\n",
				"\n",
				"variables = ['Gender', 'Age']\n",
				"levels = ['Job Advertisement', 'Sentence']\n",
				"data_types = ['Manually Annotated', 'Collected']\n",
				"\n",
				"for v, level, data_type in itertools.product(\n",
				"    variables, levels, data_types\n",
				"):\n",
				"    cols = [\n",
				"        (f'{data_type.title()} Job Advertisements', v, level.title(), 'n'),\n",
				"        (f'{data_type.title()} Job Advertisements', v, level.title(), '%'),\n",
				"        (f'{data_type.title()} Job Advertisements', v, level.title(), 'M'),\n",
				"        (f'{data_type.title()} Job Advertisements', v, level.title(), 'S.D.'),\n",
				"    ]\n",
				"\n",
				"    df_desc_genage = genage_make_descriptives_table(df=df_manual, v=v, level=level)\n",
				"\n",
				"    df_desc_genage.columns = pd.MultiIndex.from_tuples(cols)\n",
				"\n",
				"    if v == 'Gender':\n",
				"        df_desc_gender_dict[data_type].append(df_desc_genage)\n",
				"    elif v == 'Age':\n",
				"        df_desc_age_dict[data_type].append(df_desc_genage)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "2de4dfd6",
			"metadata": {},
			"outputs": [],
			"source": [
				"df_desc_genage_dict = defaultdict(list)\n",
				"\n",
				"for df, data_type in itertools.product(\n",
				"    [df_desc_gender_dict, df_desc_age_dict],\n",
				"    data_types,\n",
				"):\n",
				"    category = df[data_type][0].columns.get_level_values(level=1)[0]\n",
				"    if df[data_type][0].columns.get_level_values(level=2).str.contains('Job Advertisement').all():\n",
				"        df_desc_genage_dict[category].append(\n",
				"            pd.concat(\n",
				"                [\n",
				"                    df[data_type][0], df[data_type][1]\n",
				"                ],\n",
				"                axis='columns'\n",
				"            )\n",
				"        )\n",
				"    else:\n",
				"        df_desc_genage_dict[category].append(\n",
				"            pd.concat(\n",
				"                [\n",
				"                    df[data_type][1], df[data_type][0]\n",
				"                ],\n",
				"                axis='columns'\n",
				"            )\n",
				"        )\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "5a4f7b15",
			"metadata": {},
			"outputs": [],
			"source": [
				"for cat, df in df_desc_genage_dict.items():\n",
				"    df = pd.concat([df[0], df[1]], axis='columns')\n",
				"\n",
				"    if cat == 'Gender':\n",
				"        df_desc_gender = df\n",
				"    elif cat == 'Age':\n",
				"        df_desc_age = df\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "cd8c735b-e185-4410-be49-a776ec311452",
			"metadata": {
				"tags": []
			},
			"outputs": [],
			"source": [
				"# Save Tables\n",
				"# Gender\n",
				"df_desc_gender.to_csv(f'{table_save_path}Gender - Job Advertisement Descriptives.csv', index=True)\n",
				"df_desc_gender.to_pickle(f'{table_save_path}Gender - Job Advertisement Descriptives.pkl')\n",
				"with pd.option_context('max_colwidth', 10000000000):\n",
				"    df_desc_gender.style.to_latex(f'{table_save_path}Gender - Job Advertisement Descriptives.tex', index=True, longtable=True, escape=True, multicolumn=True, multicolumn_format='c', position='H', caption='Number and proportion of job advertisements sample, and resulting sentences for gender and age homogeneous and heterogeneous sectors', label='Descriptives')\n",
				"df_desc_gender.to_markdown(f'{table_save_path}Gender - Job Advertisement Descriptives.md', index=True)\n",
				"# save_sector_excel(df_sectors_all, data_save_dir)\n",
				"\n",
				"# Age\n",
				"df_desc_age.to_csv(f'{table_save_path}Age - Job Advertisement Descriptives.csv', index=True)\n",
				"df_desc_age.to_pickle(f'{table_save_path}Age - Job Advertisement Descriptives.pkl')\n",
				"with pd.option_context('max_colwidth', 10000000000):\n",
				"    df_desc_age.style.to_latex(f'{table_save_path}Age - Job Advertisement Descriptives.tex', index=True, longtable=True, escape=True, multicolumn=True, multicolumn_format='c', position='H', caption='Number and proportion of job advertisements sample, and resulting sentences for gender and age homogeneous and heterogeneous sectors', label='Descriptives')\n",
				"df_desc_age.to_markdown(f'{table_save_path}Age - Job Advertisement Descriptives.md', index=True)\n",
				"# save_sector_excel(df_sectors_all, data_save_dir)\n"
			]
		},
		{
			"cell_type": "markdown",
			"id": "adf8e715",
			"metadata": {
				"tags": []
			},
			"source": [
				"### Warmth and Competence tables\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "1a3b4caa",
			"metadata": {},
			"outputs": [],
			"source": [
				"variables = ['Warmth', 'Competence']\n",
				"\n",
				"cols = [\n",
				"    ('Manually Annotated Job Advertisements', 'n'),\n",
				"    ('Manually Annotated Job Advertisements', '%'),\n",
				"    ('Manually Annotated Job Advertisements', 'M'),\n",
				"    ('Manually Annotated Job Advertisements', 'S.D.'),\n",
				"]\n",
				"\n",
				"binary_order = [0, 1]\n",
				"cat_dict = {'Warmth': binary_order, 'Competence': binary_order}\n",
				"\n",
				"desc_warmcomp_dict = {\n",
				"    'Frames': list(cat_dict.keys()),\n",
				"    'n': [\n",
				"        df_manual[v].value_counts()[1]\n",
				"        for v in variables\n",
				"    ],\n",
				"    '%': [\n",
				"        df_manual[v].value_counts(normalize=True).mul(100).round(2).astype(float)[1]\n",
				"        for v in variables\n",
				"    ],\n",
				"    'M': list(\n",
				"        df_manual[variables].mean().round(2).astype(float)\n",
				"    ),\n",
				"    'S.D.': list(\n",
				"        df_manual[variables].std().round(2).astype(float)\n",
				"    )\n",
				"}\n",
				"    \n",
				"# Make df_manual from dict\n",
				"df_desc_warmcomp = pd.DataFrame(desc_warmcomp_dict)\n",
				"df_desc_warmcomp.set_index('Frames', inplace=True)\n",
				"df_desc_warmcomp.columns = pd.MultiIndex.from_tuples(cols)\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"id": "d3b6a031",
			"metadata": {},
			"outputs": [],
			"source": [
				"# Save Tables\n",
				"df_desc_warmcomp.to_csv(f'{table_save_path}Warmth and Competence - Job Advertisement Descriptives.csv', index=True)\n",
				"df_desc_warmcomp.to_pickle(f'{table_save_path}Warmth and Competence - Job Advertisement Descriptives.pkl')\n",
				"with pd.option_context('max_colwidth', 10000000000):\n",
				"    df_desc_warmcomp.style.to_latex(f'{table_save_path}Warmth and Competence - Job Advertisement Descriptives.tex', index=True, longtable=True, escape=True, multicolumn=True, multicolumn_format='c', position='H', caption='Number and proportion of job advertisements sample, and resulting sentences for gender and age homogeneous and heterogeneous sectors', label='Descriptives')\n",
				"df_desc_warmcomp.to_markdown(f'{table_save_path}Warmth and Competence - Job Advertisement Descriptives.md', index=True)\n",
				"# save_sector_excel(df_sectors_all, data_save_dir)\n"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "study1_3.10",
			"language": "python",
			"name": "study1_3.10"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.9"
		},
		"varInspector": {
			"cols": {
				"lenName": 16,
				"lenType": 16,
				"lenVar": 40
			},
			"kernels_config": {
				"python": {
					"delete_cmd_postfix": "",
					"delete_cmd_prefix": "del ",
					"library": "var_list.py",
					"varRefreshCmd": "print(var_dic_list())"
				},
				"r": {
					"delete_cmd_postfix": ") ",
					"delete_cmd_prefix": "rm(",
					"library": "var_list.r",
					"varRefreshCmd": "cat(var_dic_list()) "
				}
			},
			"types_to_exclude": [
				"module",
				"function",
				"builtin_function_or_method",
				"instance",
				"_Feature"
			],
			"window_display": false
		},
		"widgets": {
			"application/vnd.jupyter.widget-state+json": {
				"state": {},
				"version_major": 2,
				"version_minor": 0
			}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
