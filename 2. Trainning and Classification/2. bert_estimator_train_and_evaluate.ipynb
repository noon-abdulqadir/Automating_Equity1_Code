{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799bb61385fe42e68dfdb37c2e9bef4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12f586d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "\n",
    "# Sklearn\n",
    "method = 'Supervised'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc',\n",
    "          'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('PuBu_r')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# BERT variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0631dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7489ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = [0,1], y = y_train)\n",
    "    class_weights_ratio = class_weights[0]/class_weights[1]\n",
    "    class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679eb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, encoded):\n",
    "        self.encodings = encodings\n",
    "        self.encoded = encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded[idx], device=device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    ) = split_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {bert_tokenizer.__class__.__name__}.from_pretrained using {bert_tokenizer.name_or_path}.')\n",
    "\n",
    "    bert_label2id = {label: id_ for id_, label in enumerate(set(label for label in y_train))}\n",
    "    bert_id2label = {id_: label for label, id_ in bert_label2id.items()}\n",
    "\n",
    "    X_train_bert_encodings = bert_tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_train_bert_encoded = [bert_label2id[y] for y in y_train]\n",
    "    bert_train_dataset = MyDataset(X_train_bert_encodings, y_train_bert_encoded)\n",
    "\n",
    "    X_test_bert_encodings = bert_tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_test_bert_encoded = [bert_label2id[y] for y in y_test]\n",
    "    bert_test_dataset = MyDataset(X_test_bert_encodings, y_test_bert_encoded)\n",
    "\n",
    "    X_val_bert_encodings = bert_tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_val_bert_encoded = [bert_label2id[y] for y in y_val]\n",
    "    bert_val_dataset = MyDataset(X_val_bert_encodings, y_val_bert_encoded)\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set BERT encodings example:\\n{\" \".join(bert_train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Training labels after BERT encoding: {set(y_train_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set BERT encodings example:\\n{\" \".join(bert_test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing labels after BERT encoding: {set(y_test_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set BERT encodings example:\\n{\" \".join(bert_val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation labels after BERT encoding: {set(y_val_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Class weights:\\nRatio = {class_weights_ratio:.2f} (0 = {class_weights[0]:.2f}, 1 = {class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "        test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "        val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "        bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    col, vectorizer_name, classifier_name, y_test, y_test_pred,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = [1, 0]\n",
    "\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_test, y_test_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    col, vectorizer_name, classifier_name, y_test, y_test_pred, pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    # Using y_pred_prob\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{str(col)} - {plot_name} {vectorizer_name} + {classifier_name}')\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        plot_.plot(ax=ax)\n",
    "        show_and_close_plots()\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Save Plots\n",
    "        print(f'Saving {plot_name}...')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            fig.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - {plot_name} {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "                format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric.functions.BinaryClassification(\n",
    "        y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # Figures\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True)\n",
    "    plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    show_and_close_plots()\n",
    "    bc.print_report()\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        fig.savefig(\n",
    "            f'{plot_save_path}{method} plot_metric Curves {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots()\n",
    "    classifications_dict = defaultdict(int)\n",
    "    if _y_test != _y_test_pred:\n",
    "        for _y_test, _y_test_pred in zip(y_test, y_test_pred):\n",
    "            classifications_dict[(_y_test, _y_test_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {str(col)} value': _y_test,\n",
    "            f'Predicted {str(col)} value': _y_test_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_test, _y_test_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {str(col)} value', \n",
    "        columns=f'Predicted {str(col)} value', \n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap='Purples')    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    show_and_close_plots()\n",
    "\n",
    "    return (\n",
    "        cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    col, vectorizer_name, classifier_name, y_test, y_test_pred_prob, pos_label=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    average_precision = metrics.average_precision_score(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold,loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results, \n",
    "    with_y_pred=None,\n",
    "    with_y_pred_prob=None\n",
    "):\n",
    "\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = False\n",
    "    \n",
    "    # Get y_test_pred\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "    y_test_pred = y_test_pred.flatten().tolist()\n",
    "    y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "\n",
    "    # # Get y_test_pred_prob\n",
    "    # try:\n",
    "    #     y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "    #     print('Using torch.nn.functional.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "    #     print('Using scipy.special.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "    #     print('Using predicted_results.predictions[:, 1]')\n",
    "    # finally:\n",
    "    #     y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "\n",
    "    # Get metrics\n",
    "    print('-'*20)\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        print('Computing metrics using y_test_pred.')\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            col, vectorizer_name, classifier_name, y_test, y_test_pred\n",
    "        )\n",
    "    # Using y_test_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        print('Computing metrics using y_test_pred_prob.')\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            col, vectorizer_name, classifier_name, y_test, y_test_pred_prob\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # 'Mean Cross Validation Train Score': float(cv_train_scores),\n",
    "        # 'Mean Cross Validation Test Score': float(cv_test_scores),\n",
    "        # f'Mean Cross Validation Train - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Mean Cross Validation Test - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Mean Explained Train Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # f'Mean Explained Test Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        # 'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        # 'ROC': float(roc_auc),\n",
    "        # 'AUC': float(auc),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # f'{scoring.title()} Best Score': float(best_score),\n",
    "        # 'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d3da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    col, vectorizer_name, classifier_name, estimator,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = False\n",
    "\n",
    "    # Plotting\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        (\n",
    "            cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve\n",
    "        ) = plot_metrics_with_y_pred(\n",
    "            y_test, y_test_pred\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(metrics_dict, df_metrics, col, tokenizer_name, classifier_name):\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for test_metric_name, metric_value in metrics_dict.items():\n",
    "        if test_metric_name not in ['test_runtime', 'test_samples_per_second', 'test_steps_per_second']:\n",
    "            metric_name = test_metric_name.split(\"test_\")[1].replace('_', ' ').title()\n",
    "            if isinstance(metric_name, float):\n",
    "                print(f'{metric_name}: {round(float(metric_value), 2)}')\n",
    "            else:\n",
    "                print(f'{metric_name}: {metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "            # Fill Table DF\n",
    "            if isinstance(metric_value, float):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, tokenizer_name, metric_name)\n",
    "                ] = float(metric_value)\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, tokenizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        col, tokenizer_name, classifier_name, estimator,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob\n",
    "    )\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34e44624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col, curves_dict=None, cmap=plt.cm.Blues):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        'Calibration Curve': metrics.CalibrationDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{str(curve_name)}: {str(col)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{str(curve_name)}: {str(col)}')\n",
    "        for estimator in estimators_list:\n",
    "            curve = curve_package.from_estimator(\n",
    "                estimator, X_test, y_test, pos_label=1, ax=ax,\n",
    "                name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        print('Saving plots.')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            curve.figure_.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - All {str(curve_name)}s.{image_save_format}',\n",
    "                format=image_save_format,\n",
    "                dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29efbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(f'{models_save_path}*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(col, df_metrics, estimator, tokenizer_name, classifier_name):\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving Model and Table for {tokenizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}Classifiers Table.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}Classifiers Table.md')\n",
    "\n",
    "    # Save estimator\n",
    "    estimator.save_model(f'{models_save_path}{method} Estimator {str(col)} - {tokenizer_name} + {classifier_name})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# HACK REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=200).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "689d84dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "--------------------\n",
      "============================== TRAINING WARMTH ==============================\n",
      "--------------------\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "Done splitting data into training, testing, and validation sets.\n",
      "====================\n",
      "Encoding training, testing, and validation sets with BertTokenizerFast.from_pretrained using bert-base-uncased.\n",
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set shape: (600,)\n",
      "----------\n",
      "Training set example:\n",
      "Excellent analytical skills with the ability to synthesise complex information and identify key issues, concerns or trends;\n",
      "----------\n",
      "Training set BERT encodings example:\n",
      "[CLS] excellent analytical skills with the ability to synthesis ##e complex information and identify key issues , concerns or trends ; [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Training labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (80,)\n",
      "----------\n",
      "Testing set example:\n",
      "Manage client relationship by delivering high-quality, timely and value-added services.\n",
      "----------\n",
      "Testing set BERT encodings example:\n",
      "[CLS] manage client relationship by delivering high - quality , timely and value - added services . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Testing labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (120,)\n",
      "----------\n",
      "Validation set example:\n",
      "Understanding Robots Ui\n",
      "----------\n",
      "Validation set BERT encodings example:\n",
      "[CLS] understanding robots ui [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Validation labels after BERT encoding: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Class weights:\n",
      "Ratio = 0.99 (0 = 0.99, 1 = 1.01)\n",
      "====================\n",
      "Initializing BERT Model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Initializing BERT Trainer using BertTokenizerFast + BertForSequenceClassification for Warmth\n",
      "--------------------\n",
      "Passing arguments to estimator.\n",
      "--------------------\n",
      "Starting training for Warmth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a31f378c7e4e308d3fc9a80d046124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_4629/1230619439.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device) for key, val in self.encodings.items()}\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5589, 'learning_rate': 5e-05, 'epoch': 2.63}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aee351b46dd43cb9982ca3b8e848db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Computing metrics using y_test_pred.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:81\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1634\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1635\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1636\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1637\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1638\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:1979\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1976\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m epoch \u001b[39m+\u001b[39m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m steps_skipped) \u001b[39m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1977\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1979\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   1980\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1981\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_substep_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2230\u001b[0m             metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate(\n\u001b[1;32m   2231\u001b[0m                 eval_dataset\u001b[39m=\u001b[39meval_dataset,\n\u001b[1;32m   2232\u001b[0m                 ignore_keys\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2233\u001b[0m                 metric_key_prefix\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00meval_dataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2234\u001b[0m             )\n\u001b[1;32m   2235\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2236\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(ignore_keys\u001b[39m=\u001b[39;49mignore_keys_for_eval)\n\u001b[1;32m   2237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_report_to_hp_search(trial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2239\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:2932\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2929\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   2931\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 2932\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   2933\u001b[0m     eval_dataloader,\n\u001b[1;32m   2934\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   2935\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   2936\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   2937\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   2938\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[1;32m   2939\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[1;32m   2940\u001b[0m )\n\u001b[1;32m   2942\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   2943\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric_key_prefix\u001b[39m}\u001b[39;00m\u001b[39m_jit_compilation_time\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:3220\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3216\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3217\u001b[0m             EvalPrediction(predictions\u001b[39m=\u001b[39mall_preds, label_ids\u001b[39m=\u001b[39mall_labels, inputs\u001b[39m=\u001b[39mall_inputs)\n\u001b[1;32m   3218\u001b[0m         )\n\u001b[1;32m   3219\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3220\u001b[0m         metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   3221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3222\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(predicted_results, with_y_pred, with_y_pred_prob)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m with_y_pred:\n\u001b[1;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mComputing metrics using y_test_pred.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m     (\n\u001b[1;32m     37\u001b[0m         explained_variance, accuracy, balanced_accuracy, precision,\n\u001b[1;32m     38\u001b[0m         recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n\u001b[1;32m     39\u001b[0m     ) \u001b[39m=\u001b[39m compute_metrics_with_y_pred(\n\u001b[0;32m---> 40\u001b[0m         col, vectorizer_name, classifier_name, y_test, y_test_pred\n\u001b[1;32m     41\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[39m# Using y_test_pred_prob\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m with_y_pred_prob:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer_name' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Load Table DF\n",
    "df_metrics = pd.read_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "\n",
    "for col in analysis_columns:\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "        test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "        val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "        bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "    ) = encode_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    # Initialize Model\n",
    "    print(f'Initializing BERT Model.')\n",
    "    tokenizer_name = bert_tokenizer.__class__.__name__\n",
    "    classifier_name = bert_model.__class__.__name__\n",
    "\n",
    "    # Load pre-trained BERT model\n",
    "    bert_model = BertForSequenceClassification.from_pretrained(\n",
    "        bert_model_name, num_labels=len(bert_id2label)\n",
    "    ).to(device)\n",
    "    # bert_model.eval()\n",
    "\n",
    "    # Name tokenizer and classifier name\n",
    "    tokenizer_name = bert_tokenizer.__class__.__name__\n",
    "    classifier_name = bert_model.__class__.__name__\n",
    "\n",
    "    print('='*30)\n",
    "    print(f'Initializing BERT Trainer using {tokenizer_name} + {classifier_name} for {col}')\n",
    "\n",
    "    # Set BERT fine-tuning parameters\n",
    "    bert_training_args = TrainingArguments(\n",
    "        output_dir=f'{models_save_path}{method} Results',\n",
    "        logging_dir=f'{models_save_path}{method} Logs',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=20,\n",
    "        learning_rate=5e-5,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        evaluation_strategy='steps',\n",
    "        optim='adamw_torch',\n",
    "        use_mps_device=True if device.type == 'mps' else False\n",
    "    )\n",
    "\n",
    "    # Pass data to trainer \n",
    "    # FIXME: computer metrics not working\n",
    "    print('-'*20)\n",
    "    print('Passing arguments to estimator.')\n",
    "    estimator = Trainer(\n",
    "        model=bert_model,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        args=bert_training_args,\n",
    "        train_dataset=bert_train_dataset,\n",
    "        eval_dataset=bert_test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    if estimator.place_model_on_device:\n",
    "        estimator.model.to(device)\n",
    "\n",
    "    # Train trainer\n",
    "    print('-'*20)\n",
    "    print(f'Starting training for {col}')\n",
    "    estimator.train()\n",
    "    estimator.save_model(f'{models_save_path}{method} Estimator {str(col)} - {tokenizer_name} + {classifier_name})')\n",
    "    print('Done training!')\n",
    "    print()\n",
    "\n",
    "    # Get predictions\n",
    "    print('-'*20)\n",
    "    print(f'Evaluating estimator for {col}')\n",
    "    estimator.evaluate()\n",
    "    # metrics_dict\n",
    "    predicted_results = estimator.predict(bert_test_dataset)\n",
    "    print(f'Predictions shape for {col}: {predicted_results.predictions.shape}')\n",
    "    print()\n",
    "    \n",
    "    # Get y_test_pred\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_test_pred for {col}')\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "    y_test_pred = y_test_pred.flatten().tolist()\n",
    "    y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "    print(f'Length of y_test_pred: {len(y_test_pred)}')\n",
    "    print()\n",
    "\n",
    "    # Get y_test_pred_proba\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_test_pred_prob for {col}')\n",
    "    try:\n",
    "        y_test_pred_prob = torch.nn.functional.softmax(predicted_results.predictions.clone().detach().to(device), dim=-1)\n",
    "        print('Using torch.nn.functional.softmax')\n",
    "    except Exception:\n",
    "        y_test_pred_prob = scipy.special.softmax(predicted_results.predictions.clone().detach().to(device), axis=1)\n",
    "        print('Using scipy.special.softmax')\n",
    "    except Exception:\n",
    "        y_test_pred_prob = predicted_results.predictions.clone().detach().to(device)[:, 1]\n",
    "        print('Using predicted_results.predictions[:, 1]')\n",
    "    finally:\n",
    "        y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "    print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "    print()\n",
    "\n",
    "    # HACK\n",
    "    # # Get y_test_pred_proba\n",
    "    # print('-'*20)\n",
    "    # print(f'Getting y_test_pred_prob for {col}')\n",
    "    # try:\n",
    "    #     y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "    #     print('Using torch.nn.functional.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "    #     print('Using scipy.special.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "    #     print('Using predicted_results.predictions[:, 1]')\n",
    "    # finally:\n",
    "    #     y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "    # print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "    # print()\n",
    "    \n",
    "    # Examine predictions\n",
    "    print('-'*20)\n",
    "    print(f'Examining predictions for {col}')\n",
    "    print('Correctly Classified Reviews:')\n",
    "    for _y_test, _y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 20):\n",
    "        if _y_test == _y_test_pred:\n",
    "            print(f'LABEL: {_y_test}')\n",
    "            print(f'REVIEW TEXT: {_X_test[:100]}...')\n",
    "            print('-'*20)\n",
    "            print()\n",
    "    \n",
    "    print('Incorrectly Classified Reviews:')\n",
    "    for _y_test, _y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 20):\n",
    "        if _y_test != _y_test_pred:\n",
    "            print(f'TRUE LABEL: {_y_test}')\n",
    "            print(f'PREDICTED LABEL: {_y_test_pred}')\n",
    "            print(f'REVIEW TEXT: {_X_test[:100]}...')\n",
    "            print()\n",
    "\n",
    "    # Evluate estimator\n",
    "    print('-'*20)\n",
    "    print(f'Probs evaluation and ploting metrics for {col}')\n",
    "    df_metrics = evaluation(predicted_results, df_metrics, col, tokenizer_name, classifier_name)\n",
    "    print()\n",
    "\n",
    "    # Save BERT Model\n",
    "    print('-'*20)\n",
    "    print(f'Saving estimator and metrics table for {col}')\n",
    "    saving_model_and_table(df_metrics, estimator, col, tokenizer_name, classifier_name)\n",
    "    print()\n",
    "\n",
    "    # Compare Estimators\n",
    "    print('='*20)\n",
    "    print(f'Comparing estimators for {col}')\n",
    "    comparison_plots(get_fitted_estimators().append(estimator), X_test, y_test, col)\n",
    "    print('='*20)\n",
    "    print()\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print('-'*20)\n",
    "print(f'Evaluating estimator for {col}')\n",
    "estimator.evaluate()\n",
    "# metrics_dict\n",
    "predicted_results = estimator.predict(bert_test_dataset)\n",
    "print(f'Predictions shape for {col}: {predicted_results.predictions.shape}')\n",
    "\n",
    "# Get y_test_pred\n",
    "print('-'*20)\n",
    "print(f'Getting y_test_pred for {col}')\n",
    "y_test = predicted_results.label_ids\n",
    "y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "y_test_pred = y_test_pred.flatten().tolist()\n",
    "y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "print(f'Length of y_test_pred: {len(y_test_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a155ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y_test_pred_proba\n",
    "print('-'*20)\n",
    "print(f'Getting y_test_pred_prob for {col}')\n",
    "try:\n",
    "    y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "    print('Using torch.nn.functional.softmax')\n",
    "except Exception:\n",
    "    y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "    print('Using scipy.special.softmax')\n",
    "except Exception:\n",
    "    y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "    print('Using predicted_results.predictions[:, 1]')\n",
    "finally:\n",
    "    y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c27631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluate estimator\n",
    "print('-'*20)\n",
    "print(f'Probs evaluation and ploting metrics for {col}')\n",
    "df_metrics = evaluation(predicted_results, df_metrics, col, classifier_name, tokenizer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.is_model_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9419be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.args.n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Bert Preditcions'] = df_manual['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: estimator.predict(sent)\n",
    "    for sent in sentence\n",
    "    if sent and isinstance(sent, (str, list)) and isinstance(sentence, list)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
