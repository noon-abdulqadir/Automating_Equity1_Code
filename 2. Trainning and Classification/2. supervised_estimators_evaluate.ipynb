{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80da5019e4124bfba18e64abe3687fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc5a29b304e43e49f41895bbdcfa678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b893db336944cc7989e41144660ead7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7553bd96f134359b304db1e3e045d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import * # isort:skip # fmt:skip # noqa # nopep8\n",
    "from supervised_estimators_get_pipe import *  # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7d20f93e374368abfeebdfe92608f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad780708cd9f4c0eaf1ad762e606702c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variables\n",
    "warnings.filterwarnings('always')\n",
    "# Sklearn variables\n",
    "method = 'Supervised'\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
    "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
    "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Train - Mean Cross Validation Score': np.nan,\n",
    "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Test - Mean Cross Validation Score': np.nan,\n",
    "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Average Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan,\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "cores = multiprocessing.cpu_count()\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    bert_model_name, strip_accents=True\n",
    ")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name\n",
    ").to(device)\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a26e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "675cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f179e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(vectorizers_pipe, classifiers_pipe, metrics_list, analysis_columns=analysis_columns):\n",
    "\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Measures'],\n",
    "    )\n",
    "    return pd.DataFrame(index=index, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccfbba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_estimators(\n",
    "    results_save_path=results_save_path,\n",
    "    col_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if col_names_list is None:\n",
    "        col_names_list = []\n",
    "    if vectorizer_names_list is None:\n",
    "        vectorizer_names_list = []\n",
    "    if classifier_names_list is None:\n",
    "        classifier_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in dir:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in glob.glob(f'{results_save_path}*.pkl'):\n",
    "        col_names_list.append(\n",
    "            col_name_from_file := estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "        )\n",
    "        vectorizer_names_list.append(\n",
    "            vectorizer_name_from_file := estimators_file.split(f'{col_name_from_file} - ')[-1].split(' + ')[0]\n",
    "        )\n",
    "        classifier_names_list.append(\n",
    "            classifier_name_from_file := estimators_file.split(f'- {vectorizer_name_from_file} + ')[-1].split(' (Save_protocol=')[0]\n",
    "        )\n",
    "\n",
    "    estimator_names_list = [\n",
    "        f'{col_name_from_file} - {vectorizer_name_from_file} + {classifier_name_from_file}'\n",
    "        for col_name_from_file, vectorizer_name_from_file, classifier_name_from_file in tqdm_product(\n",
    "            list(set(col_names_list)),\n",
    "            list(set(vectorizer_names_list)),\n",
    "            list(set(classifier_names_list)),\n",
    "        )\n",
    "    ]\n",
    "    return (\n",
    "        list(set(col_names_list)),\n",
    "        list(set(vectorizer_names_list)),\n",
    "        list(set(classifier_names_list)),\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ab2972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def load_Xy_search_cv_estimator(\n",
    "    col, vectorizer_name, classifier_name, protocol,\n",
    "    results_save_path=results_save_path,\n",
    "    searchcv_save_path=None, method=method, \n",
    "    compression=None, saved_files_list=None,\n",
    "    path_suffix=None, data=None,\n",
    "):\n",
    "    if searchcv_save_path is None:\n",
    "        searchcv_save_path = f'{results_save_path}SearchCV/'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {str(col)} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data is None:\n",
    "        data = {}\n",
    "    if saved_files_list is None:\n",
    "        saved_files_list = [\n",
    "            'Grid Search', 'SearchCV', 'df_cv_results', 'df_train_data', 'df_test_data', 'df_val_data', 'Estimator'\n",
    "        ]\n",
    "\n",
    "    # Load data into dict\n",
    "    for file_path in glob.glob(f'{searchcv_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{searchcv_save_path}{method} ')[-1].split(path_suffix)[0]\n",
    "\n",
    "        if 'df_' in file_name:\n",
    "            data[file_name] = pd.read_pickle(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data[file_name] = joblib.load(f)\n",
    "\n",
    "    # Load estimator\n",
    "    with open(\n",
    "        f'{results_save_path}{method} Estimator{path_suffix}', 'rb'\n",
    "    ) as f:\n",
    "        data['Estimator'] = joblib.load(f)\n",
    "\n",
    "    # Assign data to variables\n",
    "    grid_search = data['Grid Search']\n",
    "    searchcv = data['SearchCV']\n",
    "    df_cv_results = data['df_cv_results']\n",
    "    df_train_data = data['df_train_data']\n",
    "    df_test_data = data['df_test_data']\n",
    "    df_val_data = data['df_val_data']\n",
    "    estimator = data['Estimator']\n",
    "    if 'df_feature_importances' in data.keys():\n",
    "        saved_files_list.append('df_feature_importances')\n",
    "        df_feature_importances = data['df_feature_importances']\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "    \n",
    "    assert set(data.keys()) == set(saved_files_list), f'Not all files were loaded! Missing: {set(data.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done loading Xy, CV data, and estimator!\\n{list(data.keys())}')\n",
    "    print('='*20)\n",
    "\n",
    "    # # Load grid_search\n",
    "    # with open(\n",
    "    #     f'{searchcv_save_path}{method} Grid Search{path_suffix}', 'rb'\n",
    "    # ) as f:\n",
    "    #     grid_search = joblib.load(f)\n",
    "    # # Load searchcv\n",
    "    # with open(\n",
    "    #     f'{searchcv_save_path}{method} SearchCV{path_suffix}', 'rb'\n",
    "    # ) as f:\n",
    "    #     searchcv = joblib.load(f)\n",
    "    # # Load searchcv data\n",
    "    # df_cv_results = pd.read_pickle(\n",
    "    #     f'{searchcv_save_path}{method} df_searchcv_results{path_suffix}'\n",
    "    # )\n",
    "    # # Load Xy data\n",
    "    # # Load train data\n",
    "    # df_train_data = pd.read_pickle(\n",
    "    #     f'{searchcv_save_path}{method} df_train_data{path_suffix}'\n",
    "    # )\n",
    "    # # Load test data\n",
    "    # df_test_data = pd.read_pickle(\n",
    "    #     f'{searchcv_save_path}{method} df_test_data{path_suffix}'\n",
    "    # )\n",
    "    # # Load val data\n",
    "    # df_val_data = pd.read_pickle(\n",
    "    #     f'{searchcv_save_path}{method} df_val_data{path_suffix}'\n",
    "    # )\n",
    "    # # Load estimator\n",
    "    # with open(\n",
    "    #     f'{results_save_path}{method} Estimator{path_suffix}', 'rb'\n",
    "    # ) as f:\n",
    "    #     estimator = joblib.load(f)\n",
    "    # # Load feature importance\n",
    "    # if os.path.isfile(f'{searchcv_save_path}{method} df_feature_importances{path_suffix}'):\n",
    "    #     df_feature_importances = pd.read_pickle(\n",
    "    #         f'{searchcv_save_path}{method} df_feature_importances{path_suffix}'\n",
    "    #     )\n",
    "    # else:\n",
    "    #     df_feature_importances = None\n",
    "\n",
    "    # Train data\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    y_train_pred = df_train_data['y_train_pred'].values\n",
    "    # Test data\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    y_test_pred = df_test_data['y_test_pred'].values\n",
    "    y_test_pred_prob = df_test_data['y_test_pred_prob'].values\n",
    "    # Val data\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    return (\n",
    "        grid_search, searchcv,\n",
    "        X_train, y_train, y_train_pred,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val,\n",
    "        df_feature_importances, estimator,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    estimator,\n",
    "    text_col=None, analysis_columns=None):\n",
    "    \n",
    "    check_consistent_length(X_train, X_test, X_val)\n",
    "    check_consistent_length(y_train, y_test, y_val, y_train_pred, y_test_pred, y_test_pred_prob)\n",
    "    check_is_fitted(estimator)\n",
    "\n",
    "    print('-'*20)\n",
    "    print('Training Confusion Matrix:\\n')\n",
    "    close_plots()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\n",
    "        f'{str(col)} - Training Confusion Matrix - {vectorizer_name} + {classifier_name}'\n",
    "    )\n",
    "    train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "        estimator, X_train, y_train, ax=ax, cmap=plt.cm.Blues\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "    print('-'*20)\n",
    "\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "    \n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    print('Done checking data.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        X_train, y_train, y_train_pred,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val,\n",
    "        estimator,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76cc9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_using_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "    cv=cv, return_train_score=None,\n",
    "):\n",
    "    if return_train_score is None:\n",
    "        return_train_score = True\n",
    "\n",
    "    # Using estimator\n",
    "    # Cross Validation\n",
    "    print('-'*20)\n",
    "    print('Cross Validating without scoring.')\n",
    "    cv_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=None,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Cross Validation with scoring\n",
    "    print('-'*20)\n",
    "    print(f'Cross Validating with {scores} scoring.')\n",
    "    cv_score_recall = cross_validate(\n",
    "        estimator,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        scoring=scores,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Get mean and std of cross validation scores\n",
    "    print('-'*20)\n",
    "    print('Getting mean and std of cross validation scores.')\n",
    "    cv_train_scores = cv_score_noscoring['train_score'].mean()\n",
    "    cv_test_scores = cv_score_noscoring['test_score'].mean()\n",
    "    cv_train_recall = cv_score_recall['train_recall'].mean()\n",
    "    cv_test_recall = cv_score_recall['test_recall'].mean()\n",
    "    cv_train_explained_variance_recall = cv_score_recall['train_explained_variance'].mean()\n",
    "    cv_test_explained_variance_recall = cv_score_recall['test_explained_variance'].mean()\n",
    "\n",
    "    # Save cross validation scores to dataframe\n",
    "    print('-'*20)\n",
    "    print('Saving cross validation scores to dataframe.')\n",
    "    df_cv_score_noscoring = pd.DataFrame(cv_score_noscoring)\n",
    "    df_cv_score_noscoring.to_pickle(f'{df_save_dir}df_cv_score_noscoring - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "    df_cv_score_recall = pd.DataFrame(cv_score_recall)\n",
    "    df_cv_score_recall.to_pickle(f'{df_save_dir}df_cv_score_recall - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
    "\n",
    "    return (\n",
    "        df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0e84026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_estimator(\n",
    "    estimator, X_test, y_test, col, vectorizer_name, classifier_name, random_state=random_state, n_jobs=n_jobs, cv=cv,\n",
    "    param_name=None, param_range=None, axis=None, alpha=None, verbose=None\n",
    "):\n",
    "    if axis is None:\n",
    "        axis = 1\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if verbose is None:\n",
    "        verbose=1\n",
    "\n",
    "    # Make param names and values\n",
    "    if param_range is None:\n",
    "        param_range = np.arange(1, 100, 10)\n",
    "    if param_name is None:\n",
    "        param_names = [\n",
    "            param_name\n",
    "            for param_dict in estimator.steps\n",
    "            for param_name in \n",
    "            [\n",
    "                name\n",
    "                for name, value in param_dict[1].get_params().items()\n",
    "                if name != 'random_state'\n",
    "                and isinstance(value, (list, int, float))\n",
    "                and not isinstance(value, bool)\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    # Using estimator\n",
    "    # Learning Curves\n",
    "    print('Plotting Learning Curve.')\n",
    "    print('-'*20)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=estimator,\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        random_state=random_state,\n",
    "        shuffle=True,\n",
    "        scoring=scoring,\n",
    "        verbose=verbose,\n",
    "        # train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    train_scores_std = np.std(train_scores, axis=axis)\n",
    "    test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    close_plots()\n",
    "    plt.figure()\n",
    "    plt.suptitle(\n",
    "        f'{str(col)} - Learning Curves for {scoring.title()} - {vectorizer_name} + {classifier_name}'\n",
    "        )\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid()\n",
    "    plt.fill_between(\n",
    "        train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    )\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    )\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # Save figure\n",
    "    for image_save_format in ['eps', 'png', 'svg']:\n",
    "        plt.savefig(f'{plot_save_path}{method} {str(col)} - Learning Curve - {vectorizer_name} + {classifier_name}.{image_save_format}', format=image_save_format)\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # # Validation Curve\n",
    "    # for param_name in param_names:\n",
    "    #     if param_name:\n",
    "    #         param_title = ' '.join(param_name.split('_')).title()\n",
    "    #         print(f'Plotting Validation Curve for {param_title}.')\n",
    "    #         print('-'*20)\n",
    "    #         train_scores, test_scores = validation_curve(\n",
    "    #             estimator=estimator,\n",
    "    #             X=X_train,\n",
    "    #             y=y_train,\n",
    "    #             param_name=param_name,\n",
    "    #             param_range=param_range,\n",
    "    #             cv=cv,\n",
    "    #             n_jobs=n_jobs,\n",
    "    #             scoring=scoring,\n",
    "    #             verbose=verbose,\n",
    "    #         )\n",
    "    #         train_scores_mean = np.mean(train_scores, axis=axis)\n",
    "    #         train_scores_std = np.std(train_scores, axis=axis)\n",
    "    #         test_scores_mean = np.mean(test_scores, axis=axis)\n",
    "    #         test_scores_std = np.std(test_scores, axis=axis)\n",
    "\n",
    "    #         # Ploting\n",
    "    #         plt.figure()\n",
    "    #         plt.suptitle(\n",
    "    #             f'{str(col)} - Validation Curve for {scoring.title()}on {param_title} - {vectorizer_name} + {classifier_name}'\n",
    "    #         )\n",
    "    #         plt.xlabel(param_name)\n",
    "    #         plt.ylabel('Score')\n",
    "    #         plt.grid()\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
    "    #         )\n",
    "    #         plt.fill_between(\n",
    "    #             param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, train_scores_mean, 'o-', color='r', label='Training score'\n",
    "    #         )\n",
    "    #         plt.plot(\n",
    "    #             param_range, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
    "    #         )\n",
    "    #         plt.legend(loc='best')\n",
    "    #         plt.show()\n",
    "\n",
    "    #         # Save figure\n",
    "    #         for image_save_format in ['eps', 'png', 'svg']:\n",
    "    #             plt.savefig(\n",
    "    #                 f'{plot_save_path}{method} {str(col)} - Validation Curve for {param_title} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #                 format=image_save_format\n",
    "    #             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd7a0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_test_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_test, y_test_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_test, y_test_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c21c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_label = 1\n",
    "# close_plots()\n",
    "# cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred\n",
    "# )\n",
    "# cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, normalize='true'\n",
    "# )\n",
    "# roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# calibration_curve = CalibrationDisplay.from_predictions(\n",
    "#     y_test, y_test_pred, pos_label=pos_label\n",
    "# )\n",
    "# show_and_close_plots()\n",
    "\n",
    "# # Plots\n",
    "# plots_dict = {\n",
    "#     'Confusion Matrix': cm_curve,\n",
    "#     'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "#     'ROC Curve': roc_curve,\n",
    "#     'Precision-Recall Curve': pr_curve,\n",
    "#     'Calibration Curve': calibration_curve,\n",
    "# }\n",
    "\n",
    "# print('=' * 20)\n",
    "# # close_plots()\n",
    "# print('Plotting metrics with y_pred_prob:')\n",
    "# print('='*20)\n",
    "# for plot_name, plot_ in plots_dict.items():\n",
    "#     close_plots()\n",
    "#     print(f'Plotting {plot_name}:')\n",
    "#     plt.figure()\n",
    "#     plt.suptitle(\n",
    "#         f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "#         )\n",
    "#     if plot_name == 'ROC Curve':\n",
    "#         plt.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "#     try:\n",
    "#         plot_.plot(color='C0')\n",
    "#     except (TypeError, AttributeError):\n",
    "#         plot_.plot()\n",
    "#         try:\n",
    "#             plt.gca().get_lines()[0].set_color('blue')\n",
    "#         except IndexError:\n",
    "#             plot_.plot(cmap=plt.cm.Blues)\n",
    "#     plt.legend(loc='best')\n",
    "#     print('=' * 20)\n",
    "\n",
    "#     # Save Plots\n",
    "#     print(f'Saving {plot_name}...')\n",
    "#     for image_save_format in ['eps', 'png', 'svg']:\n",
    "#         plt.savefig(\n",
    "#             f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "#             format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "#         )\n",
    "#     show_and_close_plots()\n",
    "#     print(f'Saved {plot_name}!')\n",
    "#     print('=' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89a7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_test_pred)\n",
    "    # Using y_pred_prob\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, display_labels=labels\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true', display_labels=labels\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    # close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "    print('='*20)\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        close_plots()\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        plot_.plot(ax=ax)\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Save Plots\n",
    "        print(f'Saving {plot_name}...')\n",
    "        for image_save_format in ['eps', 'png', 'svg']:\n",
    "            plt.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "                format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # # Visualisation with plot_metric\n",
    "    # bc = plot_metric.functions.BinaryClassification(\n",
    "    #     y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # # Figures\n",
    "    # close_plots()\n",
    "    # fig = plt.figure(figsize=(15, 10))\n",
    "    # plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    # bc.plot_confusion_matrix(colorbar=True)\n",
    "    # plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    # bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    # plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    # bc.plot_roc_curve()\n",
    "    # plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    # bc.plot_precision_recall_curve()\n",
    "    # plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    # bc.plot_class_distribution()\n",
    "    # bc.print_report()\n",
    "    # for image_save_format in ['eps', 'png', 'svg']:\n",
    "    #     plt.savefig(\n",
    "    #         f'{plot_save_path}{method} {str(col)} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #         format=image_save_format,\n",
    "    #         dpi=3000, bbox_inches='tight'\n",
    "    #     )\n",
    "    # show_and_close_plots()\n",
    "\n",
    "    # # Heatmap\n",
    "    # print('Plotting Heatmap:')\n",
    "    # close_plots()\n",
    "    # classifications_dict = defaultdict(int)\n",
    "    # for _y_test, _y_test_pred in zip(y_test, y_test_pred):\n",
    "    #     if _y_test != _y_test_pred:\n",
    "    #         classifications_dict[(_y_test, _y_test_pred)] += 1\n",
    "\n",
    "    # dicts_to_plot = [\n",
    "    #     {\n",
    "    #         f'True {str(col)} value': _y_test,\n",
    "    #         f'Predicted {str(col)} value': _y_test_pred,\n",
    "    #         'Number of Classifications': _count,\n",
    "    #     }\n",
    "    #     for (_y_test, _y_test_pred), _count in classifications_dict.items()\n",
    "    # ]\n",
    "    # df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    # df_wide = df_to_plot.pivot_table(\n",
    "    #     index=f'True {str(col)} value', \n",
    "    #     columns=f'Predicted {str(col)} value', \n",
    "    #     values='Number of Classifications'\n",
    "    # )\n",
    "\n",
    "    # plt.figure(figsize=(9,7))\n",
    "    # sns.set(style='ticks', font_scale=1.2)\n",
    "    # sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Blues, annot=True)    \n",
    "    # plt.xticks(rotation=45, ha='right')\n",
    "    # plt.yticks(rotation=0)\n",
    "    # plt.tight_layout()\n",
    "    # plt.suptitle(f'{str(col)} Heatmap - {vectorizer_name} + {classifier_name}')\n",
    "    # print('Saving Heatmap...')\n",
    "    # for image_save_format in ['eps', 'png', 'svg']:\n",
    "    #     plt.savefig(\n",
    "    #         f'{plot_save_path}{method} {str(col)} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #         format=image_save_format,\n",
    "    #         dpi=3000, bbox_inches='tight'\n",
    "    #     )\n",
    "    # print('Saved Heatmap!')\n",
    "    # show_and_close_plots()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1de7dd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    average_precision = metrics.average_precision_score(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold,loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "162b0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, cv=cv, n_jobs=n_jobs,\n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None,\n",
    "    pos_label=None, verbose=None,\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if verbose is None:\n",
    "        verbose = 1        \n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        print('Computing metrics using estimator.')\n",
    "        (\n",
    "            df_cv_score_recall,\n",
    "            cv_train_scores, cv_test_scores,\n",
    "            cv_train_recall, cv_test_recall,\n",
    "            cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
    "        ) = compute_metrics_using_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred.')\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name\n",
    "        )\n",
    "    # Using y_test_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_test_pred_prob.')\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold,loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_test, y_test_pred_prob, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "    #Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        f'{scoring.title()} Best Threshold': threshold,\n",
    "        f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Imbalanced Classification Report': imblearn_report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42fdf3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, \n",
    "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_estimator is None:\n",
    "        with_estimator = True\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Plotting\n",
    "    print('~'*20)\n",
    "    print('Plotting metrics.')\n",
    "    print('~'*20)\n",
    "    # Using estimator\n",
    "    if with_estimator:\n",
    "        plot_metrics_with_estimator(\n",
    "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        plot_metrics_with_y_pred(\n",
    "            y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f440c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_66048/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">2153300327.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">12</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_66048/2153300327.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'estimator'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_66048/\u001b[0m\u001b[1;33m2153300327.py\u001b[0m:\u001b[94m12\u001b[0m in \u001b[92m<module>\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_66048/2153300327.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'estimator'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get metrics dict\n",
    "(\n",
    "    metrics_dict, df_cv_score_recall,\n",
    "    cv_train_scores, cv_test_scores,\n",
    "    cv_train_recall, cv_test_recall,\n",
    "    cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "    explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "    f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "    average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "    loss, precision_pr, recall_pr, threshold_pr,\n",
    ") = compute_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name\n",
    ")\n",
    "\n",
    "# Print metrics\n",
    "print('=' * 20)\n",
    "print('~' * 20)\n",
    "print(f' Testing Metrics for {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "print('~' * 20)\n",
    "print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8278ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    best_train_score, df_metrics, col, vectorizer_name, classifier_name, scorig=scoring\n",
    "):\n",
    "\n",
    "    # Get metrics dict\n",
    "    (\n",
    "        metrics_dict, df_cv_score_recall,\n",
    "        cv_train_scores, cv_test_scores,\n",
    "        cv_train_recall, cv_test_recall,\n",
    "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
    "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
    "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
    "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
    "        loss, precision_pr, recall_pr, threshold_pr,\n",
    "    ) = compute_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name\n",
    "    )\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Testing Metrics for {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if 'Threshold' not in metric_name:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                print(f'{metric_name}:\\n{metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "        # Fill Table DF\n",
    "        if isinstance(metric_value, float):\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = metric_value\n",
    "        else:\n",
    "            df_metrics.loc[\n",
    "                (classifier_name), (col, vectorizer_name, metric_name)\n",
    "            ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name,\n",
    "    )\n",
    "\n",
    "    return df_metrics, metrics_dict, df_cv_score_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f33cf875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def save_estimator_and_table(\n",
    "    df_metrics, estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    table_save_path=table_save_path,\n",
    "    method=method, save_name=None,\n",
    "    compression=None, protocol=None, \n",
    "    path_suffix=None, \n",
    "):\n",
    "    if save_name is None:\n",
    "        save_name = 'Estimators Table'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {str(col)} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving fitted estimator and table for {vectorizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}{save_name}.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}{save_name}.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}{save_name}.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}{save_name}.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}{save_name}.md')\n",
    "    df_metrics.to_html(f'{table_save_path}{save_name}.html')\n",
    "\n",
    "    # Save fitted estimator\n",
    "    with open(f'{path}{method} Fitted Estimator{path_suffix}', 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print('Done saving fitted estimator and table!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fc524e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_completed_estimators(\n",
    "    results_save_path=results_save_path, method=method, classifiers_pipe=classifiers_pipe,\n",
    "    estimators_list=None, used_classifiers=None,\n",
    "):\n",
    "\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.pkl'):\n",
    "        classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "        used_classifiers.append(classifier_name)\n",
    "        with open(estimator_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    assert set(classifiers_pipe.keys()) == set(used_classifiers), 'Not all classifiers were used!'\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44148bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(estimators_list, X_test, y_test, col, curves_dict=None, cmap=plt.cm.Blues):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        'Calibration Curve': metrics.CalibrationDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{str(col)} - {str(curve_name)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{str(col)} - {str(curve_name)}')\n",
    "        for estimator in estimators_list:\n",
    "            curve = curve_package.from_estimator(\n",
    "                estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
    "                name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        print('Saving plots.')\n",
    "        for image_save_format in ['eps', 'png', 'svg']:\n",
    "            curve.figure_.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - All {str(curve_name)}s.{image_save_format}',\n",
    "                format=image_save_format,\n",
    "                dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "assert len(df_manual) == 5978, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH 5978 BUT IS OF LENGTH {len(df_manual)}'\n",
    "# TODO REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=50).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimator in dir:\n",
      "/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Supervised Results/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb459b01f8f41b29e9308db45f86ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Loading data for Warmth - CountVectorizer + KNeighborsClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Done loading Xy, CV data, and estimator!\n",
      "['df_cv_results', 'Grid Search', 'df_train_data', 'df_test_data', 'SearchCV', 'df_val_data', 'Estimator']\n",
      "====================\n",
      "--------------------\n",
      "============================== EVALUATING DATASET OF LENGTH 5978 ON WARMTH ==============================\n",
      "====================\n",
      "GridSearch - Best mean train score: M = 0.77, SD = 0.00\n",
      "\n",
      "GridSearch - Best mean test score: M = 0.67, SD = 0.00\n",
      "\n",
      "Number of splits: 30\n",
      "\n",
      "Best estimator and parameters:\n",
      "Pipeline(steps=[('CountVectorizer',\n",
      "                 CountVectorizer(lowercase=False, max_df=0.75, min_df=0.1,\n",
      "                                 ngram_range=(1, 3))),\n",
      "                ('SelectKBest', SelectKBest(k='all')),\n",
      "                ('SMOTETomek', SMOTETomek()),\n",
      "                ('KNeighborsClassifier', KNeighborsClassifier(n_neighbors=15))])\n",
      "\n",
      "Best parameters:\n",
      "{'CountVectorizer__analyzer': 'word', 'CountVectorizer__lowercase': False, 'CountVectorizer__max_df': 0.75, 'CountVectorizer__min_df': 0.1, 'CountVectorizer__ngram_range': (1, 3), 'KNeighborsClassifier__algorithm': 'auto', 'KNeighborsClassifier__n_neighbors': 15, 'KNeighborsClassifier__weights': 'uniform', 'SelectKBest__k': 'all', 'SelectKBest__score_func': <function f_classif at 0x28ea9f7f0>}\n",
      "\n",
      "Training Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.64      0.75      3365\n",
      "           1       0.42      0.78      0.55      1118\n",
      "\n",
      "    accuracy                           0.68      4483\n",
      "   macro avg       0.66      0.71      0.65      4483\n",
      "weighted avg       0.78      0.68      0.70      4483\n",
      "\n",
      "\n",
      "Best train score: 0.67\n",
      "\n",
      "Best test score: 0.74\n",
      "\n",
      "====================\n",
      "Training Confusion Matrix:\n",
      "\n",
      "====================\n",
      "====================\n",
      "Computing metrics using estimator.\n",
      "--------------------\n",
      "Cross Validating without scoring.\n",
      "--------------------\n",
      "Cross Validating with ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef'] scoring.\n",
      "--------------------\n",
      "Getting mean and std of cross validation scores.\n",
      "--------------------\n",
      "Saving cross validation scores to dataframe.\n",
      "--------------------\n",
      "Computing metrics using y_test_pred.\n",
      "--------------------\n",
      "Computing metrics using y_test_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      " Testing Metrics for Warmth - CountVectorizer + KNeighborsClassifier\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.71       446\n",
      "           1       0.39      0.74      0.51       152\n",
      "\n",
      "    accuracy                           0.64       598\n",
      "   macro avg       0.63      0.67      0.61       598\n",
      "weighted avg       0.75      0.64      0.66       598\n",
      "\n",
      "--------------------\n",
      "Train - Mean Cross Validation Score: 0.67\n",
      "--------------------\n",
      "Train - Mean Cross Validation - Recall: 0.76\n",
      "--------------------\n",
      "Train - Mean Explained Variance - Recall: -0.47\n",
      "--------------------\n",
      "Test - Mean Cross Validation Score: 0.62\n",
      "--------------------\n",
      "Test - Mean Cross Validation - Recall: 0.65\n",
      "--------------------\n",
      "Test - Mean Explained Variance - Recall: -0.7\n",
      "--------------------\n",
      "Explained Variance: -0.63\n",
      "--------------------\n",
      "Accuracy: 0.64\n",
      "--------------------\n",
      "Balanced Accuracy: 0.67\n",
      "--------------------\n",
      "Precision: 0.39\n",
      "--------------------\n",
      "Average Precision: 0.4\n",
      "--------------------\n",
      "Recall: 0.74\n",
      "--------------------\n",
      "F1-score: 0.51\n",
      "--------------------\n",
      "Matthews Correlation Coefficient: 0.3\n",
      "--------------------\n",
      "Fowlkes–Mallows Index: 0.59\n",
      "--------------------\n",
      "ROC: 0.71\n",
      "--------------------\n",
      "AUC: 0.71\n",
      "--------------------\n",
      "Recall Best Score: 0.67\n",
      "--------------------\n",
      "Log Loss/Cross Entropy: 1.19\n",
      "--------------------\n",
      "Cohen’s Kappa: 0.26\n",
      "--------------------\n",
      "Geometric Mean: 0.45\n",
      "--------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.61      0.71       446\n",
      "           1       0.39      0.74      0.51       152\n",
      "\n",
      "    accuracy                           0.64       598\n",
      "   macro avg       0.63      0.67      0.61       598\n",
      "weighted avg       0.75      0.64      0.66       598\n",
      "\n",
      "--------------------\n",
      "Imbalanced Classification Report:\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.87      0.61      0.74      0.71      0.67      0.44       446\n",
      "          1       0.39      0.74      0.61      0.51      0.67      0.45       152\n",
      "\n",
      "avg / total       0.75      0.64      0.70      0.66      0.67      0.44       598\n",
      "\n",
      "--------------------\n",
      "Confusion Matrix:\n",
      "[[270 176]\n",
      " [ 40 112]]\n",
      "--------------------\n",
      "Normalized Confusion Matrix:\n",
      "[[0.60538117 0.39461883]\n",
      " [0.26315789 0.73684211]]\n",
      "--------------------\n",
      "====================\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Plotting metrics.\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "Plotting Learning Curve.\n",
      "--------------------\n",
      "[learning_curve] Training set sizes: [ 403 1311 2218 3126 4034]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:   56.8s finished\n",
      "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Plotting metrics with y_pred_prob:\n",
      "====================\n",
      "Plotting Confusion Matrix:\n",
      "====================\n",
      "Saving Confusion Matrix...\n",
      "Saved Confusion Matrix!\n",
      "====================\n",
      "Plotting Normalized Confusion Matrix:\n",
      "====================\n",
      "Saving Normalized Confusion Matrix...\n",
      "Saved Normalized Confusion Matrix!\n",
      "====================\n",
      "Plotting ROC Curve:\n",
      "====================\n",
      "Saving ROC Curve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC Curve!\n",
      "====================\n",
      "Plotting Precision-Recall Curve:\n",
      "====================\n",
      "Saving Precision-Recall Curve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n",
      "WARNING:matplotlib.backends.backend_ps:The PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Precision-Recall Curve!\n",
      "====================\n",
      "Plotting Calibration Curve:\n",
      "====================\n",
      "Saving Calibration Curve...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [05:46<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Calibration Curve!\n",
      "====================\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/IPython/cor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">e/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">execution.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1319</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">time</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1316 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1317 │   │   │   </span>st = clock2()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1318 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1319 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exec(code, glob, local_ns)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1320 │   │   │   │   </span>out=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1321 │   │   │   │   # multi-line %%time case</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> expr_val <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;timed exec&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">SystemExit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/IPython/cor\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33me/magics/\u001b[0m\u001b[1;33mexecution.py\u001b[0m:\u001b[94m1319\u001b[0m in \u001b[92mtime\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1316 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1317 \u001b[0m\u001b[2m│   │   │   \u001b[0mst = clock2()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1318 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1319 \u001b[2m│   │   │   │   \u001b[0mexec(code, glob, local_ns)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1320 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mout=\u001b[94mNone\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1321 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# multi-line %%time case\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m expr_val \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<timed exec>\u001b[0m:\u001b[94m96\u001b[0m in \u001b[92m<module>\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mSystemExit: \u001b[0m\u001b[1;36m0\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "col_names_list, vectorizer_names_list, classifier_names_list, estimator_names_list = get_existing_estimators()\n",
    "\n",
    "# Load Table DF\n",
    "df_metrics = make_df_metrics(vectorizers_pipe, classifiers_pipe, list(metrics_dict.keys()), analysis_columns)\n",
    "\n",
    "# Identify cols, vectorizers and classifiers\n",
    "for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.pkl')):\n",
    "    col = estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "    vectorizer_name = estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "    classifier_name = estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "    protocol = int(estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[-1].split(').pkl')[0])\n",
    "\n",
    "    print('~'*20)\n",
    "    print(f'Loading data for {col} - {vectorizer_name} + {classifier_name}')\n",
    "    print('~'*20)\n",
    "    # Load X, y, search_cv, estimator\n",
    "    (\n",
    "        grid_search, searchcv,\n",
    "        X_train, y_train, y_train_pred,\n",
    "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val,\n",
    "        df_feature_importances, estimator,\n",
    "    ) = load_Xy_search_cv_estimator(\n",
    "        col, vectorizer_name, classifier_name, protocol\n",
    "    )\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} EVALUATING DATASET OF LENGTH {len(X_train)+len(X_test)+len(X_val)} ON {col.upper()} {\"=\"*30}')\n",
    "\n",
    "    # Identify and name steps in estimator\n",
    "    vectorizer = estimator[0]\n",
    "    vectorizer_params = vectorizer.get_params()\n",
    "    vectorizer_name = vectorizer.__class__.__name__\n",
    "    selector = estimator[1]\n",
    "    selector_params = selector.get_params()\n",
    "    selector_name = selector.__class__.__name__\n",
    "    classifier = estimator[-1]\n",
    "    classifier_params = classifier.get_params()\n",
    "    classifier_name = classifier.__class__.__name__\n",
    "    if col == 'Warmth':\n",
    "        resampler = estimator[-2]\n",
    "        resampler_params = resampler.get_params()\n",
    "        resampler_name = resampler.__class__.__name__\n",
    "\n",
    "    # Print results\n",
    "    print('='*20)\n",
    "    print(\n",
    "        f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'Best estimator and parameters:\\n{estimator}\\n')\n",
    "    print(\n",
    "        f'Best parameters:\\n{(best_params:=searchcv.best_params_)}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'Training Classification Report:\\n{(train_report:=metrics.classification_report(y_train, y_train_pred, labels=np.unique(y_train_pred), zero_division=0))}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'Best train score: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
    "    )\n",
    "    print(\n",
    "        f'Best test score: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
    "    )\n",
    "    print('='*20)\n",
    "    print('Training Confusion Matrix:\\n')\n",
    "    close_plots()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\n",
    "        f'{str(col)} - Training Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
    "    train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "        estimator, X_train, y_train, ax=ax, cmap=plt.cm.Blues\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "    print('='*20)\n",
    "\n",
    "    # Evaluate Model\n",
    "    df_metrics, metrics_dict, df_cv_score_recall = evaluation(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        best_train_score, df_metrics,\n",
    "        col, vectorizer_name, classifier_name, \n",
    "    )\n",
    "    sys.exit(0)\n",
    "\n",
    "    # Fit best model on validation set\n",
    "    print(f'Fitting {estimator}.')\n",
    "    estimator.set_params(**estimator.get_params())\n",
    "    estimator = estimator.fit(X_val, y_val)\n",
    "\n",
    "    # Save Vectorizer, Selector, and Classifier\n",
    "    save_estimator_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    # Compare Estimators\n",
    "    print('='*20)\n",
    "    print(f'Comparing Estimators for {col}')\n",
    "    comparison_plots(get_completed_estimators(), X_test, y_test, col)\n",
    "    print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc80e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
