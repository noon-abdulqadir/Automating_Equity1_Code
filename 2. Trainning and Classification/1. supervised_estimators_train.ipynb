{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # isort:skip # fmt:skip # noqa # nopep8 \n",
    "import sys # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615c1474ecb74cf3afabb4325580bb6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a994ae91893245d59f43f47cb60302f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45448e021ff647adb68c8b0ce5c89351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ae85e3a2d24f40b5ab29ffa87c5b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import * # isort:skip # fmt:skip # noqa # nopep8\n",
    "from supervised_estimators_get_pipe import * # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccdfed",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967a21f1106c4efda1eac64e24ef85c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d6233f5010451bb4c190957ffc9bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variables\n",
    "warnings.filterwarnings('always')\n",
    "# Sklearn variables\n",
    "method = 'Supervised'\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
    "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
    "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Train - Mean Cross Validation Score': np.nan,\n",
    "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Test - Mean Cross Validation Score': np.nan,\n",
    "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
    "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Average Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan,\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "cores = multiprocessing.cpu_count()\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    bert_model_name, strip_accents=True\n",
    ")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name\n",
    ").to(device)\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f83df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    col_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if col_names_list is None:\n",
    "        col_names_list = []\n",
    "    if vectorizer_names_list is None:\n",
    "        vectorizer_names_list = []\n",
    "    if classifier_names_list is None:\n",
    "        classifier_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in glob.glob(f'{results_save_path}*.pkl'):\n",
    "        col_names_list.append(\n",
    "            col := estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "        )\n",
    "        vectorizer_names_list.append(\n",
    "            vectorizer_name := estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "        )\n",
    "        classifier_names_list.append(\n",
    "            classifier_name := estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "        )\n",
    "\n",
    "    estimator_names_list = [\n",
    "        f'{col} - {vectorizer_name} + {classifier_name}'\n",
    "        for col, vectorizer_name, classifier_name in tqdm_product(\n",
    "            list(set(col_names_list)),\n",
    "            list(set(vectorizer_names_list)),\n",
    "            list(set(classifier_names_list)),\n",
    "        )\n",
    "    ]\n",
    "    return (\n",
    "        list(set(col_names_list)),\n",
    "        list(set(vectorizer_names_list)),\n",
    "        list(set(classifier_names_list)),\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_weights_print_Xy(\n",
    "    X_train, X_test,\n",
    "    y_train, y_test,\n",
    "    X_val, y_val\n",
    "):\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "    \n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, text_col=text_col, analysis_columns=analysis_columns, random_state=random_state):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    # Get class weights and print info\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict\n",
    "    ) = class_weights_print_Xy(\n",
    "        X_train, X_test,\n",
    "        y_train, y_test,\n",
    "        X_val, y_val\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f41e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path, method=method, protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    searchcv_save_path=None, compression=None, path_suffix=None, data=None\n",
    "):\n",
    "    if searchcv_save_path is None:\n",
    "        searchcv_save_path = f'{results_save_path}SearchCV/'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {str(col)} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data is None:\n",
    "        data = {}\n",
    "\n",
    "    print(f'Loading Xy from previous for {col} - {vectorizer_name} + {classifier_name}...')\n",
    "    # Read all dfs into \n",
    "    for file_path in glob.glob(f'{searchcv_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{searchcv_save_path}{method} ')[-1].split(path_suffix)[0]\n",
    "        if 'df_' in file_name and 'cv_results' not in file_name:\n",
    "            data[file_name] = pd.read_pickle(file_path)\n",
    "    \n",
    "    # Assign dfs to variables\n",
    "    df_train_data = data['df_train_data']\n",
    "    df_test_data = data['df_test_data']\n",
    "    df_val_data = data['df_val_data']\n",
    "\n",
    "    # Train data\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    # Test data\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    # Val data\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    print(f'Done loading Xy from previous for {col} - {vectorizer_name} + {classifier_name}.')\n",
    "\n",
    "    # Get class weights and print info\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict\n",
    "    ) = class_weights_print_Xy(\n",
    "        X_train, X_test,\n",
    "        y_train, y_test,\n",
    "        X_val, y_val\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f74f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize unusual classifiers after fitting\n",
    "def normalize_after_fitting(estimator, X_train, y_train, X_test, y_test, searchcv):\n",
    "\n",
    "    # Get feature importance if classifier provides them and use as X\n",
    "    if any(hasattr(estimator, feature_attr) for feature_attr in ['feature_importances_', 'coef_']):\n",
    "        feature_selector = SelectFromModel(estimator, prefit=True)\n",
    "        X_train = feature_selector.transform(X_train)\n",
    "        X_test = X_test[:, feature_selector.get_support()]\n",
    "        df_feature_importances = pd.DataFrame(\n",
    "            {\n",
    "                'features': X_test.values,\n",
    "                'feature_importances': estimator.feature_importances_\n",
    "            }\n",
    "        )\n",
    "        df_feature_importances = df_feature_importances.sort_values(\n",
    "                    'feature_importances', ascending=False)\n",
    "        print(df_feature_importances.head(20))\n",
    "        print(f'Best estimator has feature_importances of shape:\\n{estimator}')\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "\n",
    "    # For perceptron: calibrate classifier to get prediction probabilities\n",
    "    if not hasattr(searchcv, 'predict_proba') and not hasattr(searchcv, '_predict_proba_lr') and hasattr(searchcv, 'decision_function'):\n",
    "        searchcv = CalibratedClassifierCV(\n",
    "            searchcv, cv=cv, method='sigmoid'\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    # For Sequential classifier: compile for binary classification, optimize with adam and score on recall\n",
    "    if classifier_name == 'Sequential':\n",
    "        searchcv.compile(\n",
    "            loss='binary_crossentropy', optimizer='adam', metrics=list(scoring)\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    return (\n",
    "        estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e46bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy_search_cv_estimator(\n",
    "    grid_search, searchcv, cv_results,\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    df_feature_importances, estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, searchcv_save_path=None,\n",
    "    compression=None, protocol=None,\n",
    "    path_suffix=None, data=None\n",
    "):\n",
    "\n",
    "    if searchcv_save_path is None:\n",
    "        searchcv_save_path = f'{results_save_path}SearchCV/'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {str(col)} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "    if data is None:\n",
    "        data = {}\n",
    "\n",
    "    # Make df_cv_results\n",
    "    df_cv_results = pd.DataFrame(\n",
    "        cv_results\n",
    "    )\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': y_train_pred,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Make data dict\n",
    "    data['Grid Search'] = grid_search\n",
    "    data['SearchCV'] = searchcv\n",
    "    data['df_cv_results'] = df_cv_results\n",
    "    data['df_train_data'] = df_train_data\n",
    "    data['df_test_data'] = df_test_data\n",
    "    data['df_val_data'] = df_val_data\n",
    "    data['Estimator'] = estimator\n",
    "    if df_feature_importances is not None:\n",
    "        data['df_feature_importances'] = df_feature_importances\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    print('Saving Xy, CV data, and estimator...')\n",
    "    for file_name, file_ in data.items():\n",
    "        path = searchcv_save_path if file_name != 'Estimator' else results_save_path\n",
    "        if not isinstance(file_, pd.DataFrame) and 'df_' not in file_name:\n",
    "            with open(\n",
    "                f'{path}{method} {file_name}{path_suffix}', 'wb'\n",
    "            ) as f:\n",
    "                joblib.dump(file_, f, compress=compression, protocol=protocol)\n",
    "        elif isinstance(file_, pd.DataFrame) and 'df_' in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "    print(f'Done saving Xy, CV data, and estimator!\\n{list(data.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "assert len(df_manual) == 5978, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH 5978 BUT IS OF LENGTH {len(df_manual)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory:\n",
      "/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Supervised Results/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a46d7c27f4d4f6b8001e7b0c847bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5978 ON WARMTH ==============================\n",
      "--------------------\n",
      "Vectorizers to be used (3):\n",
      "['CountVectorizer', 'TfidfVectorizer', 'FeatureUnion']\n",
      "Total number of vectorizer parameters = 22\n",
      "Selectors to be used (1):\n",
      "['SelectKBest']\n",
      "Total number of selector parameters = 2\n",
      "Resamplers to be used (1):\n",
      "['SMOTETomek']\n",
      "Total number of resamplers parameters = 2\n",
      "Classifers to be used (14):\n",
      "['DummyClassifier', 'MultinomialNB', 'KNeighborsClassifier', 'LogisticRegression', 'PassiveAggressiveClassifier', 'Perceptron', 'LinearSVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'XGBClassifier', 'MLPClassifier', 'VotingClassifier', 'StackingClassifier']\n",
      "Total number of classifers parameters = 56\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "Done splitting data into training, testing, and validation sets.\n",
      "====================\n",
      "Training set shape: (4483,)\n",
      "----------\n",
      "Training set example:\n",
      "Factors)\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (598,)\n",
      "----------\n",
      "Testing set example:\n",
      "You’ll be part of an international consulting firm where people are lead by its values and inspired by our purpose;\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (897,)\n",
      "----------\n",
      "Validation set example:\n",
      "Also my client is HQ’d in Europe meaning you will have better visibility and find it easier to engage and collaborate with your peers and the senior stakeholders (no late night calls to the USA).\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.34 (0 = 0.67, 1 = 1.97)\n",
      "====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ba11b969a541abaeb2f635eefad7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== Using GridSearchCV ==============================\n",
      "--------------------\n",
      "GridSearchCV with:\n",
      "Pipe:\n",
      "Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()), ('SMOTETomek', SMOTETomek()),\n",
      "                ('DummyClassifier', DummyClassifier())])\n",
      "Params:\n",
      "{'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.7], 'CountVectorizer__min_df': [0.1, 0.15, 0.2, 0.25, 0.3], 'SelectKBest__score_func': [<function f_classif at 0x29779b7f0>, <function chi2 at 0x29779b910>, <function mutual_info_classif at 0x2977c6170>, <function f_regression at 0x29779ba30>, <function mutual_info_regression at 0x2977c60e0>], 'SelectKBest__k': ['all'], 'DummyClassifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform'], 'DummyClassifier__random_state': [42]}\n",
      "++++++++++++++++++++++++++++++\n",
      "Fitting GridSearchCV\n",
      "n_iterations: 4\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 120\n",
      "max_resources_: 4483\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1000\n",
      "n_resources: 120\n",
      "Fitting 30 folds for each of 1000 candidates, totalling 30000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 334\n",
      "n_resources: 360\n",
      "Fitting 30 folds for each of 334 candidates, totalling 10020 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 112\n",
      "n_resources: 1080\n",
      "Fitting 30 folds for each of 112 candidates, totalling 3360 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 38\n",
      "n_resources: 3240\n",
      "Fitting 30 folds for each of 38 candidates, totalling 1140 fits\n",
      "====================\n",
      "Saving Xy, CV data, and estimator...\n",
      "Done saving Xy, CV data, and estimator!\n",
      "['Grid Search', 'SearchCV', 'df_cv_results', 'df_train_data', 'df_test_data', 'df_val_data', 'Estimator']\n",
      "====================\n",
      "--------------------\n",
      "============================== Using GridSearchCV ==============================\n",
      "--------------------\n",
      "GridSearchCV with:\n",
      "Pipe:\n",
      "Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()), ('SMOTETomek', SMOTETomek()),\n",
      "                ('MultinomialNB', MultinomialNB())])\n",
      "Params:\n",
      "{'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.7], 'CountVectorizer__min_df': [0.1, 0.15, 0.2, 0.25, 0.3], 'SelectKBest__score_func': [<function f_classif at 0x29779b7f0>, <function chi2 at 0x29779b910>, <function mutual_info_classif at 0x2977c6170>, <function f_regression at 0x29779ba30>, <function mutual_info_regression at 0x2977c60e0>], 'SelectKBest__k': ['all'], 'MultinomialNB__fit_prior': [True, False]}\n",
      "++++++++++++++++++++++++++++++\n",
      "Fitting GridSearchCV\n",
      "n_iterations: 4\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 120\n",
      "max_resources_: 4483\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 500\n",
      "n_resources: 120\n",
      "Fitting 30 folds for each of 500 candidates, totalling 15000 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 167\n",
      "n_resources: 360\n",
      "Fitting 30 folds for each of 167 candidates, totalling 5010 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 56\n",
      "n_resources: 1080\n",
      "Fitting 30 folds for each of 56 candidates, totalling 1680 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 19\n",
      "n_resources: 3240\n",
      "Fitting 30 folds for each of 19 candidates, totalling 570 fits\n",
      "====================\n",
      "Saving Xy, CV data, and estimator...\n",
      "Done saving Xy, CV data, and estimator!\n",
      "['Grid Search', 'SearchCV', 'df_cv_results', 'df_train_data', 'df_test_data', 'df_val_data', 'Estimator']\n",
      "====================\n",
      "--------------------\n",
      "============================== Using GridSearchCV ==============================\n",
      "--------------------\n",
      "GridSearchCV with:\n",
      "Pipe:\n",
      "Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()), ('SMOTETomek', SMOTETomek()),\n",
      "                ('KNeighborsClassifier', KNeighborsClassifier())])\n",
      "Params:\n",
      "{'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.7], 'CountVectorizer__min_df': [0.1, 0.15, 0.2, 0.25, 0.3], 'SelectKBest__score_func': [<function f_classif at 0x29779b7f0>, <function chi2 at 0x29779b910>, <function mutual_info_classif at 0x2977c6170>, <function f_regression at 0x29779ba30>, <function mutual_info_regression at 0x2977c60e0>], 'SelectKBest__k': ['all'], 'KNeighborsClassifier__weights': ['uniform', 'distance'], 'KNeighborsClassifier__n_neighbors': [2, 5, 15], 'KNeighborsClassifier__algorithm': ['auto']}\n",
      "++++++++++++++++++++++++++++++\n",
      "Fitting GridSearchCV\n",
      "n_iterations: 4\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 120\n",
      "max_resources_: 4483\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1500\n",
      "n_resources: 120\n",
      "Fitting 30 folds for each of 1500 candidates, totalling 45000 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "col_names_list, vectorizer_names_list, classifier_names_list, estimator_names_list = get_existing_files()\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'Vectorizers to be used ({len(list(vectorizers_pipe.values()))}):\\n{list(vectorizers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of vectorizer parameters = {sum([len(list(vectorizers_pipe.values())[i][1]) for i in range(len(vectorizers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Selectors to be used ({len(list(selectors_pipe.values()))}):\\n{list(selectors_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of selector parameters = {sum([len(list(selectors_pipe.values())[i][1]) for i in range(len(selectors_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Resamplers to be used ({len(list(resamplers_pipe.keys()))}):\\n{list(resamplers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of resamplers parameters = {sum([len(list(resamplers_pipe.values())[i][1]) for i in range(len(resamplers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Classifers to be used ({len(list(classifiers_pipe.keys()))}):\\n{list(classifiers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of classifers parameters = {sum([len(list(classifiers_pipe.values())[i][1]) for i in range(len(classifiers_pipe))])}'\n",
    "    )\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict\n",
    "    ) = split_data(\n",
    "        df_manual, col, text_col, analysis_columns,\n",
    "    )\n",
    "\n",
    "    for (\n",
    "        vectorizer_name, vectorizer_and_params\n",
    "    ), (\n",
    "        selector_name, selector_and_params\n",
    "    ), (\n",
    "        resampler_name, resampler_and_params\n",
    "    ), (\n",
    "        classifier_name, classifier_and_params\n",
    "    ) in tqdm_product(\n",
    "        vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "    ):\n",
    "\n",
    "        if f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list:\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            # Load previous Xy\n",
    "            (\n",
    "                X_train, y_train,\n",
    "                X_test, y_test,\n",
    "                X_val, y_val,\n",
    "                train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "                test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict\n",
    "            ) = load_Xy(\n",
    "                col, vectorizer_name, classifier_name\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Identify names and params\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        selector = selector_and_params[0]\n",
    "        selector_params = selector_and_params[1]\n",
    "\n",
    "        resampler = resampler_and_params[0]\n",
    "        resampler_params = resampler_and_params[1]\n",
    "\n",
    "        classifier = classifier_and_params[0]\n",
    "        classifier_params = classifier_and_params[1]\n",
    "\n",
    "        # Pipeline\n",
    "        ## Steps\n",
    "        if col == 'Warmth':\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (resampler_name, resampler),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "        else:\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "\n",
    "        ## Params\n",
    "        param_grid = {\n",
    "            **vectorizer_params,\n",
    "            **selector_params,\n",
    "            **classifier_params,\n",
    "        }\n",
    "\n",
    "        ## Pipeline\n",
    "        pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "        # Search\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "        print('+'*30)\n",
    "\n",
    "        grid_search = HalvingGridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True,\n",
    "            verbose=1,\n",
    "            error_score='raise',\n",
    "            refit=refit,\n",
    "            random_state=random_state,\n",
    "            scoring=scorers['recall_score'],\n",
    "        )\n",
    "\n",
    "        ## Normalize unusual classifiers before fitting\n",
    "        if classifier_name == 'GaussianNB':\n",
    "            X_train = X_train.todense()\n",
    "            X_test = X_test.todense()\n",
    "            X_val = X_val.todense()\n",
    "\n",
    "        # Fit SearchCV\n",
    "        with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "            print('Fitting GridSearchCV')\n",
    "            searchcv = grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Reidentify and name best estimator and params\n",
    "            estimator = searchcv.best_estimator_\n",
    "            cv_results = searchcv.cv_results_\n",
    "            vectorizer = estimator[0]\n",
    "            vectorizer_params = vectorizer.get_params()\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            selector = estimator[1]\n",
    "            selector_params = selector.get_params()\n",
    "            selector_name = selector.__class__.__name__\n",
    "            classifier = estimator[-1]\n",
    "            classifier_params = classifier.get_params()\n",
    "            classifier_name = classifier.__class__.__name__\n",
    "            if col == 'Warmth':\n",
    "                resampler = estimator[-2]\n",
    "                resampler_params = resampler.get_params()\n",
    "                resampler_name = resampler.__class__.__name__\n",
    "\n",
    "            # Normalize unusual classifiers after fitting\n",
    "            (\n",
    "                estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "            ) = normalize_after_fitting(\n",
    "                estimator, X_train, y_train, X_test, y_test, searchcv\n",
    "            )\n",
    "\n",
    "            # Set prediction probability attribute\n",
    "            if hasattr(searchcv, 'predict_proba'):\n",
    "                searchcv_predict_attr = searchcv.predict_proba\n",
    "            elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            y_train_pred = estimator.predict(X_train)\n",
    "            y_test_pred = searchcv.predict(X_test)\n",
    "            y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "            # Save Xy and CV data\n",
    "            save_Xy_search_cv_estimator(\n",
    "                grid_search, searchcv, cv_results,\n",
    "                X_train, y_train, y_train_pred,\n",
    "                X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                X_val, y_val,\n",
    "                df_feature_importances, estimator,\n",
    "                col, vectorizer_name, classifier_name,\n",
    "            )\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8e5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
