{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # isort:skip # fmt:skip # noqa # nopep8 \n",
    "import sys # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ff168d0aa641a68787e055c5880975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743642ab33bc4a2caa64bfaae178e684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9887168787b149f285a018f7da3252e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af781418b8eb4989865b71ba302e8a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import * # isort:skip # fmt:skip # noqa # nopep8\n",
    "from supervised_estimators_get_pipe import * # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccdfed",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b36fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c4052fe58841c7a6364fa4581dc25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0693a116b66244dc88788cbe9ab2c130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Variables\n",
    "warnings.filterwarnings('always')\n",
    "# Sklearn variables\n",
    "method = 'Supervised'\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "cores = multiprocessing.cpu_count()\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    bert_model_name, strip_accents=True\n",
    ")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    bert_model_name\n",
    ").to(device)\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c5be4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f83df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    col_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if col_names_list is None:\n",
    "        col_names_list = []\n",
    "    if vectorizer_names_list is None:\n",
    "        vectorizer_names_list = []\n",
    "    if classifier_names_list is None:\n",
    "        classifier_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimator in dir:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in glob.glob(f'{results_save_path}*.pkl'):\n",
    "        col_names_list.append(\n",
    "            col_name_from_file:=estimators_file.split(f'{method} Estimator ')[-1].split(' - ')[0]\n",
    "        )\n",
    "        vectorizer_names_list.append(\n",
    "            vectorizer_name_from_file:=estimators_file.split(f'{col_name_from_file} - ')[-1].split(' + ')[0]\n",
    "        )\n",
    "        classifier_names_list.append(\n",
    "            classifier_name_from_file:=estimators_file.split(f'{vectorizer_name_from_file} + ')[-1].split('.pkl')[0]\n",
    "        )\n",
    "\n",
    "    estimator_names_list = [\n",
    "        f'{col_name_from_file} - {vectorizer_name_from_file} + {classifier_name_from_file}'\n",
    "        for col_name_from_file, vectorizer_name_from_file, classifier_name_from_file in tqdm_product(\n",
    "            list(set(col_names_list)),\n",
    "            list(set(vectorizer_names_list)),\n",
    "            list(set(classifier_names_list)),\n",
    "        )\n",
    "    ]\n",
    "    return (\n",
    "        list(set(col_names_list)),\n",
    "        list(set(vectorizer_names_list)),\n",
    "        list(set(classifier_names_list)),\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, text_col=text_col, analysis_columns=analysis_columns, random_state=random_state):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "    \n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = [0,1], y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e46bb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy_search_cv_estimator(\n",
    "    grid_search, searchcv,\n",
    "    X_train, y_train, y_train_pred,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    X_val, y_val,\n",
    "    df_feature_importances, estimator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    method=method, searchcv_save_path=None,\n",
    "    compression=None, protocol=None\n",
    "):\n",
    "\n",
    "    if searchcv_save_path is None:\n",
    "        searchcv_save_path = f'{results_save_path}/SearchCV'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "\n",
    "    df_cv_results = pd.DataFrame(searchcv.cv_results_)\n",
    "    to_save = {'df_cv_results': df_cv_results}\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'y_train_pred': y_train_pred,\n",
    "        },\n",
    "    )\n",
    "    to_save['df_train_data'] = df_train_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "        },\n",
    "    )\n",
    "    to_save['df_test_data'] = df_test_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "    to_save['df_val_data'] = df_val_data\n",
    "    if df_feature_importances is not None:\n",
    "        df_feature_importances.to_pickle(\n",
    "            f'{save_path}Save_protocol={protocol} - {method} df_feature_importances - {col}_{vectorizer_name}_{classifier_name}.pkl',\n",
    "            compression=compression, protocol=protocol\n",
    "        )\n",
    "\n",
    "    to_save['Grid Search'] = grid_search\n",
    "    to_save['SearchCV'] = searchcv\n",
    "    to_save['Estimator'] = estimator\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    print('Saving Xy, CV data, and estimator...')\n",
    "    for file_name, file_ in to_save.items():\n",
    "        path = searchcv_save_path if file_name != 'Estimator' else results_save_path\n",
    "        if not isinstance(file_, pd.DataFrame) and 'df_' not in file_name:\n",
    "            with open(\n",
    "                f'{path}Save_protocol={protocol} - {method} {file_name} {str(col)} - {vectorizer_name} + {classifier_name}.pkl',\n",
    "                'wb', compression=compression, protocol=protocol\n",
    "            ) as f:\n",
    "                joblib.dump(grid_search, f, compress=compression, protocol=protocol)\n",
    "        elif isinstance(file_, pd.DataFrame) and 'df_' in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{path}Save_protocol={protocol} - {method} {file_name} {str(col)} - {vectorizer_name} + {classifier_name}.pkl',\n",
    "                protocol=protocol\n",
    "            )\n",
    "    print('Done saving Xy, CV data, and estimator!')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1f74f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize unusual classifiers after fitting\n",
    "def normalize_after_fitting(estimator, X_train, y_train, X_test, y_test, searchcv):\n",
    "\n",
    "    # Get feature importance if classifier provides them and use as X\n",
    "    if any(hasattr(estimator, feature_attr) for feature_attr in ['feature_importances_', 'coef_']):\n",
    "        feature_selector = SelectFromModel(estimator, prefit=True)\n",
    "        X_train = feature_selector.transform(X_train)\n",
    "        X_test = X_test[:, feature_selector.get_support()]\n",
    "        df_feature_importances = pd.DataFrame(\n",
    "            {\n",
    "                'features': X_test.values,\n",
    "                'feature_importances': estimator.feature_importances_\n",
    "            }\n",
    "        )\n",
    "        df_feature_importances = df_feature_importances.sort_values(\n",
    "                    'feature_importances', ascending=False)\n",
    "        print(df_feature_importances.head(20))\n",
    "        print(f'Best estimator has feature_importances of shape:\\n{estimator}')\n",
    "    else:\n",
    "        df_feature_importances = None\n",
    "\n",
    "    # For perceptron: calibrate classifier to get prediction probabilities\n",
    "    if hasattr(searchcv, 'decision_function') and not all(hasattr(searchcv, pred_attr) for pred_attr in ['predict_proba', '_predict_proba_lr']):\n",
    "        searchcv = CalibratedClassifierCV(\n",
    "            searchcv, cv=cv, method='sigmoid'\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    # For Sequential classifier: compile for binary classification, optimize with adam and score on recall\n",
    "    if classifier_name == 'Sequential':\n",
    "        searchcv.compile(\n",
    "            loss='binary_crossentropy', optimizer='adam', metrics=list(scoring)\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "    return (\n",
    "        estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036fcf10",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "assert len(df_manual) == 5978, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH 5978 BUT IS OF LENGTH {len(df_manual)}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimator in dir:\n",
      "/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Supervised Results/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b39f18be3e488bb7f5994c3af5aadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                 | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5978 ON WARMTH ==============================\n",
      "--------------------\n",
      "Vectorizers to be used (3):\n",
      "['CountVectorizer', 'TfidfVectorizer', 'FeatureUnion']\n",
      "Total number of vectorizer parameters = 22\n",
      "Selectors to be used (1):\n",
      "['SelectKBest']\n",
      "Total number of selector parameters = 2\n",
      "Resamplers to be used (1):\n",
      "['SMOTETomek']\n",
      "Total number of resamplers parameters = 2\n",
      "Classifers to be used (14):\n",
      "['DummyClassifier', 'MultinomialNB', 'KNeighborsClassifier', 'LogisticRegression', 'PassiveAggressiveClassifier', 'Perceptron', 'LinearSVC', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'XGBClassifier', 'MLPClassifier', 'VotingClassifier', 'StackingClassifier']\n",
      "Total number of classifers parameters = 57\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "Done splitting data into training, testing, and validation sets.\n",
      "====================\n",
      "Training set shape: (4483,)\n",
      "----------\n",
      "Training set example:\n",
      "Factors)\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (598,)\n",
      "----------\n",
      "Testing set example:\n",
      "You’ll be part of an international consulting firm where people are lead by its values and inspired by our purpose;\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (897,)\n",
      "----------\n",
      "Validation set example:\n",
      "Also my client is HQ’d in Europe meaning you will have better visibility and find it easier to engage and collaborate with your peers and the senior stakeholders (no late night calls to the USA).\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.33 (0 = 0.67, 1 = 2.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.34 (0 = 0.67, 1 = 1.97)\n",
      "====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea10cf598d3d4f0695a81d7818843441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already trained Warmth - CountVectorizer + DummyClassifier\n",
      "Already trained Warmth - CountVectorizer + MultinomialNB\n",
      "Already trained Warmth - CountVectorizer + KNeighborsClassifier\n",
      "--------------------\n",
      "============================== Using GridSearchCV ==============================\n",
      "--------------------\n",
      "GridSearchCV with:\n",
      "Pipe:\n",
      "Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()), ('SMOTETomek', SMOTETomek()),\n",
      "                ('LogisticRegression', LogisticRegression())])\n",
      "Params:\n",
      "{'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.7], 'CountVectorizer__min_df': [0.1, 0.15, 0.2, 0.25, 0.3], 'SelectKBest__score_func': [<function f_classif at 0x17e736f80>, <function chi2 at 0x17e7370a0>, <function mutual_info_classif at 0x17e759900>, <function f_regression at 0x17e7371c0>, <function mutual_info_regression at 0x17e759870>], 'SelectKBest__k': ['all'], 'LogisticRegression__class_weight': ['balanced'], 'LogisticRegression__random_state': [42], 'LogisticRegression__fit_intercept': [True, False], 'LogisticRegression__multi_class': ['auto'], 'LogisticRegression__solver': ['liblinear'], 'LogisticRegression__C': [0.01, 1, 100]}\n",
      "++++++++++++++++++++++++++++++\n",
      "n_iterations: 4\n",
      "n_required_iterations: 7\n",
      "n_possible_iterations: 4\n",
      "min_resources_: 120\n",
      "max_resources_: 4483\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 1500\n",
      "n_resources: 120\n",
      "Fitting 30 folds for each of 1500 candidates, totalling 45000 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "col_names_list, vectorizer_names_list, classifier_names_list, estimator_names_list = get_existing_files()\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'Vectorizers to be used ({len(list(vectorizers_pipe.values()))}):\\n{list(vectorizers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of vectorizer parameters = {sum([len(list(vectorizers_pipe.values())[i][1]) for i in range(len(vectorizers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Selectors to be used ({len(list(selectors_pipe.values()))}):\\n{list(selectors_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of selector parameters = {sum([len(list(selectors_pipe.values())[i][1]) for i in range(len(selectors_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Resamplers to be used ({len(list(resamplers_pipe.keys()))}):\\n{list(resamplers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of resamplers parameters = {sum([len(list(resamplers_pipe.values())[i][1]) for i in range(len(resamplers_pipe))])}'\n",
    "    )\n",
    "    print(\n",
    "        f'Classifers to be used ({len(list(classifiers_pipe.keys()))}):\\n{list(classifiers_pipe.keys())}'\n",
    "    )\n",
    "    print(\n",
    "        f'Total number of classifers parameters = {sum([len(list(classifiers_pipe.values())[i][1]) for i in range(len(classifiers_pipe))])}'\n",
    "    )\n",
    "\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights,\n",
    "        train_class_weights_ratio,\n",
    "        train_class_weights_dict,\n",
    "        test_class_weights,\n",
    "        test_class_weights_ratio,\n",
    "        test_class_weights_dict\n",
    "    ) = split_data(\n",
    "        df_manual, col, text_col, analysis_columns,\n",
    "    )\n",
    "\n",
    "    for (\n",
    "        vectorizer_name, vectorizer_and_params\n",
    "    ), (\n",
    "        selector_name, selector_and_params\n",
    "    ), (\n",
    "        resampler_name, resampler_and_params\n",
    "    ), (\n",
    "        classifier_name, classifier_and_params\n",
    "    ) in tqdm_product(\n",
    "        vectorizers_pipe.items(), selectors_pipe.items(), resamplers_pipe.items(), classifiers_pipe.items()\n",
    "    ):\n",
    "\n",
    "        if f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list:\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            continue\n",
    "\n",
    "        # Identify names and params\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        selector = selector_and_params[0]\n",
    "        selector_params = selector_and_params[1]\n",
    "\n",
    "        resampler = resampler_and_params[0]\n",
    "        resampler_params = resampler_and_params[1]\n",
    "\n",
    "        classifier = classifier_and_params[0]\n",
    "        classifier_params = classifier_and_params[1]\n",
    "\n",
    "        # Pipeline\n",
    "        ## Steps\n",
    "        if col == 'Warmth':\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (resampler_name, resampler),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "        else:\n",
    "            steps = [\n",
    "                (vectorizer_name, vectorizer),\n",
    "                (selector_name, selector),\n",
    "                (classifier_name, classifier)\n",
    "            ]\n",
    "\n",
    "        ## Params\n",
    "        param_grid = {\n",
    "            **vectorizer_params,\n",
    "            **selector_params,\n",
    "            **classifier_params,\n",
    "        }\n",
    "\n",
    "        ## Pipeline\n",
    "        pipe = imblearn.pipeline.Pipeline(steps=steps)\n",
    "\n",
    "        # Search\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "        print('+'*30)\n",
    "\n",
    "        grid_search = HalvingGridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs,\n",
    "            return_train_score=True,\n",
    "            verbose=1,\n",
    "            scoring=scoring,\n",
    "            error_score='raise',\n",
    "            random_state=random_state,\n",
    "            refit=refit,\n",
    "        )\n",
    "        ## Normalize unusual classifiers before fitting\n",
    "        if classifier_name == 'GaussianNB':\n",
    "            X_train = X_train.todense()\n",
    "            X_test = X_test.todense()\n",
    "            X_val = X_val.todense()\n",
    "\n",
    "        # Fit SearchCV\n",
    "        with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "            searchcv = grid_search.fit(X_train, y_train)\n",
    "\n",
    "            # Reidentify and name best estimator and params\n",
    "            estimator = searchcv.best_estimator_\n",
    "            vectorizer = estimator[0]\n",
    "            vectorizer_params = vectorizer.get_params()\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            selector = estimator[1]\n",
    "            selector_params = selector.get_params()\n",
    "            selector_name = selector.__class__.__name__\n",
    "            classifier = estimator[-1]\n",
    "            classifier_params = classifier.get_params()\n",
    "            classifier_name = classifier.__class__.__name__\n",
    "            if col == 'Warmth':\n",
    "                resampler = estimator[-2]\n",
    "                resampler_params = resampler.get_params()\n",
    "                resampler_name = resampler.__class__.__name__\n",
    "\n",
    "            # Normalize unusual classifiers after fitting\n",
    "            (\n",
    "                estimator, X_train, y_train, X_test, y_test, searchcv, df_feature_importances\n",
    "            ) = normalize_after_fitting(\n",
    "                estimator, X_train, y_train, X_test, y_test, searchcv\n",
    "            )\n",
    "            # # Normalize unusual classifiers after fitting\n",
    "            # ## Get feature importance if classifier provides them and use as X\n",
    "            # if any(hasattr(estimator, feature_attr) for feature_attr in ['feature_importances_', 'coef_']):\n",
    "            #     feature_selector = SelectFromModel(estimator, prefit=True)\n",
    "            #     X_train = feature_selector.transform(X_train)\n",
    "            #     X_test = X_test[:, feature_selector.get_support()]\n",
    "            #     df_feature_importances = pd.DataFrame(\n",
    "            #         {\n",
    "            #             'features': X_test.values,\n",
    "            #             'feature_importances': estimator.feature_importances_\n",
    "            #         }\n",
    "            #     )\n",
    "            #     df_feature_importances = df_feature_importances.sort_values('feature_importances', ascending=False)\n",
    "            #     print(df_feature_importances.head(20))\n",
    "            #     print(f'Best estimator has feature_importances of shape:\\n{estimator}')\n",
    "            # else:\n",
    "            #     df_feature_importances = None\n",
    "            # ## For perceptron: calibrate classifier to get prediction probabilities\n",
    "            # if hasattr(searchcv, 'decision_function') and not all(hasattr(searchcv, pred_attr) for pred_attr in ['predict_proba', '_predict_proba_lr']):\n",
    "            #     searchcv = CalibratedClassifierCV(\n",
    "            #         searchcv, cv=cv, method='sigmoid'\n",
    "            #     ).fit(X_train, y_train)\n",
    "            # ## For Sequential classifier: compile for binary classification, optimize with adam and score on recall\n",
    "            # if classifier_name == 'Sequential':\n",
    "            #     searchcv.compile(\n",
    "            #         loss='binary_crossentropy', optimizer='adam', metrics=list(scoring)\n",
    "            #     ).fit(X_train, y_train)\n",
    "\n",
    "            # Set prediction probability attribute\n",
    "            if hasattr(searchcv, 'predict_proba'):\n",
    "                searchcv_predict_attr = searchcv.predict_proba\n",
    "            elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "\n",
    "            # Get predictions and probabilities\n",
    "            y_train_pred = estimator.predict(X_train)\n",
    "            y_test_pred = searchcv.predict(X_test)\n",
    "            y_test_pred_prob = searchcv_predict_attr(X_test)[:, 1]\n",
    "\n",
    "            # Save Xy and CV data\n",
    "            save_Xy_search_cv_estimator(\n",
    "                grid_search, searchcv,\n",
    "                X_train, y_train, y_train_pred,\n",
    "                X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "                X_val, y_val,\n",
    "                df_feature_importances, estimator,\n",
    "                col, vectorizer_name, classifier_name,\n",
    "            )\n",
    "\n",
    "            # Print results\n",
    "            print('='*20)\n",
    "            print(\n",
    "                f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best estimator and parameters:\\n{estimator}\\n')\n",
    "            print(\n",
    "                f'Best parameters:\\n{(best_params:=searchcv.best_params_)}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Training Classification Report:\\n{(train_report:=classification_report(y_train, y_train_pred))}\\n'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Best train score: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
    "            )\n",
    "            print(\n",
    "                f'Best test score: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
    "            )\n",
    "            print('='*20)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88c51c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
