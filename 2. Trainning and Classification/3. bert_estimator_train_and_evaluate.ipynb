{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "\n",
    "# Sklearn\n",
    "method = 'BERT'\n",
    "final_models_save_path = f'{models_save_path}{method} Results/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc',\n",
    "          'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('PuBu_r')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# BERT variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available() else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7489ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size = 1-test_split, test_size = test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight = 'balanced', classes = [0,1], y = y_train)\n",
    "    class_weights_ratio = class_weights[0]/class_weights[1]\n",
    "    class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets.')\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679eb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, encoded):\n",
    "        self.encodings = encodings\n",
    "        self.encoded = encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.encoded[idx], device=device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(df, col, analysis_columns, text_col=None):\n",
    "\n",
    "    # Split\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        class_weights,\n",
    "        class_weights_ratio,\n",
    "        class_weights_dict\n",
    "    ) = split_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "\n",
    "    if text_col is None:\n",
    "        text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {bert_tokenizer.__class__.__name__}.from_pretrained using {bert_tokenizer.name_or_path}.')\n",
    "\n",
    "    bert_label2id = {label: id_ for id_, label in enumerate(set(label for label in y_train))}\n",
    "    bert_id2label = {id_: label for label, id_ in bert_label2id.items()}\n",
    "\n",
    "    X_train_bert_encodings = bert_tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_train_bert_encoded = [bert_label2id[y] for y in y_train]\n",
    "    bert_train_dataset = MyDataset(X_train_bert_encodings, y_train_bert_encoded)\n",
    "\n",
    "    X_test_bert_encodings = bert_tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_test_bert_encoded = [bert_label2id[y] for y in y_test]\n",
    "    bert_test_dataset = MyDataset(X_test_bert_encodings, y_test_bert_encoded)\n",
    "\n",
    "    X_val_bert_encodings = bert_tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    y_val_bert_encoded = [bert_label2id[y] for y in y_val]\n",
    "    bert_val_dataset = MyDataset(X_val_bert_encodings, y_val_bert_encoded)\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set BERT encodings example:\\n{\" \".join(bert_train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Training labels after BERT encoding: {set(y_train_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set BERT encodings example:\\n{\" \".join(bert_test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing labels after BERT encoding: {set(y_test_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set BERT encodings example:\\n{\" \".join(bert_val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation labels after BERT encoding: {set(y_val_bert_encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Class weights:\\nRatio = {class_weights_ratio:.2f} (0 = {class_weights[0]:.2f}, 1 = {class_weights[1]:.2f})')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "        test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "        val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "        bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_test, y_test_pred,\n",
    "    # col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = [1, 0]\n",
    "\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_test, y_test_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_test_pred)\n",
    "    precision = metrics.precision_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_test, y_test_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_test, y_test_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_test, y_test_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_test, y_test_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_test_pred)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=0.1, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_test, y_test_pred)\n",
    "    report = metrics.classification_report(y_test, y_test_pred)\n",
    "    cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "    cm_normalized = metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_test, y_test_pred,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    # Using y_pred_prob\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_test_pred, normalize='true'\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_test, y_test_pred, pos_label=pos_label\n",
    "    )\n",
    "    close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{str(col)} - {plot_name} {vectorizer_name} + {classifier_name}')\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        plot_.plot(ax=ax)\n",
    "        show_and_close_plots()\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Save Plots\n",
    "        print(f'Saving {plot_name}...')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            plt.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - {plot_name} {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "                format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # # Visualisation with plot_metric\n",
    "    # bc = plot_metric.functions.BinaryClassification(\n",
    "    #     y_test, y_test_pred, labels=[0, 1])\n",
    "\n",
    "    # # Figures\n",
    "    # fig = plt.figure(figsize=(15, 10))\n",
    "    # plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    # bc.plot_confusion_matrix(colorbar=True)\n",
    "    # plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    # bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
    "    # plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    # bc.plot_roc_curve()\n",
    "    # plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    # bc.plot_precision_recall_curve()\n",
    "    # plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    # bc.plot_class_distribution()\n",
    "    # show_and_close_plots()\n",
    "    # bc.print_report()\n",
    "    # for image_save_format in ['eps', 'png']:\n",
    "    #     plt.savefig(\n",
    "    #         f'{plot_save_path}{method} plot_metric Curves {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "    #         format=image_save_format,\n",
    "    #         dpi=3000, bbox_inches='tight'\n",
    "    #     )\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots()\n",
    "    classifications_dict = defaultdict(int)\n",
    "    for _y_test, _y_test_pred in zip(y_test, y_test_pred):\n",
    "        if _y_test != _y_test_pred:\n",
    "            classifications_dict[(_y_test, _y_test_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {str(col)} value': _y_test,\n",
    "            f'Predicted {str(col)} value': _y_test_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_test, _y_test_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {str(col)} value', \n",
    "        columns=f'Predicted {str(col)} value', \n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap='Purples')    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    show_and_close_plots()\n",
    "\n",
    "    return (\n",
    "        cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_test, y_test_pred_prob,\n",
    "    # col, vectorizer_name, classifier_name,\n",
    "    pos_label=None\n",
    "):\n",
    "\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    # Using y_pred_prob\n",
    "    average_precision = metrics.average_precision_score(y_test, y_test_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "    loss = metrics.log_loss(y_test, y_test_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_test, y_test_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold,loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results, \n",
    "    with_y_pred=None,\n",
    "    with_y_pred_prob=None\n",
    "):\n",
    "\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = False\n",
    "    \n",
    "    # Get y_test_pred\n",
    "    y_test = predicted_results.label_ids\n",
    "    y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "    y_test_pred = y_test_pred.flatten().tolist()\n",
    "    y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "\n",
    "    # # Get y_test_pred_prob\n",
    "    # try:\n",
    "    #     y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "    #     print('Using torch.nn.functional.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "    #     print('Using scipy.special.softmax')\n",
    "    # except Exception:\n",
    "    #     y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "    #     print('Using predicted_results.predictions[:, 1]')\n",
    "    # finally:\n",
    "    #     y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "\n",
    "    # Get metrics\n",
    "    print('-'*20)\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        print('Computing metrics using y_test_pred.')\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, kappa, gmean, report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_test, y_test_pred,\n",
    "        )\n",
    "    # Using y_test_pred_prob\n",
    "    if with_y_pred_prob:\n",
    "        print('Computing metrics using y_test_pred_prob.')\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_test, y_test_pred_prob,\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # 'Mean Cross Validation Train Score': float(cv_train_scores),\n",
    "        # 'Mean Cross Validation Test Score': float(cv_test_scores),\n",
    "        # f'Mean Cross Validation Train - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Mean Cross Validation Test - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Mean Explained Train Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # f'Mean Explained Test Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        # 'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        # 'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        # 'ROC': float(roc_auc),\n",
    "        # 'AUC': float(auc),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # f'{scoring.title()} Best Score': float(best_score),\n",
    "        # 'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "    col, vectorizer_name, classifier_name, \n",
    "    with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = False\n",
    "\n",
    "    # Plotting\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        (\n",
    "            cm_curve, cm_normalized_curve, roc_curve, pr_curve, calibration_curve\n",
    "        ) = plot_metrics_with_y_pred(\n",
    "            y_test, y_test_pred,\n",
    "            col, vectorizer_name, classifier_name, \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(metrics_dict, df_metrics, col, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for test_metric_name, metric_value in metrics_dict.items():\n",
    "        if test_metric_name not in ['test_runtime', 'test_samples_per_second', 'test_steps_per_second']:\n",
    "            metric_name = test_metric_name.split(\"test_\")[1].replace('_', ' ').title()\n",
    "            if isinstance(metric_name, float):\n",
    "                print(f'{metric_name}: {round(float(metric_value), 2)}')\n",
    "            else:\n",
    "                print(f'{metric_name}: {metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "            # Fill Table DF\n",
    "            if isinstance(metric_value, float):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = float(metric_value)\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    # Plot Metrics\n",
    "    plot_metrics(\n",
    "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
    "        col, vectorizer_name, classifier_name, \n",
    "    )\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(df_metrics, estimator, col, vectorizer_name, classifier_name):\n",
    "\n",
    "    # Save metrics df\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    df_metrics.to_csv(f'{table_save_path}Classifiers Table.csv')\n",
    "    df_metrics.to_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "    df_metrics.to_excel(f'{table_save_path}Classifiers Table.xlsx')\n",
    "    df_metrics.to_latex(f'{table_save_path}Classifiers Table.tex')\n",
    "    df_metrics.to_markdown(f'{table_save_path}Classifiers Table.md')\n",
    "\n",
    "    # Save estimator\n",
    "    estimator.save_model(f'{models_save_path}{method} Estimator {str(col)} - {vectorizer_name} + {classifier_name})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def get_fitted_estimators():\n",
    "    \n",
    "    estimators_list = []\n",
    "\n",
    "    for model_path in glob.glob(f'{models_save_path}*.pkl'):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(\n",
    "    estimators_list, X_test, y_test, col,\n",
    "    curves_dict=None, cmap=plt.cm.Blues\n",
    "):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        'Calibration Curve': metrics.CalibrationDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{str(curve_name)}: {str(col)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{str(curve_name)}: {str(col)}')\n",
    "        for estimator in estimators_list:\n",
    "            curve = curve_package.from_estimator(\n",
    "                estimator, X_test, y_test, pos_label=1, ax=ax,\n",
    "                name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        print('Saving plots.')\n",
    "        for image_save_format in ['eps', 'png']:\n",
    "            curve.figure_.savefig(\n",
    "                f'{plot_save_path}{method} {str(col)} - All {str(curve_name)}s.{image_save_format}',\n",
    "                format=image_save_format,\n",
    "                dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f64ab3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "# HACK REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=200).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "with joblib.parallel_backend(backend='multiprocessing', n_jobs=n_jobs):\n",
    "    # Load Table DF\n",
    "    df_metrics = pd.read_pickle(f'{table_save_path}Classifiers Table.pkl')\n",
    "\n",
    "    for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "\n",
    "        assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0\n",
    "\n",
    "        # Split\n",
    "        (\n",
    "            train, X_train, X_train_bert_encodings, y_train, y_train_bert_encoded, bert_train_dataset,\n",
    "            test, X_test, X_test_bert_encodings, y_test, y_test_bert_encoded, bert_test_dataset,\n",
    "            val, X_val, X_val_bert_encodings, y_val, y_val_bert_encoded, bert_val_dataset,\n",
    "            bert_label2id, bert_id2label, class_weights, class_weights_ratio, class_weights_dict\n",
    "        ) = encode_data(\n",
    "            df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "        )\n",
    "\n",
    "        # Initialize Model\n",
    "        print(f'Initializing BERT Model.')\n",
    "        tokenizer_name = bert_tokenizer.__class__.__name__\n",
    "        classifier_name = bert_model.__class__.__name__\n",
    "\n",
    "        # Load pre-trained BERT model\n",
    "        bert_model = BertForSequenceClassification.from_pretrained(\n",
    "            bert_model_name, num_labels=len(bert_id2label)\n",
    "        ).to(device)\n",
    "        # bert_model.eval()\n",
    "\n",
    "        # Name tokenizer and classifier name\n",
    "        tokenizer_name = bert_tokenizer.__class__.__name__\n",
    "        classifier_name = bert_model.__class__.__name__\n",
    "\n",
    "        print('='*30)\n",
    "        print(f'Initializing BERT Trainer using {tokenizer_name} + {classifier_name} for {col}')\n",
    "\n",
    "        # Set BERT fine-tuning parameters\n",
    "        bert_training_args = TrainingArguments(\n",
    "            output_dir=f'{models_save_path}{method} Results',\n",
    "            logging_dir=f'{models_save_path}{method} Logs',\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=20,\n",
    "            learning_rate=5e-5,\n",
    "            warmup_steps=100,\n",
    "            weight_decay=0.01,\n",
    "            logging_steps=100,\n",
    "            evaluation_strategy='steps',\n",
    "            optim='adamw_torch',\n",
    "            use_mps_device=True if device.type == 'mps' else False\n",
    "        )\n",
    "\n",
    "        # Pass data to trainer \n",
    "        # FIXME: computer metrics not working\n",
    "        print('-'*20)\n",
    "        print('Passing arguments to estimator.')\n",
    "        estimator = Trainer(\n",
    "            model=bert_model,\n",
    "            tokenizer=bert_tokenizer,\n",
    "            args=bert_training_args,\n",
    "            train_dataset=bert_train_dataset,\n",
    "            eval_dataset=bert_test_dataset,\n",
    "            compute_metrics=compute_metrics,\n",
    "            accelerator=device_name\n",
    "        )\n",
    "        if estimator.place_model_on_device:\n",
    "            estimator.model.to(device)\n",
    "\n",
    "        # Train trainer\n",
    "        print('-'*20)\n",
    "        print(f'Starting training for {col}')\n",
    "        estimator.train()\n",
    "        estimator.save_model(f'{models_save_path}{method} Estimator {str(col)} - {tokenizer_name} + {classifier_name})')\n",
    "        print('Done training!')\n",
    "        print()\n",
    "\n",
    "        # Get predictions\n",
    "        print('-'*20)\n",
    "        print(f'Evaluating estimator for {col}')\n",
    "        estimator.evaluate()\n",
    "        # metrics_dict\n",
    "        predicted_results = estimator.predict(bert_test_dataset)\n",
    "        print(f'Predictions shape for {col}: {predicted_results.predictions.shape}')\n",
    "        print()\n",
    "        \n",
    "        # Get y_test_pred\n",
    "        print('-'*20)\n",
    "        print(f'Getting y_test_pred for {col}')\n",
    "        y_test = predicted_results.label_ids\n",
    "        y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "        y_test_pred = y_test_pred.flatten().tolist()\n",
    "        y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "        print(f'Length of y_test_pred: {len(y_test_pred)}')\n",
    "        print()\n",
    "\n",
    "        # Get y_test_pred_proba\n",
    "        print('-'*20)\n",
    "        print(f'Getting y_test_pred_prob for {col}')\n",
    "        try:\n",
    "            y_test_pred_prob = torch.nn.functional.softmax(predicted_results.predictions.clone().detach().to(device), dim=-1)\n",
    "            print('Using torch.nn.functional.softmax')\n",
    "        except Exception:\n",
    "            y_test_pred_prob = scipy.special.softmax(predicted_results.predictions.clone().detach().to(device), axis=1)\n",
    "            print('Using scipy.special.softmax')\n",
    "        except Exception:\n",
    "            y_test_pred_prob = predicted_results.predictions.clone().detach().to(device)[:, 1]\n",
    "            print('Using predicted_results.predictions[:, 1]')\n",
    "        finally:\n",
    "            y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "        print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "        print()\n",
    "\n",
    "        # HACK\n",
    "        # # Get y_test_pred_proba\n",
    "        # print('-'*20)\n",
    "        # print(f'Getting y_test_pred_prob for {col}')\n",
    "        # try:\n",
    "        #     y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "        #     print('Using torch.nn.functional.softmax')\n",
    "        # except Exception:\n",
    "        #     y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "        #     print('Using scipy.special.softmax')\n",
    "        # except Exception:\n",
    "        #     y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "        #     print('Using predicted_results.predictions[:, 1]')\n",
    "        # finally:\n",
    "        #     y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "        # print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "        # print()\n",
    "        \n",
    "        # Examine predictions\n",
    "        print('-'*20)\n",
    "        print(f'Examining predictions for {col}')\n",
    "        print('Correctly Classified Reviews:')\n",
    "        for y_test, y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 20):\n",
    "            if y_test == y_test_pred:\n",
    "                print(f'LABEL: {y_test}')\n",
    "                print(f'REVIEW TEXT: {_X_test[:100]}...')\n",
    "                print('-'*20)\n",
    "                print()\n",
    "        \n",
    "        print('Incorrectly Classified Reviews:')\n",
    "        for y_test, y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 20):\n",
    "            if y_test != y_test_pred:\n",
    "                print(f'TRUE LABEL: {y_test}')\n",
    "                print(f'PREDICTED LABEL: {y_test_pred}')\n",
    "                print(f'REVIEW TEXT: {_X_test[:100]}...')\n",
    "                print()\n",
    "\n",
    "        # Evluate estimator\n",
    "        print('-'*20)\n",
    "        print(f'Probs evaluation and ploting metrics for {col}')\n",
    "        df_metrics = evaluation(predicted_results, df_metrics, col, tokenizer_name, classifier_name)\n",
    "        print()\n",
    "\n",
    "        # Save BERT Model\n",
    "        print('-'*20)\n",
    "        print(f'Saving estimator and metrics table for {col}')\n",
    "        saving_model_and_table(df_metrics, estimator, col, tokenizer_name, classifier_name)\n",
    "        print()\n",
    "\n",
    "        # Compare Estimators\n",
    "        print('='*20)\n",
    "        print(f'Comparing estimators for {col}')\n",
    "        comparison_plots(get_fitted_estimators().append(estimator), X_test, y_test, col)\n",
    "        print('='*20)\n",
    "        print()\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9fb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print('-'*20)\n",
    "print(f'Evaluating estimator for {col}')\n",
    "estimator.evaluate()\n",
    "# metrics_dict\n",
    "predicted_results = estimator.predict(bert_test_dataset)\n",
    "print(f'Predictions shape for {col}: {predicted_results.predictions.shape}')\n",
    "\n",
    "# Get y_test_pred\n",
    "print('-'*20)\n",
    "print(f'Getting y_test_pred for {col}')\n",
    "y_test = predicted_results.label_ids\n",
    "y_test_pred = predicted_results.predictions.argmax(-1)\n",
    "y_test_pred = y_test_pred.flatten().tolist()\n",
    "y_test_pred = [bert_label2id[l] for l in y_test_pred]\n",
    "print(f'Length of y_test_pred: {len(y_test_pred)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a155ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y_test_pred_proba\n",
    "print('-'*20)\n",
    "print(f'Getting y_test_pred_prob for {col}')\n",
    "try:\n",
    "    y_test_pred_prob = torch.nn.functional.softmax(torch.tensor(predicted_results.predictions, device=device), dim=-1)\n",
    "    print('Using torch.nn.functional.softmax')\n",
    "except Exception:\n",
    "    y_test_pred_prob = scipy.special.softmax(predicted_results.predictions, axis=1)\n",
    "    print('Using scipy.special.softmax')\n",
    "except Exception:\n",
    "    y_test_pred_prob = predicted_results.predictions[:, 1]\n",
    "    print('Using predicted_results.predictions[:, 1]')\n",
    "finally:\n",
    "    y_test_pred_prob = y_test_pred_prob.flatten().tolist()\n",
    "print(f'Length of y_test_pred_prob: {len(y_test_pred_prob)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c27631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluate estimator\n",
    "print('-'*20)\n",
    "print(f'Probs evaluation and ploting metrics for {col}')\n",
    "df_metrics = evaluation(predicted_results, df_metrics, col, classifier_name, tokenizer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b34da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.is_model_parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9419be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.args.n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual['Bert Preditcions'] = df_manual['Job Description spacy_sentencized'].progress_apply(\n",
    "    lambda sentence: estimator.predict(sent)\n",
    "    for sent in sentence\n",
    "    if sent and isinstance(sent, (str, list)) and isinstance(sentence, list)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
