{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde1c1e53861409d90304049a4a42af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fa6ab6e3f84251a4f2d95dc53f4b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92a0604743e4c5ca370571c67864a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3111caf35e4f4cbc10bc36f985fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from transformer_estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "method = 'Transformers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa9ff21dff4b75ba9c8831f71abbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cd36630bbc4c3bab32dc9cf6c57a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformer variables\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "done_xy_save_path = f'{results_save_path}Search+Xy/'\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    f'Mean Cross Validation Train - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Cross Validation Test - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "DetectorFactory.seed = random_state\n",
    "cores = multiprocessing.cpu_count()\n",
    "accelerator = Accelerator()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ.get('TOKENIZERS_PARALLELISM')\n",
    "best_trial_args = [\n",
    "    'learning_rate', 'num_train_epochs', 'per_device_train_batch_size', 'warmup_steps', 'weight_decay'\n",
    "]\n",
    "training_args_dict = {\n",
    "    'seed': random_state,\n",
    "    # 'resume_from_checkpoint': True,\n",
    "    # 'overwrite_output_dir': True,\n",
    "    'logging_steps': 100,\n",
    "    'evaluation_strategy': 'epoch',\n",
    "    'save_strategy': 'epoch',\n",
    "    'torch_compile': bool(transformers.file_utils.is_torch_available()),\n",
    "    'use_mps_device': bool(device_name == 'mps' and torch.backends.mps.is_available()),\n",
    "    'optim': 'adamw_torch',\n",
    "    'save_total_limit': 1,\n",
    "    'load_best_model_at_end': True,\n",
    "    'metric_for_best_model': 'recall_score',\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 20,\n",
    "    'learning_rate': 5e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "}\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "with contextlib.suppress(ImportError):\n",
    "    mpl.use('MacOSX')\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d7b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    col_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if col_names_list is None:\n",
    "        col_names_list = []\n",
    "    if vectorizer_names_list is None:\n",
    "        vectorizer_names_list = []\n",
    "    if classifier_names_list is None:\n",
    "        classifier_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in glob.glob(f'{results_save_path}*.model'):\n",
    "        col_names_list.append(\n",
    "            col := estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "        )\n",
    "        vectorizer_names_list.append(\n",
    "            vectorizer_name := estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "        )\n",
    "        classifier_names_list.append(\n",
    "            classifier_name := estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "        )\n",
    "\n",
    "    estimator_names_list = [\n",
    "        f'{col} - {vectorizer_name} + {classifier_name}'\n",
    "        for col, vectorizer_name, classifier_name in tqdm_product(\n",
    "            list(set(col_names_list)),\n",
    "            list(set(vectorizer_names_list)),\n",
    "            list(set(classifier_names_list)),\n",
    "        )\n",
    "    ]\n",
    "    return (\n",
    "        list(set(col_names_list)),\n",
    "        list(set(vectorizer_names_list)),\n",
    "        list(set(classifier_names_list)),\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70db7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    col,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    compression=None, protocol=None, path_suffix=None, data_dict=None\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    # Check data\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Assign dfs to variables\n",
    "    data_dict['df_train_data'] = df_train_data\n",
    "    data_dict['df_test_data'] = df_test_data\n",
    "    data_dict['df_val_data'] = df_val_data\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = f'{results_save_path}{method} {file_name}{path_suffix}'\n",
    "        print(f'Saving Xy {file_name} at {save_path}')\n",
    "        file_.to_pickle(\n",
    "            save_path, protocol=protocol\n",
    "        )\n",
    "    print(f'Done saving Xy!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef9e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad68773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns=analysis_columns, text_col=text_col):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=1-test_split, test_size=test_split, random_state=random_state\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f07a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, encoded):\n",
    "        self.encodings = encodings\n",
    "        self.encoded = encoded\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.encoded[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy_encodings(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, test_dataset,\n",
    "    X_val, y_val, val_dataset,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, val_dataset)\n",
    "\n",
    "    # Check encodings\n",
    "    assert all(y_train == train_dataset.encoded), 'y_train and train_dataset encoded are not the same'\n",
    "    assert all(y_test == test_dataset.encoded), 'y_test and test_dataset encoded are not the same'\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set encodings example:\\n{\" \".join(train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set encoded example: {set(train_dataset.encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set encodings example:\\n{\" \".join(test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set encoded example: {set(test_dataset.encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set encodings example:\\n{\" \".join(val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation labels after encoding: {set(val_dataset.encoded)}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {tokenizer.__class__.__name__}.from_pretrained using {tokenizer.name_or_path}.')\n",
    "\n",
    "    X_train_encodings = tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    train_dataset = ToDataset(X_train_encodings, y_train)\n",
    "\n",
    "    X_test_encodings = tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    test_dataset = ToDataset(X_test_encodings, y_test)\n",
    "\n",
    "    X_val_encodings = tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    val_dataset = ToDataset(X_val_encodings, y_val)\n",
    "\n",
    "    # Print info\n",
    "    print_Xy_encodings(\n",
    "        X_train, y_train, train_dataset,\n",
    "        X_test, y_test, test_dataset,\n",
    "        X_val, y_val, val_dataset,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_encodings, train_dataset,\n",
    "        X_test_encodings, test_dataset,\n",
    "        X_val_encodings, val_dataset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ddcc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(\n",
    "    col,\n",
    "    results_save_path=results_save_path, method=method,\n",
    "    data_dict=None, protocol=None, path_suffix=None, \n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    print('+'*30)\n",
    "    print(f'{\"=\"*10} Loading Xy from previous for {col} {\"=\"*10}')\n",
    "    print('+'*30)\n",
    "    # Read all dfs\n",
    "    for file_path in glob.glob(f'{results_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{results_save_path}{method} ')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name} from {file_path}')\n",
    "        if path_suffix in file_path and 'df_' in file_name and 'cv_results' not in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    print(f'Done loading Xy from previous for {col}!')\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "\n",
    "    # Encode data\n",
    "    (\n",
    "        X_train_encodings, train_dataset,\n",
    "        X_test_encodings, test_dataset,\n",
    "        X_val_encodings, val_dataset,\n",
    "    ) = encode_data(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "    return (\n",
    "        X_train, y_train, X_train_encodings, train_dataset,\n",
    "        X_test, y_test, X_test_encodings, test_dataset,\n",
    "        X_val, y_val, X_val_encodings, val_dataset,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b78bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter tuning\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/hpo_train#hyperparameter-search-using-trainer-api\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True),\n",
    "        'num_train_epochs': trial.suggest_int('num_train_epochs', 1, 5),\n",
    "        'per_device_train_batch_size': trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'per_device_eval_batch_size': trial.suggest_categorical('per_device_eval_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'warmup_steps': trial.suggest_int('warmup_steps', 0, 500),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-12, 1e-1, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c7abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute objective for hyperparameter tuning\n",
    "# https://github.com/huggingface/transformers/issues/13019\n",
    "def compute_objective(metrics_dict):\n",
    "    return metrics_dict['eval_Recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_labels, y_pred,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "\n",
    "    print('Computing metrics using y_pred.')\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
    "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
    "    r2 = metrics.r2_score(y_labels, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_labels, y_pred)\n",
    "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_labels, y_pred_prob,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    print('Computing metrics using y_pred_prob.')\n",
    "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold, loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea5ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all(\n",
    "    y_labels, y_pred, y_pred_prob\n",
    "):\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using y_pred\n",
    "    if y_pred:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_labels, y_pred\n",
    "        )\n",
    "    # Using y_pred_prob\n",
    "    if y_pred_prob:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_labels, y_pred_prob\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # 'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        # f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # 'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        # f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'R2 Score': float(r2),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Imbalanced Classification Report': imblearn_report,\n",
    "        'Confusion Matrix': cm,\n",
    "        'Normalized Confusion Matrix': cm_normalized,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_prob': y_pred_prob,\n",
    "    }\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28661d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metrics_dict(metrics_dict, prefix_to_remove):\n",
    "    for metric_name in list(metrics_dict):\n",
    "        if metric_name.startswith(prefix_to_remove):\n",
    "            new_metric_name = ' '.join(metric_name.split(prefix_to_remove)[-1].split('_')).strip()\n",
    "        if not new_metric_name[0].isupper():\n",
    "            new_metric_name = new_metric_name.title()\n",
    "        if new_metric_name == 'Loss':\n",
    "            metrics_dict['Log Loss/Cross Entropy'] = metrics_dict.pop(metric_name)\n",
    "        else:\n",
    "            metrics_dict[new_metric_name] = metrics_dict.pop(metric_name)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f77d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred_prob(y_pred_logits, y_labels):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'Preprocessing y_pred logits and labels for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_logits, tuple):\n",
    "        y_pred_logits = y_pred_logits[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_logits):\n",
    "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    else:\n",
    "        y_pred_logits_tensor = y_pred_logits.to(device)\n",
    "\n",
    "    print(f'y_pred_logits shape: {y_pred_logits_tensor.shape}, {y_pred_logits_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    # https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits.')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).clone().detach().cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_array.shape}, {y_pred_prob_array.dtype}')\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return torch.tensor(y_pred_prob_array, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred(y_pred_prob_array):\n",
    "\n",
    "    # https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred from y_pred_prob for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_prob_array, tuple):\n",
    "        y_pred_prob_array = y_pred_prob_array[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_prob_array):\n",
    "        y_pred_prob_tensor = torch.tensor(y_pred_prob_array, device=device)\n",
    "    else:\n",
    "        y_pred_prob_tensor = y_pred_prob_array.to(device)\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_tensor.shape}. {y_pred_prob_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred through argmax of y_pred_prob.')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_prob_tensor, axis=-1).clone().detach().cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_prob.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "\n",
    "    return y_pred_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    y_pred_prob_array, y_labels_array = predicted_results_from_eval\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    (\n",
    "        y_pred_array\n",
    "    ) = preprocess_logits_for_metrics_y_pred(y_pred_prob_array)\n",
    "\n",
    "    # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
    "    print('-'*20)\n",
    "    print('Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.')\n",
    "    y_labels = y_labels_array.flatten().tolist()\n",
    "    y_pred = y_pred_array.flatten().tolist()\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred_prob length: {len(y_pred_prob)}')\n",
    "    print(f'y_labels length: {len(y_labels)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a625b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits):\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    print('Getting y_pred through argmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1).clone().detach().cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred...')\n",
    "    y_pred = [bert_label2id[l] for l in y_pred_array.flatten().tolist()]\n",
    "    print(f'y_pred length: {len(y_pred)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).clone().detach().cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
    "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
    "    print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred length: {len(y_pred_prob)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab4fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_logits(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    # Get predictions\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred logits and ids for {col}:')\n",
    "    y_pred_logits, y_labels = predicted_results_from_eval\n",
    "    print(f'y_pred_logits shape: {y_pred_logits.shape}')\n",
    "    print(f'y shape: {y_labels.shape}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_test_pred and y_test_pred_prob\n",
    "    (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    ) = preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    metrics_dict, df_metrics,\n",
    "    col, vectorizer_name, classifier_name\n",
    "):\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if metric_name not in ['Runtime', 'Samples Per Second', 'Steps Per Second']:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_name, (int, float)):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = metric_value\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "                print(f'{metric_name}: {metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93960eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy data in df and save\n",
    "def save_Xy_estimator(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "    X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "    estimator, accelerator,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None, protocol=None,\n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset)\n",
    "\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'train_dataset': train_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "            'test_dataset': test_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_pred_prob': y_val_pred_prob,\n",
    "            'val_dataset': val_dataset,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Make data dict\n",
    "    data_dict['df_train_data'] = df_train_data\n",
    "    data_dict['df_test_data'] = df_test_data\n",
    "    data_dict['df_val_data'] = df_val_data\n",
    "    data_dict['Estimator'] = estimator\n",
    "    data_dict['accelerator'] = accelerator\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = (\n",
    "            done_xy_save_path\n",
    "            if file_name not in ['Estimator', 'accelerator']\n",
    "            else results_save_path\n",
    "        )\n",
    "        print(f'Saving {file_name} at {save_path}')\n",
    "        if not isinstance(file_, pd.DataFrame) and file_name == 'Estimator' and 'df_' not in file_name:\n",
    "            # Save as .model\n",
    "            file_.save_model(f'{save_path}{method} {file_name}{path_suffix.replace(\"pkl\", \"model\")}')\n",
    "        elif not isinstance(file_, pd.DataFrame) and file_name == 'accelerator' and 'df_' not in file_name:\n",
    "            file_.save(estimator.state, f'{save_path}{method} Estimator{path_suffix.replace(\"pkl\", \"model\")}/accelerator')\n",
    "        elif isinstance(file_, pd.DataFrame) and file_name != 'Estimator' and 'df_' in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "\n",
    "    print(f'Done saving Xy, labels and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fb3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all classifiers were used\n",
    "def assert_all_classifers_used(\n",
    "    estimators_list=None, used_classifiers=None, results_save_path=results_save_path, method=method, transformers_pipe=transformers_pipe,\n",
    "):\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.model'):\n",
    "        classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "        used_classifiers.append(classifier_name)\n",
    "\n",
    "    assert set(transformers_pipe.keys()) == set(used_classifiers), 'Not all classifiers were used!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f64ab3",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n",
    "assert len(df_manual) == 5978, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH 5978 BUT IS OF LENGTH {len(df_manual)}'\n",
    "# HACK REMOVE THIS!!!!!!\n",
    "df_manual = df_manual.groupby(analysis_columns).sample(n=600).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea2f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory:\n",
      "/home/nfatahelra/Study1_Code/data/classification models/Transformers Results/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da5ce3a9582418089f1d4e0d5fe9e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 2400 ON WARMTH ==============================\n",
      "--------------------\n",
      "Classifers to be used (2):\n",
      "['BertForSequenceClassification', 'GPT2ForSequenceClassification']\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (1800,)\n",
      "----------\n",
      "Training set example:\n",
      "Always looking for better ways of doing things, improving our Legal Operations function, innovating and exchanging best practices\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (240,)\n",
      "----------\n",
      "Testing set example:\n",
      "Would you like to combine your expertise of affiliate marketing with your passion for Denim?\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (360,)\n",
      "----------\n",
      "Validation set example:\n",
      "We are proud to be named one of the most promising startups by The\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 1.01 (0 = 1.00, 1 = 1.00)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.95 (0 = 0.98, 1 = 1.03)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 1.00 (0 = 1.00, 1 = 1.00)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[ASome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with BertTokenizerFast.from_pretrained using bert-base-uncased.\n",
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "[CLS] always looking for better ways of doing things , improving our legal operations function , inn ##ova ##ting and exchanging best practices [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Training set encoded example: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "[CLS] would you like to combine your expertise of affiliate marketing with your passion for denim ? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Testing set encoded example: {0, 1}\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "[CLS] we are proud to be named one of the most promising startup ##s by the [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "----------\n",
      "Validation labels after encoding: {0, 1}\n",
      "====================\n",
      "====================\n",
      "Saving Xy df_train_data at /home/nfatahelra/Study1_Code/data/classification models/Transformers Results/Transformers df_train_data - Warmth - (Save_protocol=5).pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-04 18:31:36,519]\u001b[0m A new study created in memory with name: no-name-3724034f-ac83-456f-84ae-49a1d58458ed\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Xy df_test_data at /home/nfatahelra/Study1_Code/data/classification models/Transformers Results/Transformers df_test_data - Warmth - (Save_protocol=5).pkl\n",
      "Saving Xy df_val_data at /home/nfatahelra/Study1_Code/data/classification models/Transformers Results/Transformers df_val_data - Warmth - (Save_protocol=5).pkl\n",
      "Done saving Xy!\n",
      "['df_train_data', 'df_test_data', 'df_val_data']\n",
      "====================\n",
      "--------------------\n",
      "==============================\n",
      "============================== Initializing Trainer using BERTBASEUNCASED + BertForSequenceClassification ==============================\n",
      "++++++++++++++++++++++++++++++\n",
      "--------------------\n",
      "Passing data to Trainer.\n",
      "--------------------\n",
      "Appending trainer arguments to Trainer.\n",
      "--------------------\n",
      "Starting hyperparameter search for Warmth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\u001b[33m[W 2023-04-04 18:31:37,715]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 1.638677247123859e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'warmup_steps': 152, 'weight_decay': 2.486840537381037e-10} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:37,716]\u001b[0m Trial 0 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:37,805]\u001b[0m Trial 6 failed with parameters: {'learning_rate': 1.7630269515349673e-06, 'num_train_epochs': 3, 'per_device_train_batch_size': 64, 'per_device_eval_batch_size': 8, 'warmup_steps': 160, 'weight_decay': 2.2010436663109532e-10} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:37,806]\u001b[0m Trial 6 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:37,896]\u001b[0m Trial 1 failed with parameters: {'learning_rate': 8.638991665701433e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 128, 'per_device_eval_batch_size': 32, 'warmup_steps': 312, 'weight_decay': 0.004245385426845518} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:37,898]\u001b[0m Trial 1 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:37,986]\u001b[0m Trial 2 failed with parameters: {'learning_rate': 2.385175501504361e-05, 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 8, 'warmup_steps': 38, 'weight_decay': 3.0467633828686305e-05} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:37,987]\u001b[0m Trial 2 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,077]\u001b[0m Trial 3 failed with parameters: {'learning_rate': 9.009114136935451e-05, 'num_train_epochs': 3, 'per_device_train_batch_size': 128, 'per_device_eval_batch_size': 64, 'warmup_steps': 193, 'weight_decay': 6.477676196125809e-07} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,078]\u001b[0m Trial 3 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,166]\u001b[0m Trial 4 failed with parameters: {'learning_rate': 2.2501267504288376e-06, 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 128, 'warmup_steps': 68, 'weight_decay': 3.5162111489652013e-06} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,167]\u001b[0m Trial 4 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,256]\u001b[0m Trial 9 failed with parameters: {'learning_rate': 7.933164193706518e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 32, 'warmup_steps': 122, 'weight_decay': 0.003835729578121935} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,258]\u001b[0m Trial 9 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,345]\u001b[0m Trial 7 failed with parameters: {'learning_rate': 1.4867863665583405e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 4, 'warmup_steps': 424, 'weight_decay': 0.00022846431891732148} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,347]\u001b[0m Trial 7 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,434]\u001b[0m Trial 8 failed with parameters: {'learning_rate': 5.546023163690598e-05, 'num_train_epochs': 4, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 4, 'warmup_steps': 413, 'weight_decay': 0.0008696225237980853} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,435]\u001b[0m Trial 8 failed with value None.\u001b[0m\n",
      "\u001b[33m[W 2023-04-04 18:31:38,525]\u001b[0m Trial 5 failed with parameters: {'learning_rate': 1.209319002250436e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 4, 'warmup_steps': 134, 'weight_decay': 5.853984109919692e-05} because of the following error: RuntimeError('CUDA error: uncorrectable ECC error encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/integrations.py\", line 187, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1633, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 1902, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2645, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py\", line 2677, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 161, in forward\n",
      "    inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py\", line 178, in scatter\n",
      "    return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 53, in scatter_kwargs\n",
      "    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 44, in scatter\n",
      "    res = scatter_map(inputs)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 35, in scatter_map\n",
      "    return [type(obj)(i) for i in zip(*map(scatter_map, obj.items()))]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 31, in scatter_map\n",
      "    return list(zip(*map(scatter_map, obj)))\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py\", line 27, in scatter_map\n",
      "    return Scatter.apply(target_gpus, None, dim, obj)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 96, in forward\n",
      "    outputs = comm.scatter(input, target_gpus, chunk_sizes, ctx.dim, streams)\n",
      "  File \"/home/nfatahelra/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 189, in scatter\n",
      "    return tuple(torch._C._scatter(tensor, devices, chunk_sizes, dim, streams))\n",
      "RuntimeError: CUDA error: uncorrectable ECC error encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\u001b[33m[W 2023-04-04 18:31:38,526]\u001b[0m Trial 5 failed with value None.\u001b[0m\n",
      "  0%|          | 0/2 [00:04<?, ?it/s]\n",
      "  0%|          | 0/2 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "!!!!!!!!!!!!!!!!!!!! Failed to hyperparameter search. Setting model args and model_init + best_trail to None !!!!!!!!!!!!!!!!!!!!\n",
      "--------------------\n",
      "--------------------\n",
      "Appending trainer arguments to Trainer.\n",
      "--------------------\n",
      "Starting training for Warmth.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:196\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1630\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1632\u001b[0m )\n\u001b[0;32m-> 1633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1905\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1906\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1907\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1908\u001b[0m ):\n\u001b[1;32m   1909\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2645\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/transformers/trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2676\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2677\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2679\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamo_ctx\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orig_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:161\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj, t\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m--> 161\u001b[0m inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:178\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, kwargs, device_ids):\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:53\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m inputs \u001b[38;5;241m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 53\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(kwargs):\n\u001b[1;32m     55\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mextend(() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)))\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:44\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:35\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [obj \u001b[38;5;28;01mfor\u001b[39;00m targets \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:31\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:27\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:96\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [_get_stream(device) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n\u001b[0;32m---> 96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/study1_3.10/lib/python3.10/site-packages/torch/nn/parallel/comm.py:189\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "col_names_list, vectorizer_names_list, classifier_names_list, estimator_names_list = get_existing_files()\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'Classifers to be used ({len(list(transformers_pipe.keys()))}):\\n{list(transformers_pipe.keys())}'\n",
    "    )\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    # Split data\n",
    "    (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = split_data(\n",
    "        df=df_manual, col=col, analysis_columns=analysis_columns, text_col=text_col\n",
    "    )\n",
    "    # Get model_name, tokenizer, model and optimizer\n",
    "    for transformer_name, transformer_dict in tqdm.tqdm(transformers_pipe.items()):\n",
    "        model_name = transformer_dict['model_name']\n",
    "        config = transformer_dict['config'].from_pretrained(model_name)\n",
    "        tokenizer = transformer_dict['tokenizer'].from_pretrained(model_name)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        model = transformer_dict['model'].from_pretrained(model_name, config=config,).to(device)\n",
    "        if model.config.pad_token_id is None:\n",
    "            try:\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            except:\n",
    "                model.config.pad_token_id = model.config.eos_token_id\n",
    "        def model_init(trial): return model\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        vectorizer_name = ''.join(model.name_or_path.split('-')).upper()\n",
    "        classifier_name = model.__class__.__name__\n",
    "        output_dir = training_args_dict['output_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).model'\n",
    "        log_dir = training_args_dict['logging_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).log'\n",
    "\n",
    "        # Encode data\n",
    "        (\n",
    "            X_train_encodings, train_dataset,\n",
    "            X_test_encodings, test_dataset,\n",
    "            X_val_encodings, val_dataset,\n",
    "        ) = encode_data(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "        )\n",
    "\n",
    "        if f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list:\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            # Load previous Xy\n",
    "            (\n",
    "                X_train, y_train, X_train_encodings, train_dataset,\n",
    "                X_test, y_test, X_test_encodings, test_dataset,\n",
    "                X_val, y_val, X_val_encodings, val_dataset,\n",
    "                train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "                test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "                val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "            ) = load_Xy(\n",
    "                col\n",
    "            )\n",
    "            print('-'*20)\n",
    "            continue\n",
    "        else:\n",
    "            save_Xy(\n",
    "                X_train, y_train,\n",
    "                X_test, y_test,\n",
    "                X_val, y_val,\n",
    "                col,\n",
    "            )\n",
    "\n",
    "        # Accelerate model\n",
    "        (\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        ) = accelerator.prepare(\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        )\n",
    "        # model.eval()\n",
    "\n",
    "        # Initialize Trainer\n",
    "        print('-'*20)\n",
    "        print('='*30)\n",
    "        print(f'{\"=\"*30} Initializing Trainer using {vectorizer_name} + {classifier_name} {\"=\"*30}')\n",
    "        print('+'*30)\n",
    "\n",
    "        # Make trainer without args\n",
    "        print('-'*20)\n",
    "        print('Passing data to Trainer.')\n",
    "        estimator = Trainer(\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "            compute_metrics=compute_metrics,\n",
    "            data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "            model_init=model_init,\n",
    "        )\n",
    "        if estimator.place_model_on_device:\n",
    "            estimator.model.to(device)\n",
    "\n",
    "        # Set fine-tuning parameters\n",
    "        try:\n",
    "            training_args_dict_for_best_trial = {\n",
    "                arg_name: arg_\n",
    "                for arg_name, arg_ in training_args_dict.items()\n",
    "                if arg_name not in best_trial_args\n",
    "            }\n",
    "            training_args = TrainingArguments(**training_args_dict_for_best_trial)\n",
    "\n",
    "            # Append args to Trainer\n",
    "            print('-'*20)\n",
    "            print('Appending trainer arguments to Trainer.')\n",
    "            estimator.args = training_args\n",
    "\n",
    "            # Hyperparameter search\n",
    "            print('-'*20)\n",
    "            print(f'Starting hyperparameter search for {col}.')\n",
    "            best_trial = estimator.hyperparameter_search(\n",
    "                direction='maximize',\n",
    "                backend='optuna',\n",
    "                n_trials=10,\n",
    "                hp_space=optuna_hp_space,\n",
    "                sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "                pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "                compute_objective=compute_objective,\n",
    "                n_jobs=n_jobs,\n",
    "            )\n",
    "            estimator.save_state()\n",
    "            estimator.save_metrics('all', metrics_dict)\n",
    "            estimator.save_model(output_dir)\n",
    "            accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "            print('Done hyperparameter search!')\n",
    "            print('-'*20)\n",
    "\n",
    "            # # Train trainer\n",
    "            # print('-'*20)\n",
    "            # print(f'Starting training for {col}.')\n",
    "            # estimator.train(\n",
    "            #     trial=best_trial, \n",
    "            #     resume_from_checkpoint=True,\n",
    "            # )\n",
    "            # estimator.save_state()\n",
    "            # estimator.save_metrics('all', metrics_dict)\n",
    "            # estimator.save_model(output_dir)\n",
    "            # accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "            # print('Done training!')\n",
    "            # print('-'*20)\n",
    "\n",
    "        except:\n",
    "            print('-'*20)\n",
    "            print(f'{\"!\"*20} Failed to hyperparameter search. Setting model args and model_init + best_trail to None {\"!\"*20}')\n",
    "            print('-'*20)\n",
    "            estimator.model = model\n",
    "            estimator.model_init = None\n",
    "            best_trial = None\n",
    "            training_args = TrainingArguments(**training_args_dict)\n",
    "\n",
    "            # Appending args to Trainer\n",
    "            print('-'*20)\n",
    "            print('Appending trainer arguments to Trainer.')\n",
    "            estimator.args = training_args\n",
    "\n",
    "            # # Train trainer\n",
    "            # print('-'*20)\n",
    "            # print(f'Starting training for {col}.')\n",
    "            # estimator.train()\n",
    "            # estimator.save_state()\n",
    "            # estimator.save_metrics('all', metrics_dict)\n",
    "            # estimator.save_model(output_dir)\n",
    "            # accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "            # print('Done training!')\n",
    "            # print('-'*20)\n",
    "\n",
    "        # Train trainer\n",
    "        print('-'*20)\n",
    "        print(f'Starting training for {col}.')\n",
    "        estimator.train(\n",
    "            trial=best_trial, \n",
    "            # resume_from_checkpoint=True,\n",
    "        )\n",
    "        estimator.save_state()\n",
    "        estimator.save_metrics('all', metrics_dict)\n",
    "        estimator.save_model(output_dir)\n",
    "        accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # # Train trainer\n",
    "        # print('-'*20)\n",
    "        # print(f'Starting training for {col}.')\n",
    "        # estimator.train(\n",
    "        #     trial=best_trial, \n",
    "        #     # resume_from_checkpoint=True,\n",
    "        # )\n",
    "        # estimator.save_state()\n",
    "        # # estimator.save_metrics('all', metrics_dict)\n",
    "        # estimator.save_model(output_dir)\n",
    "        # accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "        # print('Done training!')\n",
    "        # print('-'*20)\n",
    "\n",
    "        # Evaluate\n",
    "        print('-'*20)\n",
    "        print(f'Evaluating estimator for {col}.')\n",
    "        eval_metrics_dict = estimator.evaluate()\n",
    "        y_val_pred = eval_metrics_dict.pop('eval_y_pred')\n",
    "        y_val_pred_prob = eval_metrics_dict.pop('eval_y_pred_prob')\n",
    "        eval_metrics_dict = clean_metrics_dict(eval_metrics_dict, list(eval_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done evaluating!')\n",
    "\n",
    "        # Get predictions\n",
    "        print(f'Getting prediction results for {col}.')\n",
    "        y_test_pred_logits, y_test_labels, test_metrics_dict = estimator.predict(test_dataset)\n",
    "        y_test_pred = test_metrics_dict.pop('test_y_pred')\n",
    "        y_test_pred_prob = test_metrics_dict.pop('test_y_pred_prob')\n",
    "        test_metrics_dict = clean_metrics_dict(test_metrics_dict, list(test_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done predicting!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # Save model\n",
    "        print('-'*20)\n",
    "        print(f'Saving model for {col}.')\n",
    "        save_Xy_estimator(\n",
    "            X_train, y_train, train_dataset,\n",
    "            X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "            X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "            estimator, accelerator,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # HACK\n",
    "        sys.exit(0)\n",
    "\n",
    "# Assert that all classifiers were used\n",
    "assert_all_classifers_used()\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa2a3a1e-c695-4661-8445-0f8171a3ae84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TORCH_USE_CUDA_DSA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTORCH_USE_CUDA_DSA\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TORCH_USE_CUDA_DSA' is not defined"
     ]
    }
   ],
   "source": [
    "TORCH_USE_CUDA_DSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34fbd3-2307-46b5-9ff3-88d1b0c80737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
