{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50d4c434",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fef3f604",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "55afc383",
            "metadata": {},
            "source": [
                "### Set variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b36fe18",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables\n",
                "method = 'Supervised'\n",
                "with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "    results_save_path = f.read()\n",
                "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
                "    done_xy_save_path = f.read()\n",
                "\n",
                "t = time.time()\n",
                "n_jobs = -1\n",
                "n_splits = 10\n",
                "n_repeats = 3\n",
                "random_state = 42\n",
                "refit = True\n",
                "class_weight = 'balanced'\n",
                "cv = RepeatedStratifiedKFold(\n",
                "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
                ")\n",
                "scoring = 'recall'\n",
                "scores = [\n",
                "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
                "    'explained_variance', 'matthews_corrcoef'\n",
                "]\n",
                "scorers = {\n",
                "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
                "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
                "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
                "}\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "metrics_dict = {\n",
                "    f'{scoring.title()} Best Score': np.nan,\n",
                "    f'{scoring.title()} Best Threshold': np.nan,\n",
                "    'Train - Mean Cross Validation Score': np.nan,\n",
                "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Test - Mean Cross Validation Score': np.nan,\n",
                "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Explained Variance': np.nan,\n",
                "    'Accuracy': np.nan,\n",
                "    'Balanced Accuracy': np.nan,\n",
                "    'Precision': np.nan,\n",
                "    'Average Precision': np.nan,\n",
                "    'Recall': np.nan,\n",
                "    'F1-score': np.nan,\n",
                "    'Matthews Correlation Coefficient': np.nan,\n",
                "    'Fowlkes–Mallows Index': np.nan,\n",
                "    'R2 Score': np.nan,\n",
                "    'ROC': np.nan,\n",
                "    'AUC': np.nan,\n",
                "    'Log Loss/Cross Entropy': np.nan,\n",
                "    'Cohen’s Kappa': np.nan,\n",
                "    'Geometric Mean': np.nan,\n",
                "    'Classification Report': np.nan,\n",
                "    'Imbalanced Classification Report': np.nan,\n",
                "    'Confusion Matrix': np.nan,\n",
                "    'Normalized Confusion Matrix': np.nan,\n",
                "}\n",
                "\n",
                "# Transformer variables\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
                ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "# Set random seed\n",
                "random_state = 42\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)\n",
                "torch.manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "\n",
                "# Plotting variables\n",
                "pp = pprint.PrettyPrinter(indent=4)\n",
                "tqdm.tqdm.pandas(desc='progress-bar')\n",
                "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
                "# tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
                "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
                "# pbar = progressbar.ProgressBar(maxval=10)\n",
                "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
                "mpl.rcParams['text.usetex'] = False\n",
                "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
                "mpl.rc('font', **font)\n",
                "plt.style.use('tableau-colorblind10')\n",
                "plt.set_cmap('Blues')\n",
                "pd.set_option('display.max_rows', None)\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.width', 5000)\n",
                "pd.set_option('display.colheader_justify', 'center')\n",
                "pd.set_option('display.precision', 3)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9f3c5be4",
            "metadata": {},
            "source": [
                "# Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4a26e56",
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_and_close_plots():\n",
                "    plt.show()\n",
                "    plt.clf()\n",
                "    plt.cla()\n",
                "    plt.close()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "675cf660",
            "metadata": {},
            "outputs": [],
            "source": [
                "def close_plots():\n",
                "    plt.clf()\n",
                "    plt.cla()\n",
                "    plt.close()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f179e38",
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_df_metrics(\n",
                "    vectorizers_pipe, classifiers_pipe, transformers_pipe, metrics_list,\n",
                "    col, vectorizer_name, classifier_name, protocol=None,\n",
                "    analysis_columns=analysis_columns,\n",
                "    table_save_path=table_save_path,\n",
                "    method=method, save_name=None,\n",
                "    compression=None, path_suffix=None, \n",
                "):\n",
                "    if save_name is None:\n",
                "        save_name = 'Estimators Table'\n",
                "    if compression is None:\n",
                "        compression = False\n",
                "    if protocol is None:\n",
                "        protocol = pickle.HIGHEST_PROTOCOL\n",
                "\n",
                "    combined_classifiers_list = list(classifiers_pipe.keys()) + list(transformers_pipe.keys())\n",
                "    combined_vectorizers_list = list(vectorizers_pipe.keys()) + [\n",
                "        str(tranformer_dict['tokenizer']).split('.')[-1].split(\"'>\")[0]\n",
                "        for tranformer_dict in transformers_pipe.values()\n",
                "    ]\n",
                "\n",
                "    print('='*20)\n",
                "    if os.path.exists(f'{table_save_path}{save_name}') and os.path.getsize(f'{table_save_path}{save_name}') > 0:\n",
                "        print(f'Loading table from {table_save_path}{save_name}')\n",
                "        df_metrics = pd.read_pickle(f'{table_save_path}{save_name}')\n",
                "        print('Done loading table!')\n",
                "    else:\n",
                "        print('Table does not exist, creating new table...')\n",
                "        index = pd.MultiIndex.from_product(\n",
                "            [list(map(lambda classifier_name: classifier_name, combined_classifiers_list))],\n",
                "            names=['Classifiers'],\n",
                "        )\n",
                "        columns = pd.MultiIndex.from_product(\n",
                "            [\n",
                "                analysis_columns,\n",
                "                list(map(lambda vectorizer_name: vectorizer_name, combined_vectorizers_list)),\n",
                "                metrics_list,\n",
                "            ],\n",
                "            names=['Variable', 'Vectorizer', 'Measures'],\n",
                "        )\n",
                "        df_metrics = pd.DataFrame(index=index, columns=columns)\n",
                "        print('Done creating new table!')\n",
                "    print('='*20)\n",
                "\n",
                "    return df_metrics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ccfbba94",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_existing_files(\n",
                "    results_save_path= results_save_path,\n",
                "    col_names_list=None,\n",
                "    vectorizer_names_list=None,\n",
                "    classifier_names_list=None,\n",
                "):\n",
                "    if col_names_list is None:\n",
                "        col_names_list = []\n",
                "    if vectorizer_names_list is None:\n",
                "        vectorizer_names_list = []\n",
                "    if classifier_names_list is None:\n",
                "        classifier_names_list = []\n",
                "\n",
                "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
                "\n",
                "    for estimators_file in glob.glob(f'{results_save_path}*.pkl'):\n",
                "        if f'{method} Estimator - ' in estimators_file:\n",
                "            col_names_list.append(\n",
                "                col := estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
                "            )\n",
                "            vectorizer_names_list.append(\n",
                "                vectorizer_name := estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
                "            )\n",
                "            classifier_names_list.append(\n",
                "                classifier_name := estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
                "            )\n",
                "\n",
                "    estimator_names_list = [\n",
                "        f'{col} - {vectorizer_name} + {classifier_name}'\n",
                "        for col, vectorizer_name, classifier_name in tqdm_product(\n",
                "            list(set(col_names_list)),\n",
                "            list(set(vectorizer_names_list)),\n",
                "            list(set(classifier_names_list)),\n",
                "        )\n",
                "    ]\n",
                "    return (\n",
                "        list(set(col_names_list)),\n",
                "        list(set(vectorizer_names_list)),\n",
                "        list(set(classifier_names_list)),\n",
                "        list(set(estimator_names_list))\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ab2972c",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_Xy_search_cv_estimator(\n",
                "    col, vectorizer_name, classifier_name, protocol,\n",
                "    results_save_path=results_save_path,\n",
                "    done_xy_save_path=done_xy_save_path, method=method,\n",
                "    compression=None, saved_files_list=None,\n",
                "    path_suffix=None, data_dict=None,\n",
                "):\n",
                "    if compression is None:\n",
                "        compression = False\n",
                "    if protocol is None:\n",
                "        protocol = pickle.HIGHEST_PROTOCOL\n",
                "    if path_suffix is None:\n",
                "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
                "    if data_dict is None:\n",
                "        data_dict = {}\n",
                "    if saved_files_list is None:\n",
                "        saved_files_list = []\n",
                "\n",
                "    # Load data into dict\n",
                "    for file_path in glob.glob(f'{done_xy_save_path}*{path_suffix}'):\n",
                "        file_name = file_path.split(f'{done_xy_save_path}{method} ')[-1].split(path_suffix)[0]\n",
                "        print(f'Loading {file_name} from {file_path}')\n",
                "        if 'df_' in file_name:\n",
                "            data_dict[file_name] = pd.read_pickle(file_path)\n",
                "        else:\n",
                "            with open(file_path, 'rb') as f:\n",
                "                data_dict[file_name] = joblib.load(f)\n",
                "        saved_files_list.append(file_name)\n",
                "    # Load estimator\n",
                "    print('Loading Estimator.')\n",
                "    with open(\n",
                "        f'{results_save_path}{method} Estimator{path_suffix}', 'rb'\n",
                "    ) as f:\n",
                "        data_dict['Estimator'] = joblib.load(f)\n",
                "    saved_files_list.append('Estimator')\n",
                "\n",
                "    # # Assign data to variables\n",
                "    estimator = data_dict['Estimator']\n",
                "    grid_search = data_dict['Grid Search']\n",
                "    searchcv = data_dict['SearchCV']\n",
                "    df_cv_results = data_dict['df_cv_results']\n",
                "    # Train data\n",
                "    df_train_data = data_dict['df_train_data']\n",
                "    X_train = df_train_data['X_train'].values\n",
                "    y_train = df_train_data['y_train'].values\n",
                "    y_train_pred = df_train_data['y_train_pred'].values\n",
                "    # Test data\n",
                "    df_test_data = data_dict['df_test_data']\n",
                "    X_test = df_test_data['X_test'].values\n",
                "    y_test = df_test_data['y_test'].values\n",
                "    y_test_pred = df_test_data['y_test_pred'].values\n",
                "    y_test_pred_prob = df_test_data['y_test_pred_prob'].values\n",
                "    # Val data\n",
                "    df_val_data = data_dict['df_val_data']\n",
                "    X_val = df_val_data['X_val'].values\n",
                "    y_val = df_val_data['y_val'].values\n",
                "    y_val_pred = df_val_data['y_val_pred'].values\n",
                "    y_val_pred_prob = df_val_data['y_val_pred_prob'].values\n",
                "    # Feature importances\n",
                "    if 'df_feature_importances' in data_dict.keys():\n",
                "        saved_files_list.append('df_feature_importances')\n",
                "        df_feature_importances = data_dict['df_feature_importances']\n",
                "    else:\n",
                "        df_feature_importances = None\n",
                "\n",
                "    # Check data\n",
                "    check_consistent_length(X_train, y_train, y_train_pred)\n",
                "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob)\n",
                "    check_is_fitted(estimator)\n",
                "\n",
                "    # Get class weights\n",
                "    (\n",
                "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
                "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
                "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
                "    ) = get_class_weights(\n",
                "        X_train, y_train,\n",
                "        X_test, y_test,\n",
                "        X_val, y_val,\n",
                "    )\n",
                "    # Print info\n",
                "    print_Xy(\n",
                "        X_train, y_train,\n",
                "        X_test, y_test,\n",
                "        X_val, y_val,\n",
                "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
                "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
                "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
                "    )\n",
                "\n",
                "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were loaded! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
                "    print(f'Done loading Xy, CV data, and estimator!\\n{list(data_dict.keys())}')\n",
                "    print('='*20)\n",
                "\n",
                "    return (\n",
                "        grid_search, searchcv,\n",
                "        X_train, y_train, y_train_pred,\n",
                "        X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "        X_val, y_val, y_val_pred, y_val_pred_prob,\n",
                "        df_feature_importances, df_cv_results, estimator,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1f1ef64",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_class_weights(\n",
                "    X_train, y_train,\n",
                "    X_test, y_test,\n",
                "    X_val, y_val,\n",
                "):\n",
                "    # Get train class weights\n",
                "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
                "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
                "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
                "\n",
                "    # Get train class weights\n",
                "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_test)\n",
                "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
                "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
                "\n",
                "    # Get val class weights\n",
                "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_val)\n",
                "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
                "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
                "\n",
                "    return (\n",
                "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
                "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
                "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a58ee3d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_Xy(\n",
                "    X_train, y_train,\n",
                "    X_test, y_test,\n",
                "    X_val, y_val,\n",
                "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
                "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
                "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
                "):\n",
                "    # Check for consistent length\n",
                "    check_consistent_length(X_train, y_train)\n",
                "    check_consistent_length(X_test, y_test)\n",
                "    check_consistent_length(X_val, y_val)\n",
                "\n",
                "    print('Done splitting data into training and testing sets.')\n",
                "    print('='*20)\n",
                "    print(f'Training set shape: {y_train.shape}')\n",
                "    print('-'*10)\n",
                "    print(f'Training set example:\\n{X_train[0]}')\n",
                "    print('~'*10)\n",
                "    print(f'Testing set shape: {y_test.shape}')\n",
                "    print('-'*10)\n",
                "    print(f'Testing set example:\\n{X_test[0]}')\n",
                "    print('~'*10)\n",
                "    print(f'Validation set shape: {y_val.shape}')\n",
                "    print('-'*10)\n",
                "    print(f'Validation set example:\\n{X_val[0]}')\n",
                "    print('~'*10)\n",
                "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
                "    print('-'*10)\n",
                "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
                "    print('-'*10)\n",
                "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
                "    print('='*20)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76cc9dbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics_with_estimator(\n",
                "    estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
                "    cv=cv, return_train_score=None,\n",
                "):\n",
                "    if return_train_score is None:\n",
                "        return_train_score = True\n",
                "\n",
                "    # Using estimator\n",
                "    # Cross Validation\n",
                "    print('-'*20)\n",
                "    print('Cross Validating without scoring.')\n",
                "    cv_score_noscoring = cross_validate(\n",
                "        estimator,\n",
                "        X_test,\n",
                "        y_test,\n",
                "        cv=cv,\n",
                "        scoring=None,\n",
                "        return_train_score=True,\n",
                "    )\n",
                "\n",
                "    # Cross Validation with scoring\n",
                "    print('-'*20)\n",
                "    print(f'Cross Validating with {scores} scoring.')\n",
                "    cv_score_recall = cross_validate(\n",
                "        estimator,\n",
                "        X_test,\n",
                "        y_test,\n",
                "        cv=cv,\n",
                "        scoring=scores,\n",
                "        return_train_score=True,\n",
                "    )\n",
                "\n",
                "    # Get mean and std of cross validation scores\n",
                "    print('-'*20)\n",
                "    print('Getting mean and std of cross validation scores.')\n",
                "    cv_train_scores = cv_score_noscoring['train_score'].mean()\n",
                "    cv_test_scores = cv_score_noscoring['test_score'].mean()\n",
                "    cv_train_recall = cv_score_recall['train_recall'].mean()\n",
                "    cv_test_recall = cv_score_recall['test_recall'].mean()\n",
                "    cv_train_explained_variance_recall = cv_score_recall['train_explained_variance'].mean()\n",
                "    cv_test_explained_variance_recall = cv_score_recall['test_explained_variance'].mean()\n",
                "\n",
                "    # Save cross validation scores to dataframe\n",
                "    print('-'*20)\n",
                "    print('Saving cross validation scores to dataframe.')\n",
                "    df_cv_score_noscoring = pd.DataFrame(cv_score_noscoring)\n",
                "    df_cv_score_noscoring.to_pickle(f'{df_save_dir}df_cv_score_noscoring - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
                "    df_cv_score_recall = pd.DataFrame(cv_score_recall)\n",
                "    df_cv_score_recall.to_pickle(f'{df_save_dir}df_cv_score_recall - {col}_{vectorizer_name}_{classifier_name}.pkl')\n",
                "\n",
                "    return (\n",
                "        df_cv_score_recall,\n",
                "        cv_train_scores, cv_test_scores,\n",
                "        cv_train_recall, cv_test_recall,\n",
                "        cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0e84026",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_metrics_with_estimator(\n",
                "    estimator, X_test, y_test, col, vectorizer_name, classifier_name, random_state=random_state, n_jobs=n_jobs, cv=cv,\n",
                "    params=None, axis=None, alpha=None, verbose=None\n",
                "):\n",
                "    if axis is None:\n",
                "        axis = 1\n",
                "    if alpha is None:\n",
                "        alpha = 0.1\n",
                "    if verbose is None:\n",
                "        verbose=1\n",
                "\n",
                "    # Make param names and values\n",
                "    if params is None:\n",
                "        params = {\n",
                "            param_name: classifier_params\n",
                "            for classifier_name, classifier_and_params in classifiers_pipe.items()\n",
                "            if estimator[-1].__class__.__name__ == classifier_name\n",
                "            for param_name_, classifier_params in classifier_and_params[-1].items()\n",
                "            for param_name in [param_name_.split(f'{classifier_name}__')[-1]]\n",
                "            if param_name != 'random_state' and all(isinstance(n, (list, int, float)) for n in classifier_params) and not all(isinstance(n, (bool)) for n in classifier_params)\n",
                "        }\n",
                "\n",
                "\n",
                "    # Learning Curves\n",
                "    print('Plotting Learning Curve.')\n",
                "    print('-'*20)\n",
                "    train_sizes, train_scores, test_scores = learning_curve(\n",
                "        estimator=estimator,\n",
                "        X=X_train,\n",
                "        y=y_train,\n",
                "        cv=cv,\n",
                "        n_jobs=n_jobs,\n",
                "        random_state=random_state,\n",
                "        shuffle=True,\n",
                "        scoring=scoring,\n",
                "        verbose=verbose,\n",
                "        # train_sizes=np.linspace(0.1, 1.0, 10),\n",
                "    )\n",
                "    train_scores_mean = np.mean(train_scores, axis=axis)\n",
                "    train_scores_std = np.std(train_scores, axis=axis)\n",
                "    test_scores_mean = np.mean(test_scores, axis=axis)\n",
                "    test_scores_std = np.std(test_scores, axis=axis)\n",
                "\n",
                "    close_plots()\n",
                "    plt.figure()\n",
                "    plt.title(\n",
                "        f'{col} - Learning Curves for {scoring.title()} - {vectorizer_name} + {classifier_name}'\n",
                "        )\n",
                "    plt.xlabel('Training examples')\n",
                "    plt.ylabel('Score')\n",
                "    plt.grid()\n",
                "    plt.fill_between(\n",
                "        train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
                "    )\n",
                "    plt.fill_between(\n",
                "        train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
                "    )\n",
                "    plt.plot(\n",
                "        train_sizes, train_scores_mean, 'o-', color='r', label='Training score'\n",
                "    )\n",
                "    plt.plot(\n",
                "        train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
                "    )\n",
                "    plt.legend(loc='best')\n",
                "    fig = plt.gcf()\n",
                "    fig.tight_layout()\n",
                "\n",
                "    # Save figure\n",
                "    for image_save_format in ['eps', 'png', 'svg']:\n",
                "        save_path = f'{plot_save_path}{method} {col} - Learning Curve - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "        print(f'Saving Learning Curve at {save_path}.')\n",
                "        fig.savefig(save_path, format=image_save_format)\n",
                "    show_and_close_plots()\n",
                "\n",
                "    # Validation Curve\n",
                "    for param_name, param_range in params.items():\n",
                "        param_title = ' '.join(param_name.split('_')).title()\n",
                "        print(f'Plotting Validation Curve for {param_title}.')\n",
                "        print('-'*20)\n",
                "        train_scores, test_scores = validation_curve(\n",
                "            estimator=estimator[-1],\n",
                "            X=X_train,\n",
                "            y=y_train,\n",
                "            param_name=param_name,\n",
                "            param_range=param_range,\n",
                "            cv=cv,\n",
                "            n_jobs=n_jobs,\n",
                "            scoring=scorers['recall_score'],\n",
                "            verbose=1,\n",
                "        )\n",
                "        train_scores_mean = np.mean(train_scores, axis=axis)\n",
                "        train_scores_std = np.std(train_scores, axis=axis)\n",
                "        test_scores_mean = np.mean(test_scores, axis=axis)\n",
                "        test_scores_std = np.std(test_scores, axis=axis)\n",
                "\n",
                "        # Ploting\n",
                "        plt.figure()\n",
                "        plt.title(\n",
                "            f'{col} - Validation Curve for {scoring.title()} - {col} - {vectorizer_name} + {classifier_name}'\n",
                "        )\n",
                "        plt.xlabel(param_name)\n",
                "        plt.ylabel('Score')\n",
                "        plt.grid()\n",
                "        plt.fill_between(\n",
                "            param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha, color='r'\n",
                "        )\n",
                "        plt.fill_between(\n",
                "            param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha, color='g'\n",
                "        )\n",
                "        plt.semilogx(\n",
                "            param_range, train_scores_mean, label='Training score', color='r'\n",
                "        )\n",
                "        plt.semilogx(\n",
                "            param_range, test_scores_mean, label='Cross-validation score', color='g'\n",
                "        )\n",
                "        plt.plot(\n",
                "            param_range, train_scores_mean, 'o-', color='r', label='Training score'\n",
                "        )\n",
                "        plt.plot(\n",
                "            param_range, test_scores_mean, 'o-', color='g', label='Cross-validation score'\n",
                "        )\n",
                "        plt.legend(loc='best')\n",
                "        fig = plt.gcf()\n",
                "        fig.tight_layout()\n",
                "\n",
                "        # Save figure\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - Validation Curve for {scoring.title()} - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f'Saving Validation Curve at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format\n",
                "            )\n",
                "        show_and_close_plots()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cd7a0330",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics_with_y_pred(\n",
                "    y_labels, y_pred,\n",
                "    pos_label=None, labels=None, zero_division=None, alpha=None\n",
                "):\n",
                "    if pos_label is None:\n",
                "        pos_label = 1\n",
                "    if labels is None:\n",
                "        labels = np.unique(y_pred)\n",
                "    if zero_division is None:\n",
                "        zero_division = 0\n",
                "    if alpha is None:\n",
                "        alpha = 0.1\n",
                "\n",
                "    print('Computing metrics using y_pred.')\n",
                "    # Using y_pred\n",
                "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
                "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
                "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
                "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
                "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
                "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
                "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
                "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
                "    r2 = metrics.r2_score(y_labels, y_pred)\n",
                "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
                "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
                "    gmean = gmean_iba(y_labels, y_pred)\n",
                "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
                "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
                "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
                "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
                "\n",
                "    return (\n",
                "        explained_variance, accuracy, balanced_accuracy, precision,\n",
                "        recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d89a7520",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_metrics_with_y_pred(\n",
                "    y_labels, y_pred, col, vectorizer_name, classifier_name,\n",
                "    pos_label=None, labels=None\n",
                "):\n",
                "    if pos_label is None:\n",
                "        pos_label = 1\n",
                "    if labels is None:\n",
                "        labels = np.unique(y_pred)\n",
                "\n",
                "    # Displays\n",
                "    close_plots()\n",
                "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
                "        y_labels, y_pred, display_labels=labels, cmap=plt.cm.Blues\n",
                "    )\n",
                "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
                "        y_labels, y_pred, normalize='true', display_labels=labels, cmap=plt.cm.Blues\n",
                "    )\n",
                "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
                "        y_labels, y_pred, pos_label=pos_label\n",
                "    )\n",
                "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
                "        y_labels, y_pred, pos_label=pos_label\n",
                "    )\n",
                "    calibration_curve = CalibrationDisplay.from_predictions(\n",
                "        y_labels, y_pred, pos_label=pos_label\n",
                "    )\n",
                "    show_and_close_plots()\n",
                "\n",
                "    # Plots\n",
                "    plots_dict = {\n",
                "        'Confusion Matrix': cm_curve,\n",
                "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
                "        'ROC Curve': roc_curve,\n",
                "        'Precision-Recall Curve': pr_curve,\n",
                "        'Calibration Curve': calibration_curve,\n",
                "    }\n",
                "\n",
                "    print('=' * 20)\n",
                "    close_plots()\n",
                "    print('Plotting metrics with y_pred_prob:')\n",
                "    print('='*20)\n",
                "\n",
                "    for plot_name, plot_ in plots_dict.items():\n",
                "        close_plots()\n",
                "        print(f'Plotting {plot_name}:')\n",
                "        fig, ax = plt.subplots()\n",
                "        ax.set_title(\n",
                "            f'{col} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
                "            )\n",
                "        if plot_name == 'ROC Curve':\n",
                "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
                "        try:\n",
                "            plot_.plot(ax=ax, cmap=plt.cm.Blues)\n",
                "        except Exception:\n",
                "            plot_.plot(ax=ax)\n",
                "        print('=' * 20)\n",
                "        fig = plt.gcf()\n",
                "        fig.tight_layout()\n",
                "\n",
                "        # Save Plots\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f'Saving {plot_name} at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n",
                "        show_and_close_plots()\n",
                "        print(f'Saved {plot_name}!')\n",
                "        print('=' * 20)\n",
                "\n",
                "    with contextlib.suppress(AttributeError):\n",
                "        # Visualisation with plot_metric\n",
                "        bc = plot_metric.functions.BinaryClassification(y_labels, y_pred, labels=[0, 1])\n",
                "\n",
                "        # Figures\n",
                "        close_plots()\n",
                "        fig = plt.figure(figsize=(15, 10))\n",
                "        plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
                "        bc.plot_confusion_matrix(colorbar=True)\n",
                "        plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
                "        bc.plot_confusion_matrix(normalize=True, colorbar=True)\n",
                "        plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
                "        bc.plot_roc_curve()\n",
                "        plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
                "        bc.plot_precision_recall_curve()\n",
                "        plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
                "        bc.plot_class_distribution()\n",
                "        bc.print_report()\n",
                "        fig = plt.gcf()\n",
                "        fig.tight_layout()\n",
                "\n",
                "        # Save Plots\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f'Saving plot_metric Curves at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n",
                "        show_and_close_plots()\n",
                "\n",
                "        # Heatmap\n",
                "        print('Plotting Heatmap:')\n",
                "        close_plots()\n",
                "        classifications_dict = defaultdict(int)\n",
                "        for _y_labels, _y_pred in zip(y_labels, y_pred):\n",
                "            if _y_labels != _y_pred:\n",
                "                classifications_dict[(_y_labels, _y_pred)] += 1\n",
                "\n",
                "        dicts_to_plot = [\n",
                "            {\n",
                "                f'True {col} value': _y_labels,\n",
                "                f'Predicted {col} value': _y_pred,\n",
                "                'Number of Classifications': _count,\n",
                "            }\n",
                "            for (_y_labels, _y_pred), _count in classifications_dict.items()\n",
                "        ]\n",
                "        df_to_plot = pd.DataFrame(dicts_to_plot)\n",
                "        df_wide = df_to_plot.pivot_table(\n",
                "            index=f'True {col} value', \n",
                "            columns=f'Predicted {col} value', \n",
                "            values='Number of Classifications'\n",
                "        )\n",
                "        plt.figure(figsize=(9,7))\n",
                "        sns.set(style='ticks', font_scale=1.2)\n",
                "        sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Blues, annot=True)    \n",
                "        plt.xticks(rotation=45, ha='right')\n",
                "        plt.yticks(rotation=0)\n",
                "        plt.title(f'{col} Heatmap - {vectorizer_name} + {classifier_name}')\n",
                "        fig = plt.gcf()\n",
                "        fig.tight_layout()\n",
                "\n",
                "        # Save Heatmap\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f'Saving Heatmap at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n",
                "        print('Saved Heatmap!')\n",
                "        show_and_close_plots()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1de7dd9e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics_with_y_pred_prob(\n",
                "    y_labels, y_pred_prob,\n",
                "    pos_label=None\n",
                "):\n",
                "    if pos_label is None:\n",
                "        pos_label = 1\n",
                "\n",
                "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
                "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
                "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
                "    auc = metrics.auc(fpr, tpr)\n",
                "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
                "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
                "\n",
                "    return (\n",
                "        average_precision, roc_auc, auc,\n",
                "        fpr, tpr, threshold,loss,\n",
                "        precision_pr, recall_pr, threshold_pr\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "162b0699",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(\n",
                "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "    col, vectorizer_name, classifier_name, cv=cv, n_jobs=n_jobs,\n",
                "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None,\n",
                "    pos_label=None, verbose=None,\n",
                "):\n",
                "    if pos_label is None:\n",
                "        pos_label = 1\n",
                "    if verbose is None:\n",
                "        verbose = 1\n",
                "    if with_estimator is None:\n",
                "        with_estimator = True\n",
                "    if with_y_pred is None:\n",
                "        with_y_pred = True\n",
                "    if with_y_pred_prob is None:\n",
                "        with_y_pred_prob = True\n",
                "\n",
                "    # Get metrics\n",
                "    print('='*20)\n",
                "    # Using estimator\n",
                "    if with_estimator:\n",
                "        print('Computing metrics using estimator.')\n",
                "        (\n",
                "            df_cv_score_recall,\n",
                "            cv_train_scores, cv_test_scores,\n",
                "            cv_train_recall, cv_test_recall,\n",
                "            cv_train_explained_variance_recall, cv_test_explained_variance_recall\n",
                "        ) = compute_metrics_with_estimator(\n",
                "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
                "        )\n",
                "    # Using y_test_pred\n",
                "    if with_y_pred:\n",
                "        print('-'*20)\n",
                "        print('Computing metrics using y_test_pred.')\n",
                "        (\n",
                "            explained_variance, accuracy, balanced_accuracy, precision,\n",
                "            recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
                "        ) = compute_metrics_with_y_pred(\n",
                "            y_test, y_test_pred\n",
                "        )\n",
                "    # Using y_test_pred_prob\n",
                "    if with_y_pred_prob:\n",
                "        print('-'*20)\n",
                "        print('Computing metrics using y_test_pred_prob.')\n",
                "        (\n",
                "            average_precision, roc_auc, auc,\n",
                "            fpr, tpr, threshold,loss,\n",
                "            precision_pr, recall_pr, threshold_pr\n",
                "        ) = compute_metrics_with_y_pred_prob(\n",
                "            y_test, y_test_pred_prob\n",
                "        )\n",
                "\n",
                "    #Place metrics into dict\n",
                "    print('-'*20)\n",
                "    print('Appending metrics to dict.')\n",
                "    metrics_dict = {\n",
                "        f'{scoring.title()} Best Score': float(best_train_score),\n",
                "        f'{scoring.title()} Best Threshold': threshold,\n",
                "        'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
                "        f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
                "        f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
                "        'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
                "        f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
                "        f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
                "        'Explained Variance': float(explained_variance),\n",
                "        'Accuracy': float(accuracy),\n",
                "        'Balanced Accuracy': float(balanced_accuracy),\n",
                "        'Precision': float(precision),\n",
                "        'Average Precision': float(average_precision),\n",
                "        'Recall': float(recall),\n",
                "        'F1-score': float(f1),\n",
                "        'Matthews Correlation Coefficient': float(mcc),\n",
                "        'Fowlkes–Mallows Index': float(fm),\n",
                "        'R2 Score': float(r2),\n",
                "        'ROC': float(roc_auc),\n",
                "        'AUC': float(auc),\n",
                "        'Log Loss/Cross Entropy': float(loss),\n",
                "        'Cohen’s Kappa': float(kappa),\n",
                "        'Geometric Mean': float(gmean),\n",
                "        'Classification Report': report,\n",
                "        'Imbalanced Classification Report': imblearn_report,\n",
                "        'Confusion Matrix': cm,\n",
                "        'Normalized Confusion Matrix': cm_normalized\n",
                "    }\n",
                "\n",
                "    return (\n",
                "        metrics_dict, df_cv_score_recall,\n",
                "        cv_train_scores, cv_test_scores,\n",
                "        cv_train_recall, cv_test_recall,\n",
                "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
                "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
                "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
                "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
                "        loss, precision_pr, recall_pr, threshold_pr,\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42fdf3c3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_metrics(\n",
                "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "    col, vectorizer_name, classifier_name, \n",
                "    with_estimator=None, with_y_pred=None, with_y_pred_prob=None\n",
                "):\n",
                "    if with_estimator is None:\n",
                "        with_estimator = True\n",
                "    if with_y_pred is None:\n",
                "        with_y_pred = True\n",
                "    if with_y_pred_prob is None:\n",
                "        with_y_pred_prob = True\n",
                "\n",
                "    # Plotting\n",
                "    print('~'*20)\n",
                "    print('Plotting metrics.')\n",
                "    print('~'*20)\n",
                "    # Using estimator\n",
                "    if with_estimator:\n",
                "        plot_metrics_with_estimator(\n",
                "             estimator, X_test, y_test, col, vectorizer_name, classifier_name,\n",
                "        )\n",
                "    # Using y_test_pred\n",
                "    if with_y_pred:\n",
                "        plot_metrics_with_y_pred(\n",
                "            y_test, y_test_pred, col, vectorizer_name, classifier_name,\n",
                "        )\n",
                "    print('='*20)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "585fa8a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def examine_predictions(\n",
                "    X_test, y_test, y_test_pred, col\n",
                "):\n",
                "    # Examine predictions\n",
                "    print('~'*20)\n",
                "    print(f'Examining predictions for {col}')\n",
                "    print('Incorrectly Classified Reviews:')\n",
                "    for _y_test, _y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 5):\n",
                "        if _y_test != _y_test_pred:\n",
                "            print('-'*20)\n",
                "            print(f'TRUE LABEL: {_y_test}')\n",
                "            print(f'PREDICTED LABEL: {_y_test_pred}')\n",
                "            print(f'REVIEW TEXT: {_X_test[:100]}')\n",
                "            print('-'*20)\n",
                "    print('~'*20)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8278ce83",
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluation(\n",
                "    estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "    best_train_score, df_metrics, col, vectorizer_name, classifier_name, scorig=scoring\n",
                "):\n",
                "\n",
                "    # Get metrics dict\n",
                "    (\n",
                "        metrics_dict, df_cv_score_recall,\n",
                "        cv_train_scores, cv_test_scores,\n",
                "        cv_train_recall, cv_test_recall,\n",
                "        cv_train_explained_variance_recall, cv_test_explained_variance_recall,\n",
                "        explained_variance, accuracy, balanced_accuracy, precision, recall,\n",
                "        f1, mcc, fm, kappa, gmean, report, cm, cm_normalized,\n",
                "        average_precision, roc_auc, auc, fpr, tpr, threshold, \n",
                "        loss, precision_pr, recall_pr, threshold_pr,\n",
                "    ) = compute_metrics(\n",
                "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "        col, vectorizer_name, classifier_name\n",
                "    )\n",
                "\n",
                "    # Print metrics\n",
                "    print('=' * 20)\n",
                "    print('~' * 20)\n",
                "    print(f' Testing Metrics for {col} - {vectorizer_name} + {classifier_name}')\n",
                "    print('~' * 20)\n",
                "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
                "    print('-' * 20)\n",
                "    for metric_name, metric_value in metrics_dict.items():\n",
                "        if 'Threshold' not in metric_name:\n",
                "            with contextlib.suppress(TypeError, ValueError):\n",
                "                metric_value = float(metric_value)\n",
                "            if isinstance(metric_value, (int, float)):\n",
                "                df_metrics.loc[\n",
                "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
                "                ] = metric_value\n",
                "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
                "            else:\n",
                "                print(f'{metric_name}:\\n{metric_value}')\n",
                "                df_metrics.loc[\n",
                "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
                "                ] = str(metric_value)\n",
                "            print('-' * 20)\n",
                "\n",
                "    print('=' * 20)\n",
                "\n",
                "    # Plot Metrics\n",
                "    plot_metrics(\n",
                "        estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "        col, vectorizer_name, classifier_name,\n",
                "    )\n",
                "\n",
                "    return df_metrics, metrics_dict, df_cv_score_recall\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9d5b2a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_fitted_estimator(\n",
                "    estimator,\n",
                "    col, vectorizer_name, classifier_name,\n",
                "    protocol=None,\n",
                "    results_save_path=results_save_path,\n",
                "    method=method, done_xy_save_path=done_xy_save_path,\n",
                "    path_suffix=None, data_dict=None,\n",
                "    compression=None, \n",
                "):\n",
                "    if protocol is None:\n",
                "        protocol = pickle.HIGHEST_PROTOCOL\n",
                "    if path_suffix is None:\n",
                "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
                "    if data_dict is None:\n",
                "        data_dict = {}\n",
                "    if compression is None:\n",
                "        compression = False\n",
                "\n",
                "    # Save fitted estimator\n",
                "    print('~'*20)\n",
                "    print(f'Saving fitted estimator {classifier_name} at {results_save_path}')\n",
                "    with open(\n",
                "        f'{results_save_path}{method} Fitted Estimator{path_suffix}', 'wb'\n",
                "    ) as f:\n",
                "        joblib.dump(file_, f, compress=compression, protocol=protocol)\n",
                "    print('~'*20)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f33cf875",
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_table(\n",
                "    df_metrics,\n",
                "    col, vectorizer_name, classifier_name, protocol,\n",
                "    table_save_path=table_save_path,\n",
                "    method=method, save_name=None,\n",
                "    compression=None, \n",
                "    path_suffix=None, \n",
                "):\n",
                "    if save_name is None:\n",
                "        save_name = 'Supervised Estimators Table'\n",
                "    if compression is None:\n",
                "        compression = False\n",
                "    if protocol is None:\n",
                "        protocol = pickle.HIGHEST_PROTOCOL\n",
                "    if path_suffix is None:\n",
                "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
                "\n",
                "    # Save metrics df\n",
                "    save_path = f'{table_save_path}{save_name}'\n",
                "    print(f'Saving fitted estimator and table at {save_path}')\n",
                "    df_metrics.to_csv(f'{save_path}.csv')\n",
                "    df_metrics.to_pickle(f'{save_path}.pkl')\n",
                "    df_metrics.to_excel(f'{save_path}.xlsx')\n",
                "    df_metrics.to_latex(f'{save_path}.tex')\n",
                "    df_metrics.to_markdown(f'{save_path}.md')\n",
                "    df_metrics.to_html(f'{save_path}.html')\n",
                "\n",
                "    print('Done saving fitted estimator and table!')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8fc524e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_completed_estimators(\n",
                "    results_save_path=results_save_path, method=method, classifiers_pipe=classifiers_pipe,\n",
                "    estimators_list=None, used_classifiers=None,\n",
                "):\n",
                "    if estimators_list is None:\n",
                "        estimators_list = []\n",
                "    if used_classifiers is None:\n",
                "        used_classifiers = []\n",
                "\n",
                "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.pkl'):\n",
                "        classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
                "        used_classifiers.append(classifier_name)\n",
                "        with open(estimator_path, 'rb') as f:\n",
                "            estimators_list.append(joblib.load(f))\n",
                "\n",
                "    assert set(classifiers_pipe.keys()) == set(used_classifiers), f'Not all classifiers were used! Missing: {set(classifiers_pipe.keys()) ^ set(used_classifiers)}'\n",
                "\n",
                "    return estimators_list\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a44148bd",
            "metadata": {},
            "outputs": [],
            "source": [
                "def comparison_plots(\n",
                "    estimators_list, X_test, y_test, col,\n",
                "    curves_dict=None, cmap=plt.cm.Blues\n",
                "):\n",
                "\n",
                "    curves_dict = {\n",
                "        'ROC Curve': metrics.RocCurveDisplay,\n",
                "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
                "        'Calibration Curve': CalibrationDisplay,\n",
                "    }\n",
                "\n",
                "    assert len(estimators_list) != 0\n",
                "\n",
                "    for curve_name, curve_package in curves_dict.items():\n",
                "        print('-' * 20)\n",
                "        print(f'{col} - {str(curve_name)}')\n",
                "        fig, ax = plt.subplots()\n",
                "        ax.set_title(f'{col} - {str(curve_name)}')\n",
                "        for estimator in estimators_list:\n",
                "            try:\n",
                "                curve = curve_package.from_estimator(\n",
                "                    estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
                "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
                "                )\n",
                "            except AttributeError:\n",
                "                curve = curve_package.from_estimator(\n",
                "                    estimator, X_test, y_test, pos_label=1, ax=ax,\n",
                "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
                "                )\n",
                "        show_and_close_plots()\n",
                "\n",
                "        # Save Plots\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - All {str(curve_name)}s.{image_save_format}'\n",
                "            print(f'Saving {curve_name} at {save_path}')\n",
                "            curve.figure_.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "036fcf10",
            "metadata": {},
            "source": [
                "# Evaluating"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0212dc9",
            "metadata": {
                "code_folding": [],
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "print('#'*40)\n",
                "print('Starting!')\n",
                "print('#'*40)\n",
                "\n",
                "method = 'Supervised'\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "\n",
                "# Get existing estimators\n",
                "col_names_list, vectorizer_names_list, classifier_names_list, estimator_names_list = get_existing_files()\n",
                "\n",
                "# Identify cols, vectorizers and classifiers\n",
                "for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.pkl')):\n",
                "    if ' Estimator - ' in estimators_file:\n",
                "        col = estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
                "        vectorizer_name = estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
                "        classifier_name = estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
                "        protocol = int(estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[-1].split(').pkl')[0])\n",
                "        # Load Table DF\n",
                "        df_metrics = make_df_metrics(\n",
                "            vectorizers_pipe=vectorizers_pipe, classifiers_pipe=classifiers_pipe, metrics_list=list(metrics_dict.keys()),\n",
                "            col=col, vectorizer_name=vectorizer_name, classifier_name=classifier_name, protocol=protocol\n",
                "        )\n",
                "        print('~'*20)\n",
                "        print(f'Loading data for {col} - {vectorizer_name} + {classifier_name}')\n",
                "        print('~'*20)\n",
                "        # Load X, y, search_cv, estimator\n",
                "        (\n",
                "            # grid_search, searchcv,\n",
                "            X_train, y_train, y_train_pred,\n",
                "            X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "            X_val, y_val, y_val_pred, y_val_pred_prob,\n",
                "            df_feature_importances, df_cv_results, estimator,\n",
                "        ) = load_Xy_search_cv_estimator(\n",
                "            col, vectorizer_name, classifier_name, protocol\n",
                "        )\n",
                "\n",
                "        print('-'*20)\n",
                "        print(f'{\"=\"*30} EVALUATING DATASET OF LENGTH {len(X_train)+len(X_test)+len(X_val)} ON {col.upper()} {\"=\"*30}')\n",
                "        print('='*20)\n",
                "        print(\n",
                "            f'GridSearch - Best mean train score: M = {float(best_mean_train_score:=searchcv.cv_results_[\"mean_train_score\"][best_index:=searchcv.best_index_]):.2f}, SD = {int(best_std_train_score:=searchcv.cv_results_[\"std_train_score\"][best_index]):.2f}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'GridSearch - Best mean test score: M = {float(best_mean_test_score:=searchcv.cv_results_[\"mean_test_score\"][best_index]):.2f}, SD = {int(best_std_test_score:=searchcv.cv_results_[\"std_test_score\"][best_index]):.2f}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Best train score for {scoring}: {float(best_train_score:=searchcv.best_score_):.2f}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Best test score for {scoring}: {float(best_test_score:=searchcv.score(X_test, y_test)):.2f}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Number of splits: {int(n_splits:=searchcv.n_splits_)}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Best estimator:\\n{searchcv.best_estimator_}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Best estimator and parameters:\\n{searchcv.best_params_}\\n'\n",
                "        )\n",
                "        print(\n",
                "            f'Testing Classification Report:\\n{(train_report:=metrics.classification_report(y_test, y_test_pred, labels=np.unique(y_test_pred), zero_division=0))}\\n'\n",
                "        )\n",
                "        # Examine predictions\n",
                "        examine_predictions(\n",
                "            X_test, y_test, y_test_pred, col\n",
                "        )\n",
                "        print('='*20)\n",
                "        # Train and Test Confusion Matrix\n",
                "        print('='*20)\n",
                "        print('Train and Test Confusion Matrix:\\n')\n",
                "        close_plots()\n",
                "        fig, axs = plt.subplots(1, 2)\n",
                "        fig.suptitle(f'{col} - Train and Test  Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
                "        for ax in axs:\n",
                "            ax.set_aspect('equal')\n",
                "        train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
                "            estimator, X_train, y_train, normalize='true', ax=axs[0], cmap=plt.cm.Blues, colorbar=False\n",
                "        )\n",
                "        train_cm.ax_.set_title('Trainning Data')\n",
                "        test_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
                "            estimator, X_test, y_test, normalize='true', ax=axs[1], cmap=plt.cm.Blues, colorbar=False\n",
                "        )\n",
                "        test_cm.ax_.set_title('Testing Data')\n",
                "        plt.tight_layout()\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - Train and Test Confusion Matrix - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f' Train and Test Confusion Matrix plot at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n",
                "        show_and_close_plots()\n",
                "        print('='*20)\n",
                "        # Train and Test ROC Curve\n",
                "        print('='*20)\n",
                "        print('Train and Test Scores in K Folds Cross Validation:')\n",
                "        close_plots()\n",
                "        fig = plt.figure(figsize=(10, 5))\n",
                "        plt.title(f'K-folds Cross Validation Train vs. Test Scores for {col} - {vectorizer_name} + {classifier_name}')\n",
                "        plt.plot(searchcv.cv_results_[\"mean_train_score\"], label='Train Scores')\n",
                "        plt.plot(searchcv.cv_results_[\"mean_test_score\"], label='Test Scores')\n",
                "        plt.legend(loc='best')\n",
                "        plt.xlabel('Cross Validation Steps Over K Number of Folds')\n",
                "        plt.ylabel('Recall Score')\n",
                "        fig.text(0.1, 0.01, '*Number of folds used (K) = 10', ha='center', va='center', fontsize=10)\n",
                "        for image_save_format in ['eps', 'png', 'svg']:\n",
                "            save_path = f'{plot_save_path}{method} {col} - Train and Test Scores in K Folds Cross Validation - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
                "            print(f'K Folds plot at {save_path}')\n",
                "            fig.savefig(\n",
                "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
                "            )\n",
                "        show_and_close_plots()\n",
                "        print('='*20)\n",
                "\n",
                "        # Fit estimator\n",
                "        print('~'*20)\n",
                "        print('Fitting best params to estimator')\n",
                "        estimator = estimator.set_params(**searchcv.best_params_)\n",
                "        estimator.fit(np.concatenate((X_train, X_test, X_val), axis=0), np.concatenate((y_train, y_test, y_val), axis=0))\n",
                "        print('Saving fitted estimator')\n",
                "        save_fitted_estimator(\n",
                "            estimator, col, vectorizer_name, classifier_name, protocol,\n",
                "        )\n",
                "        print('Fitted estimator saved')\n",
                "        print('~'*20)\n",
                "\n",
                "        # Evaluate Model\n",
                "        df_metrics, metrics_dict, df_cv_score_recall = evaluation(\n",
                "            estimator, X_test, y_test, y_test_pred, y_test_pred_prob,\n",
                "            best_train_score, df_metrics,\n",
                "            col, vectorizer_name, classifier_name, \n",
                "        )\n",
                "\n",
                "        # Save Vectorizer, Selector, and Classifier\n",
                "        save_table(df_metrics, col, vectorizer_name, classifier_name, protocol)\n",
                "\n",
                "# Compare Estimators\n",
                "print('='*20)\n",
                "print(f'Comparing Estimators for {col}')\n",
                "comparison_plots(get_completed_estimators(), X_test, y_test, col)\n",
                "print('='*20)\n",
                "\n",
                "print('#'*40)\n",
                "print('DONE!')\n",
                "print('#'*40)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df3db98d",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "study1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {},
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
