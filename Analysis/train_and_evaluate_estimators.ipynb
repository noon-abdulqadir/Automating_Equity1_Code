{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b240feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99116954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
    "                             balanced_accuracy_score, brier_score_loss,\n",
    "                             classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db858f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split ratios\n",
    "n_jobs = 1\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.10\n",
    "validation_ratio = 0.15\n",
    "test_split = test_size = 1 - train_ratio\n",
    "validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "# Cross-validation\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "partition = True\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "# Resampling\n",
    "class_weight = 'balanced'\n",
    "resampling_enabled = True\n",
    "resample_enn = SMOTEENN(\n",
    "    enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "resample_tome = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "# Undersampling\n",
    "rus = RandomUnderSampler(random_state=random_state, replacement=True)\n",
    "tl = RandomOverSampler(sampling_strategy='majority')\n",
    "nm = NearMiss()\n",
    "# Oversampling\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "smote = SMOTE()\n",
    "# Sampling Used\n",
    "resampling_method = resample_tome\n",
    "\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_sizes = [300, 100]\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "metrics_list = [\n",
    "    'Mean Validation Score',\n",
    "    'Explained Variance',\n",
    "    'Accuracy',\n",
    "    'Precision',\n",
    "    'Recall',\n",
    "    'F1-score',\n",
    "    'ROC',\n",
    "    'AUC',\n",
    "    'Matthews Correlation Coefficient',\n",
    "    f'{scoring.title()} Best Threshold',\n",
    "    f'{scoring.title()} Best Score',\n",
    "    'Log Loss/Cross Entropy',\n",
    "    'Classification Report',\n",
    "    'Confusion Matrix',\n",
    "    'Accuracy_opt',\n",
    "    'Precision_opt',\n",
    "    'Recall_opt',\n",
    "    'F1-score_opt',\n",
    "    'Matthews Correlation Coefficient_opt',\n",
    "    'Classification Report_opt',\n",
    "    'Confusion Matrix_opt',\n",
    "]\n",
    "\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "pickle_file_name = 'Classifiers Table.pkl'\n",
    "csv_file_name = 'Classifiers Table.csv'\n",
    "excel_file_name = 'Classifiers Table.xlsx'\n",
    "latex_file_name = 'Classifiers Table.tex'\n",
    "markdown_file_name = 'Classifiers Table.md'\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bd570",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c567a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count = CountVectorizer()\n",
    "params_count_pipe = {\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.70],\n",
    "    'CountVectorizer__min_df': [0.10, 0.15, 0.2, 0.25, 0.30],\n",
    "}\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "params_tfidf_pipe = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "#     'TfidfVectorizer__max_features': [None, 5000, 10000, 50000],\n",
    "    'TfidfVectorizer___use_idf': [True],\n",
    "#     'TfidfVectorizer___smooth_idf': [True, False],\n",
    "    'TfidfVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.70],\n",
    "    'TfidfVectorizer__min_df': [0.10, 0.15, 0.2, 0.25, 0.30],\n",
    "}\n",
    "\n",
    "### BOW FeatureUnion\n",
    "bow = FeatureUnion(\n",
    "    transformer_list=[('CountVectorizer', count), ('TfidfVectorizer', tfidf)]\n",
    ")\n",
    "params_bow_pipe = {**params_count_pipe, **params_tfidf_pipe}\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    'CountVectorizer': [count, params_count_pipe],\n",
    "    'TfidfVectorizer': [tfidf, params_tfidf_pipe],\n",
    "    'UnionBOW': [bow, params_bow_pipe],\n",
    "    # \"UnionWordEmbedding\": [em, params_em_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276d9dd",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c402f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectors\n",
    "selector = SelectKBest(score_func=chi2, k='all')\n",
    "selector_name = selector.__class__.__name__\n",
    "\n",
    "# model_selector = SelectFromModel()\n",
    "# model_selector_name = model_selector.__class__.__name__\n",
    "\n",
    "### SelectKBest\n",
    "selectkbest = SelectKBest()\n",
    "params_selectkbest_pipe = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "\n",
    "### SelectPercentile\n",
    "selectpercentile = SelectPercentile()\n",
    "params_selectpercentile_pipe = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr = SelectFpr()\n",
    "params_selectfpr_pipe = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr = SelectFdr()\n",
    "params_selectfdr_pipe = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe = SelectFwe()\n",
    "params_selectfwe_pipe = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    'SelectKBest': [selectkbest, params_selectkbest_pipe],\n",
    "    'SelectPercentile': [selectpercentile, params_selectpercentile_pipe],\n",
    "    'SelectFpr': [selectfpr, params_selectfpr_pipe],\n",
    "    'SelectFdr': [selectfdr, params_selectfdr_pipe],\n",
    "    'SelectFwe': [selectfwe, params_selectfwe_pipe],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9bde7",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c325f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy = DummyClassifier()\n",
    "params_dummy_freq = {'strategy': 'most_frequent', 'random_state': random_state}\n",
    "params_dummy_stratified = {'strategy': 'stratified', 'random_state': random_state}\n",
    "params_dummy_uniform = {'strategy': 'uniform', 'random_state': random_state}\n",
    "params_dummy_pipe = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "        'constant',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "# ### Multinomial Naive Bayes\n",
    "# nb = MultinomialNB()\n",
    "# params_nb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "# params_nb_pipe = {\n",
    "#     'MultinomialNB__fit_prior': [True],\n",
    "#     'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "# }\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "params_bnb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "params_bnb_pipe = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "params_gnb = {'var_smoothing': 1e-9}\n",
    "params_gnb_pipe = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {\n",
    "    'n_neighbors': 3,\n",
    "    'weights': 'uniform',\n",
    "    'algorithm': 'auto',\n",
    "    'leaf_size': 30,\n",
    "    'p': 2,\n",
    "    'metric': 'minkowski',\n",
    "    'metric_params': None,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_knn_pipe = {\n",
    "    'KNeighborsClassifier__weights': ['uniform'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    'KNeighborsClassifier__metric': [\n",
    "        'minkowski',\n",
    "        'euclidean',\n",
    "        'cosine',\n",
    "        'correlation',\n",
    "    ],\n",
    "    'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "### Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "params_lr = {\n",
    "    'penalty': 'l2',\n",
    "    'dual': False,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'solver': 'liblinear',\n",
    "    'max_iter': 100,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_lr_pipe = {\n",
    "    'LogisticRegression__penalty': ['l2'],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LogisticRegression__multi_class': ['ovr', 'multinomial'],\n",
    "    'LogisticRegression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "### Passive Aggressive\n",
    "pa = PassiveAggressiveClassifier()\n",
    "params_pa = {\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_pa_pipe = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': [None, 'balanced'],\n",
    "    'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd = SGDClassifier()\n",
    "params_sgd = {\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_sgd_pipe = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': [None, 'balanced'],\n",
    "    'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### SVM\n",
    "svm = LinearSVC()\n",
    "params_svm = {\n",
    "    'penalty': 'l2',\n",
    "    'loss': 'hinge',\n",
    "    'dual': True,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'max_iter': 1000,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "}\n",
    "params_svm_pipe = {\n",
    "    'LinearSVC__penalty': ['l2'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': [None, 'balanced'],\n",
    "    'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "}\n",
    "\n",
    "### Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "params_dt = {\n",
    "    'criterion': 'gini',\n",
    "    'splitter': 'best',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': random_state,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "}\n",
    "params_dt_pipe = {\n",
    "    'DecisionTreeClassifier__max_depth': [5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "params_rf = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_rf_pipe = {\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__n_jobs': [-1],\n",
    "    'RandomForestClassifier__max_depth': [5, 10],\n",
    "    'RandomForestClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "### Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "params_et = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_et_pipe = {\n",
    "    'ExtraTreesClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__n_jobs': [-1],\n",
    "    'ExtraTreesClassifier__max_depth': [5, 10],\n",
    "    'ExtraTreesClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'ExtraTreesClassifier__random_state': [42, 200],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc = GradientBoostingClassifier()\n",
    "params_gbc = {\n",
    "    'loss': 'deviance',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_depth': 3,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'init': None,\n",
    "    'random_state': random_state,\n",
    "    'max_features': None,\n",
    "    'verbose': 0,\n",
    "    'max_leaf_nodes': None,\n",
    "    'warm_start': False,\n",
    "}\n",
    "params_gbc_pipe = {\n",
    "    'GradientBoostingClassifier__max_depth': [5, 10],\n",
    "    'GradientBoostingClassifier__criterion': ['gini', 'entropy'],\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__n_estimators': [10, 20],\n",
    "    'GradientBoostingClassifier__loss': ['deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__subsample': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'GradientBoostingClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### AdaBoost\n",
    "ada = AdaBoostClassifier()\n",
    "params_ada = {\n",
    "    'base_estimator': None,\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 1.0,\n",
    "    'algorithm': 'SAMME.R',\n",
    "    'random_state': random_state,\n",
    "}\n",
    "params_ada_pipe = {\n",
    "    'AdaBoostClassifier__max_depth': [5, 10],\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(probability=True, kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "### XGBoost\n",
    "xgb = XGBClassifier()\n",
    "params_xgb = {\n",
    "    'nthread':4, #when use hyperthread, xgboost may become slower\n",
    "    'objective':'binary:logistic',\n",
    "    'learning_rate': 0.05, #so called `eta` value\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 11,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'n_estimators': 1000, #number of trees, change it to 1000 for better results\n",
    "    'missing':-999,\n",
    "    'seed': 1337,\n",
    "    'eval_metric': 'auc',\n",
    "    'sample_type': 'weighted',\n",
    "    'verbosity': '0',\n",
    "}\n",
    "params_xgb_pipe = {\n",
    "    'xgb__max_depth': [5, 10],\n",
    "    'xgb__learning_rate': [0.05],\n",
    "    'xgb__n_estimators': [1000],\n",
    "    'xgb__seed': [42],\n",
    "    'xgb__nthread': [1, 2, 3, 4],\n",
    "    'xgb__objective': ['binary:logitraw', 'binary:logistic', 'binary:hinge'],\n",
    "    'xgb__eval_metric': ['auc', 'rmse', 'rmsle', 'logloss'],\n",
    "    'xgb__sample_type': ['weighted', 'uniform'],\n",
    "}\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc = MLPClassifier()\n",
    "params_mlpc = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpc_pipe = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = MLPRegressor()\n",
    "params_mlpr = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpr_pipe = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "## Stacking and Voting Classifiers\n",
    "estimators = [\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    (\n",
    "        'Logistic Regression',\n",
    "        LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    ),\n",
    "]\n",
    "\n",
    "### Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=estimators)\n",
    "params_voting_pipe = {\n",
    "    'VotingClassifier__estimators': [\n",
    "        ('dummy', dummy, params_dummy_freq),\n",
    "        ('dummy', dummy, params_dummy_stratified),\n",
    "        ('dummy', dummy, params_dummy_uniform),\n",
    "        ('nb', nb, params_nb),\n",
    "        ('bnb', bnb, params_bnb),\n",
    "        ('gnb', gnb, params_gnb),\n",
    "        ('knn', knn, params_knn),\n",
    "        ('lr', lr, params_lr),\n",
    "        ('pa', pa, params_pa),\n",
    "        ('sgd', sgd, params_sgd),\n",
    "        ('svm', svm, params_svm),\n",
    "        ('dt', dt, params_dt),\n",
    "        ('rf', rf, params_rf),\n",
    "        ('gbc', gbc, params_gbc),\n",
    "        ('ada', ada, params_ada),\n",
    "        ('xgb', xgb, params_xgb),\n",
    "        ('mlpc', mlpc, params_mlpc),\n",
    "        ('mlpr', mlpr, params_mlpr),\n",
    "    ],\n",
    "    'VotingClassifier__voting': ['hard', 'soft'],\n",
    "    'VotingClassifier__weights': [None, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]],\n",
    "}\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_classifier = StackingClassifier(estimators=estimators)\n",
    "# final_estimator = LogisticRegression(random_state=random_state, class_weight=class_weight)\n",
    "final_estimator = RandomForestClassifier(\n",
    "    random_state=42, class_weight={0: 1, 1: 2}\n",
    ")\n",
    "params_stacking_pipe = {\n",
    "    'StackingClassifier__estimator': [\n",
    "        ('dummy', dummy),\n",
    "        ('nb', nb),\n",
    "        ('bnb', bnb),\n",
    "        ('gnb', gnb),\n",
    "        ('knn', knn),\n",
    "        ('lr', lr),\n",
    "        ('pa', pa),\n",
    "        ('sgd', sgd),\n",
    "        ('svm', svm),\n",
    "        ('dt', dt),\n",
    "        ('rf', rf),\n",
    "        ('gbc', gbc),\n",
    "        ('ada', ada),\n",
    "        ('mlpc', mlpc),\n",
    "    ],\n",
    "    'StackingClassifier__cv': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'StackingClassifier__n_jobs': [-1],\n",
    "    'StackingClassifier__stack_method': ['predict_proba', 'decision_function'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "## Classifiers Pipe dict\n",
    "classifiers_pipe = {\n",
    "    'DummyClassifier': [dummy, params_dummy_pipe],\n",
    "    'MultinomialNB': [nb, params_nb_pipe],\n",
    "    'BernoulliNB': [bnb, params_bnb_pipe],\n",
    "    'GaussianNB': [gnb, params_gnb_pipe],\n",
    "    'KNeighborsClassifier': [knn, params_knn_pipe],\n",
    "    'LogisticRegression': [lr, params_lr_pipe],\n",
    "    'PassiveAggressiveClassifier': [pa, params_pa_pipe],\n",
    "    'SGDClassifier': [sgd, params_sgd_pipe],\n",
    "    'LinearSVC': [svm, params_svm_pipe],\n",
    "    'DecisionTreeClassifier': [dt, params_dt_pipe],\n",
    "    'RandomForestClassifier': [rf, params_rf_pipe],\n",
    "    'GradientBoostingClassifier': [gbc, params_gbc_pipe],\n",
    "    'AdaBoostClassifier': [ada, params_ada_pipe],\n",
    "    'XGBClassifier': [xgb, params_xgb_pipe],\n",
    "    'MLPClassifier': [mlpc, params_mlpc_pipe],\n",
    "    'MLPRegressor': [mlpr, params_mlpr_pipe],\n",
    "    'VotingClassifier': [voting_classifier, params_voting_pipe],\n",
    "    'StackingClassifier': [stacking_classifier, params_stacking_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # BOW Split\n",
    "    print('Splitting data into training and test sets.')\n",
    "    df_manual.dropna(subset=['Warmth', 'Competence', text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([x for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([x for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{data_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1276616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Using GridSearchCV')\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ STARTING PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 50)]) != 0):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate = split_data(df_manual, col, text_col)\n",
    "\n",
    "\n",
    "        # Vectorization\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "            print(f'Vectorizer: {vectorizer_name.upper()}')\n",
    "            print('~'*40)\n",
    "            \n",
    "            # Selection\n",
    "            for selector_name, selector_and_params in selectors_pipe.items():\n",
    "                selector = selector_and_params[0]\n",
    "                selector_params = selector_and_params[1]\n",
    "\n",
    "                # Classification\n",
    "                for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                    classifier = classifier_and_params[0]\n",
    "                    classifier_params = classifier_and_params[1]\n",
    "                    print(f'Classifier: {classifier_name.upper()}')\n",
    "                    print('~'*40)\n",
    "\n",
    "                    # Pipeline\n",
    "                    ## Steps\n",
    "                    steps = [\n",
    "                        (vectorizer_name, vectorizer),\n",
    "                        (selector_name, selector),\n",
    "                        (classifier_name, classifier)\n",
    "                    ]\n",
    "                    ## Params\n",
    "                    param_grid = {\n",
    "                        **vectorizer_params,\n",
    "                        **selector_params,\n",
    "                        **classifier_params,\n",
    "                    }\n",
    "                    ## Pipeline\n",
    "                    pipe = Pipeline(steps=steps)\n",
    "\n",
    "                    ## Vectorizers, selectors, classifiers\n",
    "                    vectorizer = pipe[:-2]\n",
    "                    selector = pipe[:-1]\n",
    "                    classifier = pipe[:]\n",
    "\n",
    "                    # Search\n",
    "                    search = GridSearchCV(\n",
    "                        estimator=pipe,\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=scores,\n",
    "                        cv=cv,\n",
    "                        refit=scores[0],\n",
    "                        return_train_score=True,\n",
    "                    )\n",
    "\n",
    "                    # Fit SearchCV\n",
    "                    searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "                    # Best Parameters\n",
    "                    best_index = searchcv.best_index_\n",
    "                    cv_results = sorted(searchcv.cv_results_)\n",
    "                    best_params = searchcv.best_params_\n",
    "                    classifier = searchcv.best_estimator_\n",
    "                    y_train_pred = classifier.predict(X_train)\n",
    "                    best_score = searchcv.best_score_\n",
    "                    n_splits = searchcv.n_splits_\n",
    "\n",
    "                    print('=' * 20)\n",
    "                    print(f'Best index for {scores[0]}: {best_index}')\n",
    "                    print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                    print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                    print(f'Best score for {scores[0]}: {best_score}')\n",
    "                    print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                    print('-' * 20)\n",
    "                    report = classification_report(y_train, y_train_pred)\n",
    "                    print(f'Classification Report:\\n{report}')\n",
    "                    ConfusionMatrixDisplay.from_estimator(\n",
    "                        searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                    )\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    print('=' * 20)\n",
    "\n",
    "                    # Make the predictions\n",
    "                    score = searchcv.score(X_test, y_test)\n",
    "                    y_test_pred = searchcv.predict(X_test)\n",
    "                    if hasattr(searchcv, 'predict_proba'):\n",
    "                        y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                    elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                        y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                    # Fit Best Model\n",
    "                    print(f'Fitting {classifier}.')\n",
    "                    classifier.set_params(**classifier.get_params())\n",
    "                    classifier = classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dd6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
