{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %%\n",
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### Install packages and import\n",
    "# %%\n",
    "# #################################### PLEASE INSTALL LATEST CHROME WEBDRIVER #####################################\n",
    "# Uncomment to run as required\n",
    "# #     --install-option=\"--chromedriver-version= *.**\" \\\n",
    "#   --install-option=\"--chromedriver-checksums=4fecc99b066cb1a346035bf022607104,058cd8b7b4b9688507701b5e648fd821\"\n",
    "# %%\n",
    "# ##### COPY THE LINES IN THIS COMMENT TO THE TOP OF NEW SCRIPTS #####\n",
    "# # Function to import this package to other files\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# main_dir = str(Path(code_dir).parents[0])\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "main_dir = str(Path(code_dir).parents[0])\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from setup_module.imports import *\n",
    "from setup_module.params import *\n",
    "from setup_module.scraping import *\n",
    "from setup_module.classification import *\n",
    "from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data balance\n",
    "df_jobs_labeled.info()\n",
    "df_jobs_labeled['Warmth'].value_counts()\n",
    "df_jobs_labeled['Competence'].value_counts()\n",
    "warm_comp_count = (\n",
    "    df_jobs_labeled[analysis_columns]\n",
    "    .reset_index()\n",
    "    .groupby(analysis_columns)\n",
    "    .count()\n",
    "    .sort_values(by='index')\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle('Training Dataset: Warmth and Competence Sentence Counts', fontsize=16.0)\n",
    "warm_comp_count.plot(kind='barh', stacked=True, legend=True, color='blue', ax=ax).grid(\n",
    "    axis='y'\n",
    ")\n",
    "if save_enabled == True:\n",
    "    fig.savefig(f'{plot_save_path}Warmth and Competence Sentence Counts.{image_save_format}', format=image_save_format, dpi=3000)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_jobs_labeled, col, text_col):\n",
    "    # BOW Split\n",
    "    print('Splitting data into training and test sets.')\n",
    "    df_jobs_labeled.dropna(subset=[col, text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_jobs_labeled, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = train[f'{str(text_col)}'].astype('str').values\n",
    "    prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = test[f'{str(text_col)}'].astype('str').values\n",
    "    prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = validate[f'{str(text_col)}'].astype('str').values\n",
    "    prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    prepared_text = vectorizer.fit_transform(prepared_X_train+prepared_X_test+prepared_X_validate)\n",
    "\n",
    "    return train, test, X_train, y_train, X_test, y_test, X_validate, y_validate, prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_poed(vectorizer, y_train, y_test, feature_names):\n",
    "\n",
    "    # Get words and offsets\n",
    "    train_words, train_offsets = train_offset(X_train, 'train')\n",
    "    test_words, test_offsets = train_offset(X_test, 'test')\n",
    "    validate_words, validate_offsets = train_offset(X_validate, 'validate')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        embs = load_glove_with_vocabulary(vocabulary_map, feature_names, print_enabled=print_enabled)\n",
    "        emb_model = BagOfEmbeddings(embs, dropout=0.1, hidden_dim=75, embedding_mode='mean')\n",
    "        print(f'Embedding Model: {emb_model}')\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(emb_model.parameters(), lr = 0.001)\n",
    "        torch.manual_seed(random_state)\n",
    "        losses = run_training(epochs=1, emb_model=emb_model, optimizer=optimizer, loss_fn=loss,\n",
    "                            all_words=train_words, all_offsets=train_offsets, all_targets=y_train,\n",
    "                            batch_size=32)\n",
    "        print(\"Training avg emb_model complete!\")\n",
    "\n",
    "        print(\"Evaluating test set\")\n",
    "        batch_losses, outputs = run_test(emb_model=emb_model, loss_fn=loss,\n",
    "                            all_words=test_words, all_offsets=test_offsets, all_targets=y_train,\n",
    "                            batch_size=256)\n",
    "\n",
    "        print(\"outputs.shape\", outputs.shape)\n",
    "\n",
    "        boe_pred = outputs.detach().numpy()\n",
    "\n",
    "        best_threshold_boe, best_score_boe = calculate_best_threshold(y_test[:300], boe_pred[:300], scoring, print_enabled)\n",
    "\n",
    "        print(\"boe_pred:\\n\", boe_pred[10])\n",
    "\n",
    "        print(\"Evaluating validate outputs\")\n",
    "        _, validate_outputs = run_test(emb_model=emb_model, loss_fn=None,\n",
    "                            all_words=validate_words, all_offsets=validate_offsets, all_targets=None,\n",
    "                            batch_size=256)\n",
    "\n",
    "        boe_validate_pred = validate_outputs.detach().numpy()\n",
    "        print(\"boe_validate_pred:\\n\", boe_validate_pred[10])\n",
    "        print(\"Validate outputs done\")\n",
    "\n",
    "    else:\n",
    "        vocabulary_map = None\n",
    "        boe_pred = None\n",
    "        boe_validate_pred = None\n",
    "        best_threshold_boe = None\n",
    "        best_score_boe = None\n",
    "\n",
    "    return vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_preds(best_threshold, y_final_validate_prob_pred, test):\n",
    "\n",
    "    # Save DF of predictions\n",
    "    labels = (y_final_validate_prob_pred > best_threshold).astype(int)\n",
    "    df_preds = pd.DataFrame({f'{str(text_col)}': test[f'{str(text_col)}'], \"prediction\": labels})\n",
    "    df_preds.to_csv(f'{df_dir}df_preds_{str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format_backup}', index=False)\n",
    "\n",
    "    return df_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(test, classifier, final_classifier, X_test, y_test, X_validate, y_validate, boe_pred, boe_validate_pred):\n",
    "\n",
    "    num = len(test) // 2\n",
    "\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        y_validate_prob_pred = classifier.predict_proba(X_validate)[:, 1]\n",
    "    elif hasattr(classifier, '_predict_proba_lr'):\n",
    "        y_test_prob_pred = classifier._predict_proba_lr(X_test)[:, 1]\n",
    "        y_validate_prob_pred = classifier._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "    best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "    if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "        X_final_augmented_train = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "            \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[:num],\n",
    "            \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "        y_final_augmented_train = y_test[:num]\n",
    "\n",
    "        X_final_augmented_test = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "            \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[num:],\n",
    "            \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "        y_final_augmented_test = y_test[num:]\n",
    "\n",
    "        final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "        if hasattr(final_classifier, 'predict_proba'):\n",
    "            y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "        elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "            y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "        best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "        X_final_augmented_validate = pd.DataFrame({\n",
    "            \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "            \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "            \"num_words\": validate[\"num_words\"].values,\n",
    "            \"num_chars\": validate[\"num_chars\"].values})\n",
    "        y_final_augmented_validate = y_validate\n",
    "        y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "        df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate)\n",
    "\n",
    "    elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "        final_classifier = classifier\n",
    "        X_final_augmented_validate = X_test\n",
    "        y_final_augmented_validate = y_test\n",
    "        y_final_validate_prob_pred = y_test_prob_pred\n",
    "        df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test)\n",
    "\n",
    "    return final_classifier, X_final_augmented_validate, y_final_augmented_validate, y_final_validate_prob_pred, df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring):\n",
    "\n",
    "    # Evaluate\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'EVALUATING FITTED MODEL - {vectorizer_name} + {classifier_name}: ',\n",
    "        has_fit_parameter(classifier, 'sample_weight'),\n",
    "    )\n",
    "    # 5 cross_validation score\n",
    "    print(f'Cross Validating - {vectorizer_name} + {classifier_name}.')\n",
    "    cross_validate_score = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Mean cross_validate scores - {vectorizer_name} + {classifier_name}: {cross_validate_score_noscoring.get(\"test_score\").mean()}'\n",
    "    )\n",
    "    numberoflabels = len(set((str(e) for e in y_test.to_list())))\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Mean Validation Score')\n",
    "    ] = float(cross_validate_score_noscoring.get('test_score').mean())\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Explained Variance')\n",
    "    ] = float(\n",
    "        cross_validate_score.get('test_explained_variance').mean()\n",
    "    )\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key, values in cross_validate_score.items():\n",
    "        if 'test' in key:\n",
    "            print(key, ' mean ', values.mean())\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "\n",
    "    # Predictions\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'============================ {str(col)} PREDICTIONS FOR {vectorizer_name.upper()} WITH {classifier_name.upper()} ============================'\n",
    "    )\n",
    "    print('\\n')\n",
    "    print(f'y_test_pred - {str(col)} - {vectorizer_name} + {classifier_name}:')\n",
    "    dic_y_mapping = {n: label for n, label in enumerate(np.unique(y_train))}\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    predicted = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred]\n",
    "    acc_roc_f1 = evaluate_print(classifier_name + '   |   ', y_test, y_test_pred)\n",
    "    cm, precision, recall, accuracy, f1, mcc, best_threshold, best_score, report = evaluation(\n",
    "        y_test, y_test_pred, scoring, print_enabled\n",
    "    )\n",
    "\n",
    "    true_negative = cm[0][0]\n",
    "    false_positives = cm[0][1]\n",
    "    false_negatives = cm[1][0]\n",
    "    true_positives = cm[1][1]\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_jobs_labeled,\n",
    "        print_enabled,\n",
    "    )\n",
    "\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = float(best_threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Classification Report')\n",
    "    ] = report\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "\n",
    "    # Plot\n",
    "    heatmap = plot_confusion_matrix_percentage(col, cm, classifier_name, vectorizer_name)\n",
    "    plt.show()\n",
    "    if save_enabled == True:\n",
    "        heatmap.figure.savefig(\n",
    "            f'{plot_save_path}Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "            )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # Log Loss Cross Entropy\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        probability_of_1 = y_test_prob_pred#[:, 1]\n",
    "\n",
    "        loss = log_loss(y_test, y_test_prob_pred)\n",
    "        print('\\n')\n",
    "        print('=' * 20)\n",
    "        print(f'Log Loss / Cross Entropy = {loss}')\n",
    "        print('=' * 20)\n",
    "        print('\\n')\n",
    "        table_df.loc[\n",
    "            (classifier_name),\n",
    "            (col, vectorizer_name, 'Log Loss/Cross Entropy'),\n",
    "        ] = float(loss)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    # Optimization\n",
    "    if optimization_enabled == True and hasattr(classifier, 'predict_log_proba'):\n",
    "        classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt = optimize_model(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            probability_of_1,\n",
    "            vectorizer_name,\n",
    "            classifier_name,\n",
    "            table_df,\n",
    "            score)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred_new, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        print(f'Precision Recall Curve AFTER OPTIMIZATION - {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        get_pr_curve(X_test, y_test, recall_opt, precision_opt, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    if hasattr(classifier, 'best_estimator_'):\n",
    "        ohe_cols = list(\n",
    "            classifier.best_estimator_.named_steps['vectorizer']\n",
    "            .named_transformers_['cat']\n",
    "            .named_steps['ohe']\n",
    "            .get_feature_names(input_features=categorical)\n",
    "        )\n",
    "        num_feats = list(numerical)\n",
    "        num_feats.extend(ohe_cols)\n",
    "        feat_imp = eli5.explain_weights_df(\n",
    "            classifier.best_estimator_.named_steps['classifier'],\n",
    "            top=10,\n",
    "            feature_names=num_feats,\n",
    "        )\n",
    "        print(\n",
    "            f'feat_imp - {str(col)} - {vectorizer_name} + {classifier_name}: ',\n",
    "            feat_imp,\n",
    "        )\n",
    "        print('-' * 20)\n",
    "        print('\\n')\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_jobs_labeled,\n",
    "        print_enabled,\n",
    "    )\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    return classifier, table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "def get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col):\n",
    "    roc_curve = metrics.plot_roc_curve(classifier, X_test, y_test)\n",
    "    plt.title(\n",
    "        f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    if save_enabled == True:\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC {str(col)} - {classifier_name} - {vectorizer_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, probability_of_1, pos_label=1\n",
    "    )\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(f'AUC {str(col)} - {vectorizer_name} + {classifier_name}:\\n', auc)\n",
    "\n",
    "    print('ROC CURVE FOR PREDICTED PROBABILITIES')\n",
    "    bc = BinaryClassification(y_test, y_test_pred, labels=['0', '1'])\n",
    "    # Figures\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    bc.plot_roc_curve()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    return table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "def get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col):\n",
    "\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    pr_curve = plt.figure(figsize=(4.0, 4.0))\n",
    "    plt.plot(\n",
    "        [0, 1], [no_skill, no_skill], linestyle='--', label='No Skill'\n",
    "    )\n",
    "    plt.plot(\n",
    "        recall, precision, marker='.', label=f'AUC = {auc}'\n",
    "    )\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\n",
    "        f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=12.0,\n",
    "    )\n",
    "    plt.ylabel('Precision', fontsize=12.0)\n",
    "    plt.xlabel('Recall', fontsize=12.0)\n",
    "    plt.show()\n",
    "    if save_enabled == True:\n",
    "        pr_curve.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Model\n",
    "def optimize_model(classifier, X_test, y_test, probability_of_1, vectorizer_name, classifier_name, table_df, scoring):\n",
    "    if hasattr(classifier, 'predict_log_proba'):\n",
    "\n",
    "        y_test_prob_log_pred = classifier.predict_log_proba(X_test)[:, 1]\n",
    "\n",
    "        # calculate pr-curve\n",
    "        (\n",
    "            precision_opt,\n",
    "            recall_opt,\n",
    "            thresholds_opt,\n",
    "        ) = metrics.precision_recall_curve(\n",
    "            y_test, probability_of_1\n",
    "        )\n",
    "        # convert to f score\n",
    "        fscore_opt = (2 * precision_opt * recall_opt) / (\n",
    "            precision_opt + recall_opt\n",
    "        )\n",
    "        # locate the index of the largest f score\n",
    "        ix_opt = argmax(fscore_opt)\n",
    "        best_thresh_opt = thresholds_opt[ix_opt]\n",
    "        print('=' * 20)\n",
    "        print(\n",
    "            f'Best Threshold: {best_thresh_opt}, F-Score={fscore_opt[ix_opt]}'\n",
    "        )\n",
    "        print(f'Optimal threshold: {np.exp(best_thresh_opt)}')\n",
    "        y_test_pred_new = np.where(\n",
    "            y_test_prob_log_pred[:, 1] > best_thresh_opt, 1, 0\n",
    "        )\n",
    "        print(f'New y_test_pred {str(col)} - {vectorizer_name} + {classifier_name}:\\n{y_test_pred_new}')\n",
    "\n",
    "        print(\n",
    "            f'SCORES FOR {str(col)} - {vectorizer_name} + {classifier_name} AFTER OPTIMIZATION:'\n",
    "        )\n",
    "        cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, best_threshold_opt, best_score_opt, report_opt = evaluation(\n",
    "            y_test, y_test_pred_new, scoring, print_enabled\n",
    "        )\n",
    "\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Accuracy_opt')\n",
    "        ] = float(accuracy_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Precision_opt')\n",
    "        ] = float(precision_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Recall_opt')\n",
    "        ] = float(recall_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'F1-score_opt')\n",
    "        ] = float(f1_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient_opt'),\n",
    "        ] = float(mcc_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold_opt'),\n",
    "        ] = float(best_threshold_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score_opt'),\n",
    "        ] = float(best_score_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Classification Report_opt'),\n",
    "        ] = report_opt\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Confusion Matrix_opt'),\n",
    "        ] = str(cm_opt)\n",
    "\n",
    "        print('=' * 20)\n",
    "\n",
    "    elif hasattr(classifier, 'predict_log_proba'):\n",
    "        print('Classifier has no Attribute predict_log_proba.')\n",
    "\n",
    "    return classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name, save_enabled = True, task_enabled = False):\n",
    "    if save_enabled == True:\n",
    "        if task_enabled == False:\n",
    "            classifier_save_path = (\n",
    "                f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "            )\n",
    "            vectorizer_save_path = (\n",
    "                f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "            )\n",
    "            if select_best_enabled == True:\n",
    "                selector_save_path = (\n",
    "                    f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "                )\n",
    "\n",
    "        elif task_enabled == True:\n",
    "            classifier_save_path = (\n",
    "                f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "            )\n",
    "            vectorizer_save_path = (\n",
    "                f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "            )\n",
    "            if select_best_enabled == True:\n",
    "                selector_save_path = (\n",
    "                    f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "                )\n",
    "\n",
    "        # Save classifier\n",
    "        print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "        table_df.to_csv(table_save_path + csv_file_name)\n",
    "        table_df.to_pickle(table_save_path + pickle_file_name)\n",
    "        table_df.to_excel(table_save_path + excel_file_name)\n",
    "        table_df.to_latex(table_save_path + latex_file_name, index=True, longtable=True, escape=False, multicolumn=True, multicolumn_format='c')\n",
    "        table_df.to_markdown(table_save_path + markdown_file_name)\n",
    "\n",
    "        with open(classifier_save_path, 'wb') as f:\n",
    "            joblib.dump(classifier, f)\n",
    "        with open(vectorizer_save_path, 'wb') as f:\n",
    "            joblib.dump(vectorizer, f)\n",
    "        if select_best_enabled == True:\n",
    "            with open(selector_save_path, 'wb') as f:\n",
    "                joblib.dump(selector, f)\n",
    "\n",
    "    elif save_enabled == False:\n",
    "        print('Saving Model and Table is disabled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Supervised Model: Warmth and Competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ STARTING PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (\n",
    "        len(\n",
    "            df_jobs_labeled[\n",
    "                df_jobs_labeled[str(col)].map(\n",
    "                    df_jobs_labeled[str(col)].value_counts() > 50\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        != 0\n",
    "    ):\n",
    "\n",
    "        print('Using Search')\n",
    "        print('Using dict for classifiers and vectorizers.')\n",
    "        # Vectorization\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "            # BOW Split\n",
    "            train, test, X_train, y_train, X_test, y_test, X_validate, y_validate, prepared_text = split_data(df_jobs_labeled, col, text_col)\n",
    "\n",
    "            # Selection\n",
    "            for selector_name, selector_and_params in selectors_pipe.items():\n",
    "                selector = selector_and_params[0]\n",
    "                selector_params = selector_and_params[1]\n",
    "\n",
    "                # Classification\n",
    "                for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                    classifier = classifier_and_params[0]\n",
    "                    classifier_params = classifier_and_params[1]\n",
    "\n",
    "                    # Pipeline\n",
    "                    if select_best_enabled == False:\n",
    "                        ## Params\n",
    "                        param_grid = {\n",
    "                            **vectorizer_params,\n",
    "                            **classifier_params,\n",
    "                        }\n",
    "                        steps = [(vectorizer_name, vectorizer), (classifier_name, classifier)]\n",
    "\n",
    "                        ## Pipeline\n",
    "                        pipe = Pipeline(steps=steps)\n",
    "\n",
    "                        ## Vectorizers, selectors, classifiers\n",
    "                        vectorizer = pipe[:-1]\n",
    "                        classifier = pipe[:]\n",
    "\n",
    "                    elif select_best_enabled == True:\n",
    "                        ## Params\n",
    "                        param_grid = {\n",
    "                            **vectorizer_params,\n",
    "                            **selector_params,\n",
    "                            **classifier_params,\n",
    "                        }\n",
    "                        steps = [(vectorizer_name, vectorizer), (selector_name, selector), (classifier_name, classifier)]\n",
    "\n",
    "                        ## Pipeline\n",
    "                        pipe = Pipeline(steps=steps)\n",
    "\n",
    "                        ## Vectorizers, selectors, classifiers\n",
    "                        vectorizer = pipe[:-2]\n",
    "                        selector = pipe[:-1]\n",
    "                        classifier = pipe[:]\n",
    "\n",
    "                    # Search\n",
    "                    search = RandomizedSearchCV(\n",
    "                        estimator=pipe,\n",
    "                        param_distributions=param_grid,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=scores,\n",
    "                        cv=cv,\n",
    "                        refit=scores[0],\n",
    "                        return_train_score=True,\n",
    "                        verbose=3,\n",
    "                    )\n",
    "\n",
    "                    # Fit SearchCV\n",
    "                    searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "                    # Best Parameters\n",
    "                    best_index = searchcv.best_index_\n",
    "                    cv_results = sorted(searchcv.cv_results_)\n",
    "                    best_params = searchcv.best_params_\n",
    "                    classifier = searchcv.best_estimator_\n",
    "                    y_train_pred = classifier.predict(X_train)\n",
    "                    best_score = searchcv.best_score_\n",
    "                    n_splits = searchcv.n_splits_\n",
    "\n",
    "                    print('=' * 20)\n",
    "                    print(f'Best index for {scores[0]}: {best_index}')\n",
    "                    print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                    print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                    print(f'Best score for {scores[0]}: {best_score}')\n",
    "                    print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                    print('-' * 20)\n",
    "                    report = classification_report(y_train, y_train_pred)\n",
    "                    print(f'Classification Report:\\n{report}')\n",
    "                    ConfusionMatrixDisplay.from_estimator(\n",
    "                        searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                    )\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    print('=' * 20)\n",
    "\n",
    "                    # Make the predictions\n",
    "                    score = searchcv.score(X_test, y_test)\n",
    "                    y_test_pred = searchcv.predict(X_test)\n",
    "                    if hasattr(searchcv, 'predict_proba'):\n",
    "                        y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                    elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                        y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                    # Fit Best Model\n",
    "                    print(f'Fitting {classifier}.')\n",
    "                    classifier.set_params(**classifier.get_params())\n",
    "                    final_classifier = classifier\n",
    "                    classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "                    # Evaluate Model\n",
    "                    classifier, table_df = evaluate_model(X_train, X_test, y_train, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "                    classifier, table_df = evaluate_model(X_train, X_test, y_train, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "                    # Save Vectorizer, Selector, and Classifier\n",
    "                    if save_enabled == True:\n",
    "                        saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ FINISHED PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('study1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5b0d7544f82776c2b902af54887e7cde1aa7d2da4fd982551ffc3948bf7522f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
