{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9bfda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %%\n",
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### Install packages and import\n",
    "# %%\n",
    "# #################################### PLEASE INSTALL LATEST CHROME WEBDRIVER #####################################\n",
    "# Uncomment to run as required\n",
    "# #     --install-option=\"--chromedriver-version= *.**\" \\\n",
    "#   --install-option=\"--chromedriver-checksums=4fecc99b066cb1a346035bf022607104,058cd8b7b4b9688507701b5e648fd821\"\n",
    "# %%\n",
    "# ##### COPY THE LINES IN THIS COMMENT TO THE TOP OF NEW SCRIPTS #####\n",
    "# # Function to import this package to other files\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# main_dir = str(Path(code_dir).parents[0])\n",
    "# scraped_data = f'{code_dir}/scraped_data'\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "main_dir = str(Path(code_dir).parents[0])\n",
    "scraped_data = f'{code_dir}/scraped_data'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.post_collection_processing import *\n",
    "# from setup_module.classification import *\n",
    "from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89195e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0c5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dbe4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# # Set up Spacy\n",
    "# import spacy\n",
    "# from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "# from spacy.matcher import Matcher\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Set up NLK\n",
    "# import nltk\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "# from nltk.tag import pos_tag, pos_tag_sents\n",
    "# from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "# nltk_path = f'{llm_path}/nltk'\n",
    "# nltk.data.path.append(nltk_path)\n",
    "\n",
    "# nltk.download('words', download_dir = nltk_path)\n",
    "# nltk.download('stopwords', download_dir = nltk_path)\n",
    "# nltk.download('punkt', download_dir = nltk_path)\n",
    "# nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# punctuations = list(string.punctuation)\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Set up Gensim\n",
    "# from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "# from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "# from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "# from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# # Set up Bert\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "# bert_model_name = 'bert-base-uncased'\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "# bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b863326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{data_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4cc7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5687 entries, 0 to 5686\n",
      "Columns: 219 entries, % Sector per Workforce to Job Description gensim_123grams_sent2vec_embeddings\n",
      "dtypes: float64(35), int64(4), object(180)\n",
      "memory usage: 9.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.000    4276\n",
       "1.000    1411\n",
       "Name: Warmth, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.000    3199\n",
       "1.000    2488\n",
       "Name: Competence, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Training Dataset: Warmth and Competence Sentence Counts')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data balance\n",
    "analysis_columns = [\n",
    "    'Warmth',\n",
    "    'Competence'\n",
    "]\n",
    "df_manual.info()\n",
    "df_manual['Warmth'].value_counts()\n",
    "df_manual['Competence'].value_counts()\n",
    "warm_comp_count = (\n",
    "    df_manual[analysis_columns]\n",
    "    .reset_index()\n",
    "    .groupby(analysis_columns)\n",
    "    .count()\n",
    "    .sort_values(by='index')\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle('Training Dataset: Warmth and Competence Sentence Counts', fontsize=16.0)\n",
    "warm_comp_count.plot(kind='barh', stacked=True, legend=True, color='blue', ax=ax).grid(\n",
    "    axis='y'\n",
    ")\n",
    "\n",
    "fig.savefig(f'{data_dir}/plots/Warmth and Competence Sentence Counts.eps', format='eps', dpi=3000, bbox_inches='tight')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cabffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "    if external_glove is False and embeddings_index is None:\n",
    "        embeddings_index= get_glove()\n",
    "\n",
    "    if extra_preprocessing_enabled is False:\n",
    "        words = sentences\n",
    "\n",
    "    elif extra_preprocessing_enabled is True:\n",
    "        stop_words = set(sw.words('english'))\n",
    "        words = str(sentences).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "    M = []\n",
    "\n",
    "    try:\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis='index')\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(300)\n",
    "\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "366c0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col):\n",
    "    # BOW Split\n",
    "    print('Splitting data into training and test sets.')\n",
    "    df_manual.dropna(subset=['Warmth', 'Competence', text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([sent2vec(x) for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([sent2vec(x) for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1af0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding with Fasttext\n",
    "def word_embedding(df_jobs, X_train, feature_names):\n",
    "\n",
    "    sentences = df_jobs['2grams_gensim']\n",
    "\n",
    "    fasttext_model = FastText(sentences, window=3, min_count=1, sorted_vocab=1)\n",
    "\n",
    "    row=0\n",
    "    errors=0\n",
    "    for sent in tqdm.tqdm(sentences):\n",
    "        sent_vec = np.zeros(100)\n",
    "        weight_sum =0\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # weight = fasttext_model.wv.get_vector(word)\n",
    "                # weight_sum += weight\n",
    "                # sent_vec += weight\n",
    "                vec = fasttext_model.wv[word]\n",
    "                feat = X_train[row, feature_names.index(word)]\n",
    "                sent_vec += (vec * feat)\n",
    "                weight_sum += feat\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "        sent_vec /= weight_sum\n",
    "                # print(np.isnan(np.sum(sent_vec)))\n",
    "    sent_vectors = [sent_vec]\n",
    "    row += 1\n",
    "    print(f'errors noted: {str(errors)}')\n",
    "\n",
    "    return fasttext_model, sent_vectors\n",
    "\n",
    "def emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names):\n",
    "\n",
    "    # Get words and offsets\n",
    "    train_words, train_offsets = train_offset(X_train, 'train')\n",
    "    test_words, test_offsets = train_offset(X_test, 'test')\n",
    "    validate_words, validate_offsets = train_offset(X_validate, 'validate')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        if plots_enabled:\n",
    "            sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "\n",
    "        embs = load_glove_with_vocabulary(vocabulary_map, feature_names, print_enabled=print_enabled)\n",
    "        emb_model = BagOfEmbeddings(embs, dropout=0.1, hidden_dim=75, embedding_mode='mean')\n",
    "        print(f'Embedding Model: {emb_model}')\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(emb_model.parameters(), lr = 0.001)\n",
    "        torch.manual_seed(random_state)\n",
    "        losses = run_training(epochs=1, emb_model=emb_model, optimizer=optimizer, loss_fn=loss,\n",
    "                            all_words=train_words, all_offsets=train_offsets, all_targets=y_train,\n",
    "                            batch_size=32)\n",
    "        print(\"Training avg emb_model complete!\")\n",
    "\n",
    "        print(\"Evaluating test set\")\n",
    "        batch_losses, outputs = run_test(emb_model=emb_model, loss_fn=loss,\n",
    "                            all_words=test_words, all_offsets=test_offsets, all_targets=y_train,\n",
    "                            batch_size=256)\n",
    "\n",
    "        print(\"outputs.shape\", outputs.shape)\n",
    "\n",
    "        boe_pred = outputs.detach().numpy()\n",
    "\n",
    "        best_threshold_boe, best_score_boe = calculate_best_threshold(y_test[:300], boe_pred[:300], scoring, print_enabled)\n",
    "\n",
    "        print(\"boe_pred:\\n\", boe_pred[10])\n",
    "\n",
    "        print(\"Evaluating validate outputs\")\n",
    "        _, validate_outputs = run_test(emb_model=emb_model, loss_fn=None,\n",
    "                            all_words=validate_words, all_offsets=validate_offsets, all_targets=None,\n",
    "                            batch_size=256)\n",
    "\n",
    "        boe_validate_pred = validate_outputs.detach().numpy()\n",
    "        print(\"boe_validate_pred:\\n\", boe_validate_pred[10])\n",
    "        print(\"Validate outputs done\")\n",
    "\n",
    "    else:\n",
    "        vocabulary_map = None\n",
    "        boe_pred = None\n",
    "        boe_validate_pred = None\n",
    "        best_threshold_boe = None\n",
    "        best_score_boe = None\n",
    "\n",
    "    return vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df0beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and target arrays\n",
    "def vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate):\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} passed ============================'\n",
    "    )\n",
    "\n",
    "    refit_vectorizer = vectorizer\n",
    "\n",
    "    # prepared_X_train = X_train.to_list()\n",
    "    # prepared_y_train = y_train.to_list()\n",
    "    # prepared_X_test = X_test.to_list()\n",
    "    # prepared_y_test = y_test.to_list()\n",
    "    # prepared_X_validate = X_validate.to_list()\n",
    "    # prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    prepared_X_train, prepared_y_train, prepared_X_test, prepared_y_test, prepared_X_validate, prepared_y_validate = list(map(lambda x: x.to_list(), [X_train, y_train, X_test, y_test, X_validate, y_validate]))\n",
    "\n",
    "    prepared_text = vectorizer.fit_transform(prepared_X_train+prepared_X_test+prepared_X_validate)\n",
    "\n",
    "    # BOW fit transform\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "    # Oversampling to fix imbalance\n",
    "    if (resampling_enabled is True) and (col == 'Warmth'):\n",
    "        X_train, y_train = resample_data(X_train, y_train, col, resampling_enabled, resampling_method)\n",
    "\n",
    "    # Get feature names\n",
    "    X_train, vectorizer, dtf_features, X_names, feature_names = get_feature_name_and_refit_X_train_on_chi_test(train, X_train, vectorizer, refit_vectorizer)\n",
    "    unique_features = set(feature_names)\n",
    "\n",
    "    # BOW fit\n",
    "    print('Fitting and transforming data.')\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_validate = vectorizer.transform(X_validate)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_test = selector.transform(X_test)\n",
    "        X_validate = selector.transform(X_validate)\n",
    "        # Get feature names\n",
    "        feature_names = selector.get_feature_names_out(vocabulary=X_names)\n",
    "        unique_features = set(feature_names)\n",
    "\n",
    "    # y to numpy array\n",
    "    y_train = torch.from_numpy(np.array(y_train)).float()\n",
    "    y_test = torch.from_numpy(np.array(y_test)).float()\n",
    "    if print_enabled:\n",
    "        print(f'Train targets: {y_train}')\n",
    "        print(f'Test targets: {y_test}')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        if plots_enabled:\n",
    "            sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "\n",
    "    # vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe = emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names)\n",
    "\n",
    "    # fasttext_model, sent_vectors = word_embedding(df_jobs, X_train, feature_names)\n",
    "\n",
    "    return vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e0b8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name, save_enabled=save_enabled):\n",
    "\n",
    "    # Save DF of predictions\n",
    "    labels = (y_final_validate_prob_pred > best_threshold).astype(int)\n",
    "    df_preds = pd.DataFrame({f'{str(text_col)}': test[f'{str(text_col)}'], \"prediction\": labels})\n",
    "    if save_enabled:\n",
    "        df_preds.to_csv(f'{df_dir}df_preds_{str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format_backup}', index=False)\n",
    "\n",
    "    return df_preds\n",
    "\n",
    "\n",
    "def augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled):\n",
    "\n",
    "    num = len(test) // 2\n",
    "\n",
    "    best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "    if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "        num = len(boe_pred) // 2\n",
    "\n",
    "        X_final_augmented_train = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "            \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[:num],\n",
    "            \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "        y_final_augmented_train = y_test[:num]\n",
    "\n",
    "        X_final_augmented_test = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "            \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[num:],\n",
    "            \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "        y_final_augmented_test = y_test[num:]\n",
    "\n",
    "        final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "        if hasattr(final_classifier, 'predict_proba'):\n",
    "            y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "        elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "            y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "        best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "        X_final_augmented_validate = pd.DataFrame({\n",
    "            \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "            \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "            \"num_words\": validate[\"num_words\"].values,\n",
    "            \"num_chars\": validate[\"num_chars\"].values})\n",
    "        y_final_augmented_validate = y_validate\n",
    "\n",
    "        y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "        df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "        final_classifier = classifier\n",
    "        X_final_augmented_validate = X_test\n",
    "        y_final_augmented_validate = y_test\n",
    "        y_final_validate_prob_pred = y_test_prob_pred\n",
    "        df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    return final_classifier, X_final_augmented_validate, y_final_augmented_validate, y_final_validate_prob_pred, best_threshold, best_score, df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a69ae3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classifier\n",
    "def classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name):\n",
    "    print('\\n')\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    "    )\n",
    "    num = len(test) // 2\n",
    "\n",
    "    # BOW model\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        X_train = X_train.todense()\n",
    "        X_test = X_test.todense()\n",
    "        X_validate = X_validate.todense()\n",
    "\n",
    "    if classifier_name == 'Sequential':\n",
    "        classifier.compile(loss='categorical_crossentropy')\n",
    "    if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "        classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "    # final_classifier = classifier\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    classifier = SelectFromModel(estimator=classifier, prefit=True).fit(X_train, y_train)\n",
    "\n",
    "    ### NEW ### https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "    # summarize feature importance\n",
    "    importance = classifier.coef_\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    # plot feature importance\n",
    "    pyplot.bar([x for x in range(len(importance))], importance)\n",
    "    pyplot.show()\n",
    "\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "        y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "    elif hasattr(classifier, '_predict_proba_lr'):\n",
    "        y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "        y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "    else:\n",
    "        raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "    # final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n",
    "\n",
    "    return classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c2107c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring):\n",
    "\n",
    "    # Evaluate\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'EVALUATING FITTED MODEL - {vectorizer_name} + {classifier_name}: ',\n",
    "        has_fit_parameter(classifier, 'sample_weight'),\n",
    "    )\n",
    "    # 5 cross_validation score\n",
    "    print(f'Cross Validating - {vectorizer_name} + {classifier_name}.')\n",
    "    cross_validate_score = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Mean cross_validate scores - {vectorizer_name} + {classifier_name}: {cross_validate_score_noscoring.get(\"test_score\").mean()}'\n",
    "    )\n",
    "    numberoflabels = len(set((str(e) for e in y_test.to_list())))\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Mean Validation Score')\n",
    "    ] = float(cross_validate_score_noscoring.get('test_score').mean())\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Explained Variance')\n",
    "    ] = float(\n",
    "        cross_validate_score.get('test_explained_variance').mean()\n",
    "    )\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key, values in cross_validate_score.items():\n",
    "        if 'test' in key:\n",
    "            print(key, ' mean ', values.mean())\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "\n",
    "    # Predictions\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'============================ {str(col)} PREDICTIONS FOR {vectorizer_name.upper()} WITH {classifier_name.upper()} ============================'\n",
    "    )\n",
    "    print('\\n')\n",
    "    print(f'y_test_pred - {str(col)} - {vectorizer_name} + {classifier_name}:')\n",
    "    dic_y_mapping = dict(enumerate(np.unique(y_train)))\n",
    "    inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "    y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        # y_test_pred = y_test_pred.to_list()\n",
    "        y_test_pred = classifier.predict(X_test.todense())\n",
    "    predicted = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred]\n",
    "    acc_roc_f1 = evaluate_print(classifier_name + '   |   ', y_test, y_test_pred)\n",
    "    cm, precision, recall, accuracy, f1, mcc, best_threshold, best_score, report = evaluation(\n",
    "        y_test, y_test_pred, scoring, print_enabled\n",
    "    )\n",
    "\n",
    "    true_negative = cm[0][0]\n",
    "    false_positives = cm[0][1]\n",
    "    false_negatives = cm[1][0]\n",
    "    true_positives = cm[1][1]\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = float(best_threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Classification Report')\n",
    "    ] = report\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "\n",
    "    # Plot\n",
    "    heatmap = plot_confusion_matrix_percentage(col, cm, classifier_name, vectorizer_name)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        heatmap.figure.savefig(\n",
    "            f'{plot_save_path}Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "            )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # Log Loss Cross Entropy\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        probability_of_1 = y_test_prob_pred#[:, 1]\n",
    "\n",
    "        loss = log_loss(y_test, y_test_prob_pred)\n",
    "        print('\\n')\n",
    "        print('=' * 20)\n",
    "        print(f'Log Loss / Cross Entropy = {loss}')\n",
    "        print('=' * 20)\n",
    "        print('\\n')\n",
    "        table_df.loc[\n",
    "            (classifier_name),\n",
    "            (col, vectorizer_name, 'Log Loss/Cross Entropy'),\n",
    "        ] = float(loss)\n",
    "\n",
    "        # Explain Model\n",
    "        explained = explain_model(test, y_test, y_test_pred, y_test_prob_pred, y_train)\n",
    "        if plots_enabled:\n",
    "            explained.show_in_notebook(text=txt_instance, predict_proba=False)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    # Optimization\n",
    "    if optimization_enabled is True and hasattr(classifier, 'predict_log_proba'):\n",
    "        classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt = optimize_model(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            probability_of_1,\n",
    "            vectorizer_name,\n",
    "            classifier_name,\n",
    "            table_df,\n",
    "            score)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred_new, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        print(f'Precision Recall Curve AFTER OPTIMIZATION - {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        get_pr_curve(X_test, y_test, recall_opt, precision_opt, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    if hasattr(classifier, 'best_estimator_'):\n",
    "        ohe_cols = list(\n",
    "            classifier.best_estimator_.named_steps['vectorizer']\n",
    "            .named_transformers_['cat']\n",
    "            .named_steps['ohe']\n",
    "            .get_feature_names(input_features=categorical)\n",
    "        )\n",
    "        num_feats = list(numerical)\n",
    "        num_feats.extend(ohe_cols)\n",
    "        feat_imp = eli5.explain_weights_df(\n",
    "            classifier.best_estimator_.named_steps['classifier'],\n",
    "            top=10,\n",
    "            feature_names=num_feats,\n",
    "        )\n",
    "        print(\n",
    "            f'feat_imp - {str(col)} - {vectorizer_name} + {classifier_name}: ',\n",
    "            feat_imp,\n",
    "        )\n",
    "        print('-' * 20)\n",
    "        print('\\n')\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    return classifier, table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11cc66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "def get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col):\n",
    "    roc_curve = metrics.plot_roc_curve(classifier, X_test, y_test)\n",
    "    plt.title(\n",
    "        f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    if save_enabled is True:\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC {str(col)} - {classifier_name} - {vectorizer_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, probability_of_1, pos_label=1\n",
    "    )\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(f'AUC {str(col)} - {vectorizer_name} + {classifier_name}:\\n', auc)\n",
    "\n",
    "    print('ROC CURVE FOR PREDICTED PROBABILITIES')\n",
    "    bc = BinaryClassification(y_test, y_test_pred, labels=['0', '1'])\n",
    "    # Figures\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    bc.plot_roc_curve()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                            predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3,\n",
    "                label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                metrics.auc(fpr, tpr))\n",
    "                )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "            xlabel='False Positive Rate',\n",
    "            ylabel=\"True Positive Rate (Recall)\",\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    return table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b08c3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "def get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col):\n",
    "\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    pr_curve = plt.figure(figsize=(4.0, 4.0))\n",
    "    plt.plot(\n",
    "        [0, 1], [no_skill, no_skill], linestyle='--', label='No Skill'\n",
    "    )\n",
    "    plt.plot(\n",
    "        recall, precision, marker='.', label=f'AUC = {auc}'\n",
    "    )\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\n",
    "        f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=12.0,\n",
    "    )\n",
    "    plt.ylabel('Precision', fontsize=12.0)\n",
    "    plt.xlabel('Recall', fontsize=12.0)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        pr_curve.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "            y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3,\n",
    "                label=f'{classes[i]} (area={metrics.auc(recall, precision):0.2f})')\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "            ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a59d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Model\n",
    "def optimize_model(classifier, X_test, y_test, probability_of_1, vectorizer_name, classifier_name, table_df, scoring):\n",
    "    if hasattr(classifier, 'predict_log_proba'):\n",
    "\n",
    "        y_test_prob_log_pred = classifier.predict_log_proba(X_test)[:, 1]\n",
    "\n",
    "        # calculate pr-curve\n",
    "        (\n",
    "            precision_opt,\n",
    "            recall_opt,\n",
    "            thresholds_opt,\n",
    "        ) = metrics.precision_recall_curve(\n",
    "            y_test, probability_of_1\n",
    "        )\n",
    "        # convert to f score\n",
    "        fscore_opt = (2 * precision_opt * recall_opt) / (\n",
    "            precision_opt + recall_opt\n",
    "        )\n",
    "        # locate the index of the largest f score\n",
    "        ix_opt = argmax(fscore_opt)\n",
    "        best_thresh_opt = thresholds_opt[ix_opt]\n",
    "        print('=' * 20)\n",
    "        print(\n",
    "            f'Best Threshold: {best_thresh_opt}, F-Score={fscore_opt[ix_opt]}'\n",
    "        )\n",
    "        print(f'Optimal threshold: {np.exp(best_thresh_opt)}')\n",
    "        y_test_pred_new = np.where(\n",
    "            y_test_prob_log_pred[:, 1] > best_thresh_opt, 1, 0\n",
    "        )\n",
    "        print(f'New y_test_pred {str(col)} - {vectorizer_name} + {classifier_name}:\\n{y_test_pred_new}')\n",
    "\n",
    "        print(\n",
    "            f'SCORES FOR {str(col)} - {vectorizer_name} + {classifier_name} AFTER OPTIMIZATION:'\n",
    "        )\n",
    "        cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, best_threshold_opt, best_score_opt, report_opt = evaluation(\n",
    "            y_test, y_test_pred_new, scoring, print_enabled\n",
    "        )\n",
    "\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Accuracy_opt')\n",
    "        ] = float(accuracy_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Precision_opt')\n",
    "        ] = float(precision_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Recall_opt')\n",
    "        ] = float(recall_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'F1-score_opt')\n",
    "        ] = float(f1_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient_opt'),\n",
    "        ] = float(mcc_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold_opt'),\n",
    "        ] = float(best_threshold_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score_opt'),\n",
    "        ] = float(best_score_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Classification Report_opt'),\n",
    "        ] = report_opt\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Confusion Matrix_opt'),\n",
    "        ] = str(cm_opt)\n",
    "\n",
    "        print('=' * 20)\n",
    "\n",
    "    elif not hasattr(classifier, 'predict_log_proba'):\n",
    "        print('Classifier has no Attribute predict_log_proba.')\n",
    "\n",
    "    return classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0b9401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df, data_dir=data_dir):\n",
    "#     if save_enabled is True:\n",
    "#         if task_enabled is False:\n",
    "\n",
    "    models_save_path = f'{data_dir}classification models/'\n",
    "    table_save_path = f'{data_dir}output tables/'\n",
    "    pickle_file_name = 'Classifiers Table.pkl'\n",
    "    csv_file_name = 'Classifiers Table.csv'\n",
    "    excel_file_name = 'Classifiers Table.xlsx'\n",
    "    latex_file_name = 'Classifiers Table.tex'\n",
    "    markdown_file_name = 'Classifiers Table.md'\n",
    "\n",
    "    classifier_save_path = (\n",
    "        f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    vectorizer_save_path = (\n",
    "        f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    selector_save_path = (\n",
    "        f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    table_df.to_csv(table_save_path + csv_file_name)\n",
    "    table_df.to_pickle(table_save_path + pickle_file_name)\n",
    "    table_df.to_excel(table_save_path + excel_file_name)\n",
    "    table_df.style.to_latex(table_save_path + latex_file_name)\n",
    "    table_df.to_markdown(table_save_path + markdown_file_name)\n",
    "\n",
    "    with open(classifier_save_path, 'wb') as f:\n",
    "        joblib.dump(classifier, f)\n",
    "    with open(vectorizer_save_path, 'wb') as f:\n",
    "        joblib.dump(vectorizer, f)\n",
    "    if select_best_enabled is True:\n",
    "        with open(selector_save_path, 'wb') as f:\n",
    "            joblib.dump(selector, f)\n",
    "\n",
    "    elif save_enabled is False:\n",
    "        print('Saving Model and Table is disabled.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3936a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring):\n",
    "\n",
    "    if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "        classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "    classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name)\n",
    "    classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "    return classifier, table_df, X_test, y_test, y_test_prob_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b13febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name):\n",
    "\n",
    "    print('Not using Search')\n",
    "\n",
    "    if use_dict_for_classifiers_vectorizers is True:\n",
    "        print('Using dict for classifiers and vectorizers.')\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "            vectorizer.set_params(**vectorizer_params)\n",
    "            vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "            for classifier_name, classifier_and_params in classifiers.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "                classifier.set_params(**classifier_params)\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    elif use_dict_for_classifiers_vectorizers is False:\n",
    "        print('Using list for classifiers and vectorizers.')\n",
    "        for vectorizer in vectorizers_lst:\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "            for classifier in classifiers_lst:\n",
    "                classifier_name = classifier.__class__.__name__\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    return df_manual, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cf4591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring):\n",
    "\n",
    "    print('Using Search')\n",
    "\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "\n",
    "    models_save_path = f'{data_dir}classification models/'\n",
    "    table_save_path = f'{data_dir}output tables/'\n",
    "    pickle_file_name = 'Classifiers Table.pkl'\n",
    "    csv_file_name = 'Classifiers Table.csv'\n",
    "    excel_file_name = 'Classifiers Table.xlsx'\n",
    "    latex_file_name = 'Classifiers Table.tex'\n",
    "    markdown_file_name = 'Classifiers Table.md'\n",
    "\n",
    "    # Vectorization\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        # Selection\n",
    "        for selector_name, selector_and_params in selectors_pipe.items():\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            # Classification\n",
    "            for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "\n",
    "                # Pipeline\n",
    "#                 if select_best_enabled is True:\n",
    "                ## Steps\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "                ## Params\n",
    "                param_grid = {\n",
    "                    **vectorizer_params,\n",
    "                    **selector_params,\n",
    "                    **classifier_params,\n",
    "                }\n",
    "                ## Pipeline\n",
    "                pipe = Pipeline(steps=steps)\n",
    "\n",
    "                ## Vectorizers, selectors, classifiers\n",
    "                vectorizer = pipe[:-2]\n",
    "                selector = pipe[:-1]\n",
    "                classifier = pipe[:]\n",
    "\n",
    "#                 elif select_best_enabled is False:\n",
    "#                     ## Steps\n",
    "#                     steps = [\n",
    "#                         (vectorizer_name, vectorizer),\n",
    "#                         (classifier_name, classifier)\n",
    "#                     ]\n",
    "#                     ## Params\n",
    "#                     param_grid = {\n",
    "#                         **vectorizer_params,\n",
    "#                         **classifier_params,\n",
    "#                     }\n",
    "#                     ## Pipeline\n",
    "#                     pipe = Pipeline(steps=steps)\n",
    "\n",
    "#                     ## Vectorizers, selectors, classifiers\n",
    "#                     vectorizer = pipe[:-1]\n",
    "#                     classifier = pipe[:]\n",
    "\n",
    "                # Search\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_distributions=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scores,\n",
    "                    cv=cv,\n",
    "                    refit=scores[0],\n",
    "                    return_train_score=True,\n",
    "                    verbose=3,\n",
    "                )\n",
    "\n",
    "                # Fit SearchCV\n",
    "                searchcv = search.fit(X_train, y_train, error_score='raise')\n",
    "\n",
    "                # Best Parameters\n",
    "                best_index = searchcv.best_index_\n",
    "                cv_results = sorted(searchcv.cv_results_)\n",
    "                best_params = searchcv.best_params_\n",
    "                classifier = searchcv.best_estimator_\n",
    "                y_train_pred = classifier.predict(X_train)\n",
    "                best_score = searchcv.best_score_\n",
    "                n_splits = searchcv.n_splits_\n",
    "\n",
    "                print('=' * 20)\n",
    "                print(f'Best index for {scores[0]}: {best_index}')\n",
    "                print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                print(f'Best score for {scores[0]}: {best_score}')\n",
    "                print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                print('-' * 20)\n",
    "                report = classification_report(y_train, y_train_pred)\n",
    "                print(f'Classification Report:\\n{report}')\n",
    "                ConfusionMatrixDisplay.from_estimator(\n",
    "                    searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                )\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                print('=' * 20)\n",
    "\n",
    "                # Make the predictions\n",
    "                score = searchcv.score(X_test, y_test)\n",
    "                y_test_pred = searchcv.predict(X_test)\n",
    "                if hasattr(searchcv, 'predict_proba'):\n",
    "                    y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                    y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                # Fit Best Model\n",
    "                print(f'Fitting {classifier}.')\n",
    "                classifier.set_params(**classifier.get_params())\n",
    "                classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate Model\n",
    "                classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df)\n",
    "\n",
    "    return df_manual, searchcv, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2f86d",
   "metadata": {},
   "source": [
    "# Training Supervised Model: Warmth and Competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16d94250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "\n",
      "\n",
      "============================ STARTING PROCESSING WARMTH ============================\n",
      "\n",
      "\n",
      "--------------------\n",
      "Splitting data into training and test sets.\n",
      "Using Search\n",
      "Using dict for classifiers and vectorizers.\n",
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x114ef51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1285211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10f190040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10f6d11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10f190160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10f6d1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x117b491b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1192c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10e5a0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10eb751b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10e5a0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10eb75120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10bb44040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10c2c11b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10bb44160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10c2c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1278211b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x11754c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x117b49120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x11754c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x128018040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x128521120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x128018160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x1149b4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_regression at 0x114ef5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1149b4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1149b4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1149b4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1149b4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x127821120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1270e0040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x1270e0160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1270e0280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1270e0280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1270e0280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function f_regression at 0x1270e0280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1192c1120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function chi2 at 0x118c7c160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=constant, SelectKBest__k=1000, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x118c7c040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                     | 0/2 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 400, in fit\n    fit_params_steps = self._check_fit_params(**fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 323, in _check_fit_params\n    raise ValueError(\nValueError: Pipeline.fit does not accept the error_score parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 21\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m      9\u001b[0m         df_manual[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# BOW Split\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate \u001b[38;5;241m=\u001b[39m split_data(df_manual, col, text_col)\n\u001b[0;32m---> 21\u001b[0m     df_manual, searchcv, classifier, vectorizers, selector, table_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_manual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[52], line 83\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring)\u001b[0m\n\u001b[1;32m     71\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     72\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[1;32m     73\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Fit SearchCV\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m searchcv \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Best Parameters\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_index \u001b[38;5;241m=\u001b[39m searchcv\u001b[38;5;241m.\u001b[39mbest_index_\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 400, in fit\n    fit_params_steps = self._check_fit_params(**fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 323, in _check_fit_params\n    raise ValueError(\nValueError: Pipeline.fit does not accept the error_score parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ STARTING PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (\n",
    "        len(\n",
    "            df_manual[\n",
    "                df_manual[str(col)].map(\n",
    "                    df_manual[str(col)].value_counts() > 50\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        != 0\n",
    "    ):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate = split_data(df_manual, col, text_col)\n",
    "\n",
    "        df_manual, searchcv, classifier, vectorizers, selector, table_df = pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ FINISHED PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not using Search')\n",
    "\n",
    "if use_dict_for_classifiers_vectorizers is True:\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "        vectorizer.set_params(**vectorizer_params)\n",
    "        vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "        for classifier_name, classifier_and_params in classifiers.items():\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "            classifier.set_params(**classifier_params)\n",
    "            # classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94000ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "    classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "# classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name)\n",
    "# classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print(\n",
    "    f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    ")\n",
    "num = len(test) // 2\n",
    "\n",
    "# BOW model\n",
    "if classifier_name == 'GaussianNB':\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "    X_validate = X_validate.todense()\n",
    "\n",
    "if classifier_name == 'Sequential':\n",
    "    classifier.compile(loss='categorical_crossentropy')\n",
    "if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "    classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "final_classifier = classifier\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "if hasattr(classifier, 'predict_proba'):\n",
    "    y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "    y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "elif hasattr(classifier, '_predict_proba_lr'):\n",
    "    y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "    y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "else:\n",
    "    raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "# final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(test) // 2\n",
    "\n",
    "best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "    num = len(boe_pred) // 2\n",
    "\n",
    "    X_final_augmented_train = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "        \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[:num],\n",
    "        \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "    y_final_augmented_train = y_test[:num]\n",
    "\n",
    "    X_final_augmented_test = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "        \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[num:],\n",
    "        \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "    y_final_augmented_test = y_test[num:]\n",
    "\n",
    "    final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "    if hasattr(final_classifier, 'predict_proba'):\n",
    "        y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "    elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "        y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "    best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "    X_final_augmented_validate = pd.DataFrame({\n",
    "        \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "        \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "        \"num_words\": validate[\"num_words\"].values,\n",
    "        \"num_chars\": validate[\"num_chars\"].values})\n",
    "    y_final_augmented_validate = y_validate\n",
    "\n",
    "    y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "    df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "    final_classifier = classifier\n",
    "    X_final_augmented_validate = X_test\n",
    "    y_final_augmented_validate = y_test\n",
    "    y_final_validate_prob_pred = y_test_prob_pred\n",
    "    df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:num].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ffb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:num]\n",
    "y_test_pred = y_test_prob_pred[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = -1\n",
    "best_score = -1\n",
    "for threshold in np.arange(0.01, 0.801, 0.01):\n",
    "    threshold = np.round(threshold, 2)\n",
    "\n",
    "    # globals()[f'metrics.{scoring.lower()}_score']\n",
    "    if scoring.lower() == 'recall':\n",
    "        scorer = metrics.recall_score\n",
    "    elif scoring.lower() == 'f1 score':\n",
    "        scorer = metrics.f1_score\n",
    "    else:\n",
    "        raise ValueError(f'{scoring.title()} is not a valid score')\n",
    "\n",
    "    emb_model_score = scorer(y_true=y_test, y_pred=(y_test_pred > threshold).astype(int))\n",
    "    if emb_model_score > best_score:\n",
    "        best_score = emb_model_score\n",
    "        best_threshold = threshold\n",
    "    if print_enabled:\n",
    "        print(f'{scoring.title()} at threshold {threshold}: {emb_model_score}')\n",
    "print(f'{scoring.title()} at best threshold {best_threshold}: {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd65a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5b0d7544f82776c2b902af54887e7cde1aa7d2da4fd982551ffc3948bf7522f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
