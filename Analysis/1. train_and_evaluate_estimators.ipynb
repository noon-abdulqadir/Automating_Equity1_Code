{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5545a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay, accuracy_score,\n",
    "                             balanced_accuracy_score, brier_score_loss,\n",
    "                             classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7735d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split ratios\n",
    "n_jobs = 1\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.10\n",
    "validation_ratio = 0.15\n",
    "test_split = test_size = 1 - train_ratio\n",
    "validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "# Cross-validation\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "partition = True\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "# Resampling\n",
    "class_weight = 'balanced'\n",
    "resampling_enabled = True\n",
    "resample_enn = SMOTEENN(\n",
    "    enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "resample_tome = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "# Undersampling\n",
    "rus = RandomUnderSampler(random_state=random_state, replacement=True)\n",
    "tl = RandomOverSampler(sampling_strategy='majority')\n",
    "nm = NearMiss()\n",
    "# Oversampling\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "smote = SMOTE()\n",
    "# Sampling Used\n",
    "resampling_method = resample_tome\n",
    "\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_sizes = [300, 100]\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "metrics_list = [\n",
    "    'Mean Validation Score',\n",
    "    'Explained Variance',\n",
    "    'Accuracy',\n",
    "    'Precision',\n",
    "    'Recall',\n",
    "    'F1-score',\n",
    "    'ROC',\n",
    "    'AUC',\n",
    "    'Matthews Correlation Coefficient',\n",
    "    f'{scoring.title()} Best Threshold',\n",
    "    f'{scoring.title()} Best Score',\n",
    "    'Log Loss/Cross Entropy',\n",
    "    'Classification Report',\n",
    "    'Confusion Matrix',\n",
    "    'Accuracy_opt',\n",
    "    'Precision_opt',\n",
    "    'Recall_opt',\n",
    "    'F1-score_opt',\n",
    "    'Matthews Correlation Coefficient_opt',\n",
    "    'Classification Report_opt',\n",
    "    'Confusion Matrix_opt',\n",
    "]\n",
    "\n",
    "# Paths\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "pickle_file_name = 'Classifiers Table.pkl'\n",
    "csv_file_name = 'Classifiers Table.csv'\n",
    "excel_file_name = 'Classifiers Table.xlsx'\n",
    "latex_file_name = 'Classifiers Table.tex'\n",
    "markdown_file_name = 'Classifiers Table.md'\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df119b8",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count = CountVectorizer()\n",
    "params_count_pipe = {\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.85, 0.8, 0.75],\n",
    "    'CountVectorizer__min_df': [0.15, 0.2, 0.25],\n",
    "}\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "params_tfidf_pipe = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "    'TfidfVectorizer___use_idf': [True],\n",
    "    'TfidfVectorizer__max_df': [0.85, 0.8, 0.75],\n",
    "    'TfidfVectorizer__min_df': [0.15, 0.2, 0.25],\n",
    "}\n",
    "\n",
    "### BOW FeatureUnion\n",
    "bow = FeatureUnion(\n",
    "    transformer_list=[('CountVectorizer', count), ('TfidfVectorizer', tfidf)]\n",
    ")\n",
    "params_bow_pipe = {**params_count_pipe, **params_tfidf_pipe}\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    'CountVectorizer': [count, params_count_pipe],\n",
    "    'TfidfVectorizer': [tfidf, params_tfidf_pipe],\n",
    "    'UnionBOW': [bow, params_bow_pipe],\n",
    "    # \"UnionWordEmbedding\": [em, params_em_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaff38f",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectors\n",
    "selector = SelectKBest(score_func=chi2, k='all')\n",
    "selector_name = selector.__class__.__name__\n",
    "\n",
    "# model_selector = SelectFromModel()\n",
    "# model_selector_name = model_selector.__class__.__name__\n",
    "\n",
    "### SelectKBest\n",
    "selectkbest = SelectKBest()\n",
    "params_selectkbest_pipe = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "\n",
    "### SelectPercentile\n",
    "selectpercentile = SelectPercentile()\n",
    "params_selectpercentile_pipe = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr = SelectFpr()\n",
    "params_selectfpr_pipe = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr = SelectFdr()\n",
    "params_selectfdr_pipe = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe = SelectFwe()\n",
    "params_selectfwe_pipe = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    'SelectKBest': [selectkbest, params_selectkbest_pipe],\n",
    "    'SelectPercentile': [selectpercentile, params_selectpercentile_pipe],\n",
    "    'SelectFpr': [selectfpr, params_selectfpr_pipe],\n",
    "    'SelectFdr': [selectfdr, params_selectfdr_pipe],\n",
    "    'SelectFwe': [selectfwe, params_selectfwe_pipe],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6d49",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2222fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy = DummyClassifier()\n",
    "params_dummy_freq = {'strategy': 'most_frequent', 'random_state': random_state}\n",
    "params_dummy_stratified = {'strategy': 'stratified', 'random_state': random_state}\n",
    "params_dummy_uniform = {'strategy': 'uniform', 'random_state': random_state}\n",
    "params_dummy_pipe = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "        'constant',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "params_nb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "params_nb_pipe = {\n",
    "    'MultinomialNB__fit_prior': [True],\n",
    "    'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "params_bnb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "params_bnb_pipe = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "params_gnb = {'var_smoothing': 1e-9}\n",
    "params_gnb_pipe = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {\n",
    "    'n_neighbors': 3,\n",
    "    'weights': 'uniform',\n",
    "    'algorithm': 'auto',\n",
    "    'leaf_size': 30,\n",
    "    'p': 2,\n",
    "    'metric': 'minkowski',\n",
    "    'metric_params': None,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_knn_pipe = {\n",
    "    'KNeighborsClassifier__weights': ['uniform'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    'KNeighborsClassifier__metric': [\n",
    "        'minkowski',\n",
    "        'euclidean',\n",
    "        'cosine',\n",
    "        'correlation',\n",
    "    ],\n",
    "    'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "### Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "params_lr = {\n",
    "    'penalty': 'l2',\n",
    "    'dual': False,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'solver': 'liblinear',\n",
    "    'max_iter': 100,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_lr_pipe = {\n",
    "    'LogisticRegression__penalty': ['l2'],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LogisticRegression__multi_class': ['ovr', 'multinomial'],\n",
    "    'LogisticRegression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "}\n",
    "\n",
    "### Passive Aggressive\n",
    "pa = PassiveAggressiveClassifier()\n",
    "params_pa = {\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_pa_pipe = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': [None, 'balanced'],\n",
    "    'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd = SGDClassifier()\n",
    "params_sgd = {\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_sgd_pipe = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': [None, 'balanced'],\n",
    "    'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### SVM\n",
    "svm = LinearSVC()\n",
    "params_svm = {\n",
    "    'penalty': 'l2',\n",
    "    'loss': 'hinge',\n",
    "    'dual': True,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'max_iter': 1000,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "}\n",
    "params_svm_pipe = {\n",
    "    'LinearSVC__penalty': ['l2'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': [None, 'balanced'],\n",
    "    'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "}\n",
    "\n",
    "### Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "params_dt = {\n",
    "    'criterion': 'gini',\n",
    "    'splitter': 'best',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': random_state,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "}\n",
    "params_dt_pipe = {\n",
    "    'DecisionTreeClassifier__max_depth': [5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "params_rf = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_rf_pipe = {\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__n_jobs': [-1],\n",
    "    'RandomForestClassifier__max_depth': [5, 10],\n",
    "    'RandomForestClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "### Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "params_et = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_et_pipe = {\n",
    "    'ExtraTreesClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__n_jobs': [-1],\n",
    "    'ExtraTreesClassifier__max_depth': [5, 10],\n",
    "    'ExtraTreesClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'ExtraTreesClassifier__random_state': [random_state],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc = GradientBoostingClassifier()\n",
    "params_gbc = {\n",
    "    'loss': 'deviance',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_depth': 3,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'init': None,\n",
    "    'random_state': random_state,\n",
    "    'max_features': None,\n",
    "    'verbose': 0,\n",
    "    'max_leaf_nodes': None,\n",
    "    'warm_start': False,\n",
    "}\n",
    "params_gbc_pipe = {\n",
    "    'GradientBoostingClassifier__max_depth': [5, 10],\n",
    "    'GradientBoostingClassifier__criterion': ['gini', 'entropy'],\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__n_estimators': [10, 20],\n",
    "    'GradientBoostingClassifier__loss': ['deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__subsample': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'GradientBoostingClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### AdaBoost\n",
    "ada = AdaBoostClassifier()\n",
    "params_ada = {\n",
    "    'base_estimator': None,\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 1.0,\n",
    "    'algorithm': 'SAMME.R',\n",
    "    'random_state': random_state,\n",
    "}\n",
    "params_ada_pipe = {\n",
    "    'AdaBoostClassifier__max_depth': [5, 10],\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(probability=True, kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "### XGBoost\n",
    "xgb = XGBClassifier()\n",
    "params_xgb = {\n",
    "    'nthread':4, #when use hyperthread, xgboost may become slower\n",
    "    'objective':'binary:logistic',\n",
    "    'learning_rate': 0.05, #so called `eta` value\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 11,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'n_estimators': 1000, #number of trees, change it to 1000 for better results\n",
    "    'missing':-999,\n",
    "    'seed': 1337,\n",
    "    'eval_metric': 'auc',\n",
    "    'sample_type': 'weighted',\n",
    "    'verbosity': '0',\n",
    "}\n",
    "params_xgb_pipe = {\n",
    "    'xgb__max_depth': [5, 10],\n",
    "    'xgb__learning_rate': [0.05],\n",
    "    'xgb__n_estimators': [1000],\n",
    "    'xgb__seed': [42],\n",
    "    'xgb__nthread': [1, 2, 3, 4],\n",
    "    'xgb__objective': ['binary:logitraw', 'binary:logistic', 'binary:hinge'],\n",
    "    'xgb__eval_metric': ['auc', 'rmse', 'rmsle', 'logloss'],\n",
    "    'xgb__sample_type': ['weighted', 'uniform'],\n",
    "}\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc = MLPClassifier()\n",
    "params_mlpc = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpc_pipe = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = MLPRegressor()\n",
    "params_mlpr = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpr_pipe = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "## Stacking and Voting Classifiers\n",
    "estimators = [\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    (\n",
    "        'Logistic Regression',\n",
    "        LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    ),\n",
    "]\n",
    "\n",
    "### Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=estimators)\n",
    "params_voting_pipe = {\n",
    "    'VotingClassifier__estimators': [\n",
    "        ('dummy', dummy, params_dummy_freq),\n",
    "        ('dummy', dummy, params_dummy_stratified),\n",
    "        ('dummy', dummy, params_dummy_uniform),\n",
    "        ('nb', nb, params_nb),\n",
    "        ('bnb', bnb, params_bnb),\n",
    "        ('gnb', gnb, params_gnb),\n",
    "        ('knn', knn, params_knn),\n",
    "        ('lr', lr, params_lr),\n",
    "        ('pa', pa, params_pa),\n",
    "        ('sgd', sgd, params_sgd),\n",
    "        ('svm', svm, params_svm),\n",
    "        ('dt', dt, params_dt),\n",
    "        ('rf', rf, params_rf),\n",
    "        ('gbc', gbc, params_gbc),\n",
    "        ('ada', ada, params_ada),\n",
    "        ('xgb', xgb, params_xgb),\n",
    "        ('mlpc', mlpc, params_mlpc),\n",
    "        ('mlpr', mlpr, params_mlpr),\n",
    "    ],\n",
    "    'VotingClassifier__voting': ['hard', 'soft'],\n",
    "    'VotingClassifier__weights': [None, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]],\n",
    "}\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_classifier = StackingClassifier(estimators=estimators)\n",
    "# final_estimator = LogisticRegression(random_state=random_state, class_weight=class_weight)\n",
    "final_estimator = RandomForestClassifier(\n",
    "    random_state=42, class_weight={0: 1, 1: 2}\n",
    ")\n",
    "params_stacking_pipe = {\n",
    "    'StackingClassifier__estimator': [\n",
    "        ('dummy', dummy),\n",
    "        ('nb', nb),\n",
    "        ('bnb', bnb),\n",
    "        ('gnb', gnb),\n",
    "        ('knn', knn),\n",
    "        ('lr', lr),\n",
    "        ('pa', pa),\n",
    "        ('sgd', sgd),\n",
    "        ('svm', svm),\n",
    "        ('dt', dt),\n",
    "        ('rf', rf),\n",
    "        ('gbc', gbc),\n",
    "        ('ada', ada),\n",
    "        ('mlpc', mlpc),\n",
    "    ],\n",
    "    'StackingClassifier__cv': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'StackingClassifier__n_jobs': [-1],\n",
    "    'StackingClassifier__stack_method': ['predict_proba', 'decision_function'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "## Classifiers Pipe dict\n",
    "classifiers_pipe = {\n",
    "    'DummyClassifier': [dummy, params_dummy_pipe],\n",
    "    'MultinomialNB': [nb, params_nb_pipe],\n",
    "#     'BernoulliNB': [bnb, params_bnb_pipe],\n",
    "#     'GaussianNB': [gnb, params_gnb_pipe],\n",
    "#     'KNeighborsClassifier': [knn, params_knn_pipe],\n",
    "#     'LogisticRegression': [lr, params_lr_pipe],\n",
    "#     'PassiveAggressiveClassifier': [pa, params_pa_pipe],\n",
    "#     'SGDClassifier': [sgd, params_sgd_pipe],\n",
    "#     'LinearSVC': [svm, params_svm_pipe],\n",
    "#     'DecisionTreeClassifier': [dt, params_dt_pipe],\n",
    "#     'RandomForestClassifier': [rf, params_rf_pipe],\n",
    "#     'GradientBoostingClassifier': [gbc, params_gbc_pipe],\n",
    "#     'AdaBoostClassifier': [ada, params_ada_pipe],\n",
    "#     'XGBClassifier': [xgb, params_xgb_pipe],\n",
    "#     'MLPClassifier': [mlpc, params_mlpc_pipe],\n",
    "#     'MLPRegressor': [mlpr, params_mlpr_pipe],\n",
    "#     'VotingClassifier': [voting_classifier, params_voting_pipe],\n",
    "#     'StackingClassifier': [stacking_classifier, params_stacking_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table df\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "    names=['Classifiers'],\n",
    ")\n",
    "columns = pd.MultiIndex.from_product(\n",
    "    [\n",
    "        analysis_columns,\n",
    "        list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "        metrics_list,\n",
    "    ],\n",
    "    names=['Variable', 'Vectorizer', 'Measures'],\n",
    ")\n",
    "table_df = pd.DataFrame(index=index, columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # BOW Split\n",
    "    print('Splitting data into training, testing, and validation sets.')\n",
    "    df_manual.dropna(subset=['Warmth', 'Competence', text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([x for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([x for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets')\n",
    "    print('-'*10)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print(f'Validation set shape: {y_validate.shape}')\n",
    "    print('-'*10)\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f280accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring):\n",
    "\n",
    "    # Evaluate\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'EVALUATING FITTED MODEL - {vectorizer_name} + {classifier_name}: ',\n",
    "        has_fit_parameter(classifier, 'sample_weight'),\n",
    "    )\n",
    "    # 5 cross_validation score\n",
    "    print(f'Cross Validating - {vectorizer_name} + {classifier_name}.')\n",
    "    cross_validate_score = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Mean cross_validate scores - {vectorizer_name} + {classifier_name}: {cross_validate_score_noscoring.get(\"test_score\").mean()}'\n",
    "    )\n",
    "    numberoflabels = len(set((str(e) for e in y_test)))\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Mean Validation Score')\n",
    "    ] = float(cross_validate_score_noscoring.get('test_score').mean())\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Explained Variance')\n",
    "    ] = float(\n",
    "        cross_validate_score.get('test_explained_variance').mean()\n",
    "    )\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key, values in cross_validate_score.items():\n",
    "        if 'test' in key:\n",
    "            print(key, ' mean ', values.mean())\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "\n",
    "    # Predictions\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'============================ {str(col)} PREDICTIONS FOR {vectorizer_name.upper()} WITH {classifier_name.upper()} ============================'\n",
    "    )\n",
    "    print('\\n')\n",
    "    print(f'y_test_pred - {str(col)} - {vectorizer_name} + {classifier_name}:')\n",
    "    dic_y_mapping = dict(enumerate(np.unique(y_train)))\n",
    "    inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "    y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        # y_test_pred = y_test_pred.to_list()\n",
    "        y_test_pred = classifier.predict(X_test.todense())\n",
    "    predicted = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred]\n",
    "    acc_roc_f1 = evaluate_print(classifier_name + '   |   ', y_test, y_test_pred)\n",
    "    cm, precision, recall, accuracy, f1, mcc, best_threshold, best_score, report = evaluation(\n",
    "        y_test, y_test_pred, scoring, print_enabled\n",
    "    )\n",
    "\n",
    "    true_negative = cm[0][0]\n",
    "    false_positives = cm[0][1]\n",
    "    false_negatives = cm[1][0]\n",
    "    true_positives = cm[1][1]\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = float(best_threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Classification Report')\n",
    "    ] = report\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "\n",
    "    # Plot\n",
    "    heatmap = plot_confusion_matrix_percentage(col, cm, classifier_name, vectorizer_name)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        heatmap.figure.savefig(\n",
    "            f'{plot_save_path}Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "            )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # Log Loss Cross Entropy\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        probability_of_1 = y_test_prob_pred#[:, 1]\n",
    "\n",
    "        loss = log_loss(y_test, y_test_prob_pred)\n",
    "        print('\\n')\n",
    "        print('=' * 20)\n",
    "        print(f'Log Loss / Cross Entropy = {loss}')\n",
    "        print('=' * 20)\n",
    "        print('\\n')\n",
    "        table_df.loc[\n",
    "            (classifier_name),\n",
    "            (col, vectorizer_name, 'Log Loss/Cross Entropy'),\n",
    "        ] = float(loss)\n",
    "\n",
    "        # Explain Model\n",
    "        explained = explain_model(test, y_test, y_test_pred, y_test_prob_pred, y_train)\n",
    "        if plots_enabled:\n",
    "            explained.show_in_notebook(text=txt_instance, predict_proba=False)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    # Optimization\n",
    "    if optimization_enabled is True and hasattr(classifier, 'predict_log_proba'):\n",
    "        classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt = optimize_model(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            probability_of_1,\n",
    "            vectorizer_name,\n",
    "            classifier_name,\n",
    "            table_df,\n",
    "            score)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred_new, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        print(f'Precision Recall Curve AFTER OPTIMIZATION - {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        get_pr_curve(X_test, y_test, recall_opt, precision_opt, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    if hasattr(classifier, 'best_estimator_'):\n",
    "        ohe_cols = list(\n",
    "            classifier.best_estimator_.named_steps['vectorizer']\n",
    "            .named_transformers_['cat']\n",
    "            .named_steps['ohe']\n",
    "            .get_feature_names(input_features=categorical)\n",
    "        )\n",
    "        num_feats = list(numerical)\n",
    "        num_feats.extend(ohe_cols)\n",
    "        feat_imp = eli5.explain_weights_df(\n",
    "            classifier.best_estimator_.named_steps['classifier'],\n",
    "            top=10,\n",
    "            feature_names=num_feats,\n",
    "        )\n",
    "        print(\n",
    "            f'feat_imp - {str(col)} - {vectorizer_name} + {classifier_name}: ',\n",
    "            feat_imp,\n",
    "        )\n",
    "        print('-' * 20)\n",
    "        print('\\n')\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    return classifier, table_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df, data_dir=data_dir):\n",
    "#     if save_enabled is True:\n",
    "#         if task_enabled is False:\n",
    "\n",
    "    classifier_save_path = (\n",
    "        f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    vectorizer_save_path = (\n",
    "        f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    selector_save_path = (\n",
    "        f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    table_df.to_csv(table_save_path + csv_file_name)\n",
    "    table_df.to_pickle(table_save_path + pickle_file_name)\n",
    "    table_df.to_excel(table_save_path + excel_file_name)\n",
    "    table_df.to_latex(table_save_path + latex_file_name)\n",
    "    table_df.to_markdown(table_save_path + markdown_file_name)\n",
    "\n",
    "    with open(classifier_save_path, 'wb') as f:\n",
    "        joblib.dump(classifier, f)\n",
    "    with open(vectorizer_save_path, 'wb') as f:\n",
    "        joblib.dump(vectorizer, f)\n",
    "    if select_best_enabled is True:\n",
    "        with open(selector_save_path, 'wb') as f:\n",
    "            joblib.dump(selector, f)\n",
    "\n",
    "    elif save_enabled is False:\n",
    "        print('Saving Model and Table is disabled.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{data_dir}df_manual_for_trainning.pkl').reset_index(drop=True)[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GridSearchCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "\n",
      "\n",
      "============================ TRAINING WARMTH ============================\n",
      "\n",
      "\n",
      "--------------------\n",
      "Splitting data into training, testing, and validation sets.\n",
      "Done splitting data into training, testing, and validation sets\n",
      "Training set shape: (150,)\n",
      "Testing set shape: (20,)\n",
      "Validation set shape: (30,)\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Vectorizer: COUNTVECTORIZER\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Selector: SELECTKBEST\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Classifier: DUMMYCLASSIFIER\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "GridSearchCV with:\n",
      "Pipe: Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()),\n",
      "                ('DummyClassifier', DummyClassifier())])\n",
      "Params: {'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.85, 0.8, 0.75], 'CountVectorizer__min_df': [0.15, 0.2, 0.25], 'SelectKBest__score_func': [<function f_classif at 0x145c63f40>, <function chi2 at 0x145c800d0>, <function mutual_info_classif at 0x1465d00d0>, <function f_regression at 0x145c801f0>, <function mutual_info_regression at 0x1465d0040>], 'SelectKBest__k': ['all'], 'DummyClassifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform', 'constant'], 'DummyClassifier__random_state': [42]}\n",
      "++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "print('Using GridSearchCV')\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ TRAINING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate = split_data(df_manual, col, text_col)\n",
    "\n",
    "\n",
    "        print('~'*40)\n",
    "        # Vectorization\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "            print(f'Vectorizer: {vectorizer_name.upper()}')\n",
    "            print('~'*40)\n",
    "            \n",
    "            # Selection\n",
    "            for selector_name, selector_and_params in selectors_pipe.items():\n",
    "                selector = selector_and_params[0]\n",
    "                selector_params = selector_and_params[1]\n",
    "                print(f'Selector: {selector_name.upper()}')\n",
    "                print('~'*40)\n",
    "\n",
    "                # Classification\n",
    "                for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                    classifier = classifier_and_params[0]\n",
    "                    classifier_params = classifier_and_params[1]\n",
    "                    print(f'Classifier: {classifier_name.upper()}')\n",
    "                    print('~'*40)\n",
    "\n",
    "                    # Pipeline\n",
    "                    ## Steps\n",
    "                    steps = [\n",
    "                        (vectorizer_name, vectorizer),\n",
    "                        (selector_name, selector),\n",
    "                        (classifier_name, classifier)\n",
    "                    ]\n",
    "                    ## Params\n",
    "                    param_grid = {\n",
    "                        **vectorizer_params,\n",
    "                        **selector_params,\n",
    "                        **classifier_params,\n",
    "                    }\n",
    "                    ## Pipeline\n",
    "                    pipe = Pipeline(steps=steps)\n",
    "\n",
    "                    ## Vectorizers, selectors, classifiers\n",
    "                    vectorizer = pipe[:-2]\n",
    "                    selector = pipe[:-1]\n",
    "                    classifier = pipe[:]\n",
    "\n",
    "                    # Search\n",
    "                    print(f'GridSearchCV with:\\nPipe: {pipe}\\nParams: {param_grid}')\n",
    "                    print('+'*30)\n",
    "                    search = GridSearchCV(\n",
    "                        estimator=pipe,\n",
    "                        param_grid=param_grid,\n",
    "                        n_jobs=-1,\n",
    "                        scoring=scores,\n",
    "                        cv=cv,\n",
    "                        refit=scores[0],\n",
    "                        return_train_score=True,\n",
    "                    )\n",
    "\n",
    "                    # Fit SearchCV\n",
    "                    searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "                    # Best Parameters\n",
    "                    best_index = searchcv.best_index_\n",
    "                    cv_results = sorted(searchcv.cv_results_)\n",
    "                    best_params = searchcv.best_params_\n",
    "                    best_score = searchcv.best_score_\n",
    "                    n_splits = searchcv.n_splits_\n",
    "                    classifier = searchcv.best_estimator_\n",
    "                    y_train_pred = classifier.predict(X_train)\n",
    "\n",
    "                    print('=' * 20)\n",
    "                    print(f'Best index for {scores[0]}: {best_index}')\n",
    "                    print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                    print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                    print(f'Best score for {scores[0]}: {best_score}')\n",
    "                    print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                    print('-' * 20)\n",
    "                    report = classification_report(y_train, y_train_pred)\n",
    "                    print(f'Classification Report:\\n{report}')\n",
    "                    ConfusionMatrixDisplay.from_estimator(\n",
    "                        searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                    )\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    print('=' * 20)\n",
    "\n",
    "                    # Make the predictions\n",
    "                    score = searchcv.score(X_test, y_test)\n",
    "                    y_test_pred = searchcv.predict(X_test)\n",
    "                    if hasattr(searchcv, 'predict_proba'):\n",
    "                        y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                    elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                        y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                        y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                    # Fit Best Model\n",
    "                    print(f'Fitting {classifier}.')\n",
    "                    classifier.set_params(**classifier.get_params())\n",
    "                    classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "                    # Evaluate Model\n",
    "                    classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "                    # Save Vectorizer, Selector, and Classifier\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463f4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
