{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c3b1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# df save sir\n",
    "df_save_dir = f'{data_dir}final dfs/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "757b442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import joblib\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.3f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5545a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import (SelectFdr, SelectFpr,\n",
    "                                       SelectFromModel, SelectFwe,\n",
    "                                       SelectKBest, SelectPercentile, chi2,\n",
    "                                       f_classif, f_regression,\n",
    "                                       mutual_info_classif,\n",
    "                                       mutual_info_regression)\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.linear_model import (LogisticRegression,\n",
    "                                  PassiveAggressiveClassifier, Perceptron,\n",
    "                                  SGDClassifier)\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              RandomForestClassifier, StackingClassifier,\n",
    "                              StackingRegressor, VotingClassifier,\n",
    "                              VotingRegressor)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, LeaveOneOut,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     RepeatedStratifiedKFold, ShuffleSplit,\n",
    "                                     StratifiedKFold,\n",
    "                                     StratifiedShuffleSplit,\n",
    "                                     cross_val_score, cross_validate,\n",
    "                                     learning_curve, train_test_split)\n",
    "from sklearn.utils.validation import (check_is_fitted, column_or_1d,\n",
    "                                      has_fit_parameter)\n",
    "from sklearn import feature_selection, metrics, set_config, svm, utils\n",
    "from sklearn.metrics import (ConfusionMatrixDisplay,accuracy_score, balanced_accuracy_score,\n",
    "                             brier_score_loss, classification_report, cohen_kappa_score,\n",
    "                             confusion_matrix, f1_score, log_loss,\n",
    "                             make_scorer, matthews_corrcoef,\n",
    "                             precision_recall_curve, precision_score,\n",
    "                             recall_score, roc_auc_score)\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.utils import (check_consistent_length, check_random_state, check_X_y)\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import (EditedNearestNeighbours, NearMiss,\n",
    "                                     RandomUnderSampler, TomekLinks)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7735d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split ratios\n",
    "n_jobs = 1\n",
    "train_ratio = 0.75\n",
    "test_ratio = 0.10\n",
    "validation_ratio = 0.15\n",
    "test_split = test_size = 1 - train_ratio\n",
    "validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "# Cross-validation\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "random_state = 42\n",
    "partition = True\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=10, n_repeats=3, random_state=random_state)\n",
    "# Resampling\n",
    "class_weight = 'balanced'\n",
    "resampling_enabled = True\n",
    "resample_enn = SMOTEENN(\n",
    "    enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
    "resample_tome = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n",
    "# Undersampling\n",
    "rus = RandomUnderSampler(random_state=random_state, replacement=True)\n",
    "tl = RandomOverSampler(sampling_strategy='majority')\n",
    "nm = NearMiss()\n",
    "# Oversampling\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "smote = SMOTE()\n",
    "# Sampling Used\n",
    "resampling_method = resample_tome\n",
    "\n",
    "t = time.time()\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_sizes = [300, 100]\n",
    "scoring = 'recall'\n",
    "scores = ['recall', 'accuracy', 'f1', 'roc_auc', 'explained_variance', 'matthews_corrcoef']\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "metrics_list = [\n",
    "    'Mean Validation Score',\n",
    "    'Explained Variance',\n",
    "    'Accuracy',\n",
    "    'Precision',\n",
    "    'Recall',\n",
    "    'F1-score',\n",
    "    'ROC',\n",
    "    'AUC',\n",
    "    'Matthews Correlation Coefficient',\n",
    "    f'{scoring.title()} Best Threshold',\n",
    "    f'{scoring.title()} Best Score',\n",
    "    'Log Loss/Cross Entropy',\n",
    "    'Classification Report',\n",
    "    'Confusion Matrix',\n",
    "]\n",
    "\n",
    "# Paths\n",
    "models_save_path = f'{data_dir}classification models/'\n",
    "table_save_path = f'{data_dir}output tables/'\n",
    "plot_save_path = f'{data_dir}plots/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df119b8",
   "metadata": {},
   "source": [
    "## Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec7307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CountVectorizer\n",
    "count = CountVectorizer()\n",
    "params_count_pipe = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'CountVectorizer__analyzer': ['word'],\n",
    "    'CountVectorizer__ngram_range': [(1, 3)],\n",
    "    'CountVectorizer__lowercase': [True, False],\n",
    "    'CountVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'CountVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "\n",
    "### TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "params_tfidf_pipe = {\n",
    "#     'TfidfVectorizer__stop_words': ['english'],\n",
    "    'TfidfVectorizer__analyzer': ['word'],\n",
    "    'TfidfVectorizer__ngram_range': [(1, 3)],\n",
    "    'TfidfVectorizer__lowercase': [True, False],\n",
    "    'TfidfVectorizer___use_idf': [True, False],\n",
    "    'TfidfVectorizer__max_df': [0.90, 0.85, 0.80, 0.75, 0.70],\n",
    "    'TfidfVectorizer__min_df': [0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "}\n",
    "\n",
    "### BOW FeatureUnion\n",
    "bow = FeatureUnion(\n",
    "    transformer_list=[('CountVectorizer', count), ('TfidfVectorizer', tfidf)]\n",
    ")\n",
    "params_bow_pipe = {**params_count_pipe, **params_tfidf_pipe}\n",
    "\n",
    "## Vectorizers Dict\n",
    "vectorizers_pipe = {\n",
    "    'CountVectorizer': [count, params_count_pipe],\n",
    "    'TfidfVectorizer': [tfidf, params_tfidf_pipe],\n",
    "    'UnionBOW': [bow, params_bow_pipe],\n",
    "    # \"UnionWordEmbedding\": [em, params_em_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaff38f",
   "metadata": {},
   "source": [
    "## Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224d8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SelectKBest\n",
    "selectkbest = SelectKBest()\n",
    "params_selectkbest_pipe = {\n",
    "    'SelectKBest__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectKBest__k': ['all'],\n",
    "}\n",
    "\n",
    "### SelectPercentile\n",
    "selectpercentile = SelectPercentile()\n",
    "params_selectpercentile_pipe = {\n",
    "    'SelectPercentile__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "    'SelectPercentile__percentile': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "}\n",
    "\n",
    "### SelectFpr\n",
    "selectfpr = SelectFpr()\n",
    "params_selectfpr_pipe = {\n",
    "    'SelectFpr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFdr\n",
    "selectfdr = SelectFdr()\n",
    "params_selectfdr_pipe = {\n",
    "    'SelectFdr__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "### SelectFwe\n",
    "selectfwe = SelectFwe()\n",
    "params_selectfwe_pipe = {\n",
    "    'SelectFwe__score_func': [f_classif, chi2, mutual_info_classif, f_regression, mutual_info_regression],\n",
    "}\n",
    "\n",
    "## Selectors Dict\n",
    "selectors_pipe = {\n",
    "    'SelectKBest': [selectkbest, params_selectkbest_pipe],\n",
    "    'SelectPercentile': [selectpercentile, params_selectpercentile_pipe],\n",
    "    'SelectFpr': [selectfpr, params_selectfpr_pipe],\n",
    "    'SelectFdr': [selectfdr, params_selectfdr_pipe],\n",
    "    'SelectFwe': [selectfwe, params_selectfwe_pipe],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab6d49",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2222fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "### Dummy Classifier\n",
    "dummy = DummyClassifier()\n",
    "params_dummy_freq = {'strategy': 'most_frequent', 'random_state': random_state}\n",
    "params_dummy_stratified = {'strategy': 'stratified', 'random_state': random_state}\n",
    "params_dummy_uniform = {'strategy': 'uniform', 'random_state': random_state}\n",
    "params_dummy_pipe = {\n",
    "    'DummyClassifier__strategy': [\n",
    "        'stratified',\n",
    "        'most_frequent',\n",
    "        'prior',\n",
    "        'uniform',\n",
    "    ],\n",
    "    'DummyClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "### Multinomial Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "params_nb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "params_nb_pipe = {\n",
    "    'MultinomialNB__fit_prior': [True],\n",
    "    'MultinomialNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "### Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB()\n",
    "params_bnb = {'alpha': 0.1, 'fit_prior': True, 'class_prior': None}\n",
    "params_bnb_pipe = {\n",
    "    'BernoulliNB__fit_prior': [True],\n",
    "    'BernoulliNB__alpha': [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "params_gnb = {'var_smoothing': 1e-9}\n",
    "params_gnb_pipe = {\n",
    "    'GaussianNB__var_smoothing': [1e-9],\n",
    "}\n",
    "\n",
    "### KNeighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "params_knn = {\n",
    "    'n_neighbors': 3,\n",
    "    'weights': 'uniform',\n",
    "    'algorithm': 'auto',\n",
    "    'leaf_size': 30,\n",
    "    'p': 2,\n",
    "    'metric': 'minkowski',\n",
    "    'metric_params': None,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_knn_pipe = {\n",
    "    'KNeighborsClassifier__weights': ['uniform'],\n",
    "    'KNeighborsClassifier__n_neighbors': [2, 5, 15],\n",
    "    'KNeighborsClassifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'KNeighborsClassifier__leaf_size': [30, 50, 100, 200, 300, 500],\n",
    "    'KNeighborsClassifier__p': [1, 2, 3, 4, 5],\n",
    "    'KNeighborsClassifier__metric': [\n",
    "        'minkowski',\n",
    "        'euclidean',\n",
    "        'cosine',\n",
    "        'correlation',\n",
    "    ],\n",
    "    'KNeighborsClassifier__metric_params': [None, {'p': 2}, {'p': 3}],\n",
    "}\n",
    "\n",
    "### Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "params_lr = {\n",
    "    'penalty': 'l2',\n",
    "    'dual': False,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'solver': 'liblinear',\n",
    "    'max_iter': 100,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_lr_pipe = {\n",
    "    'LogisticRegression__penalty': ['l2'],\n",
    "    'LogisticRegression__random_state': [random_state],\n",
    "    'LogisticRegression__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'LogisticRegression__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LogisticRegression__multi_class': ['ovr', 'multinomial'],\n",
    "    'LogisticRegression__solver': ['newton-cg', 'liblinear'],\n",
    "    'LogisticRegression__C': [0.01, 1, 100],\n",
    "}\n",
    "\n",
    "### Passive Aggressive\n",
    "pa = PassiveAggressiveClassifier()\n",
    "params_pa = {\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_pa_pipe = {\n",
    "    'PassiveAggressiveClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'PassiveAggressiveClassifier__random_state': [random_state],\n",
    "    'PassiveAggressiveClassifier__fit_intercept': [True, False],\n",
    "    'PassiveAggressiveClassifier__class_weight': [None, 'balanced'],\n",
    "    'PassiveAggressiveClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### Stochastic Gradient Descent Aggressive\n",
    "sgd = SGDClassifier()\n",
    "params_sgd = {\n",
    "    'fit_intercept': True,\n",
    "    'max_iter': 1000,\n",
    "    'tol': 0.0001,\n",
    "    'class_weight': class_weight,\n",
    "    'verbose': 0,\n",
    "    'random_state': random_state,\n",
    "    'loss': 'hinge',\n",
    "    'n_jobs': 1,\n",
    "}\n",
    "params_sgd_pipe = {\n",
    "    'SGDClassifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'SGDClassifier__random_state': [random_state],\n",
    "    'SGDClassifier__fit_intercept': [True, False],\n",
    "    'SGDClassifier__class_weight': [None, 'balanced'],\n",
    "    'SGDClassifier__max_iter': [100, 200, 300, 500, 1000],\n",
    "}\n",
    "\n",
    "### SVM\n",
    "svm = LinearSVC()\n",
    "params_svm = {\n",
    "    'penalty': 'l2',\n",
    "    'loss': 'hinge',\n",
    "    'dual': True,\n",
    "    'tol': 0.0001,\n",
    "    'C': 1.0,\n",
    "    'fit_intercept': True,\n",
    "    'intercept_scaling': 1,\n",
    "    'class_weight': class_weight,\n",
    "    'random_state': random_state,\n",
    "    'max_iter': 1000,\n",
    "    'multi_class': 'ovr',\n",
    "    'verbose': 0,\n",
    "}\n",
    "params_svm_pipe = {\n",
    "    'LinearSVC__penalty': ['l2'],\n",
    "    'LinearSVC__loss': ['hinge', 'squared_hinge'],\n",
    "    'LinearSVC__random_state': [random_state],\n",
    "    'LinearSVC__max_iter': [100, 200, 300, 500, 1000],\n",
    "    'LinearSVC__fit_intercept': [True, False],\n",
    "    'LinearSVC__class_weight': [None, 'balanced'],\n",
    "    'LinearSVC__multi_class': ['ovr', 'crammer_singer'],\n",
    "}\n",
    "\n",
    "### Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "params_dt = {\n",
    "    'criterion': 'gini',\n",
    "    'splitter': 'best',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': random_state,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "}\n",
    "params_dt_pipe = {\n",
    "    'DecisionTreeClassifier__max_depth': [5, 10],\n",
    "    'DecisionTreeClassifier__criterion': ['gini', 'entropy'],\n",
    "    'DecisionTreeClassifier__random_state': [random_state],\n",
    "    'DecisionTreeClassifier__splitter': ['best', 'random'],\n",
    "    'DecisionTreeClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "params_rf = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': True,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_rf_pipe = {\n",
    "    'RandomForestClassifier__n_estimators': [10, 20],\n",
    "    'RandomForestClassifier__n_jobs': [-1],\n",
    "    'RandomForestClassifier__max_depth': [5, 10],\n",
    "    'RandomForestClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'RandomForestClassifier__random_state': [random_state],\n",
    "    'RandomForestClassifier__class_weight': [None, 'balanced'],\n",
    "    'RandomForestClassifier__oob_score': [True],\n",
    "}\n",
    "\n",
    "### Extra Trees\n",
    "et = ExtraTreesClassifier()\n",
    "params_et = {\n",
    "    'n_estimators': 10,\n",
    "    'criterion': 'log_loss',\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'bootstrap': True,\n",
    "    'oob_score': False,\n",
    "    'n_jobs': 1,\n",
    "    'random_state': random_state,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    'class_weight': class_weight,\n",
    "}\n",
    "params_et_pipe = {\n",
    "    'ExtraTreesClassifier__n_estimators': [10, 20],\n",
    "    'ExtraTreesClassifier__n_jobs': [-1],\n",
    "    'ExtraTreesClassifier__max_depth': [5, 10],\n",
    "    'ExtraTreesClassifier__max_feature': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'ExtraTreesClassifier__random_state': [random_state],\n",
    "    'ExtraTreesClassifier__criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'ExtraTreesClassifier__class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "### Gradient Boosting\n",
    "gbc = GradientBoostingClassifier()\n",
    "params_gbc = {\n",
    "    'loss': 'deviance',\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100,\n",
    "    'subsample': 1.0,\n",
    "    'criterion': 'friedman_mse',\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_depth': 3,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'init': None,\n",
    "    'random_state': random_state,\n",
    "    'max_features': None,\n",
    "    'verbose': 0,\n",
    "    'max_leaf_nodes': None,\n",
    "    'warm_start': False,\n",
    "}\n",
    "params_gbc_pipe = {\n",
    "    'GradientBoostingClassifier__max_depth': [5, 10],\n",
    "    'GradientBoostingClassifier__criterion': ['gini', 'entropy'],\n",
    "    'GradientBoostingClassifier__random_state': [random_state],\n",
    "    'GradientBoostingClassifier__n_estimators': [10, 20],\n",
    "    'GradientBoostingClassifier__loss': ['deviance', 'exponential'],\n",
    "    'GradientBoostingClassifier__subsample': [*np.arange(0.1, 1.1, 0.1)],\n",
    "    'GradientBoostingClassifier__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "### AdaBoost\n",
    "ada = AdaBoostClassifier()\n",
    "params_ada = {\n",
    "    'base_estimator': None,\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 1.0,\n",
    "    'algorithm': 'SAMME.R',\n",
    "    'random_state': random_state,\n",
    "}\n",
    "params_ada_pipe = {\n",
    "    'AdaBoostClassifier__max_depth': [5, 10],\n",
    "    'AdaBoostClassifier__criterion': ['gini', 'entropy'],\n",
    "    'AdaBoostClassifier__random_state': [random_state],\n",
    "    'AdaBoostClassifier__n_estimators': [50, 100, 150],\n",
    "    'AdaBoostClassifier__base_estimator': [\n",
    "        SVC(probability=True, kernel='linear'),\n",
    "        LogisticRegression(),\n",
    "        MultinomialNB(),\n",
    "    ],\n",
    "}\n",
    "\n",
    "### XGBoost\n",
    "xgb = XGBClassifier()\n",
    "params_xgb = {\n",
    "    'nthread':4, #when use hyperthread, xgboost may become slower\n",
    "    'objective':'binary:logistic',\n",
    "    'learning_rate': 0.05, #so called `eta` value\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 11,\n",
    "    'silent': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'n_estimators': 1000, #number of trees, change it to 1000 for better results\n",
    "    'missing':-999,\n",
    "    'seed': 1337,\n",
    "    'eval_metric': 'auc',\n",
    "    'sample_type': 'weighted',\n",
    "    'verbosity': '0',\n",
    "}\n",
    "params_xgb_pipe = {\n",
    "    'xgb__max_depth': [5, 10],\n",
    "    'xgb__learning_rate': [0.05],\n",
    "    'xgb__n_estimators': [1000],\n",
    "    'xgb__seed': [42],\n",
    "    'xgb__nthread': [1, 2, 3, 4],\n",
    "    'xgb__objective': ['binary:logitraw', 'binary:logistic', 'binary:hinge'],\n",
    "    'xgb__eval_metric': ['auc', 'rmse', 'rmsle', 'logloss'],\n",
    "    'xgb__sample_type': ['weighted', 'uniform'],\n",
    "}\n",
    "\n",
    "### MLP Classifier\n",
    "mlpc = MLPClassifier()\n",
    "params_mlpc = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpc_pipe = {\n",
    "    'MLPClassifier__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPClassifier__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPClassifier__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPClassifier__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPClassifier__random_state': [random_state],\n",
    "}\n",
    "\n",
    "mlpr = MLPRegressor()\n",
    "params_mlpr = {\n",
    "    'hidden_layer_sizes': (100,),\n",
    "    'activation': 'relu',\n",
    "    'solver': 'adam',\n",
    "    'alpha': 0.0001,\n",
    "    'batch_size': 'auto',\n",
    "    'learning_rate': 'constant',\n",
    "    'learning_rate_init': 0.001,\n",
    "    'power_t': 0.5,\n",
    "    'max_iter': 200,\n",
    "    'shuffle': True,\n",
    "    'random_state': random_state,\n",
    "    'tol': 0.0001,\n",
    "    'verbose': False,\n",
    "    'warm_start': False,\n",
    "    'momentum': 0.9,\n",
    "    'nesterovs_momentum': True,\n",
    "    'early_stopping': False,\n",
    "    'validation_fraction': 0.1,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'epsilon': 1e-08,\n",
    "}\n",
    "params_mlpr_pipe = {\n",
    "    'MLPRegressor__hidden_layer_sizes': [(100,), (50,), (25,), (10,), (5,), (1,)],\n",
    "    'MLPRegressor__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'MLPRegressor__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'MLPRegressor__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'MLPRegressor__random_state': [random_state],\n",
    "}\n",
    "\n",
    "## Stacking and Voting Classifiers\n",
    "estimators = [\n",
    "    ('Multinomial Naive Bayes', MultinomialNB()),\n",
    "    (\n",
    "        'Logistic Regression',\n",
    "        LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    ),\n",
    "]\n",
    "\n",
    "### Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=estimators)\n",
    "params_voting_pipe = {\n",
    "    'VotingClassifier__estimators': [\n",
    "        ('dummy', dummy, params_dummy_freq),\n",
    "        ('dummy', dummy, params_dummy_stratified),\n",
    "        ('dummy', dummy, params_dummy_uniform),\n",
    "        ('nb', nb, params_nb),\n",
    "        ('bnb', bnb, params_bnb),\n",
    "        ('gnb', gnb, params_gnb),\n",
    "        ('knn', knn, params_knn),\n",
    "        ('lr', lr, params_lr),\n",
    "        ('pa', pa, params_pa),\n",
    "        ('sgd', sgd, params_sgd),\n",
    "        ('svm', svm, params_svm),\n",
    "        ('dt', dt, params_dt),\n",
    "        ('rf', rf, params_rf),\n",
    "        ('gbc', gbc, params_gbc),\n",
    "        ('ada', ada, params_ada),\n",
    "        ('xgb', xgb, params_xgb),\n",
    "        ('mlpc', mlpc, params_mlpc),\n",
    "        ('mlpr', mlpr, params_mlpr),\n",
    "    ],\n",
    "    'VotingClassifier__voting': ['hard', 'soft'],\n",
    "    'VotingClassifier__weights': [None, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]],\n",
    "}\n",
    "\n",
    "### Stacking Classifier\n",
    "stacking_classifier = StackingClassifier(estimators=estimators)\n",
    "# final_estimator = LogisticRegression(random_state=random_state, class_weight=class_weight)\n",
    "final_estimator = RandomForestClassifier(\n",
    "    random_state=42, class_weight={0: 1, 1: 2}\n",
    ")\n",
    "params_stacking_pipe = {\n",
    "    'StackingClassifier__estimator': [\n",
    "        ('dummy', dummy),\n",
    "        ('nb', nb),\n",
    "        ('bnb', bnb),\n",
    "        ('gnb', gnb),\n",
    "        ('knn', knn),\n",
    "        ('lr', lr),\n",
    "        ('pa', pa),\n",
    "        ('sgd', sgd),\n",
    "        ('svm', svm),\n",
    "        ('dt', dt),\n",
    "        ('rf', rf),\n",
    "        ('gbc', gbc),\n",
    "        ('ada', ada),\n",
    "        ('mlpc', mlpc),\n",
    "    ],\n",
    "    'StackingClassifier__cv': [3, 5, 7, 9, 11, 13, 15],\n",
    "    'StackingClassifier__n_jobs': [-1],\n",
    "    'StackingClassifier__stack_method': ['predict_proba', 'decision_function'],\n",
    "    'StackingClassifier__passthrough': [True, False],\n",
    "}\n",
    "\n",
    "## Classifiers Pipe dict\n",
    "classifiers_pipe = {\n",
    "    'DummyClassifier': [dummy, params_dummy_pipe],\n",
    "    'MultinomialNB': [nb, params_nb_pipe],\n",
    "#     'BernoulliNB': [bnb, params_bnb_pipe],\n",
    "#     'GaussianNB': [gnb, params_gnb_pipe],\n",
    "#     'KNeighborsClassifier': [knn, params_knn_pipe],\n",
    "#     'LogisticRegression': [lr, params_lr_pipe],\n",
    "#     'PassiveAggressiveClassifier': [pa, params_pa_pipe],\n",
    "#     'SGDClassifier': [sgd, params_sgd_pipe],\n",
    "#     'LinearSVC': [svm, params_svm_pipe],\n",
    "#     'DecisionTreeClassifier': [dt, params_dt_pipe],\n",
    "#     'RandomForestClassifier': [rf, params_rf_pipe],\n",
    "#     'GradientBoostingClassifier': [gbc, params_gbc_pipe],\n",
    "#     'AdaBoostClassifier': [ada, params_ada_pipe],\n",
    "#     'XGBClassifier': [xgb, params_xgb_pipe],\n",
    "#     'MLPClassifier': [mlpc, params_mlpc_pipe],\n",
    "#     'MLPRegressor': [mlpr, params_mlpr_pipe],\n",
    "#     'VotingClassifier': [voting_classifier, params_voting_pipe],\n",
    "#     'StackingClassifier': [stacking_classifier, params_stacking_pipe],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_table_df(vectorizers_pipe, selectors_pipe, classifiers_pipe, analysis_columns, metrics_list):\n",
    "    \n",
    "    # Table df\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [list(map(lambda classifier: classifier, classifiers_pipe.keys()))],\n",
    "        names=['Classifiers'],\n",
    "    )\n",
    "    columns = pd.MultiIndex.from_product(\n",
    "        [\n",
    "            analysis_columns,\n",
    "            list(map(lambda vectorizer: vectorizer, vectorizers_pipe.keys())),\n",
    "            list(map(lambda selector: selector, selectors_pipe.keys())),\n",
    "            metrics_list,\n",
    "        ],\n",
    "        names=['Variable', 'Vectorizer', 'Selector', 'Measures'],\n",
    "    )\n",
    "    table_df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "    return table_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # BOW Split\n",
    "    print('Splitting data into training, testing, and validation sets.')\n",
    "    df_manual.dropna(subset=['Warmth', 'Competence', text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([x for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([x for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    print('Done splitting data into training, testing, and validation sets')\n",
    "    print('-'*10)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print(f'Validation set shape: {y_validate.shape}')\n",
    "    print('-'*10)\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999a6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_percentage(\n",
    "    cm, col, vectorizer_name, selector_name, classifier_name\n",
    "):\n",
    "    print('\\n')\n",
    "    plt.title(\n",
    "        f'Confusion Matrix Heatmap Plot {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}',\n",
    "        fontsize=14.0,\n",
    "    )\n",
    "    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n",
    "    group_counts = [f'{value:0.0f}\\n' for value in cm.flatten()]\n",
    "    group_percentages = [f'{value:.2%}' for value in cm.flatten() / np.sum(cm)]\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(cm.shape[0], cm.shape[1])\n",
    "    cm_heatmap = sns.heatmap(\n",
    "        cm, cmap=plt.cm.Blues, annot=labels, fmt='', annot_kws={'size': 12.0}\n",
    "    )\n",
    "    plt.xlabel('Predicted', fontsize=12.0)\n",
    "    plt.ylabel('Actual', fontsize=12.0)\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_heatmap.figure.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix Heatmap {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87d29eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(cm, cm_curve, roc_curve, pr_curve, calibration_curve, recall, precision, no_skill, y_validate):\n",
    "    # Plots\n",
    "    ## Confusion Matrix\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    print(f'Confusion Matrix:')\n",
    "    plt.title(\n",
    "        f'Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    cm_curve.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()        \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## Confusion Matrix Heatmap\n",
    "    print(f'Confusion Matrix Heatmap Plot:')\n",
    "    plot_confusion_matrix_percentage(cm, col, vectorizer_name, selector_name, classifier_name)\n",
    "    \n",
    "    ## ROC Curve\n",
    "    print('-' * 20)\n",
    "    print(f'ROC Curve:')      \n",
    "    plt.title(\n",
    "        f'ROC Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "    )\n",
    "    roc_curve.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()        \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    ## PR Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Precision Recall Curve:')\n",
    "    plt.title(\n",
    "        f'Precision-Recall Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "    )\n",
    "    plt.ylabel('Precision', fontsize=12.0)\n",
    "    plt.xlabel('Recall', fontsize=12.0)\n",
    "    pr_curve.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()        \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    \n",
    "    ## Calibration Curve\n",
    "    print('-' * 20)\n",
    "    print(f'Calibration Curve:')\n",
    "    plt.title(\n",
    "        f'Calibration Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16,\n",
    "    )\n",
    "    calibration_curve.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()        \n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "#     pr_curve = plt.figure(figsize=(4.0, 4.0))\n",
    "#     plt.plot(\n",
    "#         [0, 1], [no_skill, no_skill], linestyle='--', label='No Skill'\n",
    "#     )\n",
    "#     plt.plot(\n",
    "#         recall, precision, marker='.', label=f'AUC = {auc}'\n",
    "#     )\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.plot([0, 1], [0, 1], 'r--')\n",
    "#     plt.xlim([0, 1])\n",
    "#     plt.ylim([0, 1])\n",
    "#     plt.title(\n",
    "#         f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "#         fontsize=12.0,\n",
    "#     )\n",
    "#     plt.ylabel('Precision', fontsize=12.0)\n",
    "#     plt.xlabel('Recall', fontsize=12.0)\n",
    "#     plt.show()        \n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "\n",
    "    # Save Plots\n",
    "    for image_save_format in ['eps', 'png']:\n",
    "        cm_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        cm_heatmap.figure.savefig(\n",
    "            plot_save_path\n",
    "            + f'Confusion Matrix Heatmap {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        pr_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n",
    "        calibration_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'Calibration Curve {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(col, table_df, estimator, vectorizer_name, selector_name, classifier_name):\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    table_df.to_csv(table_save_path + 'Classifiers Table.csv')\n",
    "    table_df.to_pickle(table_save_path + 'Classifiers Table.pkl')\n",
    "    table_df.to_excel(table_save_path + 'Classifiers Table.xlsx')\n",
    "    table_df.to_latex(table_save_path + 'Classifiers Table.tex')\n",
    "    table_df.to_markdown(table_save_path + 'Classifiers Table.md')\n",
    "\n",
    "    with open(f'{models_save_path}Estimator {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "        joblib.dump(estimator, f)\n",
    "#     with open(f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "#         joblib.dump(vectorizer, f)\n",
    "#     with open(f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.pkl', 'wb') as f:\n",
    "#         joblib.dump(selector, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(estimator, col, vectorizer_name, selector_name, classifier_name, X_validate, y_validate, y_validate_pred, y_validate_prob_pred, best_score, scoring):\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        estimator,\n",
    "        X_validate,\n",
    "        y_validate,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    cross_validate_score = cross_validate(\n",
    "        estimator,\n",
    "        X_validate,\n",
    "        y_validate,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    no_skill = len(y_validate[y_validate == 1]) / len(y_validate)\n",
    "    mean_validation_score = cross_validate_score_noscoring.get('test_score').mean()\n",
    "    explained_variance = cross_validate_score.get('test_explained_variance').mean()\n",
    "    accuracy = metrics.accuracy_score(y_validate, y_validate_pred)\n",
    "    precision = metrics.precision_score(y_validate, y_validate_pred, pos_label=1, labels=[1, 0])\n",
    "    recall = metrics.recall_score(y_validate, y_validate_pred, pos_label=1, labels=[1, 0])\n",
    "    f1 = metrics.f1_score(y_validate, y_validate_pred)\n",
    "    mcc = metrics.matthews_corrcoef(y_validate, y_validate_pred)\n",
    "    loss = log_loss(y_validate, y_validate_prob_pred)\n",
    "    report = classification_report(y_validate, y_validate_pred)\n",
    "    cm = metrics.confusion_matrix(y_validate, y_validate_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    roc_auc = metrics.roc_auc_score(y_validate, y_validate_pred)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_validate, y_validate_prob_pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_validate, y_validate_prob_pred, pos_label=1)\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_validate, y_validate_pred,\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_validate, y_validate_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_validate, y_validate_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_validate, y_validate_pred, pos_label=1, name=f'{str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}'\n",
    "    )\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Mean Validation Score')\n",
    "    ] = float(mean_validation_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Explained Variance')\n",
    "    ] = float(explained_variance)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = str(threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Log Loss/Cross Entropy'),\n",
    "    ] = float(loss)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Classification Report')\n",
    "    ] = report\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Confusion Matrix')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Confusion Matrix')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "#     table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, selector_name, f'{scoring.title()} Normalized Confusion Matrix')\n",
    "#     ] = table_df[\n",
    "#         (classifier_name), (col, vectorizer_name, f'{scoring.title()} Normalized Confusion Matrix')\n",
    "#     ].astype(object)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'Normalized Confusion Matrix')\n",
    "    ] = str(cm_normalized)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, selector_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(f' Metrics for {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {report}')\n",
    "    print('-' * 20)\n",
    "    print(f'Mean Validation Score: {mean_validation_score}')\n",
    "    print('-' * 20)\n",
    "    print(f'Recall score: {recall}')\n",
    "    print('-' * 20)\n",
    "    print(f'Accuracy score: {accuracy}')\n",
    "    print('-' * 20)\n",
    "    print(f'Precision score: {precision}')\n",
    "    print('-' * 20)\n",
    "    print(f'F1 score: {f1}')\n",
    "    print('-' * 20)\n",
    "    print(f'Matthews correlation coefficient: {mcc}')\n",
    "    print('-' * 20)\n",
    "    print(f'{scores[0].title()} best score: {best_score}')\n",
    "    print('-' * 20)\n",
    "    print(f'{scores[0].title()} best threshold: {threshold}')\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix:\\n', cm)\n",
    "    print('-' * 20)\n",
    "    print(f'Confusion Matrix Normalized:\\n', cm_normalized)\n",
    "    print('-' * 20)\n",
    "    print('=' * 20)\n",
    "    \n",
    "    # Plot Metrics\n",
    "    plot_metrics(cm, cm_curve, roc_curve, pr_curve, calibration_curve, recall, precision, no_skill, y_validate)\n",
    "\n",
    "    return table_df, mean_validation_score, explained_variance, accuracy, precision, recall, f1, mcc, loss, report, cm, cm_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_trainning.pkl').reset_index(drop=True)[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0212dc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING WARMTH ==============================\n",
      "--------------------\n",
      "Splitting data into training, testing, and validation sets.\n",
      "Done splitting data into training, testing, and validation sets\n",
      "----------\n",
      "Training set shape: (150,)\n",
      "Testing set shape: (20,)\n",
      "Validation set shape: (30,)\n",
      "----------\n",
      "--------------------\n",
      "============================== Using GridSearchCV ==============================\n",
      "--------------------\n",
      "GridSearchCV with:\n",
      "Pipe:\n",
      "Pipeline(steps=[('CountVectorizer', CountVectorizer()),\n",
      "                ('SelectKBest', SelectKBest()),\n",
      "                ('DummyClassifier', DummyClassifier())])\n",
      "Params:\n",
      "{'CountVectorizer__analyzer': ['word'], 'CountVectorizer__ngram_range': [(1, 3)], 'CountVectorizer__lowercase': [True, False], 'CountVectorizer__max_df': [0.9, 0.85, 0.8, 0.75, 0.7], 'CountVectorizer__min_df': [0.1, 0.15, 0.2, 0.25, 0.3], 'SelectKBest__score_func': [<function f_classif at 0x15977c310>, <function chi2 at 0x15977c430>, <function mutual_info_classif at 0x15a994430>, <function f_regression at 0x15977c550>, <function mutual_info_regression at 0x15a9943a0>], 'SelectKBest__k': ['all'], 'DummyClassifier__strategy': ['stratified', 'most_frequent', 'prior', 'uniform'], 'DummyClassifier__random_state': [42]}\n",
      "++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "table_df = make_table_df(vectorizers_pipe, selectors_pipe, classifiers_pipe, analysis_columns, metrics_list)\n",
    "\n",
    "for col, (vectorizer_name, vectorizer_and_params), (selector_name, selector_and_params), (classifier_name, classifier_and_params) in itertools.product(analysis_columns, vectorizers_pipe.items(), selectors_pipe.items(), classifiers_pipe.items()):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "\n",
    "    if (len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate = split_data(df_manual, col, text_col = text_col)\n",
    "\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        selector = selector_and_params[0]\n",
    "        selector_params = selector_and_params[1]\n",
    "\n",
    "        classifier = classifier_and_params[0]\n",
    "        classifier_params = classifier_and_params[1]\n",
    "\n",
    "        # Pipeline\n",
    "        ## Steps\n",
    "        steps = [\n",
    "            (vectorizer_name, vectorizer),\n",
    "            (selector_name, selector),\n",
    "            (classifier_name, classifier)\n",
    "        ]\n",
    "\n",
    "        ## Params\n",
    "        param_grid = {\n",
    "            **vectorizer_params,\n",
    "            **selector_params,\n",
    "            **classifier_params,\n",
    "        }\n",
    "\n",
    "        ## Pipeline\n",
    "        pipe = Pipeline(steps=steps)\n",
    "\n",
    "        # Search\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} Using GridSearchCV {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(f'GridSearchCV with:\\nPipe:\\n{pipe}\\nParams:\\n{param_grid}')\n",
    "        print('+'*30)\n",
    "        search = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=param_grid,\n",
    "            n_jobs=-1,\n",
    "            scoring=scores,\n",
    "            cv=cv,\n",
    "            refit=scores[0],\n",
    "            return_train_score=True,\n",
    "        )\n",
    "\n",
    "        # Fit SearchCV\n",
    "        searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "        # Best Parameters\n",
    "        best_index = searchcv.best_index_\n",
    "        cv_results = sorted(searchcv.cv_results_)\n",
    "        best_params = searchcv.best_params_\n",
    "        best_score = searchcv.best_score_\n",
    "        n_splits = searchcv.n_splits_\n",
    "        estimator = searchcv.best_estimator_\n",
    "        y_train_pred = estimator.predict(X_train)\n",
    "        \n",
    "        vectorizer = estimator[0]\n",
    "        vectorizer_name = vectorizer.__class__.__name__\n",
    "        selector = estimator[1]\n",
    "        selector_name = selector.__class__.__name__\n",
    "        classifier = estimator[2]\n",
    "        classifier_name = classifier.__class__.__name__\n",
    "\n",
    "        print('=' * 20)\n",
    "        print(f'Best index for {scores[0]}: {best_index}')\n",
    "        print(f'Best estimator for {scores[0]}: {estimator}')\n",
    "        print(f'Best params for {scores[0]}: {best_params}')\n",
    "#         print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "        print(f'Best score for {scores[0]}: {best_score}')\n",
    "        print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "        print('-' * 20)\n",
    "        train_report = classification_report(y_train, y_train_pred)\n",
    "        print(f'Training Classification Report:\\n{train_report}')\n",
    "        print(f'Training Confusion Matrix:')\n",
    "        plt.title(\n",
    "            f'Training Confusion Matrix {str(col)} - {vectorizer_name} + {selector_name} + {classifier_name}', fontsize=16\n",
    "        )\n",
    "        train_cm = metrics.ConfusionMatrixDisplay.from_estimator(\n",
    "            estimator, X_train, y_train\n",
    "        ).plot(xticks_rotation='vertical', cmap=plt.cm.Blues)\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        print('=' * 20)\n",
    "\n",
    "        # Make the predictions\n",
    "        if hasattr(searchcv, 'predict_proba'):\n",
    "            searchcv_predict_attr = searchcv.predict_proba\n",
    "        elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "            searchcv_predict_attr = searchcv._predict_proba_lr\n",
    "        score = searchcv.score(X_test, y_test)\n",
    "        y_test_pred = searchcv.predict(X_test)    \n",
    "        y_test_prob_pred = searchcv_predict_attr(X_test)[:, 1]\n",
    "#         y_validate_pred = searchcv_predict_attr(X_validate)\n",
    "        y_validate_prob_pred = searchcv_predict_attr(X_validate)[:, 1]\n",
    "        y_validate_pred = y_validate_prob_pred\n",
    "\n",
    "        # Fit Best Model\n",
    "        print(f'Fitting {estimator}.')\n",
    "        estimator.set_params(**estimator.get_params())\n",
    "        estimator = estimator.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate Model\n",
    "        table_df, mean_validation_score, explained_variance, accuracy, precision, recall, f1, mcc, loss, report, cm, cm_normalized = evaluation(\n",
    "            estimator, col, vectorizer_name, selector_name, classifier_name, X_validate, y_validate, y_validate_pred, y_validate_prob_pred, best_score, scoring\n",
    "        )\n",
    "\n",
    "        # Save Vectorizer, Selector, and Classifier\n",
    "        saving_model_and_table(col, table_df, estimator, vectorizer_name, selector_name, classifier_name)\n",
    "\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa476da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f101e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
