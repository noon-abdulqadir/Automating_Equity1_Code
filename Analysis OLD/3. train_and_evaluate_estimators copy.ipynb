{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9bfda79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Cannot change to a different GUI toolkit: widget. Using notebook instead.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %%\n",
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### Install packages and import\n",
    "# %%\n",
    "# #################################### PLEASE INSTALL LATEST CHROME WEBDRIVER #####################################\n",
    "# Uncomment to run as required\n",
    "# #     --install-option=\"--chromedriver-version= *.**\" \\\n",
    "#   --install-option=\"--chromedriver-checksums=4fecc99b066cb1a346035bf022607104,058cd8b7b4b9688507701b5e648fd821\"\n",
    "# %%\n",
    "# ##### COPY THE LINES IN THIS COMMENT TO THE TOP OF NEW SCRIPTS #####\n",
    "# # Function to import this package to other files\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# main_dir = str(Path(code_dir).parents[0])\n",
    "# scraped_data = f'{code_dir}/scraped_data'\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#
    "#
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "main_dir = str(Path(code_dir).parents[0])\n",
    "scraped_data = f'{code_dir}/scraped_data'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.post_collection_processing import *\n",
    "# from setup_module.classification import *\n",
    "from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "
    "
    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b6e3cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bcc3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e6c7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_pickle(f'{data_dir}df_manual_for_trainning.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4cc7116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Warmth Counts:\n",
      "0.000    4463\n",
      "1.000    1515\n",
      "Name: Warmth, dtype: int64\n",
      "Warmth Mean: 0.25\n",
      "Warmth Standard Deviation: 0.44\n",
      "--------------------\n",
      "--------------------\n",
      "Competence Counts:\n",
      "0.000    3346\n",
      "1.000    2632\n",
      "Name: Competence, dtype: int64\n",
      "Competence Mean: 0.44\n",
      "Competence Standard Deviation: 0.5\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Training Dataset: Warmth and Competence Sentence Counts')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize data balance\n",
    "analysis_columns = [\n",
    "    'Warmth',\n",
    "    'Competence'\n",
    "]\n",
    "\n",
    "# print('-'*20)\n",
    "# df_manual.info()\n",
    "for col in analysis_columns:\n",
    "    print('-'*20)\n",
    "    print(f'{col} Counts:\\n{df_manual[col].value_counts()}')\n",
    "    print(f'{col} Mean: {df_manual[col].mean().round(2).astype(float)}')\n",
    "    print(f'{col} Standard Deviation: {df_manual[col].std().round(2).astype(float)}')\n",
    "    print('-'*20)\n",
    "\n",
    "warm_comp_count = (\n",
    "    df_manual[analysis_columns]\n",
    "    .reset_index()\n",
    "    .groupby(analysis_columns)\n",
    "    .count()\n",
    "    .sort_values(by='index')\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle('Training Dataset: Warmth and Competence Sentence Counts', fontsize=16.0)\n",
    "warm_comp_count.plot(kind='barh', stacked=True, legend=True, color='blue', ax=ax).grid(\n",
    "    axis='y'\n",
    ")\n",
    "\n",
    "# for save_format in ['eps', 'png']:\n",
    "#     fig.savefig(f'{data_dir}/plots/Manual Warmth and Competence Sentence Counts.{save_format}', format=save_format, dpi=3000, bbox_inches='tight')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed58def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(sentences, embeddings_index=None, external_glove=True, extra_preprocessing_enabled=False):\n",
    "\n",
    "    if external_glove is False and embeddings_index is None:\n",
    "        embeddings_index= get_glove()\n",
    "\n",
    "    if extra_preprocessing_enabled is False:\n",
    "        words = sentences\n",
    "\n",
    "    elif extra_preprocessing_enabled is True:\n",
    "        stop_words = set(sw.words('english'))\n",
    "        words = str(sentences).lower()\n",
    "        words = word_tokenize(words)\n",
    "        words = [w for w in words if (w not in stop_words) and (w.isalpha())]\n",
    "\n",
    "    M = []\n",
    "\n",
    "    try:\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(embeddings_index[w])\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        M = np.array(M)\n",
    "        v = M.sum(axis='index')\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(300)\n",
    "\n",
    "        return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    except Exception:\n",
    "        return np.zeros(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "366c0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_manual, col, text_col):\n",
    "    # BOW Split\n",
    "    print('Splitting data into training and test sets.')\n",
    "    df_manual.dropna(subset=['Warmth', 'Competence', text_col], how='any', inplace=True)\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_manual, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([x for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([x for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([x for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "#     prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "#     prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1af0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding with Fasttext\n",
    "def word_embedding(df_jobs, X_train, feature_names):\n",
    "\n",
    "    sentences = df_jobs['2grams_gensim']\n",
    "\n",
    "    fasttext_model = FastText(sentences, window=3, min_count=1, sorted_vocab=1)\n",
    "\n",
    "    row=0\n",
    "    errors=0\n",
    "    for sent in tqdm.tqdm(sentences):\n",
    "        sent_vec = np.zeros(100)\n",
    "        weight_sum =0\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # weight = fasttext_model.wv.get_vector(word)\n",
    "                # weight_sum += weight\n",
    "                # sent_vec += weight\n",
    "                vec = fasttext_model.wv[word]\n",
    "                feat = X_train[row, feature_names.index(word)]\n",
    "                sent_vec += (vec * feat)\n",
    "                weight_sum += feat\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "        sent_vec /= weight_sum\n",
    "                # print(np.isnan(np.sum(sent_vec)))\n",
    "    sent_vectors = [sent_vec]\n",
    "    row += 1\n",
    "    print(f'errors noted: {str(errors)}')\n",
    "\n",
    "    return fasttext_model, sent_vectors\n",
    "\n",
    "def emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names):\n",
    "\n",
    "    # Get words and offsets\n",
    "    train_words, train_offsets = train_offset(X_train, 'train')\n",
    "    test_words, test_offsets = train_offset(X_test, 'test')\n",
    "    validate_words, validate_offsets = train_offset(X_validate, 'validate')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        if plots_enabled:\n",
    "            sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "\n",
    "        embs = load_glove_with_vocabulary(vocabulary_map, feature_names, print_enabled=print_enabled)\n",
    "        emb_model = BagOfEmbeddings(embs, dropout=0.1, hidden_dim=75, embedding_mode='mean')\n",
    "        print(f'Embedding Model: {emb_model}')\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(emb_model.parameters(), lr = 0.001)\n",
    "        torch.manual_seed(random_state)\n",
    "        losses = run_training(epochs=1, emb_model=emb_model, optimizer=optimizer, loss_fn=loss,\n",
    "                            all_words=train_words, all_offsets=train_offsets, all_targets=y_train,\n",
    "                            batch_size=32)\n",
    "        print(\"Training avg emb_model complete!\")\n",
    "\n",
    "        print(\"Evaluating test set\")\n",
    "        batch_losses, outputs = run_test(emb_model=emb_model, loss_fn=loss,\n",
    "                            all_words=test_words, all_offsets=test_offsets, all_targets=y_train,\n",
    "                            batch_size=256)\n",
    "\n",
    "        print(\"outputs.shape\", outputs.shape)\n",
    "\n",
    "        boe_pred = outputs.detach().numpy()\n",
    "\n",
    "        best_threshold_boe, best_score_boe = calculate_best_threshold(y_test[:300], boe_pred[:300], scoring, print_enabled)\n",
    "\n",
    "        print(\"boe_pred:\\n\", boe_pred[10])\n",
    "\n",
    "        print(\"Evaluating validate outputs\")\n",
    "        _, validate_outputs = run_test(emb_model=emb_model, loss_fn=None,\n",
    "                            all_words=validate_words, all_offsets=validate_offsets, all_targets=None,\n",
    "                            batch_size=256)\n",
    "\n",
    "        boe_validate_pred = validate_outputs.detach().numpy()\n",
    "        print(\"boe_validate_pred:\\n\", boe_validate_pred[10])\n",
    "        print(\"Validate outputs done\")\n",
    "\n",
    "    else:\n",
    "        vocabulary_map = None\n",
    "        boe_pred = None\n",
    "        boe_validate_pred = None\n",
    "        best_threshold_boe = None\n",
    "        best_score_boe = None\n",
    "\n",
    "    return vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df0beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and target arrays\n",
    "def vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate):\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} passed ============================'\n",
    "    )\n",
    "\n",
    "    refit_vectorizer = vectorizer\n",
    "\n",
    "    # prepared_X_train = X_train.to_list()\n",
    "    # prepared_y_train = y_train.to_list()\n",
    "    # prepared_X_test = X_test.to_list()\n",
    "    # prepared_y_test = y_test.to_list()\n",
    "    # prepared_X_validate = X_validate.to_list()\n",
    "    # prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    prepared_X_train, prepared_y_train, prepared_X_test, prepared_y_test, prepared_X_validate, prepared_y_validate = list(map(lambda x: x.to_list(), [X_train, y_train, X_test, y_test, X_validate, y_validate]))\n",
    "\n",
    "    prepared_text = vectorizer.fit_transform(prepared_X_train+prepared_X_test+prepared_X_validate)\n",
    "\n",
    "    # BOW fit transform\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "    # Oversampling to fix imbalance\n",
    "    if (resampling_enabled is True) and (col == 'Warmth'):\n",
    "        X_train, y_train = resample_data(X_train, y_train, col, resampling_enabled, resampling_method)\n",
    "\n",
    "    # Get feature names\n",
    "    X_train, vectorizer, dtf_features, X_names, feature_names = get_feature_name_and_refit_X_train_on_chi_test(train, X_train, vectorizer, refit_vectorizer)\n",
    "    unique_features = set(feature_names)\n",
    "\n",
    "    # BOW fit\n",
    "    print('Fitting and transforming data.')\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_validate = vectorizer.transform(X_validate)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_test = selector.transform(X_test)\n",
    "        X_validate = selector.transform(X_validate)\n",
    "        # Get feature names\n",
    "        feature_names = selector.get_feature_names_out(vocabulary=X_names)\n",
    "        unique_features = set(feature_names)\n",
    "\n",
    "    # y to numpy array\n",
    "    y_train = torch.from_numpy(np.array(y_train)).float()\n",
    "    y_test = torch.from_numpy(np.array(y_test)).float()\n",
    "    if print_enabled:\n",
    "        print(f'Train targets: {y_train}')\n",
    "        print(f'Test targets: {y_test}')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        if plots_enabled:\n",
    "            sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "\n",
    "    # vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe = emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names)\n",
    "\n",
    "    # fasttext_model, sent_vectors = word_embedding(df_jobs, X_train, feature_names)\n",
    "\n",
    "    return vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9e0b8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name, save_enabled=save_enabled):\n",
    "\n",
    "    # Save DF of predictions\n",
    "    labels = (y_final_validate_prob_pred > best_threshold).astype(int)\n",
    "    df_preds = pd.DataFrame({f'{str(text_col)}': test[f'{str(text_col)}'], \"prediction\": labels})\n",
    "    if save_enabled:\n",
    "        df_preds.to_csv(f'{df_dir}df_preds_{str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format_backup}', index=False)\n",
    "\n",
    "    return df_preds\n",
    "\n",
    "\n",
    "def augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled):\n",
    "\n",
    "    num = len(test) // 2\n",
    "\n",
    "    best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "    if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "        num = len(boe_pred) // 2\n",
    "\n",
    "        X_final_augmented_train = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "            \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[:num],\n",
    "            \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "        y_final_augmented_train = y_test[:num]\n",
    "\n",
    "        X_final_augmented_test = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "            \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[num:],\n",
    "            \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "        y_final_augmented_test = y_test[num:]\n",
    "\n",
    "        final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "        if hasattr(final_classifier, 'predict_proba'):\n",
    "            y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "        elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "            y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "        best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "        X_final_augmented_validate = pd.DataFrame({\n",
    "            \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "            \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "            \"num_words\": validate[\"num_words\"].values,\n",
    "            \"num_chars\": validate[\"num_chars\"].values})\n",
    "        y_final_augmented_validate = y_validate\n",
    "\n",
    "        y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "        df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "        final_classifier = classifier\n",
    "        X_final_augmented_validate = X_test\n",
    "        y_final_augmented_validate = y_test\n",
    "        y_final_validate_prob_pred = y_test_prob_pred\n",
    "        df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    return final_classifier, X_final_augmented_validate, y_final_augmented_validate, y_final_validate_prob_pred, best_threshold, best_score, df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a69ae3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classifier\n",
    "def classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name):\n",
    "    print('\\n')\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    "    )\n",
    "    num = len(test) // 2\n",
    "\n",
    "    # BOW model\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        X_train = X_train.todense()\n",
    "        X_test = X_test.todense()\n",
    "        X_validate = X_validate.todense()\n",
    "\n",
    "    if classifier_name == 'Sequential':\n",
    "        classifier.compile(loss='categorical_crossentropy')\n",
    "    if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "        classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "    # final_classifier = classifier\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    classifier = SelectFromModel(estimator=classifier, prefit=True).fit(X_train, y_train)\n",
    "\n",
    "    ### NEW ### https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "    # summarize feature importance\n",
    "    importance = classifier.coef_\n",
    "    for i,v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "    # plot feature importance\n",
    "    pyplot.bar([x for x in range(len(importance))], importance)\n",
    "    pyplot.show()\n",
    "\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "        y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "    elif hasattr(classifier, '_predict_proba_lr'):\n",
    "        y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "        y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "    else:\n",
    "        raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "    # final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n",
    "\n",
    "    return classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c2107c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring):\n",
    "\n",
    "    # Evaluate\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'EVALUATING FITTED MODEL - {vectorizer_name} + {classifier_name}: ',\n",
    "        has_fit_parameter(classifier, 'sample_weight'),\n",
    "    )\n",
    "    # 5 cross_validation score\n",
    "    print(f'Cross Validating - {vectorizer_name} + {classifier_name}.')\n",
    "    cross_validate_score = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Mean cross_validate scores - {vectorizer_name} + {classifier_name}: {cross_validate_score_noscoring.get(\"test_score\").mean()}'\n",
    "    )\n",
    "    numberoflabels = len(set((str(e) for e in y_test.to_list())))\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Mean Validation Score')\n",
    "    ] = float(cross_validate_score_noscoring.get('test_score').mean())\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Explained Variance')\n",
    "    ] = float(\n",
    "        cross_validate_score.get('test_explained_variance').mean()\n",
    "    )\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key, values in cross_validate_score.items():\n",
    "        if 'test' in key:\n",
    "            print(key, ' mean ', values.mean())\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "\n",
    "    # Predictions\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'============================ {str(col)} PREDICTIONS FOR {vectorizer_name.upper()} WITH {classifier_name.upper()} ============================'\n",
    "    )\n",
    "    print('\\n')\n",
    "    print(f'y_test_pred - {str(col)} - {vectorizer_name} + {classifier_name}:')\n",
    "    dic_y_mapping = dict(enumerate(np.unique(y_train)))\n",
    "    inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "    y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        # y_test_pred = y_test_pred.to_list()\n",
    "        y_test_pred = classifier.predict(X_test.todense())\n",
    "    predicted = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred]\n",
    "    acc_roc_f1 = evaluate_print(classifier_name + '   |   ', y_test, y_test_pred)\n",
    "    cm, precision, recall, accuracy, f1, mcc, best_threshold, best_score, report = evaluation(\n",
    "        y_test, y_test_pred, scoring, print_enabled\n",
    "    )\n",
    "\n",
    "    true_negative = cm[0][0]\n",
    "    false_positives = cm[0][1]\n",
    "    false_negatives = cm[1][0]\n",
    "    true_positives = cm[1][1]\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = float(best_threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Classification Report')\n",
    "    ] = report\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "\n",
    "    # Plot\n",
    "    heatmap = plot_confusion_matrix_percentage(col, cm, classifier_name, vectorizer_name)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        heatmap.figure.savefig(\n",
    "            f'{plot_save_path}Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "            )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # Log Loss Cross Entropy\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        probability_of_1 = y_test_prob_pred#[:, 1]\n",
    "\n",
    "        loss = log_loss(y_test, y_test_prob_pred)\n",
    "        print('\\n')\n",
    "        print('=' * 20)\n",
    "        print(f'Log Loss / Cross Entropy = {loss}')\n",
    "        print('=' * 20)\n",
    "        print('\\n')\n",
    "        table_df.loc[\n",
    "            (classifier_name),\n",
    "            (col, vectorizer_name, 'Log Loss/Cross Entropy'),\n",
    "        ] = float(loss)\n",
    "\n",
    "        # Explain Model\n",
    "        explained = explain_model(test, y_test, y_test_pred, y_test_prob_pred, y_train)\n",
    "        if plots_enabled:\n",
    "            explained.show_in_notebook(text=txt_instance, predict_proba=False)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    # Optimization\n",
    "    if optimization_enabled is True and hasattr(classifier, 'predict_log_proba'):\n",
    "        classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt = optimize_model(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            probability_of_1,\n",
    "            vectorizer_name,\n",
    "            classifier_name,\n",
    "            table_df,\n",
    "            score)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred_new, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        print(f'Precision Recall Curve AFTER OPTIMIZATION - {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        get_pr_curve(X_test, y_test, recall_opt, precision_opt, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    if hasattr(classifier, 'best_estimator_'):\n",
    "        ohe_cols = list(\n",
    "            classifier.best_estimator_.named_steps['vectorizer']\n",
    "            .named_transformers_['cat']\n",
    "            .named_steps['ohe']\n",
    "            .get_feature_names(input_features=categorical)\n",
    "        )\n",
    "        num_feats = list(numerical)\n",
    "        num_feats.extend(ohe_cols)\n",
    "        feat_imp = eli5.explain_weights_df(\n",
    "            classifier.best_estimator_.named_steps['classifier'],\n",
    "            top=10,\n",
    "            feature_names=num_feats,\n",
    "        )\n",
    "        print(\n",
    "            f'feat_imp - {str(col)} - {vectorizer_name} + {classifier_name}: ',\n",
    "            feat_imp,\n",
    "        )\n",
    "        print('-' * 20)\n",
    "        print('\\n')\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_manual,\n",
    "        print_enabled,\n",
    "    )\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    return classifier, table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11cc66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "def get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col):\n",
    "    roc_curve = metrics.plot_roc_curve(classifier, X_test, y_test)\n",
    "    plt.title(\n",
    "        f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    if save_enabled is True:\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC {str(col)} - {classifier_name} - {vectorizer_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, probability_of_1, pos_label=1\n",
    "    )\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(f'AUC {str(col)} - {vectorizer_name} + {classifier_name}:\\n', auc)\n",
    "\n",
    "    print('ROC CURVE FOR PREDICTED PROBABILITIES')\n",
    "    bc = BinaryClassification(y_test, y_test_pred, labels=['0', '1'])\n",
    "    # Figures\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    bc.plot_roc_curve()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                            predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3,\n",
    "                label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                metrics.auc(fpr, tpr))\n",
    "                )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "            xlabel='False Positive Rate',\n",
    "            ylabel=\"True Positive Rate (Recall)\",\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    return table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b08c3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "def get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col):\n",
    "\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    pr_curve = plt.figure(figsize=(4.0, 4.0))\n",
    "    plt.plot(\n",
    "        [0, 1], [no_skill, no_skill], linestyle='--', label='No Skill'\n",
    "    )\n",
    "    plt.plot(\n",
    "        recall, precision, marker='.', label=f'AUC = {auc}'\n",
    "    )\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\n",
    "        f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=12.0,\n",
    "    )\n",
    "    plt.ylabel('Precision', fontsize=12.0)\n",
    "    plt.xlabel('Recall', fontsize=12.0)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        pr_curve.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "            y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3,\n",
    "                label=f'{classes[i]} (area={metrics.auc(recall, precision):0.2f})')\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "            ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a59d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Model\n",
    "def optimize_model(classifier, X_test, y_test, probability_of_1, vectorizer_name, classifier_name, table_df, scoring):\n",
    "    if hasattr(classifier, 'predict_log_proba'):\n",
    "\n",
    "        y_test_prob_log_pred = classifier.predict_log_proba(X_test)[:, 1]\n",
    "\n",
    "        # calculate pr-curve\n",
    "        (\n",
    "            precision_opt,\n",
    "            recall_opt,\n",
    "            thresholds_opt,\n",
    "        ) = metrics.precision_recall_curve(\n",
    "            y_test, probability_of_1\n",
    "        )\n",
    "        # convert to f score\n",
    "        fscore_opt = (2 * precision_opt * recall_opt) / (\n",
    "            precision_opt + recall_opt\n",
    "        )\n",
    "        # locate the index of the largest f score\n",
    "        ix_opt = argmax(fscore_opt)\n",
    "        best_thresh_opt = thresholds_opt[ix_opt]\n",
    "        print('=' * 20)\n",
    "        print(\n",
    "            f'Best Threshold: {best_thresh_opt}, F-Score={fscore_opt[ix_opt]}'\n",
    "        )\n",
    "        print(f'Optimal threshold: {np.exp(best_thresh_opt)}')\n",
    "        y_test_pred_new = np.where(\n",
    "            y_test_prob_log_pred[:, 1] > best_thresh_opt, 1, 0\n",
    "        )\n",
    "        print(f'New y_test_pred {str(col)} - {vectorizer_name} + {classifier_name}:\\n{y_test_pred_new}')\n",
    "\n",
    "        print(\n",
    "            f'SCORES FOR {str(col)} - {vectorizer_name} + {classifier_name} AFTER OPTIMIZATION:'\n",
    "        )\n",
    "        cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, best_threshold_opt, best_score_opt, report_opt = evaluation(\n",
    "            y_test, y_test_pred_new, scoring, print_enabled\n",
    "        )\n",
    "\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Accuracy_opt')\n",
    "        ] = float(accuracy_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Precision_opt')\n",
    "        ] = float(precision_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Recall_opt')\n",
    "        ] = float(recall_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'F1-score_opt')\n",
    "        ] = float(f1_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient_opt'),\n",
    "        ] = float(mcc_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold_opt'),\n",
    "        ] = float(best_threshold_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score_opt'),\n",
    "        ] = float(best_score_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Classification Report_opt'),\n",
    "        ] = report_opt\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Confusion Matrix_opt'),\n",
    "        ] = str(cm_opt)\n",
    "\n",
    "        print('=' * 20)\n",
    "\n",
    "    elif not hasattr(classifier, 'predict_log_proba'):\n",
    "        print('Classifier has no Attribute predict_log_proba.')\n",
    "\n",
    "    return classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e0b9401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df, data_dir=data_dir):\n",
    "#     if save_enabled is True:\n",
    "#         if task_enabled is False:\n",
    "\n",
    "    classifier_save_path = (\n",
    "        f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    vectorizer_save_path = (\n",
    "        f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "    selector_save_path = (\n",
    "        f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "    )\n",
    "\n",
    "    # Save classifier\n",
    "    print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "    table_df.to_csv(table_save_path + csv_file_name)\n",
    "    table_df.to_pickle(table_save_path + pickle_file_name)\n",
    "    table_df.to_excel(table_save_path + excel_file_name)\n",
    "    table_df.to_latex(table_save_path + latex_file_name)\n",
    "    table_df.to_markdown(table_save_path + markdown_file_name)\n",
    "\n",
    "    with open(classifier_save_path, 'wb') as f:\n",
    "        joblib.dump(classifier, f)\n",
    "    with open(vectorizer_save_path, 'wb') as f:\n",
    "        joblib.dump(vectorizer, f)\n",
    "    if select_best_enabled is True:\n",
    "        with open(selector_save_path, 'wb') as f:\n",
    "            joblib.dump(selector, f)\n",
    "\n",
    "    elif save_enabled is False:\n",
    "        print('Saving Model and Table is disabled.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3936a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring):\n",
    "\n",
    "    if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "        classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "    classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name)\n",
    "    classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "    return classifier, table_df, X_test, y_test, y_test_prob_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1b13febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name):\n",
    "\n",
    "    print('Not using Search')\n",
    "\n",
    "    if use_dict_for_classifiers_vectorizers is True:\n",
    "        print('Using dict for classifiers and vectorizers.')\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "            vectorizer.set_params(**vectorizer_params)\n",
    "            vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "            for classifier_name, classifier_and_params in classifiers.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "                classifier.set_params(**classifier_params)\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    elif use_dict_for_classifiers_vectorizers is False:\n",
    "        print('Using list for classifiers and vectorizers.')\n",
    "        for vectorizer in vectorizers_lst:\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "            for classifier in classifiers_lst:\n",
    "                classifier_name = classifier.__class__.__name__\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    return df_manual, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6cf4591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring):\n",
    "\n",
    "    print('Using Search')\n",
    "\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "\n",
    "    models_save_path = f'{data_dir}classification models/'\n",
    "    table_save_path = f'{data_dir}output tables/'\n",
    "    pickle_file_name = 'Classifiers Table.pkl'\n",
    "    csv_file_name = 'Classifiers Table.csv'\n",
    "    excel_file_name = 'Classifiers Table.xlsx'\n",
    "    latex_file_name = 'Classifiers Table.tex'\n",
    "    markdown_file_name = 'Classifiers Table.md'\n",
    "\n",
    "    # Vectorization\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        # Selection\n",
    "        for selector_name, selector_and_params in selectors_pipe.items():\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            # Classification\n",
    "            for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "\n",
    "                # Pipeline\n",
    "#                 if select_best_enabled is True:\n",
    "                ## Steps\n",
    "                steps = [\n",
    "                    (vectorizer_name, vectorizer),\n",
    "                    (selector_name, selector),\n",
    "                    (classifier_name, classifier)\n",
    "                ]\n",
    "                ## Params\n",
    "                param_grid = {\n",
    "                    **vectorizer_params,\n",
    "                    **selector_params,\n",
    "                    **classifier_params,\n",
    "                }\n",
    "                ## Pipeline\n",
    "                pipe = Pipeline(steps=steps)\n",
    "\n",
    "                ## Vectorizers, selectors, classifiers\n",
    "                vectorizer = pipe[:-2]\n",
    "                selector = pipe[:-1]\n",
    "                classifier = pipe[:]\n",
    "\n",
    "#                 elif select_best_enabled is False:\n",
    "#                     ## Steps\n",
    "#                     steps = [\n",
    "#                         (vectorizer_name, vectorizer),\n",
    "#                         (classifier_name, classifier)\n",
    "#                     ]\n",
    "#                     ## Params\n",
    "#                     param_grid = {\n",
    "#                         **vectorizer_params,\n",
    "#                         **classifier_params,\n",
    "#                     }\n",
    "#                     ## Pipeline\n",
    "#                     pipe = Pipeline(steps=steps)\n",
    "\n",
    "#                     ## Vectorizers, selectors, classifiers\n",
    "#                     vectorizer = pipe[:-1]\n",
    "#                     classifier = pipe[:]\n",
    "\n",
    "                # Search\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_distributions=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scores,\n",
    "                    cv=cv,\n",
    "                    refit=scores[0],\n",
    "                    return_train_score=True,\n",
    "                    verbose=3,\n",
    "                )\n",
    "\n",
    "                # Fit SearchCV\n",
    "                searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "                # Best Parameters\n",
    "                best_index = searchcv.best_index_\n",
    "                cv_results = sorted(searchcv.cv_results_)\n",
    "                best_params = searchcv.best_params_\n",
    "                classifier = searchcv.best_estimator_\n",
    "                y_train_pred = classifier.predict(X_train)\n",
    "                best_score = searchcv.best_score_\n",
    "                n_splits = searchcv.n_splits_\n",
    "\n",
    "                print('=' * 20)\n",
    "                print(f'Best index for {scores[0]}: {best_index}')\n",
    "                print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                print(f'Best score for {scores[0]}: {best_score}')\n",
    "                print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                print('-' * 20)\n",
    "                report = classification_report(y_train, y_train_pred)\n",
    "                print(f'Classification Report:\\n{report}')\n",
    "                ConfusionMatrixDisplay.from_estimator(\n",
    "                    searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                )\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                print('=' * 20)\n",
    "\n",
    "                # Make the predictions\n",
    "                score = searchcv.score(X_test, y_test)\n",
    "                y_test_pred = searchcv.predict(X_test)\n",
    "                if hasattr(searchcv, 'predict_proba'):\n",
    "                    y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                    y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                # Fit Best Model\n",
    "                print(f'Fitting {classifier}.')\n",
    "                classifier.set_params(**classifier.get_params())\n",
    "                classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate Model\n",
    "                classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_df)\n",
    "\n",
    "    return df_manual, searchcv, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2f86d",
   "metadata": {},
   "source": [
    "# Training Supervised Model: Warmth and Competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16d94250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "\n",
      "\n",
      "============================ STARTING PROCESSING WARMTH ============================\n",
      "\n",
      "\n",
      "--------------------\n",
      "Splitting data into training and test sets.\n",
      "Using Search\n",
      "Using dict for classifiers and vectorizers.\n",
      "Fitting 30 folds for each of 10 candidates, totalling 300 fits\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.9s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   1.2s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x12717d120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x1268c4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x12717d1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x1268c4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x1268c4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   1.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.9s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.8s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1289f5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1289f51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x128418040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x128418160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x128418280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   1.4s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.8s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 19/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.8s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 21/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x1209c5120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x1209c51b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x117a18040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x117a18160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x117a18280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=all, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=1000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 27/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 7/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.0s\n",
      "[CV 6/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.9s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   1.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 30/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=5000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 16/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=100, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 9/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 15/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=stratified, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 1/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 25/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=uniform, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_regression at 0x10e2ed120>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 11/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 26/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=prior, SelectKBest__k=500, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 18/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=stratified, SelectKBest__k=all, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 20/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.2, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=10000, SelectKBest__score_func=<function mutual_info_classif at 0x10e2ed1b0>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.1s\n",
      "[CV 2/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n",
      "[CV 12/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 28/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=uniform, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 8/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 14/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 24/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.8, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_classif at 0x10dde4040>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 3/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.5s\n",
      "[CV 10/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 17/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.7s\n",
      "[CV 23/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=True, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=42, DummyClassifier__strategy=most_frequent, SelectKBest__k=5000, SelectKBest__score_func=<function chi2 at 0x10dde4160>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 4/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 22/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.3s\n",
      "[CV 29/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.85, CountVectorizer__min_df=0.15, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=most_frequent, SelectKBest__k=500, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.2s\n",
      "[CV 5/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.4s\n",
      "[CV 13/30] END CountVectorizer__analyzer=word, CountVectorizer__lowercase=False, CountVectorizer__max_df=0.75, CountVectorizer__min_df=0.25, CountVectorizer__ngram_range=(1, 3), DummyClassifier__random_state=200, DummyClassifier__strategy=constant, SelectKBest__k=10000, SelectKBest__score_func=<function f_regression at 0x10dde4280>; accuracy: (train=nan, test=nan) explained_variance: (train=nan, test=nan) f1: (train=nan, test=nan) matthews_corrcoef: (train=nan, test=nan) precision: (train=nan, test=nan) recall: (train=nan, test=nan) roc_auc: (train=nan, test=nan) total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                     | 0/2 [00:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1400, in fit_transform\n    X, self.stop_words_ = self._limit_features(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1252, in _limit_features\n    raise ValueError(\nValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 21\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m      9\u001b[0m         df_manual[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# BOW Split\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate \u001b[38;5;241m=\u001b[39m split_data(df_manual, col, text_col)\n\u001b[0;32m---> 21\u001b[0m     df_manual, searchcv, classifier, vectorizers, selector, table_df \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_manual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[85], line 83\u001b[0m, in \u001b[0;36mpipe\u001b[0;34m(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring)\u001b[0m\n\u001b[1;32m     71\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     72\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipe,\n\u001b[1;32m     73\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Fit SearchCV\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m searchcv \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Best Parameters\u001b[39;00m\n\u001b[1;32m     86\u001b[0m best_index \u001b[38;5;241m=\u001b[39m searchcv\u001b[38;5;241m.\u001b[39mbest_index_\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 300 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n300 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1400, in fit_transform\n    X, self.stop_words_ = self._limit_features(\n  File \"/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1252, in _limit_features\n    raise ValueError(\nValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ STARTING PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (\n",
    "        len(\n",
    "            df_manual[\n",
    "                df_manual[str(col)].map(\n",
    "                    df_manual[str(col)].value_counts() > 50\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        != 0\n",
    "    ):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate = split_data(df_manual, col, text_col)\n",
    "\n",
    "        df_manual, searchcv, classifier, vectorizers, selector, table_df = pipe(df_manual, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, vectorizers, selector, classifiers, col, text_col, scoring)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ FINISHED PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not using Search')\n",
    "\n",
    "if use_dict_for_classifiers_vectorizers is True:\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "        vectorizer.set_params(**vectorizer_params)\n",
    "        vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map = vectorize(vectorizer, vectorizer_name, selector, df_manual, col, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate)\n",
    "\n",
    "        for classifier_name, classifier_and_params in classifiers.items():\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "            classifier.set_params(**classifier_params)\n",
    "            # classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94000ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "    classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "# classifier, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, y_test_prob_pred, y_validate_prob_pred = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name)\n",
    "# classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print(\n",
    "    f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    ")\n",
    "num = len(test) // 2\n",
    "\n",
    "# BOW model\n",
    "if classifier_name == 'GaussianNB':\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "    X_validate = X_validate.todense()\n",
    "\n",
    "if classifier_name == 'Sequential':\n",
    "    classifier.compile(loss='categorical_crossentropy')\n",
    "if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "    classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "final_classifier = classifier\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "if hasattr(classifier, 'predict_proba'):\n",
    "    y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "    y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "elif hasattr(classifier, '_predict_proba_lr'):\n",
    "    y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "    y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "else:\n",
    "    raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "# final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(test) // 2\n",
    "\n",
    "best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "    num = len(boe_pred) // 2\n",
    "\n",
    "    X_final_augmented_train = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "        \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[:num],\n",
    "        \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "    y_final_augmented_train = y_test[:num]\n",
    "\n",
    "    X_final_augmented_test = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "        \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[num:],\n",
    "        \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "    y_final_augmented_test = y_test[num:]\n",
    "\n",
    "    final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "    if hasattr(final_classifier, 'predict_proba'):\n",
    "        y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "    elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "        y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "    best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "    X_final_augmented_validate = pd.DataFrame({\n",
    "        \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "        \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "        \"num_words\": validate[\"num_words\"].values,\n",
    "        \"num_chars\": validate[\"num_chars\"].values})\n",
    "    y_final_augmented_validate = y_validate\n",
    "\n",
    "    y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "    df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "    final_classifier = classifier\n",
    "    X_final_augmented_validate = X_test\n",
    "    y_final_augmented_validate = y_test\n",
    "    y_final_validate_prob_pred = y_test_prob_pred\n",
    "    df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:num].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ffb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:num]\n",
    "y_test_pred = y_test_prob_pred[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = -1\n",
    "best_score = -1\n",
    "for threshold in np.arange(0.01, 0.801, 0.01):\n",
    "    threshold = np.round(threshold, 2)\n",
    "\n",
    "    # globals()[f'metrics.{scoring.lower()}_score']\n",
    "    if scoring.lower() == 'recall':\n",
    "        scorer = metrics.recall_score\n",
    "    elif scoring.lower() == 'f1 score':\n",
    "        scorer = metrics.f1_score\n",
    "    else:\n",
    "        raise ValueError(f'{scoring.title()} is not a valid score')\n",
    "\n",
    "    emb_model_score = scorer(y_true=y_test, y_pred=(y_test_pred > threshold).astype(int))\n",
    "    if emb_model_score > best_score:\n",
    "        best_score = emb_model_score\n",
    "        best_threshold = threshold\n",
    "    if print_enabled:\n",
    "        print(f'{scoring.title()} at threshold {threshold}: {emb_model_score}')\n",
    "print(f'{scoring.title()} at best threshold {best_threshold}: {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd65a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5b0d7544f82776c2b902af54887e7cde1aa7d2da4fd982551ffc3948bf7522f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
