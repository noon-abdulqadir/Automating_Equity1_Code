{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %%\n",
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### Install packages and import\n",
    "# %%\n",
    "# #################################### PLEASE INSTALL LATEST CHROME WEBDRIVER #####################################\n",
    "# Uncomment to run as required\n",
    "# #     --install-option=\"--chromedriver-version= *.**\" \\\n",
    "#   --install-option=\"--chromedriver-checksums=4fecc99b066cb1a346035bf022607104,058cd8b7b4b9688507701b5e648fd821\"\n",
    "# %%\n",
    "# ##### COPY THE LINES IN THIS COMMENT TO THE TOP OF NEW SCRIPTS #####\n",
    "# # Function to import this package to other files\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# main_dir = str(Path(code_dir).parents[0])\n",
    "# scraped_data = f'{code_dir}/scraped_data'\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "main_dir = str(Path(code_dir).parents[0])\n",
    "scraped_data = f'{code_dir}/scraped_data'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from setup_module.imports import *\n",
    "from setup_module.scraping import *\n",
    "from setup_module.post_collection_processing import *\n",
    "from setup_module.params import *\n",
    "from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_word2vec(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    w2v_model.train(\n",
    "        sentences,\n",
    "        total_examples=w2v_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build w2v_vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    w2v_vocab = list(w2v_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "            # print(f'{sector} 300: {w2v_model_300.wv[word]}')\n",
    "            # print(f'{sector} 100: {w2v_model_100.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(w2v_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {w2v_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {w2v_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return w2v_vocab, w2v_model\n",
    "\n",
    "def word2vec_embeddings(sentences, w2v_vocab, w2v_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in w2v_vocab]\n",
    "\n",
    "    return np.mean(w2v_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_fasttext(\n",
    "    df, ngram_number, embedding_library, size = 300,\n",
    "    words = ['she', 'he', 'support', 'leader', 'management', 'team', 'business', 'customer', 'risk', 'build', 'computer', 'programmer'],\n",
    "    t = time.time(), cores = multiprocessing.cpu_count(),\n",
    "):\n",
    "    sentences = df[f'Job Description {embedding_library}_{ngram_number}grams_original_list'].values\n",
    "\n",
    "    ft_model = FastText(\n",
    "        sentences=sentences,\n",
    "        vector_size=size,\n",
    "        min_count=0,\n",
    "        window=2,\n",
    "        sample=6e-5,\n",
    "        alpha=0.03,\n",
    "        min_alpha=0.0007,\n",
    "        negative=20,\n",
    "        workers=cores - 1,\n",
    "        sg = 1,\n",
    "    )\n",
    "\n",
    "    ft_model.build_vocab(sentences, progress_per=10000)\n",
    "    print(f'Time to train the model for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "\n",
    "    ft_model.train(\n",
    "        sentences,\n",
    "        total_examples=ft_model.corpus_count,\n",
    "        epochs=30,\n",
    "        report_delay=1,\n",
    "    )\n",
    "\n",
    "    print(f'Time to build vocab for {size}: {round((time.time() - t) / 60, 2)} mins')\n",
    "    ft_vocab = list(ft_model.wv.index_to_key)\n",
    "\n",
    "    print(f'Checking words form list of length {len(words)}')\n",
    "    print(f'WORDS LIST: {words}')\n",
    "\n",
    "    for word in words:\n",
    "        print(f'Checking word:\\n{word.upper()}:')\n",
    "        try:\n",
    "            # print(f'{sector} 300: {ft_model_300.wv[word]}')\n",
    "            # print(f'{sector} 100: {ft_model_100.wv[word]}')\n",
    "            print(f'Length of {size} model vobal: {len(ft_vocab)}')\n",
    "            print(f'{size} - Positive most similar to {word}: {ft_model.wv.most_similar(positive=word, topn=5)}')\n",
    "            print(f'{size} - Negative most similar to {word}: {ft_model.wv.most_similar(negative=word, topn=5)}')\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "    return ft_vocab, ft_model\n",
    "\n",
    "def fasttext_embeddings(sentences, ft_vocab, ft_model, size=300):\n",
    "\n",
    "    sentences = [word for word in sentences if word in ft_vocab]\n",
    "\n",
    "    return np.mean(ft_model.wv[sentences], axis=0) if len(sentences) >= 1 else np.zeros(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(glove_file = f'{llm_path}/gensim/glove/glove.840B.300d.txt'):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf8') as glove:\n",
    "\n",
    "        for line in glove:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessed unlabeled dataframe\n",
    "print('Analyzing DF.')\n",
    "df_all = pd.read_pickle(f'{data_dir}df_manual_for_trainning.pkl')\n",
    "# n job ads = 21204\n",
    "\n",
    "# df_name = 'df_manual_mean'\n",
    "# df_all = pd.read_pickle(f'{df_dir}{df_name}_outliers.{file_save_format}')\n",
    "\n",
    "try:\n",
    "    df_all = df_all.drop(\n",
    "        ['Task_Mentioned', 'Task_Warmth', 'Task_Competence'],\n",
    "        axis=1,\n",
    "    )\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "\n",
    "# df_all = df_all.dropna(subset=dv_cols)\n",
    "print('DF Processed:')\n",
    "df_all.info()\n",
    "df_gender_age_info(df_all)\n",
    "df_all['English Requirement'].value_counts()\n",
    "df_all['Dutch Requirement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec Model\n",
    "for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "        build_train_func, embed_func, model_loader = embed_func_list\n",
    "\n",
    "        model = model_loader.load(\n",
    "            validate_path(\n",
    "                f'{data_dir}embeddings models/{embedding_library}_{ngram_number}grams_{embed_model_name}_model.model'\n",
    "            )\n",
    "        )\n",
    "\n",
    "        setattr(mod, f'model_{embed_model_name}_{ngram_number}grams', model)\n",
    "\n",
    "# ft_model_gensim = FastText.load(\n",
    "#     validate_path(\n",
    "#         f'{args[\"embeddings_save_path\"]}123grams_{embedding_library}_ft_model.model'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# word2vec_model300 = gensim_api.load('word2vec-google-news-300')\n",
    "# glove_model300 = gensim_api.load('glove-wiki-gigaword-300')\n",
    "# fasttext_model300 = gensim_api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# word_embedding_models = {'Word2Vec': word2vec_model300, 'GLoVe': glove_model300, 'fastText': fasttext_model300}\n",
    "\n",
    "embedding_models_dict['w2v'].append(gensim_api.load('word2vec-google-news-300'))\n",
    "embedding_models_dict['glove'] = [gensim_api.load('glove-wiki-gigaword-300')]\n",
    "embedding_models_dict['ft'].append(gensim_api.load('fasttext-wiki-news-subwords-300'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_models_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender_age_info(df_all)\n",
    "print('='*20)\n",
    "print('English Requirement:')\n",
    "df_all['English Requirement'].value_counts()\n",
    "print('='*20)\n",
    "print('Dutch Requirement:')\n",
    "df_all['Dutch Requirement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(df_all, n, sampling_enabled = True, random_state=random_state):\n",
    "    if sampling_enabled is True:\n",
    "        df_all_sample = df_all.sample(n=n, random_state=random_state).reset_index(drop=True)\n",
    "    elif sampling_enabled is False:\n",
    "        df_all_sample = df_all\n",
    "\n",
    "    print(f'Sample size: {len(df_all_sample)}')\n",
    "    # df_all_sample['Search Keyword'].isnull().values.any()\n",
    "#     df_all_sample.duplicated(subset=[\"Job ID\", 'Job Description_cleaned']).value_counts()\n",
    "\n",
    "    return df_all_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample_dfs(\n",
    "    df_all, model_sizes = model_sizes, n=300, sampling_enabled = True,\n",
    "    dfs_dict = {\n",
    "        # 'All Sample': {'categories': {'All Sample': defaultdict()}},\n",
    "        'Gender': {'categories': {'Female': defaultdict(), 'Mixed Gender': defaultdict(), 'Male': defaultdict(), }},\n",
    "        'Age': {'categories': {'Older': defaultdict(), 'Mixed Age': defaultdict(), 'Younger': defaultdict()}}\n",
    "    }\n",
    "):\n",
    "\n",
    "    df_all_sample = make_sample(df_all, n=n, sampling_enabled=sampling_enabled)\n",
    "\n",
    "    for gen in order_gender:\n",
    "        df_gen = df_all_sample.loc[df_all_sample['Gender'] == gen]\n",
    "        dfs_dict['Gender']['categories'][gen]['df'] = df_gen\n",
    "        print(f'Length of {gen}: {len(df_gen)}')\n",
    "\n",
    "    for age in order_age:\n",
    "        df_age = df_all_sample.loc[df_all_sample['Age'] == age]\n",
    "        dfs_dict['Age']['categories'][age]['df'] = df_age\n",
    "        print(f'Length of {age}: {len(df_age)}')\n",
    "\n",
    "    return dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict = make_sample_dfs(df_all, sampling_enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "            value['frequencies'] = defaultdict()\n",
    "            value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_freq'] = convert_frequency(\n",
    "                value,\n",
    "                f'Job Description {embedding_library}_{ngram_number}grams_abs_word_freq'\n",
    "            )\n",
    "            value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_perc'] = convert_frequency(\n",
    "                value,\n",
    "                f'Job Description {embedding_library}_{ngram_number}grams_abs_word_perc'\n",
    "            )\n",
    "            value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_perc_cum'] = convert_frequency(\n",
    "                value,\n",
    "                f'Job Description {embedding_library}_{ngram_number}grams_abs_word_perc_cum'\n",
    "            )\n",
    "            value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_freq_df'] = pd.DataFrame(\n",
    "                data=[\n",
    "                    value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_freq'],\n",
    "                    value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_perc'],\n",
    "                    value['frequencies'][f'{embedding_library}_{ngram_number}grams_abs_word_perc_cum']\n",
    "                ]).T.sort_values(0, ascending=False).rename(columns={0: 'abs_word_freq', 1: 'abs_word_perc', 2: 'abs_word_perc_cum'})\n",
    "\n",
    "            print('='*80)\n",
    "            print(f'{iv_cat.upper()}:')\n",
    "            print(f'{iv_cat} word frequency length: {len(value[\"frequencies\"][f\"{embedding_library}_{ngram_number}grams_abs_word_freq_df\"])}')\n",
    "            print(f'{iv_cat} word frequency sorted: {sorted(value[\"frequencies\"][f\"{embedding_library}_{ngram_number}grams_abs_word_freq\"], key=value[\"frequencies\"][f\"{embedding_library}_{ngram_number}grams_abs_word_freq\"].get, reverse=True)[:5]}')\n",
    "            print('-'*20)\n",
    "            print(f'{iv_cat} {embedding_library} {ngram_number}grams abs_word_freq_df:')\n",
    "            print('-'*20)\n",
    "            print(f'{value[\"frequencies\"][f\"{embedding_library}_{ngram_number}grams_abs_word_freq_df\"].head()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iv_cats['categories'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for (iv_cat, value), model_size in itertools.product(iv_cats['categories'].items(), model_sizes):\n",
    "        for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "            print(iv_cat.upper())\n",
    "            print('+'*30)\n",
    "            value[f'w2v_model_size_{model_size}'] = defaultdict()\n",
    "            value[f'w2v_model_size_{model_size}']['w2v_vocab'], value[f'w2v_model_size_{model_size}']['w2v_model'] = build_train_word2vec(\n",
    "                value['df'], size=model_size, ngram_number=ngram_number, embedding_library=embedding_library\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for (iv_cat, value), model_size in itertools.product(iv_cats['categories'].items(), model_sizes):\n",
    "        for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "            print(iv_cat.upper())\n",
    "            print('+'*30)\n",
    "            value[f'ft_model_size_{model_size}'] = defaultdict()\n",
    "            value[f'ft_model_size_{model_size}']['ft_vocab'], value[f'ft_model_size_{model_size}']['ft_model'] = build_train_fasttext(\n",
    "                value['df'], size=model_size, ngram_number=ngram_number, embedding_library=embedding_library\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_dict.keys() #iv\n",
    "dfs_dict['Gender'].keys() #iv_cat\n",
    "dfs_dict['Gender']['categories'].keys()\n",
    "dfs_dict['Gender']['categories']['Female'].keys() #iv_cat\n",
    "dfs_dict['Gender']['categories']['Female']['df'].keys() #df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    sentences_list = []\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        lst = value['df'][f'{n_gram}'].to_list()\n",
    "        sentences_list.append(' '.join([sentence for sentences in value['df'][f'{n_gram}'].to_list() for sentence in sentences if sentence]))\n",
    "        # print(value['df'][f'{n_gram}'].to_list())\n",
    "        # for df_name, df in value['df'].items():\n",
    "len(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "\n",
    "    print(f'Soft Cosine Similarity for {iv}:')\n",
    "\n",
    "    dfs_dict[f'{iv}']['word_embeddings'] = defaultdict()\n",
    "\n",
    "    sentence_list = []\n",
    "\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        sentences_list.append(' '.join([sentence for sentences in value['df'][f'{n_gram}'].to_list() for sentence in sentences if sentence]))\n",
    "\n",
    "    dictionary = corpora.Dictionary([sentences for sentences in sentences_list if sentences])\n",
    "    dfs_dict[f'{iv}']['word_embeddings']['dictionary'] = dictionary\n",
    "    print(f'{iv} Dictionary:\\n{dictionary}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    sentences_list = []\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        lst = value['df'][f'{n_gram}'].to_list()\n",
    "        sentences_list.append(' '.join([sentence for sentences in value['df'][f'{n_gram}'].to_list() for sentence in sentences if sentence]))\n",
    "        # print(value['df'][f'{n_gram}'].to_list())\n",
    "        # for df_name, df in value['df'].items():\n",
    "len(sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "\n",
    "    print(f'Soft Cosine Similarity for {iv}:')\n",
    "\n",
    "    dfs_dict[f'{iv}']['word_embeddings'] = defaultdict()\n",
    "\n",
    "    sentence_list = []\n",
    "\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        sentences_list.append(' '.join([sentence for sentences in value['df'][f'{n_gram}'].to_list() for sentence in sentences if sentence]))\n",
    "\n",
    "    dictionary = corpora.Dictionary([sent for sentences in sentences_list for sent in sentences if len(sent) > 0])\n",
    "    dfs_dict[f'{iv}']['word_embeddings']['dictionary'] = dictionary\n",
    "    print(f'{iv} Dictionary:\\n{dictionary}')\n",
    "\n",
    "    bow_vectors = [\n",
    "    dictionary.doc2bow(sent) for sentences in sentences_list for sent in sentences\n",
    "    ]\n",
    "    dfs_dict[f'{iv}']['word_embeddings']['bow_vectors'] = bow_vectors\n",
    "    print(f'{iv} TOP 5 BOW:\\n{bow_vectors[0][:5]}')\n",
    "\n",
    "    tfidf_vectors = TfidfModel(corpus=bow_vectors, dictionary=dictionary)\n",
    "    dfs_dict[f'{iv}']['word_embeddings']['tfidf_vectors'] = tfidf_vectors\n",
    "\n",
    "    # Soft Cosine Similarities\n",
    "    for embed_model_name, embed_func_list in embedding_models_dict.items():\n",
    "        build_train_func, embed_func, model_loader, model = embed_func_list\n",
    "\n",
    "        similarity_matrix = SparseTermSimilarityMatrix(WordEmbeddingSimilarityIndex(model), dictionary)\n",
    "        dfs_dict[f'{iv}']['word_embeddings'][f'{embed_model_name}_similarity_matrix'] = similarity_matrix\n",
    "\n",
    "        softcos_index = SoftCosineSimilarity(bow_vectors, similarity_matrix, num_best=10)\n",
    "        dfs_dict[f'{iv}']['word_embeddings'][f'{embed_model_name}_docsim_index'] = softcos_index\n",
    "\n",
    "        scm1 = similarity_matrix.inner_product(\n",
    "            bow_vectors[0],\n",
    "            bow_vectors[1],\n",
    "            normalized=(True, True)\n",
    "        )\n",
    "        scm2 = similarity_matrix.inner_product(\n",
    "            bow_vectors[0],\n",
    "            bow_vectors[2],\n",
    "            normalized=(True, True)\n",
    "        )\n",
    "        scm3 = similarity_matrix.inner_product(\n",
    "            bow_vectors[1],\n",
    "            bow_vectors[2],\n",
    "            normalized=(True, True)\n",
    "        )\n",
    "        print(f'{model_name} Soft Cosine Similarity between:\\n{list(iv_cats[\"categories\"].keys())[0]} Dominated Sectors <-> {list(iv_cats[\"categories\"].keys())[1]} Dominated Sectors: {scm1:.2f}\\n{list(iv_cats[\"categories\"].keys())[0]} Dominated Sectors <-> {list(iv_cats[\"categories\"].keys())[2]} Dominated Sectors: {scm2:.2f}\\n{list(iv_cats[\"categories\"].keys())[1]} Dominated Sectors <-> {list(iv_cats[\"categories\"].keys())[2]} Dominated Sectors: {scm3:.2f}')\n",
    "\n",
    "        df_sims = create_soft_cossim_matrix(bow_vectors, list(iv_cats[\"categories\"].keys()), similarity_matrix)\n",
    "        dfs_dict[f'{iv}']['word_embeddings'][f'{model_name}_df_sims'] = df_sims\n",
    "        print(f'DF of cosine similarities:\\n{df_sims.head()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary([sent for sentences in sentences_list for sent in sentences if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectors = [\n",
    "    dictionary.doc2bow(sent) for sentences in sentences_list for sent in sentences\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfs_dict[f'{iv}']['categories'][f'{list(iv_cats[\"categories\"].keys())[0]}']['df'][f'{n_gram}'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ''\n",
    "for token_list in x:\n",
    "    if token_list:\n",
    "        for token in token_list:\n",
    "            sent += ' ' + token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sent[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_all_sentences = [\n",
    "    f' {token}' for token in token_list for token_list in dfs_dict[f'{iv}']['categories'][f'{list(iv_cats[\"categories\"].keys())[0]}']['df'][f'{n_gram}'].to_list()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_list = [\n",
    "        dfs_dict[f'{iv}']['categories'][f'{list(iv_cats[\"categories\"].keys())[0]}']['df'][f'{n_gram}'].to_list(),\n",
    "        dfs_dict[f'{iv}']['categories'][f'{list(iv_cats[\"categories\"].keys())[1]}']['df'][f'{n_gram}'].to_list(),\n",
    "        dfs_dict[f'{iv}']['categories'][f'{list(iv_cats[\"categories\"].keys())[2]}']['df'][f'{n_gram}'].to_list(),\n",
    "        ]\n",
    "\n",
    "dictionary = corpora.Dictionary([sent for sentences in sentences_list for sent in sentences if len(sent) > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_sample(sector, size, model, vocab):\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'PCA for {sector} model {size}')\n",
    "    pca = PCA(n_components=3, random_state=random_state)\n",
    "    X = model.wv[vocab]\n",
    "    pca_clf = pca.fit_transform(X)\n",
    "    pca_components = pca.components_\n",
    "    pca_eigenvalues = pca.explained_variance_ratio_\n",
    "    pca_perc_explained_variance=np.cumsum(np.round(pca_eigenvalues, decimals=3)*100)\n",
    "    pca_tmp = pd.DataFrame(pca_clf, index=vocab, columns=['x', 'y', 'z'])\n",
    "    print(pca_tmp.head(3))\n",
    "    try:\n",
    "        pca_tmp = pca_tmp.sample(150)\n",
    "    except ValueError as e:\n",
    "        print(f'For {sector} model {size}, error: {e}')\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(pca_tmp['x'], pca_tmp['y'], pca_tmp['z'], alpha = 0.5)\n",
    "\n",
    "    for word, row in pca_tmp.iterrows():\n",
    "        x, y, z = row\n",
    "        pos = (x, y, z)\n",
    "        ax.text(x, y, z, s=word, size=8, zorder=1, color='k')\n",
    "\n",
    "    plt.title('Word2Vec map - PCA')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.figure.savefig(f'{plot_save_path}w2v_{sector}_{size}_map_PCA.{str(image_save_format)}', format=image_save_format, dpi=3000, bbox_inches='tight')\n",
    "\n",
    "\n",
    "    return pca_clf, pca_tmp, pca_components, pca_eigenvalues, pca_perc_explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for (iv_cat, value), model_size in itertools.product(iv_cats['categories'].items(), model_sizes):\n",
    "        # Word2Vec\n",
    "        value[f'w2v_model_size_{model_size}']['w2v_pca_clf'], value[f'w2v_model_size_{model_size}']['w2v_pca_tmp'], value[f'w2v_model_size_{model_size}']['w2v_pca_components'], value[f'w2v_model_size_{model_size}']['w2v_pca_eigenvalues'], value[f'w2v_model_size_{model_size}']['w2v_pca_perc_explained_variance'] = pca_sample(iv_cat, model_size, value[f'w2v_model_size_{model_size}']['w2v_model'], value[f'w2v_model_size_{model_size}']['w2v_vocab'])\n",
    "        # FastText\n",
    "        value[f'ft_model_size_{model_size}']['ft_pca_clf'], value[f'ft_model_size_{model_size}']['ft_pca_tmp'], value[f'ft_model_size_{model_size}']['ft_pca_components'], value[f'ft_model_size_{model_size}']['ft_pca_eigenvalues'], value[f'ft_model_size_{model_size}']['ft_pca_perc_explained_variance'] = pca_sample(iv_cat, model_size, value[f'ft_model_size_{model_size}']['ft_model'], value[f'ft_model_size_{model_size}']['ft_vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_sample(sector, size, model, vocab):\n",
    "\n",
    "    print(f'TSNE for {sector} model {size}')\n",
    "    tsne = TSNE(perplexity=40, n_components=3, random_state=random_state, init='pca')\n",
    "    X = model.wv[vocab]\n",
    "    tsne_clf = tsne.fit_transform(X)\n",
    "    tsne_tmp = pd.DataFrame(tsne_clf, index=vocab, columns=['x', 'y', 'z'])\n",
    "    tsne_tmp['input'] = 0\n",
    "    tsne_tmp['input'].iloc[0:1] = 1\n",
    "    print(tsne_tmp.head(3))\n",
    "    try:\n",
    "        tsne_tmp = tsne_tmp.sample(150)\n",
    "    except ValueError as e:\n",
    "        print(f'For {sector} model {size}, error: {e}')\n",
    "\n",
    "    # Plotting TSNE\n",
    "    # 3D Plot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(tsne_tmp[tsne_tmp[\"input\"]==0]['x'],\n",
    "            tsne_tmp[tsne_tmp[\"input\"]==0]['y'],\n",
    "            tsne_tmp[tsne_tmp[\"input\"]==0]['z'],\n",
    "            c='black', alpha = 0.5)\n",
    "    ax.scatter(tsne_tmp[tsne_tmp[\"input\"]==1]['x'],\n",
    "            tsne_tmp[tsne_tmp[\"input\"]==1]['y'],\n",
    "            tsne_tmp[tsne_tmp[\"input\"]==1]['z'],\n",
    "            c='red', alpha = 0.5)\n",
    "    ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[],\n",
    "        yticklabels=[], zticklabels=[])\n",
    "\n",
    "    for word, row in tsne_tmp[[\"x\",\"y\",\"z\"]].iterrows():\n",
    "        x, y, z = row\n",
    "        pos = (x, y, z)\n",
    "        ax.text(x, y, z, s=word, size=8, zorder=1, color='k')\n",
    "\n",
    "    plt.title('Word2Vec map - TSNE')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.figure.savefig(\n",
    "        f'{plot_save_path}w2v_{sector}_{size}_map_TSNE.{image_save_format}', format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in tsne_clf:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "        plt.annotate(\n",
    "            labels[i],\n",
    "            xy=(x[i], y[i]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"right\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "    plt.title('Word2Vec plot - TSNE')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(\n",
    "        f'{plot_save_path}w2v_{sector}_{size}_plot_TSNE.{image_save_format}', format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "    )\n",
    "\n",
    "    return tsne_clf, tsne_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for (iv_cat, value), model_size in itertools.product(iv_cats['categories'].items(), model_sizes):\n",
    "        value[f'w2v_model_size_{model_size}']['tsne_clf'], value[f'w2v_model_size_{model_size}']['tsne_tmp'] = tsne_sample(iv_cat, model_size, value[f'w2v_model_size_{model_size}']['w2v_model'], value[f'w2v_model_size_{model_size}']['w2v_vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(sector, size, model, words):\n",
    "    # trained word2vec model dimention\n",
    "    dim_size = model.wv.vectors.shape[1]\n",
    "\n",
    "    arrays = np.empty((0, dim_size), dtype='f')\n",
    "    for word in words:\n",
    "        try:\n",
    "            print(f'For {sector} model {size}, analyzing word: {word.title()}')\n",
    "            word_labels = [word]\n",
    "            color_list  = ['red']\n",
    "\n",
    "            # adds the vector of the query word\n",
    "            arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "\n",
    "            # gets list of most similar words\n",
    "            sim_words = model.wv.most_similar(word, topn=10)\n",
    "\n",
    "            # adds the vector for each of the closest words to the array\n",
    "            for wrd_score in sim_words:\n",
    "                wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "                word_labels.append(wrd_score[0])\n",
    "                color_list.append('green')\n",
    "                arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "\n",
    "            #---------------------- Apply PCA and tsne to reduce dimention --------------\n",
    "\n",
    "            # fit 2d PCA model to the similar word vectors\n",
    "            model_pca = PCA(n_components = 10).fit_transform(arrays)\n",
    "\n",
    "            # Finds 2d coordinates t-SNE\n",
    "            np.set_printoptions(suppress=True)\n",
    "            Y = TSNE(n_components=2, random_state=random_state, perplexity=15).fit_transform(model_pca)\n",
    "\n",
    "            try:\n",
    "                # Sets everything up to plot\n",
    "                df_plot = pd.DataFrame({'x': list(Y[:, 0]), 'y': list(Y[:, 1]), 'words_name': word_labels, 'words_color': color_list})\n",
    "\n",
    "\n",
    "                #------------------------- tsne plot Python -----------------------------------\n",
    "\n",
    "                # plot dots with color and position\n",
    "                plot_dot = sns.regplot(data=df_plot,\n",
    "                                x=\"x\",\n",
    "                                y=\"y\",\n",
    "                                fit_reg=False,\n",
    "                                marker=\"o\",\n",
    "                                scatter_kws={'s': 40,\n",
    "                                            'facecolors': df_plot['words_color']\n",
    "                                            }\n",
    "                                )\n",
    "\n",
    "                # Adds annotations with color one by one with a loop\n",
    "                for line in range(df_plot.shape[0]):\n",
    "                    plot_dot.text(df_plot[\"x\"][line],\n",
    "                            df_plot['y'][line],\n",
    "                            '  ' + df_plot[\"words_name\"][line].title(),\n",
    "                            horizontalalignment='left',\n",
    "                            verticalalignment='bottom', size='medium',\n",
    "                            color=df_plot['words_color'][line],\n",
    "                            weight='normal'\n",
    "                            ).set_size(15)\n",
    "\n",
    "\n",
    "                plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "                plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "\n",
    "                plt.title(f'{sector} model {size} t-SNE visualization for word {word.title()}')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except ValueError as e:\n",
    "                print(f'{sector} model {size} error for {word}: {e}')\n",
    "        except KeyError as e:\n",
    "            print(f'{sector} model {size} error for {word}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for (iv_cat, value), model_size in itertools.product(iv_cats['categories'].items(), model_sizes):\n",
    "        tsne_plot(iv_cat, model_size, value[f'w2v_model_size_{model_size}']['w2v_model'], value['frequencies']['abs_word_freq_df'].index.values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "for iv, iv_cats in dfs_dict.items():\n",
    "    for iv_cat, value in iv_cats['categories'].items():\n",
    "        print(f'{iv_cat} model wordcloud:')\n",
    "        Cloud = WordCloud(colormap='viridis', background_color=\"white\", max_words=50, random_state=random_state).generate_from_frequencies(value['frequencies']['abs_word_freq'])\n",
    "\n",
    "        plt.imshow(Cloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        plt.savefig(f'{plot_save_path}w2v_{iv_cat}_wordcloud.{image_save_format}', format=image_save_format, dpi=3000, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['save_enabled'] is True:\n",
    "    with open(validate_path(f'{args[\"embeddings_save_path\"]}dfs_dict.json'), 'w') as f:\n",
    "        for key in dfs_dict.keys():\n",
    "            f.write(f\"{key}, {dfs_dict[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True,\n",
    "                        stop_words='english',\n",
    "                        ngram_range = (1,1),\n",
    "                        tokenizer = tokenizer.tokenize)\n",
    "\n",
    "# Fit and Transform the documents\n",
    "train_data = tfidf.fit_transform(documents_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics or components\n",
    "num_components=10\n",
    "\n",
    "# Create SVD object\n",
    "lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n",
    "\n",
    "# Fit SVD model on data\n",
    "lsa.fit_transform(train_data)\n",
    "\n",
    "# Get Singular values and Components \n",
    "Sigma = lsa.singular_values_ \n",
    "V_transpose = lsa.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the topics with their terms\n",
    "terms = tfidf.get_feature_names()\n",
    "\n",
    "for index, component in enumerate(lsa.components_):\n",
    "    zipped = zip(terms, component)\n",
    "    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n",
    "    top_terms_list=list(dict(top_terms_key).keys())\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e64b55c31e662d3b8ca165241f15a246a93354fe580fc7a1249b2f351dbc5a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
