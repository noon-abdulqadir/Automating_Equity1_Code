{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# %%\n",
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### Install packages and import\n",
    "# %%\n",
    "# #################################### PLEASE INSTALL LATEST CHROME WEBDRIVER #####################################\n",
    "# Uncomment to run as required\n",
    "# #     --install-option=\"--chromedriver-version= *.**\" \\\n",
    "#   --install-option=\"--chromedriver-checksums=4fecc99b066cb1a346035bf022607104,058cd8b7b4b9688507701b5e648fd821\"\n",
    "# %%\n",
    "# ##### COPY THE LINES IN THIS COMMENT TO THE TOP OF NEW SCRIPTS #####\n",
    "# # Function to import this package to other files\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# code_dir = None\n",
    "# code_dir_name = 'Code'\n",
    "# unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "# for _ in range(5):\n",
    "\n",
    "#     parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "#     if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "#         code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "#         if code_dir is not None:\n",
    "#             break\n",
    "\n",
    "# main_dir = str(Path(code_dir).parents[0])\n",
    "# sys.path.append(code_dir)\n",
    "\n",
    "# from setup_module.imports import *\n",
    "# from setup_module.params import *\n",
    "# from setup_module.scraping import *\n",
    "# from setup_module.classification import *\n",
    "# from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# %matplotlib notebook\n",
    "# %matplotlib inline\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "main_dir = str(Path(code_dir).parents[0])\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "from setup_module.imports import *\n",
    "from setup_module.params import *\n",
    "from setup_module.scraping import *\n",
    "from setup_module.classification import *\n",
    "from setup_module.vectorizers_classifiers import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib notebook\n",
    "%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data balance\n",
    "df_jobs_labeled.info()\n",
    "df_jobs_labeled['Warmth'].value_counts()\n",
    "df_jobs_labeled['Competence'].value_counts()\n",
    "warm_comp_count = (\n",
    "    df_jobs_labeled[analysis_columns]\n",
    "    .reset_index()\n",
    "    .groupby(analysis_columns)\n",
    "    .count()\n",
    "    .sort_values(by='index')\n",
    ")\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle('Training Dataset: Warmth and Competence Sentence Counts', fontsize=16.0)\n",
    "warm_comp_count.plot(kind='barh', stacked=True, legend=True, color='blue', ax=ax).grid(\n",
    "    axis='y'\n",
    ")\n",
    "if save_enabled is True:\n",
    "    fig.savefig(f'{plot_save_path}Warmth and Competence Sentence Counts.{image_save_format}', format=image_save_format, dpi=3000)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97861e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'Job Description_w2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c0125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_jobs_labeled, vectorizer, col, text_col):\n",
    "    # BOW Split\n",
    "    print('Splitting data into training and test sets.')\n",
    "    df_jobs_labeled = df_jobs_labeled.dropna(subset=['Warmth', 'Competence', text_col], how='any')\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df_jobs_labeled, test_size=test_split, train_size = 1-test_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    validate, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array([sent2vec(x) for x in train[f'{str(text_col)}'].astype('str').values])\n",
    "    prepared_X_train = X_train.to_list()\n",
    "\n",
    "    y_train = column_or_1d(train[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_train = y_train.to_list()\n",
    "\n",
    "    X_test = np.array([sent2vec(x) for x in test[f'{str(text_col)}'].astype('str').values])\n",
    "    prepared_X_test = X_test.to_list()\n",
    "\n",
    "    y_test = column_or_1d(test[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_test = y_test.to_list()\n",
    "\n",
    "    X_validate = np.array([sent2vec(x) for x in validate[f'{str(text_col)}'].astype('str').values])\n",
    "    prepared_X_validate = X_validate.to_list()\n",
    "\n",
    "    y_validate = column_or_1d(validate[str(col)].astype('int64').values, warn=True)\n",
    "    prepared_y_validate = y_validate.to_list()\n",
    "\n",
    "    prepared_text = vectorizer.fit_transform(prepared_X_train+prepared_X_test+prepared_X_validate)\n",
    "\n",
    "    return train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f9c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding with Fasttext\n",
    "def word_embedding(df_jobs, X_train, feature_names):\n",
    "\n",
    "    sentences = df_jobs['2grams_gensim']\n",
    "\n",
    "    fasttext_model = FastText(sentences, window=3, min_count=1, sorted_vocab=1)\n",
    "\n",
    "    row=0\n",
    "    errors=0\n",
    "    for sent in tqdm.tqdm(sentences):\n",
    "        sent_vec = np.zeros(100)\n",
    "        weight_sum =0\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # weight = fasttext_model.wv.get_vector(word)\n",
    "                # weight_sum += weight\n",
    "                # sent_vec += weight\n",
    "                vec = fasttext_model.wv[word]\n",
    "                feat = X_train[row, feature_names.index(word)]\n",
    "                sent_vec += (vec * feat)\n",
    "                weight_sum += feat\n",
    "            except Exception:\n",
    "                errors += 1\n",
    "        sent_vec /= weight_sum\n",
    "                # print(np.isnan(np.sum(sent_vec)))\n",
    "    sent_vectors = [sent_vec]\n",
    "    row += 1\n",
    "    print(f'errors noted: {str(errors)}')\n",
    "\n",
    "    return fasttext_model, sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names):\n",
    "\n",
    "    # Get words and offsets\n",
    "    train_words, train_offsets = train_offset(X_train, 'train')\n",
    "    test_words, test_offsets = train_offset(X_test, 'test')\n",
    "    validate_words, validate_offsets = train_offset(X_validate, 'validate')\n",
    "\n",
    "    if hasattr(vectorizer, 'vocabulary_'):\n",
    "        vocabulary_map = vectorizer.vocabulary_\n",
    "        if plots_enabled:\n",
    "            sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')\n",
    "\n",
    "        embs = load_glove_with_vocabulary(vocabulary_map, feature_names, print_enabled=print_enabled)\n",
    "        emb_model = BagOfEmbeddings(embs, dropout=0.1, hidden_dim=75, embedding_mode='mean')\n",
    "        print(f'Embedding Model: {emb_model}')\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(emb_model.parameters(), lr = 0.001)\n",
    "        torch.manual_seed(random_state)\n",
    "        losses = run_training(epochs=1, emb_model=emb_model, optimizer=optimizer, loss_fn=loss,\n",
    "                            all_words=train_words, all_offsets=train_offsets, all_targets=y_train,\n",
    "                            batch_size=32)\n",
    "        print(\"Training avg emb_model complete!\")\n",
    "\n",
    "        print(\"Evaluating test set\")\n",
    "        batch_losses, outputs = run_test(emb_model=emb_model, loss_fn=loss,\n",
    "                            all_words=test_words, all_offsets=test_offsets, all_targets=y_train,\n",
    "                            batch_size=256)\n",
    "\n",
    "        print(\"outputs.shape\", outputs.shape)\n",
    "\n",
    "        boe_pred = outputs.detach().numpy()\n",
    "\n",
    "        best_threshold_boe, best_score_boe = calculate_best_threshold(y_test[:300], boe_pred[:300], scoring, print_enabled)\n",
    "\n",
    "        print(\"boe_pred:\\n\", boe_pred[10])\n",
    "\n",
    "        print(\"Evaluating validate outputs\")\n",
    "        _, validate_outputs = run_test(emb_model=emb_model, loss_fn=None,\n",
    "                            all_words=validate_words, all_offsets=validate_offsets, all_targets=None,\n",
    "                            batch_size=256)\n",
    "\n",
    "        boe_validate_pred = validate_outputs.detach().numpy()\n",
    "        print(\"boe_validate_pred:\\n\", boe_validate_pred[10])\n",
    "        print(\"Validate outputs done\")\n",
    "\n",
    "    else:\n",
    "        vocabulary_map = None\n",
    "        boe_pred = None\n",
    "        boe_validate_pred = None\n",
    "        best_threshold_boe = None\n",
    "        best_score_boe = None\n",
    "\n",
    "    return vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and target arrays\n",
    "def vectorize(vectorizer, vectorizer_name, selector, df_jobs_labeled, col, text_col):\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} passed ============================'\n",
    "    )\n",
    "\n",
    "    refit_vectorizer = vectorizer\n",
    "\n",
    "    # BOW Split\n",
    "    train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, prepared_text = split_data(df_jobs_labeled, vectorizer, col, text_col)\n",
    "\n",
    "    # BOW fit transform\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "    # Oversampling to fix imbalance\n",
    "    if (resampling_enabled is True) and (col == 'Warmth'):\n",
    "        X_train, y_train = resample_data(X_train, y_train, col, resampling_enabled, resampling_method)\n",
    "\n",
    "    # Get feature names\n",
    "    X_train, vectorizer, dtf_features, X_names, feature_names = get_feature_name_and_refit_X_train_on_chi_test(train, X_train, vectorizer, refit_vectorizer)\n",
    "    unique_features = set(feature_names)\n",
    "\n",
    "    # BOW fit\n",
    "    print('Fitting and transforming data.')\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    X_validate = vectorizer.transform(X_validate)\n",
    "    # Selecting best features\n",
    "    if select_best_enabled is True:\n",
    "        X_test = selector.transform(X_test)\n",
    "        X_validate = selector.transform(X_validate)\n",
    "        # Get feature names\n",
    "        feature_names = selector.get_feature_names_out(vocabulary=X_names)\n",
    "        unique_features = set(feature_names)\n",
    "\n",
    "    # y to numpy array\n",
    "    y_train = torch.from_numpy(np.array(y_train)).float()\n",
    "    y_test = torch.from_numpy(np.array(y_test)).float()\n",
    "    if print_enabled:\n",
    "        print(f'Train targets: {y_train}')\n",
    "        print(f'Test targets: {y_test}')\n",
    "\n",
    "    vocabulary_map, boe_pred, boe_validate_pred, best_threshold_boe, best_score_boe = emb_poed(vectorizer, X_train, y_train, X_test, y_test, X_validate, y_validate, feature_names)\n",
    "\n",
    "    # fasttext_model, sent_vectors = word_embedding(df_jobs, X_train, feature_names)\n",
    "\n",
    "    return df_jobs_labeled, vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map, best_threshold_boe, best_score_boe, boe_pred, boe_validate_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name, save_enabled=save_enabled):\n",
    "\n",
    "    # Save DF of predictions\n",
    "    labels = (y_final_validate_prob_pred > best_threshold).astype(int)\n",
    "    df_preds = pd.DataFrame({f'{str(text_col)}': test[f'{str(text_col)}'], \"prediction\": labels})\n",
    "    if save_enabled:\n",
    "        df_preds.to_csv(f'{df_dir}df_preds_{str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}', index=False)\n",
    "\n",
    "    return df_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled):\n",
    "\n",
    "    num = len(test) // 2\n",
    "\n",
    "    best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "    if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "        num = len(boe_pred) // 2\n",
    "\n",
    "        X_final_augmented_train = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "            \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[:num],\n",
    "            \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "        y_final_augmented_train = y_test[:num]\n",
    "\n",
    "        X_final_augmented_test = pd.DataFrame({\n",
    "            \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "            \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "            \"num_words\": test[\"num_words\"].values[num:],\n",
    "            \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "        y_final_augmented_test = y_test[num:]\n",
    "\n",
    "        final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "        if hasattr(final_classifier, 'predict_proba'):\n",
    "            y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "        elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "            y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "        best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "        X_final_augmented_validate = pd.DataFrame({\n",
    "            \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "            \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "            \"num_words\": validate[\"num_words\"].values,\n",
    "            \"num_chars\": validate[\"num_chars\"].values})\n",
    "        y_final_augmented_validate = y_validate\n",
    "\n",
    "        y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "        df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "        final_classifier = classifier\n",
    "        X_final_augmented_validate = X_test\n",
    "        y_final_augmented_validate = y_test\n",
    "        y_final_validate_prob_pred = y_test_prob_pred\n",
    "        df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n",
    "\n",
    "    return final_classifier, X_final_augmented_validate, y_final_augmented_validate, y_final_validate_prob_pred, best_threshold, best_score, df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69ae3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classifier\n",
    "def classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred):\n",
    "    print('\\n')\n",
    "    print(\n",
    "        f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    "    )\n",
    "    num = len(test) // 2\n",
    "\n",
    "    # BOW model\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        X_train = X_train.todense()\n",
    "        X_test = X_test.todense()\n",
    "        X_validate = X_validate.todense()\n",
    "\n",
    "    if classifier_name == 'Sequential':\n",
    "        classifier.compile(loss='categorical_crossentropy')\n",
    "    if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "        classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "    final_classifier = classifier\n",
    "    classifier = classifier.fit(X_train, y_train)\n",
    "    classifier = SelectFromModel(estimator=classifier, prefit=True).fit(X_train, y_train)\n",
    "\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "        y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "    elif hasattr(classifier, '_predict_proba_lr'):\n",
    "        y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "        y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "    else:\n",
    "        raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "    final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n",
    "\n",
    "    return final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2107c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "def evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring):\n",
    "\n",
    "    # Evaluate\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'EVALUATING FITTED MODEL - {vectorizer_name} + {classifier_name}: ',\n",
    "        has_fit_parameter(classifier, 'sample_weight'),\n",
    "    )\n",
    "    # 5 cross_validation score\n",
    "    print(f'Cross Validating - {vectorizer_name} + {classifier_name}.')\n",
    "    cross_validate_score = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "        scoring=scores,\n",
    "    )\n",
    "\n",
    "    cross_validate_score_noscoring = cross_validate(\n",
    "        classifier,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        cv=cv,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f'Mean cross_validate scores - {vectorizer_name} + {classifier_name}: {cross_validate_score_noscoring.get(\"test_score\").mean()}'\n",
    "    )\n",
    "    numberoflabels = len(set((str(e) for e in y_test.to_list())))\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Mean Validation Score')\n",
    "    ] = float(cross_validate_score_noscoring.get('test_score').mean())\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Explained Variance')\n",
    "    ] = float(\n",
    "        cross_validate_score.get('test_explained_variance').mean()\n",
    "    )\n",
    "\n",
    "    print('-' * 20)\n",
    "    for key, values in cross_validate_score.items():\n",
    "        if 'test' in key:\n",
    "            print(key, ' mean ', values.mean())\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "\n",
    "    # Predictions\n",
    "    print('-' * 20)\n",
    "    print(\n",
    "        f'============================ {str(col)} PREDICTIONS FOR {vectorizer_name.upper()} WITH {classifier_name.upper()} ============================'\n",
    "    )\n",
    "    print('\\n')\n",
    "    print(f'y_test_pred - {str(col)} - {vectorizer_name} + {classifier_name}:')\n",
    "    dic_y_mapping = {n: label for n, label in enumerate(np.unique(y_train))}\n",
    "    inverse_dic = {v:k for k,v in dic_y_mapping.items()}\n",
    "    y_train = np.array([inverse_dic[y] for y in y_train])\n",
    "\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    if classifier_name == 'GaussianNB':\n",
    "        # y_test_pred = y_test_pred.to_list()\n",
    "        y_test_pred = classifier.predict(X_test.todense())\n",
    "    predicted = [dic_y_mapping[np.argmax(pred)] for pred in y_test_pred]\n",
    "    acc_roc_f1 = evaluate_print(classifier_name + '   |   ', y_test, y_test_pred)\n",
    "    cm, precision, recall, accuracy, f1, mcc, best_threshold, best_score, report = evaluation(\n",
    "        y_test, y_test_pred, scoring, print_enabled\n",
    "    )\n",
    "\n",
    "    true_negative = cm[0][0]\n",
    "    false_positives = cm[0][1]\n",
    "    false_negatives = cm[1][0]\n",
    "    true_positives = cm[1][1]\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_jobs_labeled,\n",
    "        print_enabled,\n",
    "    )\n",
    "\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Accuracy')\n",
    "    ] = float(accuracy)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Precision')\n",
    "    ] = float(precision)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Recall')\n",
    "    ] = float(recall)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'F1-score')\n",
    "    ] = float(f1)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient'),\n",
    "    ] = float(mcc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold'),\n",
    "    ] = float(best_threshold)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score'),\n",
    "    ] = float(best_score)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Classification Report')\n",
    "    ] = report\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'Confusion Matrix')\n",
    "    ] = str(cm)\n",
    "\n",
    "    # Plot\n",
    "    heatmap = plot_confusion_matrix_percentage(col, cm, classifier_name, vectorizer_name)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        heatmap.figure.savefig(\n",
    "            f'{plot_save_path}Confusion Matrix {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "            )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    # Log Loss Cross Entropy\n",
    "    if hasattr(classifier, 'predict_proba'):\n",
    "        y_test_prob_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "        probability_of_1 = y_test_prob_pred#[:, 1]\n",
    "\n",
    "        loss = log_loss(y_test, y_test_prob_pred)\n",
    "        print('\\n')\n",
    "        print('=' * 20)\n",
    "        print(f'Log Loss / Cross Entropy = {loss}')\n",
    "        print('=' * 20)\n",
    "        print('\\n')\n",
    "        table_df.loc[\n",
    "            (classifier_name),\n",
    "            (col, vectorizer_name, 'Log Loss/Cross Entropy'),\n",
    "        ] = float(loss)\n",
    "\n",
    "        # Explain Model\n",
    "        explained = explain_model(test, y_test, y_test_pred, y_test_prob_pred, y_train)\n",
    "        if plots_enabled:\n",
    "            explained.show_in_notebook(text=txt_instance, predict_proba=False)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    # Optimization\n",
    "    if optimization_enabled is True and hasattr(classifier, 'predict_log_proba'):\n",
    "        classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt = optimize_model(\n",
    "            classifier,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            probability_of_1,\n",
    "            vectorizer_name,\n",
    "            classifier_name,\n",
    "            table_df,\n",
    "            score)\n",
    "\n",
    "        # ROC Curve\n",
    "        table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc = get_roc_curve(classifier, X_test, y_test, y_test_pred_new, probability_of_1, vectorizer_name, classifier_name, col)\n",
    "\n",
    "        # Precision Recall Curve\n",
    "        print(f'Precision Recall Curve AFTER OPTIMIZATION - {str(col)} - {vectorizer_name} + {classifier_name}')\n",
    "        get_pr_curve(X_test, y_test, recall_opt, precision_opt, auc, vectorizer_name, classifier_name, col)\n",
    "\n",
    "    if hasattr(classifier, 'best_estimator_'):\n",
    "        ohe_cols = list(\n",
    "            classifier.best_estimator_.named_steps['vectorizer']\n",
    "            .named_transformers_['cat']\n",
    "            .named_steps['ohe']\n",
    "            .get_feature_names(input_features=categorical)\n",
    "        )\n",
    "        num_feats = list(numerical)\n",
    "        num_feats.extend(ohe_cols)\n",
    "        feat_imp = eli5.explain_weights_df(\n",
    "            classifier.best_estimator_.named_steps['classifier'],\n",
    "            top=10,\n",
    "            feature_names=num_feats,\n",
    "        )\n",
    "        print(\n",
    "            f'feat_imp - {str(col)} - {vectorizer_name} + {classifier_name}: ',\n",
    "            feat_imp,\n",
    "        )\n",
    "        print('-' * 20)\n",
    "        print('\\n')\n",
    "\n",
    "    report_test = predict(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        classifier,\n",
    "        classifier_name,\n",
    "        col,\n",
    "        scoring,\n",
    "        df_jobs_labeled,\n",
    "        print_enabled,\n",
    "    )\n",
    "    print(f'REPORT TEST {str(col)} - {vectorizer_name} + {classifier_name}:\\n', report_test)\n",
    "    print('-' * 20)\n",
    "\n",
    "    return classifier, table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "def get_roc_curve(classifier, X_test, y_test, y_test_pred, probability_of_1, vectorizer_name, classifier_name, col):\n",
    "    roc_curve = metrics.plot_roc_curve(classifier, X_test, y_test)\n",
    "    plt.title(\n",
    "        f'ROC Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=16,\n",
    "    )\n",
    "    if save_enabled is True:\n",
    "        roc_curve.figure_.savefig(\n",
    "            plot_save_path\n",
    "            + f'ROC {str(col)} - {classifier_name} - {vectorizer_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(\n",
    "        y_test, probability_of_1, pos_label=1\n",
    "    )\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'ROC')\n",
    "    ] = float(roc_auc)\n",
    "    table_df.loc[\n",
    "        (classifier_name), (col, vectorizer_name, 'AUC')\n",
    "    ] = float(auc)\n",
    "\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    print(f'AUC {str(col)} - {vectorizer_name} + {classifier_name}:\\n', auc)\n",
    "\n",
    "    print('ROC CURVE FOR PREDICTED PROBABILITIES')\n",
    "    bc = BinaryClassification(y_test, y_test_pred, labels=['0', '1'])\n",
    "    # Figures\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    bc.plot_roc_curve()\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],\n",
    "                            predicted_prob[:,i])\n",
    "        ax[0].plot(fpr, tpr, lw=3,\n",
    "                label='{0} (area={1:0.2f})'.format(classes[i],\n",
    "                                metrics.auc(fpr, tpr))\n",
    "                )\n",
    "    ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n",
    "    ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05],\n",
    "            xlabel='False Positive Rate',\n",
    "            ylabel=\"True Positive Rate (Recall)\",\n",
    "            title=\"Receiver operating characteristic\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    return table_df, fpr, tpr, thresholds, auc, roc_curve, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Recall Curve\n",
    "def get_pr_curve(X_test, y_test, recall, precision, auc, vectorizer_name, classifier_name, col):\n",
    "\n",
    "    no_skill = len(y_test[y_test == 1]) / len(y_test)\n",
    "    pr_curve = plt.figure(figsize=(4.0, 4.0))\n",
    "    plt.plot(\n",
    "        [0, 1], [no_skill, no_skill], linestyle='--', label='No Skill'\n",
    "    )\n",
    "    plt.plot(\n",
    "        recall, precision, marker='.', label=f'AUC = {auc}'\n",
    "    )\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.title(\n",
    "        f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}',\n",
    "        fontsize=12.0,\n",
    "    )\n",
    "    plt.ylabel('Precision', fontsize=12.0)\n",
    "    plt.xlabel('Recall', fontsize=12.0)\n",
    "    plt.show()\n",
    "    if save_enabled is True:\n",
    "        pr_curve.savefig(\n",
    "            plot_save_path\n",
    "            + f'Precision Recall Curve {str(col)} - {vectorizer_name} + {classifier_name}.{image_save_format}',\n",
    "            format=image_save_format,\n",
    "            dpi=3000,\n",
    "        )\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    for i in range(len(classes)):\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(\n",
    "            y_test_array[:,i], predicted_prob[:,i])\n",
    "        ax[1].plot(recall, precision, lw=3,\n",
    "                label=f'{classes[i]} (area={metrics.auc(recall, precision):0.2f})')\n",
    "    ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall',\n",
    "            ylabel=\"Precision\", title=\"Precision-Recall curve\")\n",
    "    ax[1].legend(loc=\"best\")\n",
    "    ax[1].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Model\n",
    "def optimize_model(classifier, X_test, y_test, probability_of_1, vectorizer_name, classifier_name, table_df, scoring):\n",
    "    if hasattr(classifier, 'predict_log_proba'):\n",
    "\n",
    "        y_test_prob_log_pred = classifier.predict_log_proba(X_test)[:, 1]\n",
    "\n",
    "        # calculate pr-curve\n",
    "        (\n",
    "            precision_opt,\n",
    "            recall_opt,\n",
    "            thresholds_opt,\n",
    "        ) = metrics.precision_recall_curve(\n",
    "            y_test, probability_of_1\n",
    "        )\n",
    "        # convert to f score\n",
    "        fscore_opt = (2 * precision_opt * recall_opt) / (\n",
    "            precision_opt + recall_opt\n",
    "        )\n",
    "        # locate the index of the largest f score\n",
    "        ix_opt = argmax(fscore_opt)\n",
    "        best_thresh_opt = thresholds_opt[ix_opt]\n",
    "        print('=' * 20)\n",
    "        print(\n",
    "            f'Best Threshold: {best_thresh_opt}, F-Score={fscore_opt[ix_opt]}'\n",
    "        )\n",
    "        print(f'Optimal threshold: {np.exp(best_thresh_opt)}')\n",
    "        y_test_pred_new = np.where(\n",
    "            y_test_prob_log_pred[:, 1] > best_thresh_opt, 1, 0\n",
    "        )\n",
    "        print(f'New y_test_pred {str(col)} - {vectorizer_name} + {classifier_name}:\\n{y_test_pred_new}')\n",
    "\n",
    "        print(\n",
    "            f'SCORES FOR {str(col)} - {vectorizer_name} + {classifier_name} AFTER OPTIMIZATION:'\n",
    "        )\n",
    "        cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, best_threshold_opt, best_score_opt, report_opt = evaluation(\n",
    "            y_test, y_test_pred_new, scoring, print_enabled\n",
    "        )\n",
    "\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Accuracy_opt')\n",
    "        ] = float(accuracy_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Precision_opt')\n",
    "        ] = float(precision_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Recall_opt')\n",
    "        ] = float(recall_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'F1-score_opt')\n",
    "        ] = float(f1_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Matthews Correlation Coefficient_opt'),\n",
    "        ] = float(mcc_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Threshold_opt'),\n",
    "        ] = float(best_threshold_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, f'{scoring.title()} Best Score_opt'),\n",
    "        ] = float(best_score_opt)\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Classification Report_opt'),\n",
    "        ] = report_opt\n",
    "        table_df.loc[\n",
    "            (classifier_name), (col, vectorizer_name, 'Confusion Matrix_opt'),\n",
    "        ] = str(cm_opt)\n",
    "\n",
    "        print('=' * 20)\n",
    "\n",
    "    elif not hasattr(classifier, 'predict_log_proba'):\n",
    "        print('Classifier has no Attribute predict_log_proba.')\n",
    "\n",
    "    return classifier, table_df, y_test_prob_log_pred, y_test_pred_new, cm_opt, precision_opt, recall_opt, accuracy_opt, f1_opt, mcc_opt, report_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "def saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name, save_enabled = True, task_enabled = False):\n",
    "    if save_enabled is True:\n",
    "        if task_enabled is False:\n",
    "            classifier_save_path = (\n",
    "                f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "            )\n",
    "            vectorizer_save_path = (\n",
    "                f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "            )\n",
    "            if select_best_enabled is True:\n",
    "                selector_save_path = (\n",
    "                    f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}.{file_save_format}'\n",
    "                )\n",
    "\n",
    "        elif task_enabled is True:\n",
    "            classifier_save_path = (\n",
    "                f'{models_save_path}Model {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "            )\n",
    "            vectorizer_save_path = (\n",
    "                f'{models_save_path}Vectorizer {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "            )\n",
    "            if select_best_enabled is True:\n",
    "                selector_save_path = (\n",
    "                    f'{models_save_path}Selector {str(col)} - {vectorizer_name} + {classifier_name}_WITH_TASK.{file_save_format}'\n",
    "                )\n",
    "\n",
    "        # Save classifier\n",
    "        print(f'Saving Model and Table for {vectorizer_name} + {classifier_name}.')\n",
    "        table_df.to_csv(table_save_path + csv_file_name)\n",
    "        table_df.to_pickle(table_save_path + pickle_file_name)\n",
    "        table_df.to_excel(table_save_path + excel_file_name)\n",
    "        table_df.to_latex(table_save_path + latex_file_name)\n",
    "        table_df.to_markdown(table_save_path + markdown_file_name)\n",
    "\n",
    "        with open(classifier_save_path, 'wb') as f:\n",
    "            joblib.dump(classifier, f)\n",
    "        with open(vectorizer_save_path, 'wb') as f:\n",
    "            joblib.dump(vectorizer, f)\n",
    "        if select_best_enabled is True:\n",
    "            with open(selector_save_path, 'wb') as f:\n",
    "                joblib.dump(selector, f)\n",
    "\n",
    "    elif save_enabled is False:\n",
    "        print('Saving Model and Table is disabled.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring):\n",
    "\n",
    "    if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "        classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "    classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred)\n",
    "    classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "    return classifier, table_df, X_test, y_test, y_test_prob_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_pipe(df_jobs_labeled, vectorizers, selector, classifiers, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name):\n",
    "\n",
    "    print('Not using Search')\n",
    "\n",
    "    if use_dict_for_classifiers_vectorizers is True:\n",
    "        print('Using dict for classifiers and vectorizers.')\n",
    "        for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "            vectorizer = vectorizer_and_params[0]\n",
    "            vectorizer_params = vectorizer_and_params[1]\n",
    "            vectorizer.set_params(**vectorizer_params)\n",
    "            df_jobs_labeled, vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map, best_threshold_boe, best_score_boe, boe_pred, boe_validate_pred = vectorize(vectorizer, vectorizer_name, selector, df_jobs_labeled, col, text_col)\n",
    "\n",
    "            for classifier_name, classifier_and_params in classifiers.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "                classifier.set_params(**classifier_params)\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    elif use_dict_for_classifiers_vectorizers is False:\n",
    "        print('Using list for classifiers and vectorizers.')\n",
    "        for vectorizer in vectorizers_lst:\n",
    "            vectorizer_name = vectorizer.__class__.__name__\n",
    "            df_jobs_labeled, vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map, best_threshold_boe, best_score_boe, boe_pred, boe_validate_pred = vectorize(vectorizer, vectorizer_name, selector, df_jobs_labeled, col, text_col)\n",
    "\n",
    "            for classifier in classifiers_lst:\n",
    "                classifier_name = classifier.__class__.__name__\n",
    "                classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    return df_jobs_labeled, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf4591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipe(df_jobs_labeled, vectorizers_pipe, selectors_pipe, classifiers_pipe, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name):\n",
    "\n",
    "    print('Using Search')\n",
    "\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "\n",
    "    # Vectorization\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers_pipe.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "\n",
    "        # Selection\n",
    "        for selector_name, selector_and_params in selectors_pipe.items():\n",
    "            selector = selector_and_params[0]\n",
    "            selector_params = selector_and_params[1]\n",
    "\n",
    "            # Classification\n",
    "            for classifier_name, classifier_and_params in classifiers_pipe.items():\n",
    "                classifier = classifier_and_params[0]\n",
    "                classifier_params = classifier_and_params[1]\n",
    "\n",
    "                # Pipeline\n",
    "                if select_best_enabled is True:\n",
    "                    ## Steps\n",
    "                    steps = [\n",
    "                        (vectorizer_name, vectorizer),\n",
    "                        (selector_name, selector),\n",
    "                        (classifier_name, classifier)\n",
    "                    ]\n",
    "                    ## Params\n",
    "                    param_grid = {\n",
    "                        **vectorizer_params,\n",
    "                        **selector_params,\n",
    "                        **classifier_params,\n",
    "                    }\n",
    "                    ## Pipeline\n",
    "                    pipe = Pipeline(steps=steps)\n",
    "\n",
    "                    ## Vectorizers, selectors, classifiers\n",
    "                    vectorizer = pipe[:-2]\n",
    "                    selector = pipe[:-1]\n",
    "                    classifier = pipe[:]\n",
    "\n",
    "                elif select_best_enabled is False:\n",
    "                    ## Steps\n",
    "                    steps = [\n",
    "                        (vectorizer_name, vectorizer),\n",
    "                        (classifier_name, classifier)\n",
    "                    ]\n",
    "                    ## Params\n",
    "                    param_grid = {\n",
    "                        **vectorizer_params,\n",
    "                        **classifier_params,\n",
    "                    }\n",
    "                    ## Pipeline\n",
    "                    pipe = Pipeline(steps=steps)\n",
    "\n",
    "                    ## Vectorizers, selectors, classifiers\n",
    "                    vectorizer = pipe[:-1]\n",
    "                    classifier = pipe[:]\n",
    "\n",
    "                # Search\n",
    "                search = RandomizedSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_distributions=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    scoring=scores,\n",
    "                    cv=cv,\n",
    "                    refit=scores[0],\n",
    "                    return_train_score=True,\n",
    "                    verbose=3,\n",
    "                )\n",
    "\n",
    "                # Fit SearchCV\n",
    "                searchcv = search.fit(X_train, y_train)\n",
    "\n",
    "                # Best Parameters\n",
    "                best_index = searchcv.best_index_\n",
    "                cv_results = sorted(searchcv.cv_results_)\n",
    "                best_params = searchcv.best_params_\n",
    "                classifier = searchcv.best_estimator_\n",
    "                y_train_pred = classifier.predict(X_train)\n",
    "                best_score = searchcv.best_score_\n",
    "                n_splits = searchcv.n_splits_\n",
    "\n",
    "                print('=' * 20)\n",
    "                print(f'Best index for {scores[0]}: {best_index}')\n",
    "                print(f'Best classifier for {scores[0]}: {classifier}')\n",
    "                print(f'Best y_train_pred for {scores[0]}: {y_train_pred}')\n",
    "                print(f'Best score for {scores[0]}: {best_score}')\n",
    "                print(f'Number of splits for {scores[0]}: {n_splits}')\n",
    "\n",
    "                print('-' * 20)\n",
    "                report = classification_report(y_train, y_train_pred)\n",
    "                print(f'Classification Report:\\n{report}')\n",
    "                ConfusionMatrixDisplay.from_estimator(\n",
    "                    searchcv, X_test, y_test, xticks_rotation=\"vertical\"\n",
    "                )\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                print('=' * 20)\n",
    "\n",
    "                # Make the predictions\n",
    "                score = searchcv.score(X_test, y_test)\n",
    "                y_test_pred = searchcv.predict(X_test)\n",
    "                if hasattr(searchcv, 'predict_proba'):\n",
    "                    y_test_prob_pred = searchcv.predict_proba(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv.predict_proba(X_validate)[:, 1]\n",
    "                elif hasattr(searchcv, '_predict_proba_lr'):\n",
    "                    y_test_prob_pred = searchcv._predict_proba_lr(X_test)[:, 1]\n",
    "                    y_validate_prob_pred = searchcv._predict_proba_lr(X_validate)[:, 1]\n",
    "\n",
    "                # Fit Best Model\n",
    "                print(f'Fitting {classifier}.')\n",
    "                classifier.set_params(**classifier.get_params())\n",
    "                classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "                # Evaluate Model\n",
    "                classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n",
    "\n",
    "                # Save Vectorizer, Selector, and Classifier\n",
    "                if save_enabled is True:\n",
    "                    saving_model_and_table(vectorizer, vectorizer_name, selector, selector_name, classifier, classifier_name, col, table_save_path, table_df, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    return df_jobs_labeled, searchcv, classifier, vectorizers, selector, table_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b2f86d",
   "metadata": {},
   "source": [
    "# Training Supervised Model: Warmth and Competence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d94250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ STARTING PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "    if (\n",
    "        len(\n",
    "            df_jobs_labeled[\n",
    "                df_jobs_labeled[str(col)].map(\n",
    "                    df_jobs_labeled[str(col)].value_counts() > 50\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        != 0\n",
    "    ):\n",
    "\n",
    "        # BOW Split\n",
    "        train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, prepared_text = split_data(df_jobs_labeled, vectorizer, col, text_col)\n",
    "\n",
    "        if search_enabled is False:\n",
    "            df_jobs_labeled, classifier, vectorizers, selector, table_df = no_pipe(df_jobs_labeled, vectorizers, selector, classifiers, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "        elif search_enabled is True:\n",
    "            df_jobs_labeled, searchcv, classifier, vectorizers, selector, table_df = pipe(df_jobs_labeled, vectorizers_pipe, selectors_pipe, classifiers_pipe, col, text_col, scoring, table_df, table_save_path, models_save_path, csv_file_name, pickle_file_name, excel_file_name, latex_file_name, markdown_file_name)\n",
    "\n",
    "    print('-' * 20)\n",
    "    print('\\n')\n",
    "    print(f'============================ FINISHED PROCESSING {col.upper()} ============================')\n",
    "    print('\\n')\n",
    "    print('-' * 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Not using Search')\n",
    "\n",
    "if use_dict_for_classifiers_vectorizers is True:\n",
    "    print('Using dict for classifiers and vectorizers.')\n",
    "    for vectorizer_name, vectorizer_and_params in vectorizers.items():\n",
    "        vectorizer = vectorizer_and_params[0]\n",
    "        vectorizer_params = vectorizer_and_params[1]\n",
    "        vectorizer.set_params(**vectorizer_params)\n",
    "        df_jobs_labeled, vectorizer, selector, train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, feature_names, unique_features, vocabulary_map, best_threshold_boe, best_score_boe, boe_pred, boe_validate_pred = vectorize(vectorizer, vectorizer_name, selector, df_jobs_labeled, col, text_col)\n",
    "\n",
    "        for classifier_name, classifier_and_params in classifiers.items():\n",
    "            classifier = classifier_and_params[0]\n",
    "            classifier_params = classifier_and_params[1]\n",
    "            classifier.set_params(**classifier_params)\n",
    "            # classifier, table_df, X_test, y_test, y_test_prob_pred = classify_and_evaluate(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred, table_df, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94000ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier_name == 'DummyClassifier' and use_dict_for_classifiers_vectorizers is False:\n",
    "    classifier_name += f' - {str(classifier.strategy).title()}'\n",
    "# classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score = classify(train, X_train, y_train, test, X_test, y_test, validate, X_validate, y_validate, classifier, classifier_name, vectorizer, vectorizer_name, boe_pred, boe_validate_pred)\n",
    "# classifier, table_df = evaluate_model(X_train, y_train, X_test, y_test, table_df, classifier, classifier_name, vectorizer, vectorizer_name, col, text_col, scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825727f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print(\n",
    "    f'============================ {str(col)}: {vectorizer_name} + {classifier_name} passed ============================'\n",
    ")\n",
    "num = len(test) // 2\n",
    "\n",
    "# BOW model\n",
    "if classifier_name == 'GaussianNB':\n",
    "    X_train = X_train.todense()\n",
    "    X_test = X_test.todense()\n",
    "    X_validate = X_validate.todense()\n",
    "\n",
    "if classifier_name == 'Sequential':\n",
    "    classifier.compile(loss='categorical_crossentropy')\n",
    "if hasattr(classifier, 'decision_function') and not hasattr(classifier, 'predict_proba'):\n",
    "    classifier = CalibratedClassifierCV(classifier, cv = cv, method = 'sigmoid')\n",
    "\n",
    "final_classifier = classifier\n",
    "classifier = classifier.fit(X_train, y_train)\n",
    "\n",
    "if hasattr(classifier, 'predict_proba'):\n",
    "    y_test_prob_pred = classifier.predict_proba(X_test)\n",
    "    y_validate_prob_pred = classifier.predict_proba(X_validate)\n",
    "elif hasattr(classifier, '_predict_proba_lr'):\n",
    "    y_test_prob_pred = classifier._predict_proba_lr(X_test)\n",
    "    y_validate_prob_pred = classifier._predict_proba_lr(X_validate)\n",
    "else:\n",
    "    raise(f'{classifier_name} has neither predict_proba nor _predict_proba_lr attributes.')\n",
    "\n",
    "# final_classifier, X_test, y_test, y_test_prob_pred, best_threshold, best_score, df_preds = augment(classifier, classifier_name, vectorizer, vectorizer_name, final_classifier, test, y_test, y_test_prob_pred, validate, y_validate, y_validate_prob_pred, boe_pred, boe_validate_pred, scoring, print_enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b8173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(test) // 2\n",
    "\n",
    "best_threshold, best_score = calculate_best_threshold(y_test[:num], y_test_prob_pred[:num], scoring, print_enabled)\n",
    "\n",
    "if (boe_pred is not None) and (boe_validate_pred is not None):\n",
    "    num = len(boe_pred) // 2\n",
    "\n",
    "    X_final_augmented_train = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[:num],\n",
    "        \"boe_pred\": boe_pred[:num].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[:num],\n",
    "        \"num_chars\": test[\"num_chars\"].values[:num]})\n",
    "    y_final_augmented_train = y_test[:num]\n",
    "\n",
    "    X_final_augmented_test = pd.DataFrame({\n",
    "        \"y_test_prob_pred\": y_test_prob_pred[num:],\n",
    "        \"boe_pred\": boe_pred[num:].squeeze(),\n",
    "        \"num_words\": test[\"num_words\"].values[num:],\n",
    "        \"num_chars\": test[\"num_chars\"].values[num:]})\n",
    "    y_final_augmented_test = y_test[num:]\n",
    "\n",
    "    final_classifier = final_classifier.fit(X_final_augmented_train, y_final_augmented_train)\n",
    "\n",
    "    if hasattr(final_classifier, 'predict_proba'):\n",
    "        y_final_test_prob_pred = final_classifier.predict_proba(X_final_augmented_test)[:, 1]\n",
    "    elif hasattr(final_classifier, '_predict_proba_lr'):\n",
    "        y_final_test_prob_pred = final_classifier._predict_proba_lr(X_final_augmented_test)[:, 1]\n",
    "\n",
    "    best_threshold_final, best_score_final = calculate_best_threshold(y_final_augmented_test, y_final_test_prob_pred, scoring, print_enabled)\n",
    "\n",
    "    X_final_augmented_validate = pd.DataFrame({\n",
    "        \"y_validate_prob_pred\": y_validate_prob_pred,\n",
    "        \"boe_validate_pred\": boe_validate_pred.squeeze(),\n",
    "        \"num_words\": validate[\"num_words\"].values,\n",
    "        \"num_chars\": validate[\"num_chars\"].values})\n",
    "    y_final_augmented_validate = y_validate\n",
    "\n",
    "    y_final_validate_prob_pred = final_classifier.predict_proba(X_final_augmented_validate)[:,1]\n",
    "\n",
    "    df_preds = get_df_preds(best_threshold_final, y_final_validate_prob_pred, validate, col, vectorizer_name, classifier_name)\n",
    "\n",
    "elif (boe_pred is None) and (boe_validate_pred is None):\n",
    "    final_classifier = classifier\n",
    "    X_final_augmented_validate = X_test\n",
    "    y_final_augmented_validate = y_test\n",
    "    y_final_validate_prob_pred = y_test_prob_pred\n",
    "    df_preds = get_df_preds(best_threshold, y_final_validate_prob_pred, test, col, vectorizer_name, classifier_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:num].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ffb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test[:num]\n",
    "y_test_pred = y_test_prob_pred[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = -1\n",
    "best_score = -1\n",
    "for threshold in np.arange(0.01, 0.801, 0.01):\n",
    "    threshold = np.round(threshold, 2)\n",
    "\n",
    "    # globals()[f'metrics.{scoring.lower()}_score']\n",
    "    if scoring.lower() == 'recall':\n",
    "        scorer = metrics.recall_score\n",
    "    elif scoring.lower() == 'f1 score':\n",
    "        scorer = metrics.f1_score\n",
    "    else:\n",
    "        raise ValueError(f'{scoring.title()} is not a valid score')\n",
    "\n",
    "    emb_model_score = scorer(y_true=y_test, y_pred=(y_test_pred > threshold).astype(int))\n",
    "    if emb_model_score > best_score:\n",
    "        best_score = emb_model_score\n",
    "        best_threshold = threshold\n",
    "    if print_enabled:\n",
    "        print(f'{scoring.title()} at threshold {threshold}: {emb_model_score}')\n",
    "print(f'{scoring.title()} at best threshold {best_threshold}: {best_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd65a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('study1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5b0d7544f82776c2b902af54887e7cde1aa7d2da4fd982551ffc3948bf7522f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
