{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "50d4c434",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
                "    for _ in range(5):\n",
                "\n",
                "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "            code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "            if code_dir is not None:\n",
                "                break\n",
                "else:\n",
                "    code_dir = str(Path.cwd())\n",
                "sys.path.append(code_dir)\n",
                "\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "fef3f604",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0a59f439af894b19bbef18688024bbbf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 640x480 with 0 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "55afc383",
            "metadata": {},
            "source": [
                "### Set variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "7b36fe18",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                }
            ],
            "source": [
                "# Variables\n",
                "t = time.time()\n",
                "n_jobs = -1\n",
                "n_splits = 10\n",
                "n_repeats = 3\n",
                "random_state = 42\n",
                "refit = True\n",
                "class_weight = 'balanced'\n",
                "cv = RepeatedStratifiedKFold(\n",
                "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
                ")\n",
                "scoring = 'recall'\n",
                "scores = [\n",
                "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
                "    'explained_variance', 'matthews_corrcoef'\n",
                "]\n",
                "scorers = {\n",
                "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
                "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
                "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
                "}\n",
                "protocol = pickle.HIGHEST_PROTOCOL\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "classified_columns = ['Warmth_Probability', 'Competence_Probability']\n",
                "metrics_dict = {\n",
                "    f'{scoring.title()} Best Score': np.nan,\n",
                "    f'{scoring.title()} Best Threshold': np.nan,\n",
                "    'Train - Mean Cross Validation Score': np.nan,\n",
                "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Test - Mean Cross Validation Score': np.nan,\n",
                "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Explained Variance': np.nan,\n",
                "    'Accuracy': np.nan,\n",
                "    'Balanced Accuracy': np.nan,\n",
                "    'Precision': np.nan,\n",
                "    'Average Precision': np.nan,\n",
                "    'Recall': np.nan,\n",
                "    'F1-score': np.nan,\n",
                "    'Matthews Correlation Coefficient': np.nan,\n",
                "    'Brier Score': np.nan,\n",
                "    'Fowlkes–Mallows Index': np.nan,\n",
                "    'R2 Score': np.nan,\n",
                "    'ROC': np.nan,\n",
                "    'AUC': np.nan,\n",
                "    'Log Loss/Cross Entropy': np.nan,\n",
                "    'Cohen’s Kappa': np.nan,\n",
                "    'Geometric Mean': np.nan,\n",
                "    'Classification Report': np.nan,\n",
                "    'Imbalanced Classification Report': np.nan,\n",
                "    'Confusion Matrix': np.nan,\n",
                "    'Normalized Confusion Matrix': np.nan,\n",
                "}\n",
                "\n",
                "# Transformer variables\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
                ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "# Set random seed\n",
                "random_state = 42\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)\n",
                "torch.manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "torch.Generator(device_name).manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "accelerator = Accelerator()\n",
                "torch.autograd.set_detect_anomaly(True)\n",
                "os.environ.get('TOKENIZERS_PARALLELISM')\n",
                "os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO')\n",
                "os.environ.get('TRANSFORMERS_CACHE')\n",
                "openai_token = os.environ['OPENAI_API_KEY']\n",
                "huggingface_token = os.environ['HUGGINGFACE_API_KEY']\n",
                "# load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4'\n",
                "quantization_config_dict = {\n",
                "    'load_in_8bit': True,\n",
                "    'llm_int8_skip_modules': ['lm_head'],\n",
                "}\n",
                "hyperparameter_tuning = True\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2a84d153",
            "metadata": {},
            "source": [
                "# Functions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d468ccc0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_classified_df(\n",
                "    done_dfs_name, df_jobs_len, df_save_dir\n",
                "):\n",
                "    print(f'Loading {done_dfs_name}...')\n",
                "    df_jobs = pd.read_pickle(f'{df_save_dir}{done_dfs_name}.pkl')\n",
                "    assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "    print(f'Dataframe {done_dfs_name} loaded with shape: {df_jobs.shape}')\n",
                "\n",
                "    return df_jobs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "d7036357",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, encodings):\n",
                "        self.encodings = encodings\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return {\n",
                "            key: val[idx].clone().detach().to(device)\n",
                "            for key, val in self.encodings.items()\n",
                "        }\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.encodings['input_ids'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "f7286eeb",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImbTrainer(Trainer):\n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.class_weights = self._calculate_class_weights(self.train_dataset)\n",
                "\n",
                "    def _calculate_class_weights(self, dataset):\n",
                "        # Count the number of samples in each class\n",
                "        class_counts = torch.zeros(self.model.config.num_labels)\n",
                "        for label in dataset.labels:\n",
                "            class_counts[label] += 1\n",
                "\n",
                "        # Calculate the inverse frequency of each class\n",
                "        inv_frequencies = 1 / class_counts\n",
                "\n",
                "        # Normalize the inverse frequencies so that they sum up to 1\n",
                "        sum_inv_frequencies = torch.sum(inv_frequencies)\n",
                "        return inv_frequencies / sum_inv_frequencies\n",
                "\n",
                "    def compute_loss(self, model, inputs, return_outputs=False):\n",
                "        labels = inputs.pop(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(device))\n",
                "        loss = loss_fct(outputs.logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "9ad2b64f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to get y_pred and y_pred_prob\n",
                "def preprocess_logits_for_metrics_from_logits(y_pred_logits):\n",
                "\n",
                "    # Get y_pred\n",
                "    print('-'*20)\n",
                "    y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
                "    print('Getting y_pred through argmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1).cpu().numpy()\n",
                "        print('Using torch.argmax.')\n",
                "    except Exception:\n",
                "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
                "        print('Using np.argmax.')\n",
                "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
                "    print('-'*20)\n",
                "    print('Flattening y_pred...')\n",
                "    y_pred = y_pred_array.flatten().tolist()\n",
                "    print(f'y_pred length: {len(y_pred)}')\n",
                "    print('-'*20)\n",
                "\n",
                "    # Get y_pred_prob\n",
                "    print('-'*20)\n",
                "    print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
                "        print('Using torch.nn.functional.softmax.')\n",
                "    except Exception:\n",
                "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
                "        print('Using scipy.special.softmax.')\n",
                "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
                "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
                "    print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
                "    print('-'*20)\n",
                "    print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
                "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
                "    print(f'y_pred length: {len(y_pred_prob)}')\n",
                "    print('-'*20)\n",
                "\n",
                "    y_pred_logits_tensor.clone().detach()\n",
                "\n",
                "    return (\n",
                "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "e048cbc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def prob_confirmatory_tests(y_pred, y_pred_prob):\n",
                "\n",
                "    # Confirmatory Regression\n",
                "    print('+'*20)\n",
                "    print('Confirmatory Tests validating the linear relationship between y_pred and y_pred_prob')\n",
                "    print('-'*20)\n",
                "    print('T-Test y_pred_prob ~ y_pred:')\n",
                "    levene = scipy.stats.levene(y_pred_prob, y_pred)\n",
                "    equal_var_levene = levene.pvalue < 0.05\n",
                "    print(scipy.stats.ttest_ind(y_pred_prob, y_pred, equal_var=equal_var_levene))\n",
                "\n",
                "    print('\\n')\n",
                "    print('-'*20)\n",
                "    print('Logit y_pred ~ y_pred_prob:')\n",
                "    try:\n",
                "        logit_model = sm.Logit(endog=y_pred, exog=y_pred_prob)\n",
                "        logit_results = logit_model.fit()\n",
                "        std_coef = logit_results.params[0] / np.std(y_pred_prob)\n",
                "        std_err = logit_results.bse[0]\n",
                "        log_likelihood = logit_results.llf\n",
                "        print(logit_results.summary())\n",
                "        print('-'*20)\n",
                "        print(f'Std Coef: {std_coef}')\n",
                "        print(f'Std Err: {std_err}')\n",
                "        print(f'Log Likelihood: {log_likelihood}')\n",
                "    except np.linalg.LinAlgError:\n",
                "        print('Logit LinAlgError: Singular matrix. Skipping confirmatory tests.')\n",
                "\n",
                "    print('-'*20)\n",
                "    print('\\n')\n",
                "    print('-'*20)\n",
                "    print('OLS y_pred_prob ~ y_pred:')\n",
                "    try:\n",
                "        ols_model = sm.OLS(endog=y_pred_prob, exog=y_pred)\n",
                "        ols_results = ols_model.fit()\n",
                "        std_coef = ols_results.params[0] / np.std(y_pred)\n",
                "        std_err = ols_results.bse[0]\n",
                "        print(ols_results.summary())\n",
                "        print('-'*20)\n",
                "        print(f'Std Coef: {std_coef}')\n",
                "        print(f'Std Err: {std_err}')\n",
                "    except np.linalg.LinAlgError:\n",
                "        print('OLS LinAlgError: Singular matrix. Skipping confirmatory tests.')\n",
                "\n",
                "    print('-'*20)\n",
                "    print('+'*20)\n",
                "    print('\\n')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "036fcf10",
            "metadata": {},
            "source": [
                "# Classifying"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fd32c435",
            "metadata": {},
            "source": [
                "### READ DATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "858310d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # ATTN: IF THIS IS THE FIRST TIME YOU ARE CLASSIFYING JOBS, UNCOMMENT AND RUN THIS CODE\n",
                "# with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "#     df_jobs_len = int(f.read())\n",
                "# df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "# assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "1e310829",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading df_jobs_for_classification...\n",
                        "Dataframe df_jobs_for_classification loaded with shape: (307154, 83)\n"
                    ]
                }
            ],
            "source": [
                "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "    df_jobs_len = int(f.read())\n",
                "\n",
                "for done_dfs_name in ['df_jobs_classified', 'df_jobs_classified_Warmth_Competence', 'df_jobs_classified_Warmth']:\n",
                "    if os.path.exists(f'{df_save_dir}{done_dfs_name}.pkl') and os.path.getsize(f'{df_save_dir}{done_dfs_name}.pkl') > 0:\n",
                "\n",
                "        df_jobs = pd.read_pickle(f'{df_save_dir}{done_dfs_name}.pkl')\n",
                "        assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "\n",
                "        if (\n",
                "            done_dfs_name == 'df_jobs_classified'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' in df_jobs.columns\n",
                "            and 'Competence_Probability' in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(done_dfs_name, df_jobs_len, df_save_dir)\n",
                "            break\n",
                "\n",
                "        elif (\n",
                "            done_dfs_name == 'df_jobs_classified_Warmth_Competence'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' in df_jobs.columns\n",
                "            and 'Competence_Probability' in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(done_dfs_name, df_jobs_len, df_save_dir)\n",
                "            break\n",
                "\n",
                "        elif (\n",
                "            done_dfs_name == 'df_jobs_classified_Warmth'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' not in df_jobs.columns\n",
                "            and 'Competence_Probability' not in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(done_dfs_name, df_jobs_len, df_save_dir)\n",
                "            break\n",
                "\n",
                "else:\n",
                "    print('Loading df_jobs_for_classification...')\n",
                "    df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "    assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "    print(f'Dataframe df_jobs_for_classification loaded with shape: {df_jobs.shape}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "a0212dc9",
            "metadata": {
                "code_folding": [],
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "########################################\n",
                        "Starting!\n",
                        "########################################\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------\n",
                        "--------------------\n",
                        "Using BertForSequenceClassification from Transformers pipeline.\n",
                        "Loading Fitted Transformer BertForSequenceClassification from pretrained.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Done loading Fitted Transformer BertForSequenceClassification from pretrained!\n",
                        "Transformers pipeline caused RuntimeError. Using Trainer instead.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Getting estimator for Warmth.\n",
                        "--------------------\n",
                        "Classifying data using BertForSequenceClassification for Warmth.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "de4efc5e59c347e587580dfb4d35ecf5",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/15358 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "%%time\n",
                "print('#'*40)\n",
                "print('Starting!')\n",
                "print('#'*40)\n",
                "\n",
                "done_cols = ''\n",
                "final_estimators_dict = {\n",
                "    'Warmth': {\n",
                "        'vectorizer_name': 'BERTBASEUNCASED',\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "    },\n",
                "    'Competence': {\n",
                "        'vectorizer_name': 'BERTBASEUNCASED',\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "    },\n",
                "}\n",
                "\n",
                "for col in tqdm.tqdm(analysis_columns):\n",
                "    if col not in df_jobs.columns:\n",
                "        print('-'*20)\n",
                "        final_estimators_dict[col]['path_suffix'] = path_suffix = f' - {col} - {(vectorizer_name := final_estimators_dict[col][\"vectorizer_name\"])} + {(classifier_name := final_estimators_dict[col][\"classifier_name\"])} (Save_protocol={protocol})'\n",
                "\n",
                "        if classifier_name in list(classifiers_pipe.keys()):\n",
                "            method = 'Supervised'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read()\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            print('Loading Supervised Estimator.')\n",
                "            with open(\n",
                "                f'{results_save_path}{method} Fitted Estimator {path_suffix}.pkl', 'rb'\n",
                "            ) as f:\n",
                "                estimator = joblib.load(f)\n",
                "            print('Done loading Supervised Estimator!')\n",
                "\n",
                "            print('-'*20)\n",
                "            print('Classifying data.')\n",
                "            X = np.array(list(df_jobs[text_col].astype('str').values))\n",
                "            df_jobs[col] = estimator.predict(X)\n",
                "            if hasattr(estimator, 'predict_proba'):\n",
                "                # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
                "                df_jobs[f'{col}_Probability'] = estimator.predict_proba(X)[:, -1]\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "        elif classifier_name in list(transformers_pipe.keys()):\n",
                "            method = 'Transformers'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read().strip('\\n')\n",
                "            with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
                "                done_xy_save_path = f.read().strip('\\n')\n",
                "            with open(f'{done_xy_save_path}{method} training_args_dict - {col} - {vectorizer_name} + {classifier_name}.json', 'r') as f:\n",
                "                training_args_dict = json.load(f)\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            model = transformers_pipe[classifier_name]['model']\n",
                "            tokenizer = transformers_pipe[classifier_name]['tokenizer']\n",
                "            config = transformers_pipe[classifier_name]['config']\n",
                "\n",
                "            print(f'Loading Fitted Transformer {classifier_name} from pretrained.')\n",
                "            estimator_dir = f'{results_save_path}{method} Fitted Estimator{path_suffix}.model'\n",
                "            fitted_estimator = model.from_pretrained(estimator_dir, trust_remote_code=True)\n",
                "            if hasattr(fitted_estimator, 'to'):\n",
                "                fitted_estimator = fitted_estimator.to(device)\n",
                "            tokenizer = tokenizer.from_pretrained(estimator_dir, trust_remote_code=True)\n",
                "            config = config.from_pretrained(f'{estimator_dir}/config.json', trust_remote_code=True)\n",
                "            print(f'Done loading Fitted Transformer {classifier_name} from pretrained!')\n",
                "\n",
                "            # try:\n",
                "            #     # Get predictions\n",
                "            #     # Accelerate model\n",
                "            #     (\n",
                "            #         fitted_estimator, tokenizer\n",
                "            #     ) = accelerator.prepare(\n",
                "            #         fitted_estimator, tokenizer\n",
                "            #     )\n",
                "            #     classifier = transformers.pipeline(model=fitted_estimator, tokenizer=tokenizer, function_to_apply='softmax', device=device, framework='pt', task='text-classification', return_all_scores=False)\n",
                "            #     df_jobs[col] = df_jobs[text_col].astype(str).apply(lambda x: [pred['label'].split('LABEL_')[1] for pred in classifier(x)][0])\n",
                "            #     df_jobs[f'{col}_Probability'] = df_jobs[text_col].astype(str).apply(lambda x: [pred['score'] for pred in classifier(x)][0])\n",
                "\n",
                "            # except RuntimeError:\n",
                "            print('Transformers pipeline caused RuntimeError. Using Trainer instead.')\n",
                "            # Tokenize\n",
                "            X = df_jobs[text_col].astype('str').values.tolist()\n",
                "            encodings = tokenizer(\n",
                "            X, truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor, add_special_tokens=True\n",
                "            ).to(device)\n",
                "            dataset = ToDataset(encodings)\n",
                "            # Accelerate model\n",
                "            (\n",
                "                fitted_estimator, tokenizer, dataset\n",
                "            ) = accelerator.prepare(\n",
                "                fitted_estimator, tokenizer, dataset\n",
                "            )\n",
                "\n",
                "            print(f'Getting estimator for {col}.')\n",
                "            estimator = Trainer(\n",
                "                model=fitted_estimator,\n",
                "                tokenizer=tokenizer,\n",
                "                args=TrainingArguments(**training_args_dict),\n",
                "            )\n",
                "            if estimator.place_model_on_device:\n",
                "                estimator.model.to(device)\n",
                "\n",
                "            print('-'*20)\n",
                "            print(f'Classifying data using {classifier_name} for {col}.')\n",
                "            (y_pred_logits, y_labels, metrics) = estimator.predict(dataset)\n",
                "            y_pred_array, y_pred, y_pred_prob_array, y_pred_prob = preprocess_logits_for_metrics_from_logits(y_pred_logits)\n",
                "\n",
                "            # Assign to dataframe\n",
                "            df_jobs[col] = y_pred\n",
                "            df_jobs[f'{col}_Probability'] = y_pred_prob\n",
                "\n",
                "            # Confirmatory Regression\n",
                "            prob_confirmatory_tests(df_jobs[col].values, df_jobs[f'{col}_Probability'].values)\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "        done_cols += f'_{col}'\n",
                "        assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "        df_jobs.to_pickle(f'{df_save_dir}df_jobs_classified{done_cols}.pkl')\n",
                "        df_jobs.to_csv(f'{df_save_dir}df_jobs_classified{done_cols}.csv', index=False)\n",
                "    else:\n",
                "        print('-'*20)\n",
                "        print(f'Column {col} already exists in dataframe. Skipping.')\n",
                "        print('-'*20)\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1154fedc",
            "metadata": {},
            "source": [
                "## Inspect classified data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0d5dd82",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_classified.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_classified.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06e26f9b",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "    df_jobs_len = int(f.read())\n",
                "\n",
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_classified.pkl')\n",
                "assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82288367",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ddc3eba",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "094dd690",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Job ID', 'Job Description spacy_sentencized']].info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c201136",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Job ID', 'Job Description spacy_sentencized']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32e95c5c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Conver Job ID and Sentence to str\n",
                "str_cols = [\n",
                "    'Job ID',\n",
                "    'Job Description',\n",
                "    'Job Description spacy_sentencized',\n",
                "]\n",
                "\n",
                "for col in str_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(str, errors='ignore').progress_apply(lambda x: x.strip().replace('[', '').replace(']', ''))\n",
                "    df_jobs[col] = df_jobs[col].apply(lambda x: x.strip())\n",
                "    df_jobs[col] = df_jobs[col].apply(lambda x: unicodedata.normalize('NFKD', x.encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n",
                "    print(f'{col} converted to str.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, str))) else f'{col} NOT converted to str.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df35f2d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Warmth and Competence to int\n",
                "# Warmth 1 = 1741, Competence 1 = 3043\n",
                "int_cols = [\n",
                "    'Warmth',\n",
                "    'Competence',\n",
                "]\n",
                "\n",
                "for col in int_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(np.int64, errors='ignore')\n",
                "    print(f'{col} converted to int.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, int))) else f'{col} NOT converted to int.')\n",
                "    print(f'{col} value counts:\\n{df_jobs[col].value_counts()}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "514efabb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Warmth and Competence to int\n",
                "# Warmth 1 = 1741, Competence 1 = 3043\n",
                "float_cols = [\n",
                "    'Warmth_Probability',\n",
                "    'Competence_Probability',\n",
                "]\n",
                "\n",
                "for col in float_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(np.float64, errors='ignore')\n",
                "    print(f'{col} converted to float.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, float))) else f'{col} NOT converted to int.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fac73e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48b1df0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.dropna(\n",
                "    subset=[\n",
                "        'Job ID', 'Job Description spacy_sentencized', 'Warmth', 'Warmth_Probability', 'Competence', 'Competence_Probability'\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7be9264f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.drop_duplicates(subset=['Job ID', 'Job Description spacy_sentencized'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e947093",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6ed0fd9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b78bbb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Warmth', 'Warmth_Probability', 'Competence', 'Competence_Probability']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8085df26",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Warmth', 'Warmth_Probability', 'Competence', 'Competence_Probability']].describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59154094",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcf92d6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving classified df_jobs length {len(df_jobs)} to txt file.')\n",
                "with open(f'{data_dir}df_jobs_for_analysis_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_jobs)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e252eb4",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=analysis_columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1635aac9",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=classified_columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dde4ff43",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=dvs_all)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "adc3c701",
            "metadata": {},
            "source": [
                "### Save dataframe\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3057728d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de943922",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving classified df_jobs length {len(df_jobs)} to txt file.')\n",
                "with open(f'{data_dir}df_jobs_for_analysis_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_jobs)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f2e8dea",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Automating_Equity1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {},
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
