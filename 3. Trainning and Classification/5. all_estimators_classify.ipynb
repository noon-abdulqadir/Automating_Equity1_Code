{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50d4c434",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
                "    for _ in range(5):\n",
                "\n",
                "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "            code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "            if code_dir is not None:\n",
                "                break\n",
                "else:\n",
                "    code_dir = str(Path.cwd())\n",
                "sys.path.append(code_dir)\n",
                "\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fef3f604",
            "metadata": {},
            "outputs": [],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from setup_module.estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "55afc383",
            "metadata": {},
            "source": [
                "### Set variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b36fe18",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variables\n",
                "t = time.time()\n",
                "n_jobs = -1\n",
                "n_splits = 10\n",
                "n_repeats = 3\n",
                "random_state = 42\n",
                "refit = True\n",
                "class_weight = 'balanced'\n",
                "cv = RepeatedStratifiedKFold(\n",
                "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
                ")\n",
                "scoring = 'recall'\n",
                "scores = [\n",
                "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
                "    'explained_variance', 'matthews_corrcoef'\n",
                "]\n",
                "scorers = {\n",
                "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
                "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
                "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
                "}\n",
                "protocol = pickle.HIGHEST_PROTOCOL\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "classified_columns = ['Warmth_Probability', 'Competence_Probability']\n",
                "metrics_dict = {\n",
                "    f'{scoring.title()} Best Score': np.nan,\n",
                "    f'{scoring.title()} Best Threshold': np.nan,\n",
                "    'Train - Mean Cross Validation Score': np.nan,\n",
                "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Test - Mean Cross Validation Score': np.nan,\n",
                "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Explained Variance': np.nan,\n",
                "    'Accuracy': np.nan,\n",
                "    'Balanced Accuracy': np.nan,\n",
                "    'Precision': np.nan,\n",
                "    'Average Precision': np.nan,\n",
                "    'Recall': np.nan,\n",
                "    'F1-score': np.nan,\n",
                "    'Matthews Correlation Coefficient': np.nan,\n",
                "    'Brier Score': np.nan,\n",
                "    'Fowlkes–Mallows Index': np.nan,\n",
                "    'R2 Score': np.nan,\n",
                "    'ROC': np.nan,\n",
                "    'AUC': np.nan,\n",
                "    'Log Loss/Cross Entropy': np.nan,\n",
                "    'Cohen’s Kappa': np.nan,\n",
                "    'Geometric Mean': np.nan,\n",
                "    'Classification Report': np.nan,\n",
                "    'Imbalanced Classification Report': np.nan,\n",
                "    'Confusion Matrix': np.nan,\n",
                "    'Normalized Confusion Matrix': np.nan,\n",
                "}\n",
                "\n",
                "# Transformer variables\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
                ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "# Set random seed\n",
                "random_state = 42\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)\n",
                "torch.manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "torch.Generator(device_name).manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "accelerator = Accelerator()\n",
                "torch.autograd.set_detect_anomaly(True)\n",
                "os.environ.get('TOKENIZERS_PARALLELISM')\n",
                "os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO')\n",
                "os.environ.get('TRANSFORMERS_CACHE')\n",
                "openai_token = os.environ['OPENAI_API_KEY']\n",
                "huggingface_token = os.environ['HUGGINGFACE_API_KEY']\n",
                "# load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4'\n",
                "quantization_config_dict = {\n",
                "    'load_in_8bit': True,\n",
                "    'llm_int8_skip_modules': ['lm_head'],\n",
                "}\n",
                "hyperparameter_tuning = True\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2a84d153",
            "metadata": {},
            "source": [
                "# Functions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d468ccc0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_classified_df(\n",
                "    df, df_name, df_len, done_dfs_name, df_save_dir\n",
                "):\n",
                "    print(f'Loading {df_name}_{done_dfs_name}...')\n",
                "    df = pd.read_pickle(f'{df_save_dir}{df_name}_{done_dfs_name}.pkl')\n",
                "    assert len(df) == df_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_len} BUT IS OF LENGTH {len(df)}'\n",
                "    print(f'Dataframe {df_name}_{done_dfs_name} loaded with shape: {df.shape}')\n",
                "\n",
                "    return df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d94f389",
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_prediction(text):\n",
                "    if pred := classifier(text):\n",
                "        return pd.Series([pred[0]['label'].split('LABEL_')[1], pred[0]['score']])\n",
                "    else:\n",
                "        return None, None\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d7036357",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, encodings, labels):\n",
                "        self.encodings = encodings\n",
                "        self.labels = labels\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        # check is encodings and labels are tensors\n",
                "        for key, val in self.encodings.items():\n",
                "            if not torch.is_tensor(val[idx]):\n",
                "                self.encodings[key][idx] = torch.tensor(val[idx], dtype=torch.long, device=device)\n",
                "        if not torch.is_tensor(self.labels[idx]):\n",
                "            self.labels[idx] = torch.tensor(self.labels[idx], dtype=torch.long, device=device)\n",
                "        item = {key: val[idx].to(device) for key, val in self.encodings.items()}\n",
                "        item['labels'] = self.labels[idx]\n",
                "        return item\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.encodings['input_ids'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7286eeb",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImbTrainer(Trainer):\n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.class_weights, self.weight, self.pos_weight = self._calculate_class_weights(self.train_dataset)\n",
                "\n",
                "    def _calculate_class_weights(self, dataset):\n",
                "        # Count the number of samples in each class\n",
                "        class_counts = torch.bincount(torch.tensor(dataset.labels, device=device))\n",
                "\n",
                "        # Calculate the inverse frequency of each class with Laplace smoothing\n",
                "        inv_frequencies = (len(dataset) + 1) / (class_counts + 1)\n",
                "        class_weights = inv_frequencies / torch.sum(inv_frequencies)\n",
                "\n",
                "        # Calculate weight and pos_weight\n",
                "        num_negative = class_counts[0].item()\n",
                "        num_positive = class_counts[1].item()\n",
                "        weight_neg = num_positive / (num_negative + 1e-5)\n",
                "        weight_pos = num_negative / (num_positive + 1e-5)\n",
                "        weight = torch.tensor([weight_neg, weight_pos], device=device)\n",
                "        pos_weight = torch.tensor([weight_pos], device=device)\n",
                "\n",
                "        return class_weights, weight, pos_weight\n",
                "\n",
                "    def _calculate_calibration_loss(self, logits, labels):\n",
                "        return nn.BCEWithLogitsLoss(\n",
                "            weight=self.weight.to(device)\n",
                "        )(\n",
                "            logits.to(device),\n",
                "            torch.nn.functional.one_hot(labels, logits.size(-1)).long().float().to(device)\n",
                "        )\n",
                "\n",
                "    def compute_loss(self, model, inputs, return_outputs=False):\n",
                "        loss = super().compute_loss(model, inputs, return_outputs=True)[0]\n",
                "        labels = inputs.pop('labels').to(device)\n",
                "        outputs = model(**inputs)\n",
                "        logits = outputs.get('logits').to(device)\n",
                "        y_pred = torch.argmax(logits, dim=-1).to(device)\n",
                "        y_pred_prob = torch.softmax(logits, dim=-1)[:, 1].to(device)\n",
                "\n",
                "        # Get recall and precision\n",
                "        accuracy = binary_accuracy(y_pred, labels).to(device)\n",
                "        recall = binary_recall(y_pred, labels).to(device)\n",
                "        precision = binary_precision(y_pred, labels).to(device)\n",
                "\n",
                "        if accuracy < 0.7 and recall < 0.7 and precision < 0.7:\n",
                "            loss = self._calculate_calibration_loss(logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9ad2b64f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to get y_pred and y_pred_prob\n",
                "def preprocess_logits_for_metrics_from_logits(y_pred_logits, print_enabled=None):\n",
                "\n",
                "    if print_enabled is None:\n",
                "        print_enabled = True\n",
                "\n",
                "    # Get y_pred\n",
                "    if not torch.is_tensor(y_pred_logits):\n",
                "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
                "    if print_enabled:\n",
                "        print('-'*20)\n",
                "        print('Getting y_pred through argmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1)\n",
                "        if print_enabled: print('Using torch.argmax.')\n",
                "    except Exception:\n",
                "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
                "        if print_enabled: print('Using np.argmax.')\n",
                "    if print_enabled:\n",
                "        print(f'y_pred_array shape: {y_pred_array.shape}')\n",
                "        print('-'*20)\n",
                "        print('Flattening y_pred...')\n",
                "    y_pred = y_pred_array.flatten().tolist()\n",
                "    if print_enabled:\n",
                "        print(f'y_pred length: {len(y_pred)}')\n",
                "        print('-'*20)\n",
                "\n",
                "    # Get y_pred_prob\n",
                "    if print_enabled:\n",
                "        print('-'*20)\n",
                "        print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1)\n",
                "        if print_enabled: print('Using torch.nn.functional.softmax.')\n",
                "    except Exception:\n",
                "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
                "        if print_enabled: print('Using scipy.special.softmax.')\n",
                "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
                "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
                "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
                "    if print_enabled:\n",
                "        print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
                "        print('-'*20)\n",
                "        print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
                "        print(f'y_pred length: {len(y_pred_prob)}')\n",
                "        print('-'*20)\n",
                "\n",
                "    return (\n",
                "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e048cbc8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def prob_confirmatory_tests(y_pred, y_pred_prob):\n",
                "\n",
                "    # Confirmatory Regression\n",
                "    print('+'*20)\n",
                "    print('Confirmatory Tests validating the linear relationship between y_pred and y_pred_prob')\n",
                "    print('-'*20)\n",
                "    print('T-Test y_pred_prob ~ y_pred:')\n",
                "    levene = scipy.stats.levene(y_pred_prob, y_pred)\n",
                "    equal_var_levene = levene.pvalue < 0.05\n",
                "    print(scipy.stats.ttest_ind(y_pred_prob, y_pred, equal_var=equal_var_levene))\n",
                "\n",
                "    print('\\n')\n",
                "    print('-'*20)\n",
                "    print('Logit y_pred ~ y_pred_prob:')\n",
                "    try:\n",
                "        logit_model = sm.Logit(endog=y_pred, exog=y_pred_prob)\n",
                "        logit_results = logit_model.fit()\n",
                "        std_coef = logit_results.params[0] / np.std(y_pred_prob)\n",
                "        std_err = logit_results.bse[0]\n",
                "        log_likelihood = logit_results.llf\n",
                "        print(logit_results.summary())\n",
                "        print('-'*20)\n",
                "        print(f'Std Coef: {std_coef}')\n",
                "        print(f'Std Err: {std_err}')\n",
                "        print(f'Log Likelihood: {log_likelihood}')\n",
                "    except Exception as e:\n",
                "        print(type(e).__name__)\n",
                "\n",
                "    print('-'*20)\n",
                "    print('\\n')\n",
                "    print('-'*20)\n",
                "    print('OLS y_pred_prob ~ y_pred:')\n",
                "    try:\n",
                "        ols_model = sm.OLS(endog=y_pred_prob, exog=y_pred)\n",
                "        ols_results = ols_model.fit()\n",
                "        std_coef = ols_results.params[0] / np.std(y_pred)\n",
                "        std_err = ols_results.bse[0]\n",
                "        print(ols_results.summary())\n",
                "        print('-'*20)\n",
                "        print(f'Std Coef: {std_coef}')\n",
                "        print(f'Std Err: {std_err}')\n",
                "    except Exception as e:\n",
                "        print(type(e).__name__)\n",
                "\n",
                "    print('-'*20)\n",
                "    print('+'*20)\n",
                "    print('\\n')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "036fcf10",
            "metadata": {},
            "source": [
                "# Classifying"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fd32c435",
            "metadata": {},
            "source": [
                "### READ DATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "858310d0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# # ATTN: IF THIS IS THE FIRST TIME YOU ARE CLASSIFYING JOBS, UNCOMMENT AND RUN THIS CODE\n",
                "# with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "#     df_jobs_len = int(f.read())\n",
                "# df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "# assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "# print(f'Dataframe df_jobs loaded with shape: {df_jobs.shape}')\n",
                "# with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
                "#     df_manual_len = int(f.read())\n",
                "# df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_training.pkl')\n",
                "# assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
                "# print(f'Dataframe df_manual loaded with shape: {df_manual.shape}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e310829",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "    df_jobs_len = int(f.read())\n",
                "\n",
                "with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
                "    df_manual_len = int(f.read())\n",
                "\n",
                "for done_dfs_name in ['classified', 'classified_Warmth_Competence', 'classified_Warmth']:\n",
                "    if (os.path.exists(f'{df_save_dir}df_jobs_{done_dfs_name}.pkl') and os.path.getsize(f'{df_save_dir}df_jobs_{done_dfs_name}.pkl') > 0) and (os.path.exists(f'{df_save_dir}df_manual_{done_dfs_name}.pkl') and os.path.getsize(f'{df_save_dir}df_manual_{done_dfs_name}.pkl') > 0):\n",
                "\n",
                "        df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_{done_dfs_name}.pkl')\n",
                "        assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "        df_manual = pd.read_pickle(f'{df_save_dir}df_manual_{done_dfs_name}.pkl')\n",
                "        assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
                "\n",
                "        if (\n",
                "            done_dfs_name == 'classified'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' in df_jobs.columns\n",
                "            and 'Competence_Probability' in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(df_jobs, 'df_jobs', df_jobs_len, done_dfs_name, df_save_dir)\n",
                "            df_manual = load_classified_df(df_manual, 'df_manual', df_manual_len, done_dfs_name, df_save_dir)\n",
                "            break\n",
                "\n",
                "        elif (\n",
                "            done_dfs_name == 'classified_Warmth_Competence'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' in df_jobs.columns\n",
                "            and 'Competence_Probability' in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(df_jobs, 'df_jobs', df_jobs_len, done_dfs_name, df_save_dir)\n",
                "            df_manual = load_classified_df(df_manual, 'df_manual', df_manual_len, done_dfs_name, df_save_dir)\n",
                "            break\n",
                "\n",
                "        elif (\n",
                "            done_dfs_name == 'classified_Warmth'\n",
                "            and 'Warmth' in df_jobs.columns\n",
                "            and 'Warmth_Probability' in df_jobs.columns\n",
                "            and 'Competence' not in df_jobs.columns\n",
                "            and 'Competence_Probability' not in df_jobs.columns\n",
                "        ):\n",
                "            df_jobs = load_classified_df(df_jobs, 'df_jobs', df_jobs_len, done_dfs_name, df_save_dir)\n",
                "            df_manual = load_classified_df(df_manual, 'df_manual', df_manual_len, done_dfs_name, df_save_dir)\n",
                "            break\n",
                "\n",
                "else:\n",
                "    print('Loading df_jobs_for_classification...')\n",
                "    df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "    assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "    print(f'Dataframe df_jobs_for_classification loaded with shape: {df_jobs.shape}')\n",
                "    df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_training.pkl')\n",
                "    assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
                "    print(f'Dataframe df_manual_for_training loaded with shape: {df_manual.shape}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a0212dc9",
            "metadata": {
                "code_folding": [],
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%%time\n",
                "print('#'*40)\n",
                "print('Starting!')\n",
                "print('#'*40)\n",
                "\n",
                "done_cols = ''\n",
                "\n",
                "final_estimators_dict = {\n",
                "    'Warmth': {\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "        'vectorizer_name': ''.join(transformers_pipe[\"BertForSequenceClassification\"][\"model_name\"].split('-')).upper(),\n",
                "    },\n",
                "    'Competence': {\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "        'vectorizer_name': ''.join(transformers_pipe[\"BertForSequenceClassification\"][\"model_name\"].split('-')).upper(),\n",
                "    },\n",
                "}\n",
                "\n",
                "for col in tqdm.tqdm(analysis_columns):\n",
                "    if col not in df_jobs.columns and f'{col}_predicted' not in df_manual.columns:\n",
                "        print('-'*20)\n",
                "        final_estimators_dict[col]['path_suffix'] = path_suffix = f' - {col} - {(vectorizer_name := final_estimators_dict[col][\"vectorizer_name\"])} + {(classifier_name := final_estimators_dict[col][\"classifier_name\"])} (Save_protocol={protocol})'\n",
                "\n",
                "        if classifier_name in list(classifiers_pipe.keys()):\n",
                "            method = 'Supervised'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read()\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            print('Loading Supervised Estimator.')\n",
                "            with open(\n",
                "                f'{results_save_path}{method} Fitted Estimator {path_suffix}.pkl', 'rb'\n",
                "            ) as f:\n",
                "                estimator = joblib.load(f)\n",
                "            print('Done loading Supervised Estimator!')\n",
                "\n",
                "            print('-'*20)\n",
                "            print('Classifying data.')\n",
                "            # df_jobs\n",
                "            print('Classifying df_jobs.')\n",
                "            X = np.array(list(df_jobs[text_col].astype('str').values))\n",
                "            df_jobs[col] = estimator.predict(X)\n",
                "            if hasattr(estimator, 'predict_proba'):\n",
                "                # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
                "                df_jobs[f'{col}_Probability'] = estimator.predict_proba(X)[:, -1]\n",
                "            # df_manual\n",
                "            print('Classifying df_manual to generate instrumental variables.')\n",
                "            X_instrument = np.array(list(df_manual[text_col].astype('str').values))\n",
                "            df_manual[f'{col}_predicted'] = estimator.predict(X_instrument)\n",
                "            if hasattr(estimator, 'predict_proba'):\n",
                "                # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
                "                df_manual[f'{col}_Probability_predicted'] = estimator.predict_proba(X_instrument)[:, -1]\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "        elif classifier_name in list(transformers_pipe.keys()):\n",
                "            method = 'Transformers'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read().strip('\\n')\n",
                "            with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
                "                done_xy_save_path = f.read().strip('\\n')\n",
                "            with open(f'{done_xy_save_path}{method} training_args_dict - {col} - {vectorizer_name} + {classifier_name}.json', 'r') as f:\n",
                "                training_args_dict = json.load(f)\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            model = transformers_pipe[classifier_name]['model']\n",
                "            tokenizer = transformers_pipe[classifier_name]['tokenizer']\n",
                "            config = transformers_pipe[classifier_name]['config']\n",
                "\n",
                "            print(f'Loading Fitted Transformer {classifier_name} from pretrained.')\n",
                "            estimator_dir = f'{results_save_path}{method} Fitted Estimator{path_suffix}.model'\n",
                "            fitted_estimator = model.from_pretrained(estimator_dir, trust_remote_code=True)\n",
                "            if hasattr(fitted_estimator, 'to'):\n",
                "                fitted_estimator = fitted_estimator.to(device)\n",
                "            tokenizer = tokenizer.from_pretrained(estimator_dir, trust_remote_code=True)\n",
                "            config = config.from_pretrained(f'{estimator_dir}/config.json', trust_remote_code=True)\n",
                "            print(f'Done loading Fitted Transformer {classifier_name} from pretrained!')\n",
                "\n",
                "            try:\n",
                "                print('Using transformers pipeline.')\n",
                "                # Get predictions\n",
                "                # Accelerate model\n",
                "                (\n",
                "                    fitted_estimator, tokenizer\n",
                "                ) = accelerator.prepare(\n",
                "                    fitted_estimator, tokenizer\n",
                "                )\n",
                "                classifier = transformers.pipeline(\n",
                "                    model=fitted_estimator, tokenizer=tokenizer, function_to_apply='softmax', device=device, framework='pt', task='text-classification', return_all_scores=False\n",
                "                )\n",
                "                # df_jobs\n",
                "                print('Classifying df_jobs.')\n",
                "                df_jobs[[col, f'{col}_Probability']] = df_jobs[text_col].astype(str).progress_apply(extract_prediction)\n",
                "                df_jobs[col] = df_jobs[col].astype(int)\n",
                "                df_jobs[f'{col}_Probability'] = df_jobs[f'{col}_Probability'].astype(float)\n",
                "                # df_manual\n",
                "                print('Classifying df_manual to generate instrumental variables.')\n",
                "                df_manual[[f'{col}_predicted', f'{col}_Probability_predicted']] = df_manual[text_col].astype(str).progress_apply(extract_prediction)\n",
                "                df_manual[f'{col}_predicted'] = df_manual[f'{col}_predicted'].astype(int)\n",
                "                df_manual[f'{col}_Probability_predicted'] = df_manual[f'{col}_Probability_predicted'].astype(float)\n",
                "                # for idx_, row in tqdm.tqdm(df_jobs[text_col].items()):\n",
                "                #     pred = classifier(row)\n",
                "                #     df_jobs.loc[idx_, col] = pred[0]['label'].split('LABEL_')[1]\n",
                "                #     df_jobs.loc[idx_, f'{col}_Probability'] = pred[0]['score']\n",
                "                # df_jobs[col] = df_jobs[text_col].astype(str).progress_apply(lambda x: [pred['label'].split('LABEL_')[1] for pred in classifier(x)][0])\n",
                "                # df_jobs[f'{col}_Probability'] = df_jobs[text_col].astype(str).progress_apply(lambda x: [pred['score'] for pred in classifier(x)][0])\n",
                "\n",
                "            except Exception as e:\n",
                "                print(f'Transformers pipeline caused {type(e).__name__}. Using Trainer instead.')\n",
                "                # Tokenize df_jobs\n",
                "                X = df_jobs[text_col].astype('str').values.tolist()\n",
                "                encodings = tokenizer(\n",
                "                X, truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
                "                ).to(device)\n",
                "                dataset = ToDataset(encodings)\n",
                "                # Tokenize df_manual\n",
                "                X_instrument = df_manual[text_col].astype('str').values.tolist()\n",
                "                encodings_instrument = tokenizer(\n",
                "                X_instrument, truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
                "                ).to(device)\n",
                "                dataset_instrument = ToDataset(encodings_instrument)\n",
                "                # Accelerate model\n",
                "                (\n",
                "                    fitted_estimator, tokenizer, dataset, dataset_instrument\n",
                "                ) = accelerator.prepare(\n",
                "                    fitted_estimator, tokenizer, dataset, dataset_instrument\n",
                "                )\n",
                "\n",
                "                print(f'Getting estimator for {col}.')\n",
                "                estimator = ImbTrainer(\n",
                "                    model=fitted_estimator,\n",
                "                    tokenizer=tokenizer,\n",
                "                    args=TrainingArguments(**training_args_dict),\n",
                "                )\n",
                "                if estimator.place_model_on_device:\n",
                "                    estimator.model.to(device)\n",
                "\n",
                "                # df_jobs\n",
                "                print('-'*20)\n",
                "                print(f'Classifying data using {classifier_name} for {col}.')\n",
                "                print('Classifying df_jobs.')\n",
                "                (y_pred_logits, y_labels, metrics) = estimator.predict(dataset)\n",
                "                y_pred_array, y_pred, y_pred_prob_array, y_pred_prob = preprocess_logits_for_metrics_from_logits(y_pred_logits)\n",
                "\n",
                "                # Assign to dataframe\n",
                "                df_jobs[col] = y_pred\n",
                "                df_jobs[f'{col}_Probability'] = y_pred_prob\n",
                "                df_jobs[col] = df_jobs[col].astype(int)\n",
                "                df_jobs[f'{col}_Probability'] = df_jobs[f'{col}_Probability'].astype(float)\n",
                "\n",
                "                # df_manual\n",
                "                print('Classifying df_manual to generate instrumental variables.')\n",
                "                (y_pred_logits_instrument, y_labels_instrument, metrics_instrument) = estimator.predict(dataset_instrument)\n",
                "                y_pred_array_instrument, y_pred_instrument, y_pred_prob_array_instrument, y_pred_prob_instrument = preprocess_logits_for_metrics_from_logits(y_pred_logits_instrument)\n",
                "\n",
                "                # Assign to dataframe\n",
                "                df_manual[f'{col}_predicted'] = y_pred_instrument\n",
                "                df_manual[f'{col}_Probability_predicted'] = y_pred_prob_instrument\n",
                "                df_manual[f'{col}_predicted'] = df_manual[f'{col}_predicted'].astype(int)\n",
                "                df_manual[f'{col}_Probability_predicted'] = df_manual[f'{col}_Probability_predicted'].astype(float)\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "        done_cols += f'_{col}'\n",
                "        assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "        # df_jobs\n",
                "        df_jobs.to_pickle(f'{df_save_dir}df_jobs_classified{done_cols}.pkl')\n",
                "        df_jobs.to_csv(f'{df_save_dir}df_jobs_classified{done_cols}.csv', index=False)\n",
                "        # df_manual\n",
                "        df_manual.to_pickle(f'{df_save_dir}df_manual_classified{done_cols}.pkl')\n",
                "        df_manual.to_csv(f'{df_save_dir}df_manual_classified{done_cols}.csv', index=False)\n",
                "    else:\n",
                "        print('-'*20)\n",
                "        print(f'Column {col} already exists in dataframe. Skipping.')\n",
                "        print('-'*20)\n",
                "    # Confirmatory Regression\n",
                "    print('='*20)\n",
                "    print(f'Confirmatory test for df_jobs {col}')\n",
                "    prob_confirmatory_tests(df_jobs[col].values, df_jobs[f'{col}_Probability'].values)\n",
                "    print(f'Confirmatory test for df_manual {col}')\n",
                "    prob_confirmatory_tests(df_manual[col], df_manual[f'{col}_predicted'].values)\n",
                "    print('='*20)\n",
                "\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1154fedc",
            "metadata": {},
            "source": [
                "## Inspect classified data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0d5dd82",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_classified.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_classified.csv', index=False)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "06e26f9b",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "    df_jobs_len = int(f.read())\n",
                "\n",
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_classified.pkl')\n",
                "assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "60ce42ee",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_classified.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_classified.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bcb23852",
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
                "    df_manual_len = int(f.read())\n",
                "\n",
                "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_classified.pkl')\n",
                "assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0ddc3eba",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "82288367",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a8aa616",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "094dd690",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Job ID', 'Job Description spacy_sentencized']].info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c201136",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Job ID', 'Job Description spacy_sentencized']].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db0bd246",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "aa579bcf",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab91527a",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual[['Job ID', 'Job Description spacy_sentencized']].info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75581892",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_manual[['Job ID', 'Job Description spacy_sentencized']].head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2195bf07",
            "metadata": {},
            "source": [
                "## Merge df_jobs with df_manual"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3b7c409",
            "metadata": {},
            "outputs": [],
            "source": [
                "merge_on_cols_list = ['Job ID', 'Job Description spacy_sentencized', 'Search Keyword', 'Platform', 'Job Title', 'Company Name', 'Location']\n",
                "fill_on_cols_list = ['Job Description', 'Industry', 'Rating', 'Employment Type', 'Company URL', 'Job URL', 'Job Age', 'Job Age Number', 'Collection Date', 'Data Row', 'Tracking ID', 'Job Date', 'Type of ownership', 'Language', 'Job Description_num_words', 'Job Description_num_unique_words', 'Job Description_num_chars', 'Job Description_num_chars_no_whitespact_and_punt', 'Job Description_num_punctuations']\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "05bbcd1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.merge(\n",
                "    df_jobs,\n",
                "    df_manual,\n",
                "    how='outer', on=merge_on_cols_list, suffixes=('', '_actual')\n",
                ").drop_duplicates(\n",
                "    subset=merge_on_cols_list\n",
                ").reset_index(drop=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "77d2e051",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "019a39ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.isna().sum()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f45ad212",
            "metadata": {},
            "outputs": [],
            "source": [
                "def fill_contradicting_cols(df_jobs):\n",
                "\n",
                "    for col in dvs:\n",
                "        fill_contradicting_dict = {\n",
                "            col: f'{col}_actual',\n",
                "            f'{col}_Probability': f'{col}_Probability_predicted'\n",
                "        }\n",
                "        for main_col, fill_col in fill_contradicting_dict.items():\n",
                "            print(f'Filling {main_col} with values from {fill_col}')\n",
                "            df_jobs[main_col] = df_jobs[main_col].fillna(df_jobs[fill_col])\n",
                "            mask = (\n",
                "                (df_jobs[main_col] != df_jobs[fill_col])\n",
                "                & (~df_jobs[fill_col].isna())\n",
                "                | (df_jobs[main_col].isna())\n",
                "            )\n",
                "            df_jobs.loc[mask, main_col] = df_jobs.loc[mask, fill_col]\n",
                "            assert df_jobs[main_col].isna().sum() == 0, f'Missing values found in {main_col} column'\n",
                "\n",
                "    return df_jobs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "63bee6e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted'\n",
                "    ]\n",
                "].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd3cc585",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted'\n",
                "    ]\n",
                "].tail()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8be31771",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = fill_contradicting_cols(df_jobs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "57ab83a1",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted'\n",
                "    ]\n",
                "].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2a08ccaf",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted'\n",
                "    ]\n",
                "].tail()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab7f10a2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fill in other nan values from columns with '_actual' suffix from df_manual\n",
                "for col in df_jobs.columns:\n",
                "    if '_actual' not in col\\\n",
                "        and '_predicted' not in col\\\n",
                "            and '_Probability' not in col\\\n",
                "                and col not in dvs\\\n",
                "                    and col not in merge_on_cols_list + fill_on_cols_list\\\n",
                "                        and df_jobs[col].isna().sum() != 0:\n",
                "                        df_jobs[col] = df_jobs[col].fillna(df_jobs[f'{col}_actual'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8278ff8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "287cbdc7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48a1421f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.isna().sum()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9cf46fdd",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.drop(\n",
                "    columns=\n",
                "    [\n",
                "        col\n",
                "        for col in df_jobs.columns\n",
                "        if '_actual' in col\n",
                "        and col not in ['Warmth_actual', 'Competence_actual']\n",
                "        or col in fill_on_cols_list\n",
                "    ]\n",
                ").reset_index(drop=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d5f00ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4d4d5e74",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.isna().sum()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0a91b329",
            "metadata": {},
            "source": [
                "## Clean df_jobs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32e95c5c",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Conver Job ID and Sentence to str\n",
                "str_cols = [\n",
                "    'Job ID',\n",
                "    'Job Description spacy_sentencized',\n",
                "]\n",
                "\n",
                "for col in str_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(str, errors='ignore').progress_apply(lambda x: x.strip().replace('[', '').replace(']', ''))\n",
                "    df_jobs[col] = df_jobs[col].apply(lambda x: x.strip())\n",
                "    df_jobs[col] = df_jobs[col].apply(lambda x: unicodedata.normalize('NFKD', x.encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n",
                "    print(f'{col} converted to str.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, str))) else f'{col} NOT converted to str.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df35f2d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Warmth and Competence to int\n",
                "# Warmth 1 = 1741, Competence 1 = 3043\n",
                "int_cols = [\n",
                "    'Warmth',\n",
                "    'Competence',\n",
                "    'Warmth_actual',\n",
                "    'Competence_actual',\n",
                "    'Warmth_predicted',\n",
                "    'Competence_predicted',\n",
                "]\n",
                "\n",
                "for col in int_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(np.int64, errors='ignore')\n",
                "    print(f'{col} converted to int.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, int))) else f'{col} NOT converted to int.')\n",
                "    print(f'{col} value counts:\\n{df_jobs[col].value_counts()}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "514efabb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert Warmth and Competence to int\n",
                "# Warmth 1 = 1741, Competence 1 = 3043\n",
                "float_cols = [\n",
                "    'Warmth_Probability',\n",
                "    'Competence_Probability',\n",
                "    'Warmth_Probability_predicted',\n",
                "    'Competence_Probability_predicted',\n",
                "]\n",
                "\n",
                "for col in float_cols:\n",
                "    df_jobs[col] = df_jobs[col].astype(np.float64, errors='ignore')\n",
                "    print(f'{col} converted to float.' if all(df_jobs[col].progress_apply(lambda x: isinstance(x, float))) else f'{col} NOT converted to int.')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7fac73e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48b1df0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.dropna(\n",
                "    subset=[\n",
                "        col\n",
                "        for col in df_jobs.columns\n",
                "        if '_actual' not in col and '_predicted' not in col\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7be9264f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.drop_duplicates(subset=merge_on_cols_list)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e947093",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "39e0d9fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.isna().sum()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6ed0fd9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b78bbb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted',\n",
                "    ]\n",
                "].head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8085df26",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[\n",
                "    [\n",
                "        'Warmth', 'Warmth_Probability', 'Warmth_actual', 'Warmth_predicted', 'Warmth_Probability_predicted',\n",
                "        'Competence', 'Competence_Probability', 'Competence_actual', 'Competence_predicted', 'Competence_Probability_predicted',\n",
                "    ]\n",
                "].describe()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59154094",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dcf92d6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving classified df_jobs length {len(df_jobs)} to txt file.')\n",
                "with open(f'{data_dir}df_jobs_for_analysis_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_jobs)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e252eb4",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(\n",
                "    df_jobs,\n",
                "    ivs_all=analysis_columns + [f'{col}_actual' for col in analysis_columns] + [f'{col}_predicted' for col in analysis_columns]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1635aac9",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(\n",
                "    df_jobs,\n",
                "    ivs_all=classified_columns + [f'{col}_predicted' for col in classified_columns ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dde4ff43",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(\n",
                "    df_jobs,\n",
                "    ivs_all=dvs_all + [f'{col}_actual' for col in dvs_all if '_Probability' not in col] + [f'{col}_predicted' for col in dvs_all]\n",
                ")\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "adc3c701",
            "metadata": {},
            "source": [
                "### Save dataframe\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3057728d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de943922",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving classified df_jobs length {len(df_jobs)} to txt file.')\n",
                "with open(f'{data_dir}df_jobs_for_analysis_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_jobs)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f2e8dea",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_manual) > 0 and isinstance(df_manual, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_manual)}'\n",
                "df_manual.to_pickle(f'{df_save_dir}df_manual_for_analysis.pkl')\n",
                "df_manual.to_csv(f'{df_save_dir}df_manual_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "49fb48d1",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Saving classified df_manual length {len(df_manual)} to txt file.')\n",
                "with open(f'{data_dir}df_manual_for_analysis_len.txt', 'w') as f:\n",
                "    f.write(str(len(df_manual)))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "470d7949",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Automating_Equity1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {},
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
