{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "50d4c434",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "sys.path.append(code_dir)\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "fef3f604",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a8aa52dc94d740b8aed664799822a6a4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fa9e47e3899d4552a0629641855225f8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 640x480 with 0 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from setup_module.imports import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
                "from setup_module import specification_curve_fork as specy # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "55afc383",
            "metadata": {},
            "source": [
                "### Set variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "7b36fe18",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using MPS\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "38671f10eb1a4e5dbcf5f485b0c6917e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "0it [00:00, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "<Figure size 640x480 with 0 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Variables\n",
                "method = 'Supervised'\n",
                "with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "    results_save_path = f.read()\n",
                "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
                "    done_xy_save_path = f.read()\n",
                "t = time.time()\n",
                "n_jobs = -1\n",
                "n_splits = 10\n",
                "n_repeats = 3\n",
                "random_state = 42\n",
                "refit = True\n",
                "class_weight = 'balanced'\n",
                "cv = RepeatedStratifiedKFold(\n",
                "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
                ")\n",
                "scoring = 'recall'\n",
                "scores = [\n",
                "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
                "    'explained_variance', 'matthews_corrcoef'\n",
                "]\n",
                "scorers = {\n",
                "    'precision_score': make_scorer(precision_score, zero_division=0),\n",
                "    'recall_score': make_scorer(recall_score, zero_division=0),\n",
                "    'accuracy_score': make_scorer(accuracy_score, zero_division=0),\n",
                "}\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "metrics_dict = {\n",
                "    f'{scoring.title()} Best Score': np.nan,\n",
                "    f'{scoring.title()} Best Threshold': np.nan,\n",
                "    'Train - Mean Cross Validation Score': np.nan,\n",
                "    f'Train - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Train - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Test - Mean Cross Validation Score': np.nan,\n",
                "    f'Test - Mean Cross Validation - {scoring.title()}': np.nan,\n",
                "    f'Test - Mean Explained Variance - {scoring.title()}': np.nan,\n",
                "    'Explained Variance': np.nan,\n",
                "    'Accuracy': np.nan,\n",
                "    'Balanced Accuracy': np.nan,\n",
                "    'Precision': np.nan,\n",
                "    'Average Precision': np.nan,\n",
                "    'Recall': np.nan,\n",
                "    'F1-score': np.nan,\n",
                "    'Matthews Correlation Coefficient': np.nan,\n",
                "    'Fowlkes–Mallows Index': np.nan,\n",
                "    'R2 Score': np.nan,\n",
                "    'ROC': np.nan,\n",
                "    'AUC': np.nan,\n",
                "    'Log Loss/Cross Entropy': np.nan,\n",
                "    'Cohen’s Kappa': np.nan,\n",
                "    'Geometric Mean': np.nan,\n",
                "    'Classification Report': np.nan,\n",
                "    'Imbalanced Classification Report': np.nan,\n",
                "    'Confusion Matrix': np.nan,\n",
                "    'Normalized Confusion Matrix': np.nan,\n",
                "}\n",
                "\n",
                "# Transformer variables\n",
                "max_length = 512\n",
                "returned_tensor = 'pt'\n",
                "cpu_counts = torch.multiprocessing.cpu_count()\n",
                "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
                ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "device_name = str(device.type)\n",
                "print(f'Using {device_name.upper()}')\n",
                "# Set random seed\n",
                "random_state = 42\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)\n",
                "torch.manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "torch.Generator(device_name).manual_seed(random_state)\n",
                "cores = multiprocessing.cpu_count()\n",
                "accelerator = Accelerator()\n",
                "torch.autograd.set_detect_anomaly(True)\n",
                "os.environ.get('TOKENIZERS_PARALLELISM')\n",
                "best_trial_args = [\n",
                "    'num_train_epochs', 'learning_rate',\n",
                "]\n",
                "training_args_dict = {\n",
                "    'seed': random_state,\n",
                "    'resume_from_checkpoint': True,\n",
                "    'overwrite_output_dir': True,\n",
                "    'logging_steps': 500,\n",
                "    'evaluation_strategy': 'steps',\n",
                "    'eval_steps': 500,\n",
                "    'save_strategy': 'steps',\n",
                "    'save_steps': 500,\n",
                "    # 'metric_for_best_model': 'recall',\n",
                "    # 'torch_compile': bool(transformers.file_utils.is_torch_available()),\n",
                "    'use_mps_device': bool(device_name == 'mps' and torch.backends.mps.is_available()),\n",
                "    'optim': 'adamw_torch',\n",
                "    'load_best_model_at_end': True,\n",
                "    'per_device_train_batch_size': 16,\n",
                "    'per_device_eval_batch_size': 20,\n",
                "    'warmup_steps': 100,\n",
                "    'weight_decay': 0.01,\n",
                "    # The below metrics are used by hyperparameter search\n",
                "    'num_train_epochs': 3,\n",
                "    'learning_rate': 5e-5,\n",
                "}\n",
                "training_args_dict_for_best_trial = {\n",
                "    arg_name: arg_\n",
                "    for arg_name, arg_ in training_args_dict.items()\n",
                "    if arg_name not in best_trial_args\n",
                "}\n",
                "\n",
                "# Plotting variables\n",
                "pp = pprint.PrettyPrinter(indent=4)\n",
                "tqdm.tqdm.pandas(desc='progress-bar')\n",
                "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
                "# tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
                "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
                "# pbar = progressbar.ProgressBar(maxval=10)\n",
                "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
                "mpl.rcParams['text.usetex'] = False\n",
                "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
                "mpl.rc('font', **font)\n",
                "plt.style.use('tableau-colorblind10')\n",
                "plt.set_cmap('Blues')\n",
                "pd.set_option('display.max_rows', None)\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.width', 5000)\n",
                "pd.set_option('display.colheader_justify', 'center')\n",
                "pd.set_option('display.precision', 3)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2a84d153",
            "metadata": {},
            "source": [
                "# Functions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "d7036357",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToDataset(torch.utils.data.Dataset):\n",
                "    def __init__(self, encodings):\n",
                "        self.encodings = encodings\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return {\n",
                "            key: val[idx].clone().detach().to(device)\n",
                "            for key, val in self.encodings.items()\n",
                "        }\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.encodings['input_ids'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "f7286eeb",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ImbTrainer(Trainer):\n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.class_weights = self._calculate_class_weights(self.train_dataset)\n",
                "\n",
                "    def _calculate_class_weights(self, dataset):\n",
                "        # Count the number of samples in each class\n",
                "        class_counts = torch.zeros(self.model.config.num_labels)\n",
                "        for label in dataset.labels:\n",
                "            class_counts[label] += 1\n",
                "\n",
                "        # Calculate the inverse frequency of each class\n",
                "        inv_frequencies = 1 / class_counts\n",
                "\n",
                "        # Normalize the inverse frequencies so that they sum up to 1\n",
                "        sum_inv_frequencies = torch.sum(inv_frequencies)\n",
                "        return inv_frequencies / sum_inv_frequencies\n",
                "\n",
                "    def compute_loss(self, model, inputs, return_outputs=False):\n",
                "        labels = inputs.pop(\"labels\")\n",
                "        outputs = model(**inputs)\n",
                "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(device))\n",
                "        loss = loss_fct(outputs.logits, labels)\n",
                "        return (loss, outputs) if return_outputs else loss\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "9ad2b64f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to get y_pred and y_pred_prob\n",
                "def preprocess_logits_for_metrics_from_logits(y_pred_logits):\n",
                "\n",
                "    # Get y_pred\n",
                "    print('-'*20)\n",
                "    y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
                "    print('Getting y_pred through argmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1).cpu().numpy()\n",
                "        print('Using torch.argmax.')\n",
                "    except Exception:\n",
                "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
                "        print('Using np.argmax.')\n",
                "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
                "    print('-'*20)\n",
                "    print('Flattening y_pred...')\n",
                "    y_pred = y_pred_array.flatten().tolist()\n",
                "    print(f'y_pred length: {len(y_pred)}')\n",
                "    print('-'*20)\n",
                "\n",
                "    # Get y_pred_prob\n",
                "    print('-'*20)\n",
                "    print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
                "    try:\n",
                "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
                "        print('Using torch.nn.functional.softmax.')\n",
                "    except Exception:\n",
                "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
                "        print('Using scipy.special.softmax.')\n",
                "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
                "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
                "    print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
                "    print('-'*20)\n",
                "    print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
                "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
                "    print(f'y_pred length: {len(y_pred_prob)}')\n",
                "    print('-'*20)\n",
                "\n",
                "    y_pred_logits_tensor.clone().detach()\n",
                "\n",
                "    return (\n",
                "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
                "    )\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "036fcf10",
            "metadata": {},
            "source": [
                "# Classifying"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "fd32c435",
            "metadata": {},
            "source": [
                "### READ DATA"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "1e310829",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataframe loaded with shape: (307300, 75)\n"
                    ]
                }
            ],
            "source": [
                "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
                "    df_jobs_len = int(f.read())\n",
                "\n",
                "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
                "assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
                "print(f'Dataframe loaded with shape: {df_jobs.shape}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a0212dc9",
            "metadata": {
                "code_folding": [],
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "########################################\n",
                        "Starting!\n",
                        "########################################\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/2 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------\n",
                        "--------------------\n",
                        "Using BertForSequenceClassification from Transformers pipeline.\n",
                        "Loading Transformer Estimator.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Done loading Transformer Estimator!\n",
                        "Getting prediction results for Warmth.\n",
                        "--------------------\n",
                        "Classifying data.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "075d272f8cf947f99c5a24bd77819935",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/15365 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------\n",
                        "Getting y_pred through argmax of y_pred_logits...\n",
                        "Using torch.argmax.\n",
                        "y_pred_array shape: (307300,)\n",
                        "--------------------\n",
                        "Flattening y_pred...\n",
                        "y_pred length: 307300\n",
                        "--------------------\n",
                        "--------------------\n",
                        "Getting y_pred_prob through softmax of y_pred_logits...\n",
                        "Using torch.nn.functional.softmax.\n",
                        "y_pred_prob shape: (307300, 2)\n",
                        "--------------------\n",
                        "Flattening y_pred_prob and extracting probabilities of 1...\n",
                        "y_pred length: 307300\n",
                        "--------------------\n",
                        "Done classifying data using BertForSequenceClassification for Warmth!\n",
                        "--------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 50%|█████     | 1/2 [7:08:15<7:08:15, 25695.83s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------\n",
                        "--------------------\n",
                        "Using BertForSequenceClassification from Transformers pipeline.\n",
                        "Loading Transformer Estimator.\n",
                        "Done loading Transformer Estimator!\n",
                        "Getting prediction results for Competence.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--------------------\n",
                        "Classifying data.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "3c501277577a40c694b2e31c6b449cf4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "  0%|          | 0/15365 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "%%time\n",
                "print('#'*40)\n",
                "print('Starting!')\n",
                "print('#'*40)\n",
                "\n",
                "protocol = pickle.HIGHEST_PROTOCOL\n",
                "analysis_columns = ['Warmth', 'Competence']\n",
                "text_col = 'Job Description spacy_sentencized'\n",
                "classified_columns = ['Warmth_Probability', 'Competence_Probability']\n",
                "final_estimators_dict = {\n",
                "    'Warmth': {\n",
                "        'vectorizer_name': 'BERTBASEUNCASED',\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "    },\n",
                "    'Competence': {\n",
                "        'vectorizer_name': 'BERTBASEUNCASED',\n",
                "        'classifier_name': 'BertForSequenceClassification',\n",
                "    },\n",
                "}\n",
                "\n",
                "for col in tqdm.tqdm(analysis_columns):\n",
                "    if col not in df_jobs.columns:\n",
                "        print('-'*20)\n",
                "        final_estimators_dict[col]['path_suffix'] = path_suffix = f' - {col} - {(vectorizer_name := final_estimators_dict[col][\"vectorizer_name\"])} + {(classifier_name := final_estimators_dict[col][\"classifier_name\"])} (Save_protocol={protocol})'\n",
                "\n",
                "        if classifier_name in list(classifiers_pipe.keys()):\n",
                "            method = 'Supervised'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read()\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            print('Loading Supervised Estimator.')\n",
                "            with open(\n",
                "                f'{results_save_path}{method} Fitted Estimator {path_suffix}.pkl', 'rb'\n",
                "            ) as f:\n",
                "                estimator = joblib.load(f)\n",
                "            print('Done loading Supervised Estimator!')\n",
                "\n",
                "            print('-'*20)\n",
                "            print('Classifying data.')\n",
                "            X = np.array(list(df_jobs[text_col].astype('str').values))\n",
                "            df_jobs[col] = estimator.predict(X)\n",
                "            if hasattr(estimator, 'predict_proba'):\n",
                "                # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
                "                df_jobs[f'{col}_Probability'] = estimator.predict_proba(X)[:, -1]\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "        elif classifier_name in list(transformers_pipe.keys()):\n",
                "            method = 'Transformers'\n",
                "            with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
                "                results_save_path = f.read()\n",
                "            print('-'*20)\n",
                "            print(f'Using {classifier_name} from {method} pipeline.')\n",
                "            model = transformers_pipe[classifier_name]['model']\n",
                "            tokenizer = transformers_pipe[classifier_name]['tokenizer']\n",
                "            config = transformers_pipe[classifier_name]['config']\n",
                "\n",
                "            print(f'Loading Fitted Transformer {classifier_name} from pretrained.')\n",
                "            estimator_dir = f'{results_save_path}{method} Fitted Estimator{path_suffix}.model'\n",
                "            output_dir = training_args_dict['output_dir'] = training_args_dict_for_best_trial['output_dir'] = final_estimators_dict[col]['output_dir'] = f'{results_save_path}{method} Final Estimator{path_suffix}.model'\n",
                "            fitted_estimator = model.from_pretrained(estimator_dir)\n",
                "            tokenizer = tokenizer.from_pretrained(estimator_dir)\n",
                "            config = config.from_pretrained(f'{estimator_dir}/config.json')\n",
                "            print(f'Done loading Fitted Transformer {classifier_name} from pretrained!')\n",
                "\n",
                "            # Tokenize\n",
                "            X = df_jobs[text_col].astype('str').values.tolist()\n",
                "            encodings = tokenizer(\n",
                "            X, truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
                "            ).to(device)\n",
                "            dataset = ToDataset(encodings)\n",
                "            # Accelerate model\n",
                "            (\n",
                "                fitted_estimator, tokenizer, dataset\n",
                "            ) = accelerator.prepare(\n",
                "                fitted_estimator, tokenizer, dataset\n",
                "            )\n",
                "\n",
                "            # Get predictions\n",
                "            print(f'Getting estimator for {col}.')\n",
                "            estimator = Trainer(\n",
                "                model=fitted_estimator,\n",
                "                tokenizer=tokenizer,\n",
                "                args=TrainingArguments(**training_args_dict),\n",
                "                # preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
                "                # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
                "                # data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
                "            )\n",
                "            if estimator.place_model_on_device:\n",
                "                estimator.model.to(device)\n",
                "\n",
                "            print('-'*20)\n",
                "            print(f'Classifying data using {classifier_name} for {col}.')\n",
                "            (y_pred_logits, y_labels, metrics) = estimator.predict(dataset)\n",
                "            y_pred_array, y_pred, y_pred_prob_array, y_pred_prob = preprocess_logits_for_metrics_from_logits(y_pred_logits)\n",
                "            df_jobs[col] = y_pred\n",
                "            df_jobs[f'{col}_Probability'] = y_pred_prob\n",
                "\n",
                "            print(f'Done classifying data using {classifier_name} for {col}!')\n",
                "            print('-'*20)\n",
                "\n",
                "            assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "            df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "            df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n",
                "            print(f'Dataframe saved to {df_save_dir}df_jobs_for_analysis.pkl')\n",
                "            print('-'*20)\n",
                "    else:\n",
                "        print(f'Column {col} already exists in dataframe. Skipping.')\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "1154fedc",
            "metadata": {},
            "source": [
                "## Inspect classified data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48b1df0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = df_jobs.dropna(subset=['Warmth', 'Competence', 'Warmth_Probability', 'Competence_Probability'])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "59154094",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e947093",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.info()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e6ed0fd9",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.describe()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b78bbb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Warmth', 'Warmth_Probability', 'Competence', 'Competence_Probability']].head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8085df26",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs[['Warmth', 'Warmth_Probability', 'Competence', 'Competence_Probability']].describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e252eb4",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=analysis_columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1635aac9",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=classified_columns)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dde4ff43",
            "metadata": {},
            "outputs": [],
            "source": [
                "get_df_info(df_jobs, ivs_all=dvs_all)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "9e7aea9a",
            "metadata": {},
            "source": [
                "### Plot classified data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d1aa329",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Counts plot of classifed warmthh and competence\n",
                "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
                "sns.countplot(x='Warmth', data=df_jobs, ax=ax[0], palette='colorblind')\n",
                "sns.countplot(x='Competence', data=df_jobs, ax=ax[1], palette='colorblind')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "368511db",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Box plot of warmth and competence probabilities\n",
                "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
                "sns.boxplot(x='Warmth', y='Warmth_Probability', data=df_jobs, ax=ax[0], palette='colorblind')\n",
                "sns.boxplot(x='Competence', y='Competence_Probability', data=df_jobs, ax=ax[1], palette='colorblind')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fd87577f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Specification curve analysis\n",
                "print(f'Running Logit specification curve analysis with:\\nDEPENDENT VARIABLES = {dvs}\\nINDEPENDENT VARIABLES = {ivs_dummy_and_perc}\\nCONTROLS = {controls}')\n",
                "sc = specy.SpecificationCurve(df=dj_jobs, y_endog=dvs, x_exog=ivs_dummy, controls=controls)\n",
                "sc.fit(estimator=sm.Logit)\n",
                "sc.plot(show_plot=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a9e10d45",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Specification curve analysis\n",
                "print(f'Running OLS specification curve analysis with:\\nDEPENDENT VARIABLES = {dvs_prob}\\nINDEPENDENT VARIABLES = {ivs_dummy_and_perc}\\nCONTROLS = {controls}')\n",
                "sc = specy.SpecificationCurve(df=dj_jobs, y_endog=dvs_prob, x_exog=ivs_perc, controls=controls)\n",
                "sc.fit(estimator=sm.OLS)\n",
                "sc.plot(show_plot=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "adc3c701",
            "metadata": {},
            "source": [
                "### Save dataframe\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3057728d",
            "metadata": {},
            "outputs": [],
            "source": [
                "assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
                "df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
                "df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de943922",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "study1_3.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "state": {},
                "version_major": 2,
                "version_minor": 0
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
