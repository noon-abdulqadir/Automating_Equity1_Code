{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "if code_dir_name not in str(Path.cwd()).split('/')[-1]:\n",
    "    for _ in range(5):\n",
    "\n",
    "        parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "        if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "            code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "            if code_dir is not None:\n",
    "                break\n",
    "else:\n",
    "    code_dir = str(Path.cwd())\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module.estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module.plot_metric_fork import functions as plot_metric_functions # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e858544b",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "method = 'Transformers'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
    "    results_save_path = f.read().strip('\\n')\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
    "    done_xy_save_path = f.read().strip('\\n')\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    f'Mean Cross Validation Train - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Cross Validation Test - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Brier Score': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "torch.Generator(device_name).manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "accelerator = Accelerator()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ.get('TOKENIZERS_PARALLELISM')\n",
    "os.environ.get('PYTORCH_MPS_HIGH_WATERMARK_RATIO')\n",
    "os.environ.get('TRANSFORMERS_CACHE')\n",
    "openai_token = os.environ['OPENAI_API_KEY']\n",
    "huggingface_token = os.environ['HUGGINGFACE_API_KEY']\n",
    "# load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type='nf4'\n",
    "quantization_config_dict = {\n",
    "    'load_in_8bit': True,\n",
    "    'llm_int8_skip_modules': ['lm_head'],\n",
    "}\n",
    "hyperparameter_tuning = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe416deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_close_plots():\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def close_plots():\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_metrics(\n",
    "    vectorizers_pipe, classifiers_pipe, transformers_pipe, metrics_list,\n",
    "    col, vectorizer_name, classifier_name, protocol=None,\n",
    "    analysis_columns=analysis_columns,\n",
    "    table_save_path=table_save_path,\n",
    "    method=method, save_name=None,\n",
    "    compression=None, path_suffix=None,\n",
    "):\n",
    "    if save_name is None:\n",
    "        save_name = f'{method} Estimators Table'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if isinstance(metrics_list, dict):\n",
    "        metrics_list = list(metrics_list.keys())\n",
    "\n",
    "    transformers_tokenizers_list = [\n",
    "        str(tranformer_dict['tokenizer']).split('.')[-1].split(\"'>\")[0]\n",
    "        for tranformer_dict in transformers_pipe.values()\n",
    "    ]\n",
    "    combined_classifiers_list = list(classifiers_pipe.keys()) + list(transformers_pipe.keys())\n",
    "    combined_vectorizers_list = list(vectorizers_pipe.keys()) + transformers_tokenizers_list\n",
    "\n",
    "    print('='*20)\n",
    "    if os.path.exists(f'{table_save_path}{save_name}.pkl') and os.path.getsize(f'{table_save_path}{save_name}.pkl') > 0:\n",
    "        print(f'Loading table from {table_save_path}{save_name}.pkl')\n",
    "        df_metrics = pd.read_pickle(f'{table_save_path}{save_name}.pkl')\n",
    "        print('Done loading table!')\n",
    "    else:\n",
    "        print('Table does not exist, creating new table...')\n",
    "        if method == 'Transformers':\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [list(map(lambda classifier_name: classifier_name, list(transformers_pipe.keys())))],\n",
    "                names=['Classifiers'],\n",
    "            )\n",
    "            columns = pd.MultiIndex.from_product(\n",
    "                [\n",
    "                    analysis_columns,\n",
    "                    metrics_list,\n",
    "                ],\n",
    "                names=['Variable', 'Measures'],\n",
    "            )\n",
    "        elif method == 'Supervised':\n",
    "            index = pd.MultiIndex.from_product(\n",
    "                [list(map(lambda classifier_name: classifier_name, list(classifiers_pipe.keys())))],\n",
    "                names=['Classifiers'],\n",
    "            )\n",
    "            columns = pd.MultiIndex.from_product(\n",
    "                [\n",
    "                    analysis_columns,\n",
    "                    list(map(lambda vectorizer_name: vectorizer_name, list(vectorizers_pipe.keys()))),\n",
    "                    metrics_list,\n",
    "                ],\n",
    "                names=['Variable', 'Vectorizer', 'Measures'],\n",
    "            )\n",
    "        # Make df\n",
    "        df_metrics = pd.DataFrame(index=index, columns=columns)\n",
    "        print('Done creating new table!')\n",
    "    print('='*20)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab49d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path=results_save_path,\n",
    "    estimator_names_list=None,\n",
    "):\n",
    "    if estimator_names_list is None:\n",
    "        estimator_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.*')):\n",
    "        if f'{method} Estimator - ' in estimators_file:\n",
    "\n",
    "            col=estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "            vectorizer_name=estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "            classifier_name=estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "            estimator_names_list.append(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "\n",
    "    return (\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights, self.weight, self.pos_weight = self._calculate_class_weights(self.train_dataset)\n",
    "\n",
    "    def _calculate_class_weights(self, dataset):\n",
    "        # Count the number of samples in each class\n",
    "        class_counts = torch.bincount(torch.tensor(dataset.labels, device=device))\n",
    "\n",
    "        # Calculate the inverse frequency of each class with Laplace smoothing\n",
    "        inv_frequencies = (len(dataset) + 1) / (class_counts + 1)\n",
    "        class_weights = inv_frequencies / torch.sum(inv_frequencies)\n",
    "\n",
    "        # Calculate weight and pos_weight\n",
    "        num_negative = class_counts[0].item()\n",
    "        num_positive = class_counts[1].item()\n",
    "        weight_neg = num_positive / (num_negative + 1e-5)\n",
    "        weight_pos = num_negative / (num_positive + 1e-5)\n",
    "        weight = torch.tensor([weight_neg, weight_pos], device=device)\n",
    "        pos_weight = torch.tensor([weight_pos], device=device)\n",
    "\n",
    "        return class_weights, weight, pos_weight\n",
    "\n",
    "    def _calculate_calibration_loss(self, logits, labels):\n",
    "        return nn.BCEWithLogitsLoss(\n",
    "            weight=self.weight.to(device)\n",
    "        )(\n",
    "            logits.to(device),\n",
    "            torch.nn.functional.one_hot(labels, logits.size(-1)).to(device).long().float()\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        original_loss = super().compute_loss(model, inputs, return_outputs=True)[0]\n",
    "        if self.label_smoother is not None and 'labels' in inputs:\n",
    "            labels = inputs.pop('labels')\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "        if labels is not None:\n",
    "            loss = torch.tensor(self._calculate_calibration_loss(logits, labels))\n",
    "        else:\n",
    "            loss = original_loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy_estimator(\n",
    "    model, tokenizer, config,\n",
    "    col, vectorizer_name, classifier_name, protocol,\n",
    "    results_save_path=results_save_path,\n",
    "    done_xy_save_path=done_xy_save_path, method=method,\n",
    "    compression=None, saved_files_list=None,\n",
    "    path_suffix=None, data_dict=None,\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {str(col)} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol})'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if saved_files_list is None:\n",
    "        saved_files_list = []\n",
    "\n",
    "    print(f'Loading Xy from previous for {col}...')\n",
    "    # Read all dfs into\n",
    "    for file_path in glob.glob(f'{done_xy_save_path}{method}*{path_suffix}*'):\n",
    "        file_name = file_path.split(f'{done_xy_save_path}{method} ')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name} from {file_path}')\n",
    "        if path_suffix in file_path and 'df_' in file_name or 'metrics_dict' in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "            saved_files_list.append(file_name)\n",
    "\n",
    "    # Load estimator and accelator\n",
    "    print('Loading Estimator.')\n",
    "    estimator_dir = f'{results_save_path}{method} Estimator{path_suffix}.model'\n",
    "    if os.path.exists(f'{estimator_dir}/checkpoint-500') and not os.listdir(f'{estimator_dir}/checkpoint-500') and training_args_dict['resume_from_checkpoint']:\n",
    "        saved_estimator = model.from_pretrained(f'{estimator_dir}/checkpoint-500', trust_remote_code=True)\n",
    "    else:\n",
    "        saved_estimator = model.from_pretrained(f'{estimator_dir}', trust_remote_code=True)\n",
    "    tokenizer = tokenizer.from_pretrained(estimator_dir, trust_remote_code=True)\n",
    "    config = config.from_pretrained(f'{estimator_dir}/config.json', trust_remote_code=True)\n",
    "    saved_files_list.extend(['Estimator', 'config'])#, 'accelerator'])\n",
    "\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    train_dataset = df_train_data['train_dataset'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    test_dataset = df_test_data['test_dataset'].values\n",
    "    y_test_pred = df_test_data['y_test_pred'].values\n",
    "    y_test_pred_prob = df_test_data['y_test_pred_prob'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "    val_dataset = df_val_data['val_dataset'].values\n",
    "    y_val_pred = df_val_data['y_val_pred'].values\n",
    "    y_val_pred_prob = df_val_data['y_val_pred_prob'].values\n",
    "\n",
    "    # Metrics dicts\n",
    "    for key, value in data_dict.items():\n",
    "        if 'metrics_dict' in key:\n",
    "            for key, value in metrics_dict.items():\n",
    "                if isinstance(value, str):\n",
    "                    metrics_dict[key] = np.fromstring(value[1:-1], sep=' ')\n",
    "    eval_metrics_dict = data_dict['eval_metrics_dict']\n",
    "    test_metrics_dict = data_dict['test_metrics_dict']\n",
    "    # for metrics_dict in [eval_metrics_dict, test_metrics_dict]:\n",
    "    #     for key, value in metrics_dict.items():\n",
    "    #         if isinstance(value, str):\n",
    "    #             metrics_dict[key] = np.fromstring(value[1:-1], sep=' ')\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "\n",
    "    assert set(list(data_dict.keys())+['Estimator', 'config']) == set(saved_files_list), f'Not all files were loaded! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done loading Xy and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n",
    "\n",
    "    return (\n",
    "        X_train, y_train, train_dataset,\n",
    "        X_test, y_test, test_dataset, y_test_pred, y_test_pred_prob,\n",
    "        X_val, y_val, val_dataset, y_val_pred, y_val_pred_prob,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "        saved_estimator, tokenizer, config, eval_metrics_dict, test_metrics_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # check is encodings and labels are tensors\n",
    "        for key, val in self.encodings.items():\n",
    "            if not torch.is_tensor(val[idx]):\n",
    "                self.encodings[key][idx] = torch.tensor(val[idx], dtype=torch.long, device=device)\n",
    "        if not torch.is_tensor(self.labels[idx]):\n",
    "            self.labels[idx] = torch.tensor(self.labels[idx], dtype=torch.long, device=device)\n",
    "        item = {key: val[idx].to(device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy_encodings(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, test_dataset,\n",
    "    X_val, y_val, val_dataset,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, val_dataset)\n",
    "\n",
    "    # Check encodings\n",
    "    assert all(y_train == train_dataset.encoded), 'y_train and train_dataset encoded are not the same'\n",
    "    assert all(y_test == test_dataset.encoded), 'y_test and test_dataset encoded are not the same'\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set encodings example:\\n{\" \".join(train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set encoded example: {set(train_dataset.encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set encodings example:\\n{\" \".join(test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set encoded example: {set(test_dataset.encoded)}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set encodings example:\\n{\" \".join(val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation labels after encoding: {set(val_dataset.encoded)}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    tokenizer,\n",
    "):\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {tokenizer.__class__.__name__}.from_pretrained using {tokenizer.name_or_path}.')\n",
    "\n",
    "    X_train_encodings = tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    train_dataset = ToDataset(X_train_encodings, y_train)\n",
    "\n",
    "    X_test_encodings = tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    test_dataset = ToDataset(X_test_encodings, y_test)\n",
    "\n",
    "    X_val_encodings = tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    val_dataset = ToDataset(X_val_encodings, y_val)\n",
    "\n",
    "    # Print info\n",
    "    print_Xy_encodings(\n",
    "        X_train, y_train, train_dataset,\n",
    "        X_test, y_test, test_dataset,\n",
    "        X_val, y_val, val_dataset,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_encodings, train_dataset,\n",
    "        X_test_encodings, test_dataset,\n",
    "        X_val_encodings, val_dataset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_labels, y_pred,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None, print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_pred.')\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
    "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
    "    brier = metrics.brier_score_loss(y_labels, y_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
    "    r2 = metrics.r2_score(y_labels, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_labels, y_pred)\n",
    "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, brier, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f234a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_with_y_pred(\n",
    "    y_labels, y_pred, col, vectorizer_name, classifier_name,\n",
    "    pos_label=None, labels=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "\n",
    "    # Displays\n",
    "    close_plots()\n",
    "    cm_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_labels, y_pred, display_labels=labels, cmap=plt.cm.Blues, colorbar=True\n",
    "    )\n",
    "    cm_normalized_curve = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "        y_labels, y_pred, normalize='true', display_labels=labels, cmap=plt.cm.Blues, colorbar=True\n",
    "    )\n",
    "    roc_curve = metrics.RocCurveDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='C0'\n",
    "    )\n",
    "    pr_curve = metrics.PrecisionRecallDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='C0'\n",
    "    )\n",
    "    calibration_curve = CalibrationDisplay.from_predictions(\n",
    "        y_labels, y_pred, pos_label=pos_label, color='C0'\n",
    "    )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Plots\n",
    "    plots_dict = {\n",
    "        'Confusion Matrix': cm_curve,\n",
    "        'Normalized Confusion Matrix': cm_normalized_curve,\n",
    "        'ROC Curve': roc_curve,\n",
    "        'Precision-Recall Curve': pr_curve,\n",
    "        'Calibration Curve': calibration_curve,\n",
    "    }\n",
    "\n",
    "    print('=' * 20)\n",
    "    close_plots()\n",
    "    print('Plotting metrics with y_pred_prob:')\n",
    "    print('='*20)\n",
    "\n",
    "    for plot_name, plot_ in plots_dict.items():\n",
    "        close_plots()\n",
    "        print(f'Plotting {plot_name}:')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\n",
    "            f'{col} - {plot_name} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "        if plot_name == 'ROC Curve':\n",
    "            ax.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "        try:\n",
    "            plot_.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "        except Exception:\n",
    "            plot_.plot(ax=ax, color='C0')\n",
    "        print('=' * 20)\n",
    "        fig = plt.gcf()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Save Plots\n",
    "        for image_save_format in ['eps', 'png', 'svg']:\n",
    "            save_path = f'{plot_save_path}{method} {col} - {plot_name} - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            print(f'Saving {plot_name} at {save_path}')\n",
    "            fig.savefig(\n",
    "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n",
    "        show_and_close_plots()\n",
    "        print(f'Saved {plot_name}!')\n",
    "        print('=' * 20)\n",
    "\n",
    "    # with contextlib.suppress(AttributeError):\n",
    "    # Visualisation with plot_metric\n",
    "    bc = plot_metric_functions.BinaryClassification(y_labels, y_pred, labels=[0, 1], matplotlib_style='tableau-colorblind10', seaborn_style='whitegrid')\n",
    "\n",
    "    # Figures\n",
    "    close_plots()\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    fig.suptitle(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "    plt.subplot2grid((2, 6), (1, 1), colspan=2)\n",
    "    bc.plot_confusion_matrix(colorbar=True, cmap=plt.cm.Blues)\n",
    "    plt.subplot2grid((2, 6), (1, 3), colspan=2)\n",
    "    bc.plot_confusion_matrix(normalize=True, colorbar=True, cmap=plt.cm.Blues)\n",
    "    plt.subplot2grid(shape=(2, 6), loc=(0, 0), colspan=2)\n",
    "    bc.plot_roc_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 2), colspan=2)\n",
    "    bc.plot_precision_recall_curve()\n",
    "    plt.subplot2grid((2, 6), (0, 4), colspan=2)\n",
    "    bc.plot_class_distribution()\n",
    "    bc.print_report()\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save Plots\n",
    "    for image_save_format in ['eps', 'png', 'svg']:\n",
    "        save_path = f'{plot_save_path}{method} {col} - plot_metric Curves - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "        print(f'Saving plot_metric Curves at {save_path}')\n",
    "        fig.savefig(\n",
    "            save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    show_and_close_plots()\n",
    "\n",
    "    # Heatmap\n",
    "    print('Plotting Heatmap:')\n",
    "    close_plots()\n",
    "    classifications_dict = defaultdict(int)\n",
    "    for _y_labels, _y_pred in zip(y_labels, y_pred):\n",
    "        if _y_labels != _y_pred:\n",
    "            classifications_dict[(_y_labels, _y_pred)] += 1\n",
    "\n",
    "    dicts_to_plot = [\n",
    "        {\n",
    "            f'True {col} value': _y_labels,\n",
    "            f'Predicted {col} value': _y_pred,\n",
    "            'Number of Classifications': _count,\n",
    "        }\n",
    "        for (_y_labels, _y_pred), _count in classifications_dict.items()\n",
    "    ]\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(\n",
    "        index=f'True {col} value',\n",
    "        columns=f'Predicted {col} value',\n",
    "        values='Number of Classifications'\n",
    "    )\n",
    "    plt.figure(figsize=(9,7))\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    sns.heatmap(df_wide, linewidths=1, cmap=plt.cm.Blues, annot=True)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.title(f'{col} Heatmap - {vectorizer_name} + {classifier_name}')\n",
    "    fig = plt.gcf()\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save Heatmap\n",
    "    for image_save_format in ['eps', 'png', 'svg']:\n",
    "        save_path = f'{plot_save_path}{method} {col} - Heatmap - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "        print(f'Saving Heatmap at {save_path}')\n",
    "        fig.savefig(\n",
    "            save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "        )\n",
    "    print('Saved Heatmap!')\n",
    "    show_and_close_plots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_labels, y_pred_prob,\n",
    "    pos_label=None,\n",
    "    print_enabled=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Computing metrics using y_pred_prob.')\n",
    "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold, loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359e3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all(\n",
    "    y_labels, y_pred, y_pred_prob, print_enabled=None\n",
    "):\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "    # Get metrics\n",
    "    # Using y_pred\n",
    "    if y_pred:\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, brier, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_labels, y_pred, print_enabled=print_enabled\n",
    "        )\n",
    "    # Using y_pred_prob\n",
    "    if y_pred_prob:\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_labels, y_pred_prob, print_enabled=print_enabled\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    if print_enabled:\n",
    "        print('='*20)\n",
    "        print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # 'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        # f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # 'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        # f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Brier Score': float(brier),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'R2 Score': float(r2),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Imbalanced Classification Report': str(imblearn_report),\n",
    "        'Confusion Matrix': str(cm),\n",
    "        'Normalized Confusion Matrix': str(cm_normalized),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_prob': y_pred_prob,\n",
    "    }\n",
    "    if print_enabled: print('Done appending metrics to dict.')\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metrics_dict(metrics_dict, prefix_to_remove):\n",
    "    for metric_name in list(metrics_dict):\n",
    "        if metric_name.startswith(prefix_to_remove):\n",
    "            new_metric_name = ' '.join(metric_name.split(prefix_to_remove)[-1].split('_')).strip()\n",
    "        if not new_metric_name[0].isupper():\n",
    "            new_metric_name = new_metric_name.title()\n",
    "        if new_metric_name == 'Loss':\n",
    "            metrics_dict['Log Loss/Cross Entropy'] = metrics_dict.pop(metric_name)\n",
    "        else:\n",
    "            metrics_dict[new_metric_name] = metrics_dict.pop(metric_name)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ac304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred_prob(y_pred_logits, y_labels):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'Preprocessing y_pred logits and labels for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_logits, tuple):\n",
    "        y_pred_logits = y_pred_logits[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_logits):\n",
    "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    else:\n",
    "        y_pred_logits_tensor = y_pred_logits.to(device)\n",
    "\n",
    "    print(f'y_pred_logits shape: {y_pred_logits_tensor.shape}, {y_pred_logits_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    # https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits.')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1)\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_array.shape}, {y_pred_prob_array.dtype}')\n",
    "\n",
    "    return torch.tensor(y_pred_prob_array, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca2e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred(y_pred_prob_array):\n",
    "\n",
    "    # https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred from y_pred_prob for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_prob_array, tuple):\n",
    "        y_pred_prob_array = y_pred_prob_array[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_prob_array):\n",
    "        y_pred_prob_tensor = torch.tensor(y_pred_prob_array, device=device)\n",
    "    else:\n",
    "        y_pred_prob_tensor = y_pred_prob_array.to(device)\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_tensor.shape}. {y_pred_prob_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred through argmax of y_pred_prob.')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_prob_tensor, dim=-1)\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_prob.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "\n",
    "    return y_pred_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    y_pred_prob_array, y_labels_array = predicted_results_from_eval\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    (\n",
    "        y_pred_array\n",
    "    ) = preprocess_logits_for_metrics_y_pred(y_pred_prob_array)\n",
    "\n",
    "    # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
    "    print('-'*20)\n",
    "    print('Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.')\n",
    "    y_labels = y_labels_array.flatten().tolist()\n",
    "    y_pred = y_pred_array.flatten().tolist()\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred_prob length: {len(y_pred_prob)}')\n",
    "    print(f'y_labels length: {len(y_labels)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_from_logits(y_pred_logits, print_enabled=None):\n",
    "\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "\n",
    "    # Get y_pred\n",
    "    if not torch.is_tensor(y_pred_logits):\n",
    "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Getting y_pred through argmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1)\n",
    "        if print_enabled: print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
    "        if print_enabled: print('Using np.argmax.')\n",
    "    if print_enabled:\n",
    "        print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "        print('-'*20)\n",
    "        print('Flattening y_pred...')\n",
    "    y_pred = y_pred_array.flatten().tolist()\n",
    "    if print_enabled:\n",
    "        print(f'y_pred length: {len(y_pred)}')\n",
    "        print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1)\n",
    "        if print_enabled: print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        if print_enabled: print('Using scipy.special.softmax.')\n",
    "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
    "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    if print_enabled:\n",
    "        print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
    "        print('-'*20)\n",
    "        print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
    "        print(f'y_pred length: {len(y_pred_prob)}')\n",
    "        print('-'*20)\n",
    "\n",
    "    return (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1121fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_logits(\n",
    "    predicted_results_from_eval, print_enabled=None\n",
    "):\n",
    "    if print_enabled is None:\n",
    "        print_enabled = True\n",
    "    # Get predictions\n",
    "    y_pred_logits, y_labels = predicted_results_from_eval\n",
    "    if print_enabled:\n",
    "        print('-'*20)\n",
    "        print(f'Getting y_pred logits and ids for {col}:')\n",
    "        print(f'y_pred_logits shape: {y_pred_logits.shape}')\n",
    "        print(f'y shape: {y_labels.shape}')\n",
    "        print('-'*20)\n",
    "\n",
    "    # Get y_test_pred and y_test_pred_prob\n",
    "    (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    ) = preprocess_logits_for_metrics_from_logits(y_pred_logits, print_enabled=print_enabled)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob, print_enabled=print_enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d3da10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(\n",
    "    y_labels, y_pred,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    with_y_pred=None, with_y_pred_prob=None\n",
    "):\n",
    "    if with_y_pred is None:\n",
    "        with_y_pred = True\n",
    "    if with_y_pred_prob is None:\n",
    "        with_y_pred_prob = True\n",
    "\n",
    "    # Plotting\n",
    "    # Using y_test_pred\n",
    "    if with_y_pred:\n",
    "        plot_metrics_with_y_pred(\n",
    "            y_labels, y_pred,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeec3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_predictions(\n",
    "    X_test, y_test, y_test_pred, col\n",
    "):\n",
    "    # Examine predictions\n",
    "    print('~'*20)\n",
    "    print(f'Examining predictions for {col}')\n",
    "    print('Incorrectly Classified Reviews:')\n",
    "    for _y_test, _y_test_pred, _X_test in random.sample(list(zip(y_test, y_test_pred, X_test)), 50):\n",
    "        if _y_test != _y_test_pred:\n",
    "            print('-'*20)\n",
    "            print(f'TRUE LABEL: {_y_test}')\n",
    "            print(f'PREDICTED LABEL: {_y_test_pred}')\n",
    "            print(f'REVIEW TEXT: {_X_test[:100]}')\n",
    "            print('-'*20)\n",
    "    print('~'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    y_labels, y_pred,\n",
    "    metrics_dict, df_metrics,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    plot_enabled=True,\n",
    "):\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {test_metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if metric_name not in ['Runtime', 'Samples Per Second', 'Steps Per Second']:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, metric_name)\n",
    "                ] = metric_value\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, metric_name)\n",
    "                ] = str(metric_value)\n",
    "                print(f'{metric_name}:\\n{metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    if plot_enabled:\n",
    "        # Plot Metrics\n",
    "        plot_metrics(\n",
    "            y_labels, y_pred,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6552d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_confirmatory_tests(y_pred, y_pred_prob):\n",
    "\n",
    "    # Confirmatory Regression\n",
    "    print('+'*20)\n",
    "    print('Confirmatory Tests validating the linear relationship between y_pred and y_pred_prob')\n",
    "    print('-'*20)\n",
    "    print('T-Test y_pred_prob ~ y_pred:')\n",
    "    levene = scipy.stats.levene(y_pred_prob, y_pred)\n",
    "    equal_var_levene = levene.pvalue < 0.05\n",
    "    print(scipy.stats.ttest_ind(y_pred_prob, y_pred, equal_var=equal_var_levene))\n",
    "\n",
    "\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    print('Logit y_pred ~ y_pred_prob:')\n",
    "    try:\n",
    "        logit_model = sm.Logit(endog=y_pred, exog=y_pred_prob)\n",
    "        logit_results = logit_model.fit()\n",
    "        std_coef = logit_results.params[0] / np.std(y_pred_prob)\n",
    "        std_err = logit_results.bse[0]\n",
    "        log_likelihood = logit_results.llf\n",
    "        print(logit_results.summary())\n",
    "        print('-'*20)\n",
    "        print(f'Std Coef: {std_coef}')\n",
    "        print(f'Std Err: {std_err}')\n",
    "        print(f'Log Likelihood: {log_likelihood}')\n",
    "    except Exception as e:\n",
    "        print(type(e).__name__)\n",
    "\n",
    "    print('-'*20)\n",
    "    print('\\n')\n",
    "    print('-'*20)\n",
    "    print('OLS y_pred_prob ~ y_pred:')\n",
    "    try:\n",
    "        ols_model = sm.OLS(endog=y_pred_prob, exog=y_pred)\n",
    "        ols_results = ols_model.fit()\n",
    "        std_coef = ols_results.params[0] / np.std(y_pred)\n",
    "        std_err = ols_results.bse[0]\n",
    "        print(ols_results.summary())\n",
    "        print('-'*20)\n",
    "        print(f'Std Coef: {std_coef}')\n",
    "        print(f'Std Err: {std_err}')\n",
    "    except Exception as e:\n",
    "        print(type(e).__name__)\n",
    "\n",
    "    print('-'*20)\n",
    "    print('+'*20)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93960eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy data in df and save\n",
    "def save_Xy_estimator(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "    X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "    estimator, accelerator, eval_metrics_dict, test_metrics_dict,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None, protocol=None,\n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset)\n",
    "\n",
    "    # Make data dict\n",
    "    data_dict['Estimator'] = estimator\n",
    "    data_dict['accelerator'] = accelerator\n",
    "    data_dict['eval_metrics_dict'] = eval_metrics_dict\n",
    "    data_dict['test_metrics_dict'] = test_metrics_dict\n",
    "\n",
    "    # Make df_train_data\n",
    "    data_dict['df_train_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'train_dataset': train_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    data_dict['df_test_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "            'test_dataset': test_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    data_dict['df_val_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_pred_prob': y_val_pred_prob,\n",
    "            'val_dataset': val_dataset,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    saved_files_list = []\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = (\n",
    "            done_xy_save_path\n",
    "            if file_name not in ['Estimator', 'accelerator']\n",
    "            else results_save_path\n",
    "        )\n",
    "        print(f'Saving {file_name} at {save_path}')\n",
    "        if not isinstance(file_, pd.DataFrame) and file_name == 'Estimator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            # Save as .model\n",
    "            file_.save_model(f'{save_path}{method} {file_name}{path_suffix.replace(\"pkl\", \"model\")}')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif not isinstance(file_, pd.DataFrame) and file_name == 'accelerator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.save(estimator.state, f'{save_path}{method} Estimator{path_suffix.replace(\"pkl\", \"model\")}/accelerator')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, dict) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' not in file_name and 'metrics_dict' in file_name:\n",
    "            with open(f'{save_path}{method} {file_name}{path_suffix}', 'wb') as f:\n",
    "                pickle.dump(file_, f, protocol=protocol)\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, pd.DataFrame) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "            saved_files_list.append(file_name)\n",
    "\n",
    "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were saved! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done saving Xy, labels and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06545bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fitted_estimator(\n",
    "    estimator, metrics_dict,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    protocol=None,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None,\n",
    "):\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).model'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "\n",
    "    # Save fitted estimator\n",
    "    print('~'*20)\n",
    "    print(f'Saving fitted estimator {classifier_name} at {results_save_path}')\n",
    "    estimator.save_state()\n",
    "    estimator.save_model(f'{results_save_path}{method} Fitted Estimator{path_suffix}')\n",
    "    estimator.save_metrics('all', metrics_dict)\n",
    "    print('~'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b48de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(\n",
    "    df_metrics,\n",
    "    col, vectorizer_name, classifier_name, protocol,\n",
    "    table_save_path=table_save_path,\n",
    "    method=method, save_name=None,\n",
    "    compression=None,\n",
    "    path_suffix=None,\n",
    "):\n",
    "    if save_name is None:\n",
    "        save_name = f'{method} Estimators Table'\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Save metrics df\n",
    "    save_path = f'{table_save_path}{save_name}'\n",
    "    print(f'Saving fitted estimator and table at {save_path}')\n",
    "    df_metrics.to_csv(f'{save_path}.csv')\n",
    "    df_metrics.to_pickle(f'{save_path}.pkl')\n",
    "    df_metrics.to_excel(f'{save_path}.xlsx')\n",
    "    df_metrics.style.to_latex(f'{save_path}.tex', hrules=True)\n",
    "    df_metrics.to_markdown(f'{save_path}.md')\n",
    "    df_metrics.to_html(f'{save_path}.html')\n",
    "\n",
    "    print('Done saving fitted estimator and table!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efbbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completed_estimators(results_save_path=results_save_path, method=method):\n",
    "\n",
    "    estimators_list = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.model'):\n",
    "        with open(estimator_path, 'rb') as f:\n",
    "            estimators_list.append(joblib.load(f))\n",
    "\n",
    "    return estimators_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plots(\n",
    "    estimators_list, X_test, y_test, col,\n",
    "    curves_dict=None, cmap=plt.cm.Blues\n",
    "):\n",
    "\n",
    "    curves_dict = {\n",
    "        'ROC Curve': metrics.RocCurveDisplay,\n",
    "        'Precision Recall Curve': metrics.PrecisionRecallDisplay,\n",
    "        # 'Calibration Curve': CalibrationDisplay,\n",
    "        # 'Validation Curve': ValidationCurveDisplay,\n",
    "        # 'Learning Curve': LearningCurveDisplay,\n",
    "    }\n",
    "\n",
    "    assert len(estimators_list) != 0\n",
    "\n",
    "    for curve_name, curve_package in curves_dict.items():\n",
    "        print('-' * 20)\n",
    "        print(f'{col} - {str(curve_name)}')\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(f'{col} - {str(curve_name)}')\n",
    "        for estimator in estimators_list:\n",
    "            try:\n",
    "                curve = curve_package.from_estimator(\n",
    "                    estimator, X_test, y_test, pos_label=1, ax=ax, cmap=cmap,\n",
    "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "                )\n",
    "            except AttributeError:\n",
    "                curve = curve_package.from_estimator(\n",
    "                    estimator, X_test, y_test, pos_label=1, ax=ax,\n",
    "                    name=f'{estimator.steps[0][0]} + {estimator.steps[1][0]} + {estimator.steps[-1][0]}'\n",
    "                )\n",
    "        show_and_close_plots()\n",
    "\n",
    "        # Save Plots\n",
    "        for image_save_format in ['eps', 'png', 'svg']:\n",
    "            save_path = f'{plot_save_path}{method} {col} - All {str(curve_name)}s.{image_save_format}'\n",
    "            print(f'Saving {curve_name} at {save_path}')\n",
    "            curve.figure_.savefig(\n",
    "                save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "707bd733",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe77e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "estimator_names_list = get_existing_files()\n",
    "done_estimators = glob.glob(f'{done_xy_save_path}*')\n",
    "done_files = [\n",
    "    'df_train_data', 'df_test_data', 'df_val_data', 'eval_metrics_dict', 'test_metrics_dict'\n",
    "]\n",
    "\n",
    "# Identify cols, vectorizers and classifiers\n",
    "for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}{method} Estimator - *.model')):\n",
    "    assert f'{method} Estimator - ' in estimators_file, f'Estimators file name {estimators_file} does not contain {method} Estimator - '\n",
    "    col = estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "    vectorizer_name = estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "    classifier_name = estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "    if classifier_name in transformers_pipe.keys():\n",
    "        model = transformers_pipe[classifier_name]['model']\n",
    "        tokenizer = transformers_pipe[classifier_name]['tokenizer']\n",
    "        config = transformers_pipe[classifier_name]['config']\n",
    "        protocol = int(estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[-1].split(').model')[0])\n",
    "        with open(f'{done_xy_save_path}{method} training_args_dict - {col} - {vectorizer_name} + {classifier_name}.json', 'r') as f:\n",
    "            training_args_dict = json.load(f)\n",
    "        output_dir = training_args_dict['output_dir'] = estimators_file\n",
    "\n",
    "        # Load Table DF\n",
    "        df_metrics = make_df_metrics(\n",
    "            vectorizers_pipe=vectorizers_pipe, classifiers_pipe=classifiers_pipe, transformers_pipe=transformers_pipe,\n",
    "            metrics_list=metrics_dict,\n",
    "            col=col, vectorizer_name=vectorizer_name, classifier_name=classifier_name, protocol=protocol\n",
    "        )\n",
    "        print('~'*20)\n",
    "        print(f'Loading data for {col} - {vectorizer_name} + {classifier_name}')\n",
    "        print('~'*20)\n",
    "        # Load X, y and estimator\n",
    "        (\n",
    "            X_train, y_train, train_dataset,\n",
    "            X_test, y_test, test_dataset, y_test_pred, y_test_pred_prob,\n",
    "            X_val, y_val, val_dataset, y_val_pred, y_val_pred_prob,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "            saved_estimator, tokenizer, config, eval_metrics_dict, test_metrics_dict,\n",
    "        ) = load_Xy_estimator(\n",
    "            model, tokenizer, config,\n",
    "            col, vectorizer_name, classifier_name, protocol,\n",
    "        )\n",
    "\n",
    "        if hasattr(saved_estimator, 'to'):\n",
    "            saved_estimator = saved_estimator.to(device)\n",
    "\n",
    "        print('-'*20)\n",
    "        print(f'{\"=\"*30} EVALUATING DATASET OF LENGTH {len(X_train)+len(X_test)+len(X_val)} ON {col.upper()} {\"=\"*30}')\n",
    "        print('-'*20)\n",
    "        print(\n",
    "            f'Testing Classification Report:\\n{(train_report:=metrics.classification_report(y_test, y_test_pred, labels=np.unique(y_test_pred), zero_division=0))}\\n'\n",
    "        )\n",
    "        # Examine predictions\n",
    "        examine_predictions(\n",
    "            X_test, y_test, y_test_pred, col\n",
    "        )\n",
    "        print('='*20)\n",
    "        # Train and Test Confusion Matrix\n",
    "        print('='*20)\n",
    "        print('Evaluation and Test Confusion Matrix:\\n')\n",
    "        close_plots()\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        fig.suptitle(f'{col} - Evaluation and Test Confusion Matrix - {vectorizer_name} + {classifier_name}')\n",
    "        for ax in axs:\n",
    "            ax.set_aspect('equal')\n",
    "        val_cm = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            y_val, y_val_pred, normalize='true', ax=axs[0], cmap=plt.cm.Blues, colorbar=False\n",
    "        )\n",
    "        val_cm.ax_.set_title('Evaluation Data')\n",
    "        test_cm = metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test, y_test_pred, normalize='true', ax=axs[1], cmap=plt.cm.Blues, colorbar=False\n",
    "        )\n",
    "        test_cm.ax_.set_title('Testing Data')\n",
    "        plt.tight_layout()\n",
    "        for image_save_format in ['eps', 'png', 'svg']:\n",
    "            save_path = f'{plot_save_path}{method} {col} - Evaluation and Test Confusion Matrix - {vectorizer_name} + {classifier_name}.{image_save_format}'\n",
    "            try:\n",
    "                print(f'Evaluation and Test Confusion Matrix plot at {save_path}')\n",
    "                fig.savefig(\n",
    "                    save_path, format=image_save_format, dpi=3000, bbox_inches='tight'\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        show_and_close_plots()\n",
    "        print('='*20)\n",
    "\n",
    "        # Fit estimator\n",
    "        print('~'*20)\n",
    "        print('Fitting best params to estimator')\n",
    "        X = np.concatenate((X_test, X_val), axis=0)\n",
    "        y = np.concatenate((y_test, y_val), axis=0)\n",
    "        X_encodings = tokenizer(\n",
    "            X.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "        ).to(device)\n",
    "        dataset = ToDataset(X_encodings, y)\n",
    "\n",
    "        # Accelerate model\n",
    "        (\n",
    "            saved_estimator, tokenizer, dataset\n",
    "        ) = accelerator.prepare(\n",
    "            saved_estimator, tokenizer, dataset\n",
    "        )\n",
    "        # saved_estimator.eval()\n",
    "\n",
    "        # Initalize trainer\n",
    "        estimator = Trainer(\n",
    "            model=saved_estimator,\n",
    "            tokenizer=tokenizer,\n",
    "            args=TrainingArguments(**training_args_dict),\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)], # [TensorBoardCallback()]\n",
    "            data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "        )\n",
    "\n",
    "        # Get prediction results\n",
    "        if estimator.place_model_on_device:\n",
    "            estimator.model.to(device)\n",
    "        y_pred_logits, y_labels, metrics_dict = estimator.predict(dataset)\n",
    "        y_pred = metrics_dict.pop('test_y_pred')\n",
    "        y_pred_prob = metrics_dict.pop('test_y_pred_prob')\n",
    "        metrics_dict = clean_metrics_dict(metrics_dict, list(metrics_dict.keys())[0].split('_')[0])\n",
    "        save_fitted_estimator(estimator, metrics_dict, col, vectorizer_name, classifier_name, protocol)\n",
    "        print('Done predicting!')\n",
    "\n",
    "        # Evaluate Model\n",
    "        df_metrics = evaluation(\n",
    "            y_labels, y_pred,\n",
    "            metrics_dict, df_metrics,\n",
    "            col, vectorizer_name, classifier_name, plot_enabled=False,\n",
    "        )\n",
    "\n",
    "        # Confirmatory Regression\n",
    "        prob_confirmatory_tests(y_pred, y_pred_prob)\n",
    "\n",
    "        # Save Vectorizer, Selector, and Classifier\n",
    "        save_table(df_metrics, col, vectorizer_name, classifier_name, protocol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b166a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Automating_Equity1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
