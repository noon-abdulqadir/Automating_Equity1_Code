{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01a89f4838c483c82135cf3e7f708c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b981da6f4a5473580e177a3ede142fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7e134ed44d46f7a78e3083b15ebab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Transformer variables\n",
    "method = 'Transformers'\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'w') as f:\n",
    "    f.write(results_save_path)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "done_xy_save_path = f'{results_save_path}Search+Xy/'\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'w') as f:\n",
    "    f.write(done_xy_save_path)\n",
    "if not os.path.exists(done_xy_save_path):\n",
    "    os.makedirs(done_xy_save_path)\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    f'Mean Cross Validation Train - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Cross Validation Test - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "torch.Generator(device_name).manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "accelerator = Accelerator()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ.get('TOKENIZERS_PARALLELISM')\n",
    "best_trial_args = [\n",
    "    'num_train_epochs', 'per_device_train_batch_size', 'per_device_eval_batch_size', 'learning_rate', 'warmup_steps', 'weight_decay'\n",
    "]\n",
    "training_args_dict = {\n",
    "    'seed': random_state,\n",
    "    'resume_from_checkpoint': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'logging_steps': 500,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'eval_steps': 500,\n",
    "    'save_strategy': 'steps',\n",
    "    'save_steps': 500,\n",
    "    # 'metric_for_best_model': 'recall',\n",
    "    # 'torch_compile': bool(transformers.file_utils.is_torch_available()),\n",
    "    'use_mps_device': bool(device_name == 'mps' and torch.backends.mps.is_available()),\n",
    "    'optim': 'adamw_torch',\n",
    "    'load_best_model_at_end': True,\n",
    "    # The below metrics are used by hyperparameter search\n",
    "    'num_train_epochs': 3,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 20,\n",
    "    'learning_rate': 5e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "}\n",
    "training_args_dict_for_best_trial = {\n",
    "    arg_name: arg_\n",
    "    for arg_name, arg_ in training_args_dict.items()\n",
    "    if arg_name not in best_trial_args\n",
    "}\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "# tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d7b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    estimator_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if estimator_names_list is None:\n",
    "        estimator_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.pkl')):\n",
    "        if f'{method} Estimator - ' in estimators_file:\n",
    "\n",
    "            col=estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "            vectorizer_name=estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "            classifier_name=estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "            estimator_names_list.append(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "\n",
    "    return (\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70db7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    col,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    compression=None, protocol=None, path_suffix=None, data_dict=None\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    # Check data\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Assign dfs to variables\n",
    "    data_dict['df_train_data'] = df_train_data\n",
    "    data_dict['df_test_data'] = df_test_data\n",
    "    data_dict['df_val_data'] = df_val_data\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = f'{models_save_path}{file_name}{path_suffix}'\n",
    "        print(f'Saving Xy {file_name} at {save_path}')\n",
    "        file_.to_pickle(\n",
    "            save_path, protocol=protocol\n",
    "        )\n",
    "    print(f'Done saving Xy!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef9e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad68773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns=analysis_columns, text_col=text_col):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.loc[df[text_col].apply(len) >= 5]\n",
    "    print(f'DF length: {len(df)}')\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=1-test_split, test_size=test_split, random_state=random_state\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f07a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], device=device).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy_encodings(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, test_dataset,\n",
    "    X_val, y_val, val_dataset,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, val_dataset)\n",
    "\n",
    "    # Check encodings\n",
    "    assert all(y_train == train_dataset.labels), 'y_train and train_dataset labels are not the same'\n",
    "    assert all(y_test == test_dataset.labels), 'y_test and test_dataset labels are not the same'\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set encodings example:\\n{\" \".join(train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set encodings example:\\n{\" \".join(test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set encodings example:\\n{\" \".join(val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    tokenizer,\n",
    "):\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {f\"{tokenizer=}\".split(\"=\")[0]}.from_pretrained using {tokenizer.name_or_path}.')\n",
    "\n",
    "    X_train_encodings = tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    train_dataset = ToDataset(X_train_encodings, y_train)\n",
    "\n",
    "    X_test_encodings = tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    test_dataset = ToDataset(X_test_encodings, y_test)\n",
    "\n",
    "    X_val_encodings = tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    val_dataset = ToDataset(X_val_encodings, y_val)\n",
    "\n",
    "    # Print info\n",
    "    print_Xy_encodings(\n",
    "        X_train, y_train, train_dataset,\n",
    "        X_test, y_test, test_dataset,\n",
    "        X_val, y_val, val_dataset,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_encodings, train_dataset,\n",
    "        X_test_encodings, test_dataset,\n",
    "        X_val_encodings, val_dataset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ddcc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(\n",
    "    col,\n",
    "    results_save_path=results_save_path, method=method,\n",
    "    data_dict=None, protocol=None, path_suffix=None, \n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    print('+'*30)\n",
    "    print(f'{\"=\"*10} Loading Xy from previous for {col} {\"=\"*10}')\n",
    "    print('+'*30)\n",
    "    # Read all dfs\n",
    "    for file_path in glob.glob(f'{models_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{models_save_path}')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name} from {file_path}')\n",
    "        if path_suffix in file_path and 'df_' in file_name and 'cv_results' not in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    print(f'Done loading Xy from previous for {col}!')\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b78bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter tuning\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/hpo_train#hyperparameter-search-using-trainer-api\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-6, 1e-4, log=True),\n",
    "        'num_train_epochs': trial.suggest_int('num_train_epochs', 1, 5),\n",
    "        'per_device_train_batch_size': trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'per_device_eval_batch_size': trial.suggest_categorical('per_device_eval_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'warmup_steps': trial.suggest_int('warmup_steps', 0, 500),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-12, 1e-1, log=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c7abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute objective for hyperparameter tuning\n",
    "# https://github.com/huggingface/transformers/issues/13019\n",
    "def compute_objective(metrics_dict):\n",
    "    return metrics_dict['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_labels, y_pred,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "\n",
    "    print('Computing metrics using y_pred.')\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
    "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
    "    r2 = metrics.r2_score(y_labels, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_labels, y_pred)\n",
    "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_labels, y_pred_prob,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    print('Computing metrics using y_pred_prob.')\n",
    "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold, loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea5ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all(\n",
    "    y_labels, y_pred, y_pred_prob\n",
    "):\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using y_pred\n",
    "    if y_pred:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_labels, y_pred\n",
    "        )\n",
    "    # Using y_pred_prob\n",
    "    if y_pred_prob:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_labels, y_pred_prob\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # 'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        # f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # 'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        # f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'R2 Score': float(r2),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': str(report),\n",
    "        'Imbalanced Classification Report': str(imblearn_report),\n",
    "        'Confusion Matrix': str(cm),\n",
    "        'Normalized Confusion Matrix': str(cm_normalized),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_prob': y_pred_prob,\n",
    "    }\n",
    "    print('Done appending metrics to dict.')\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28661d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metrics_dict(metrics_dict, prefix_to_remove):\n",
    "    for metric_name in list(metrics_dict):\n",
    "        if metric_name.startswith(prefix_to_remove):\n",
    "            new_metric_name = ' '.join(metric_name.split(prefix_to_remove)[-1].split('_')).strip()\n",
    "        if not new_metric_name[0].isupper():\n",
    "            new_metric_name = new_metric_name.title()\n",
    "        if new_metric_name == 'Loss':\n",
    "            metrics_dict['Log Loss/Cross Entropy'] = metrics_dict.pop(metric_name)\n",
    "        else:\n",
    "            metrics_dict[new_metric_name] = metrics_dict.pop(metric_name)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f77d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred_prob(y_pred_logits, y_labels):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'Preprocessing y_pred logits and labels for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_logits, tuple):\n",
    "        y_pred_logits = y_pred_logits[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_logits):\n",
    "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    else:\n",
    "        y_pred_logits_tensor = y_pred_logits.to(device)\n",
    "\n",
    "    print(f'y_pred_logits shape: {y_pred_logits_tensor.shape}, {y_pred_logits_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    # https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits.')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_array.shape}, {y_pred_prob_array.dtype}')\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return torch.tensor(y_pred_prob_array, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred(y_pred_prob_array):\n",
    "\n",
    "    # https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred from y_pred_prob for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_prob_array, tuple):\n",
    "        y_pred_prob_array = y_pred_prob_array[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_prob_array):\n",
    "        y_pred_prob_tensor = torch.tensor(y_pred_prob_array, device=device)\n",
    "    else:\n",
    "        y_pred_prob_tensor = y_pred_prob_array.to(device)\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_tensor.shape}. {y_pred_prob_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred through argmax of y_pred_prob.')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_prob_tensor, axis=-1).cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_prob.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "\n",
    "    return y_pred_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    y_pred_prob_array, y_labels_array = predicted_results_from_eval\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    (\n",
    "        y_pred_array\n",
    "    ) = preprocess_logits_for_metrics_y_pred(y_pred_prob_array)\n",
    "\n",
    "    # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
    "    print('-'*20)\n",
    "    print('Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.')\n",
    "    y_labels = y_labels_array.flatten().tolist()\n",
    "    y_pred = y_pred_array.flatten().tolist()\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred_prob length: {len(y_pred_prob)}')\n",
    "    print(f'y_labels length: {len(y_labels)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a625b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits):\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    print('Getting y_pred through argmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1).cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred...')\n",
    "    y_pred = [bert_label2id[l] for l in y_pred_array.flatten().tolist()]\n",
    "    print(f'y_pred length: {len(y_pred)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
    "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
    "    print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred length: {len(y_pred_prob)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ab4fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_logits(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    # Get predictions\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred logits and ids for {col}:')\n",
    "    y_pred_logits, y_labels = predicted_results_from_eval\n",
    "    print(f'y_pred_logits shape: {y_pred_logits.shape}')\n",
    "    print(f'y shape: {y_labels.shape}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_test_pred and y_test_pred_prob\n",
    "    (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    ) = preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    metrics_dict, df_metrics,\n",
    "    col, vectorizer_name, classifier_name\n",
    "):\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if metric_name not in ['Runtime', 'Samples Per Second', 'Steps Per Second']:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_name, (int, float)):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = metric_value\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "                print(f'{metric_name}: {metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93960eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy data in df and save\n",
    "def save_Xy_estimator(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "    X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "    estimator, accelerator, eval_metrics_dict, test_metrics_dict,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None, protocol=None,\n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset)\n",
    "\n",
    "    # Make data dict\n",
    "    data_dict['Estimator'] = estimator\n",
    "    data_dict['accelerator'] = accelerator\n",
    "    data_dict['eval_metrics_dict'] = eval_metrics_dict\n",
    "    data_dict['test_metrics_dict'] = test_metrics_dict\n",
    "\n",
    "    # Make df_train_data\n",
    "    data_dict['df_train_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'train_dataset': train_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    data_dict['df_test_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "            'test_dataset': test_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    data_dict['df_val_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_pred_prob': y_val_pred_prob,\n",
    "            'val_dataset': val_dataset,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    saved_files_list = []\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = (\n",
    "            done_xy_save_path\n",
    "            if file_name not in ['Estimator', 'accelerator']\n",
    "            else results_save_path\n",
    "        )\n",
    "        print(f'Saving {file_name} at {save_path}')\n",
    "        if not isinstance(file_, pd.DataFrame) and file_name == 'Estimator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            # Save as .model\n",
    "            file_.save_model(f'{save_path}{method} {file_name}{path_suffix.replace(\"pkl\", \"model\")}')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif not isinstance(file_, pd.DataFrame) and file_name == 'accelerator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.save(estimator.state, f'{save_path}{method} Estimator{path_suffix.replace(\"pkl\", \"model\")}/accelerator')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, dict) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' not in file_name and 'metrics_dict' in file_name:\n",
    "            with open(f'{save_path}{method} {file_name}{path_suffix}', 'wb') as f:\n",
    "                pickle.dump(file_, f, protocol=protocol)\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, pd.DataFrame) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "            saved_files_list.append(file_name)\n",
    "\n",
    "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were saved! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done saving Xy, labels and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fb3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all classifiers were used\n",
    "def assert_all_classifiers_used(\n",
    "    estimators_list=None, used_classifiers=None, results_save_path=results_save_path, method=method, classifiers_pipe=transformers_pipe,\n",
    "):\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.pkl'):\n",
    "        classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "        used_classifiers.append(classifier_name)\n",
    "\n",
    "    assert set(list(classifiers_pipe.keys())) == set(used_classifiers), f'Not all classifiers were used!\\nAvaliable Classifiers:\\n{set(list(classifiers_pipe.keys()))}\\nUsed Classifiers:\\n{set(used_classifiers)}\\nLeftout Classifiers:\\n{set(list(classifiers_pipe.keys())) ^ set(used_classifiers)}'\n",
    "    print('All classifiers were used!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f64ab3",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded with shape: (5947, 68)\n"
     ]
    }
   ],
   "source": [
    "with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
    "    df_manual_len = int(f.read())\n",
    "\n",
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_training.pkl')\n",
    "assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
    "print(f'Dataframe loaded with shape: {df_manual.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ea2f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory:\n",
      "/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5947 ON COMPETENCE ==============================\n",
      "--------------------\n",
      "classifiers to be used (1):\n",
      "['GPT2ForSequenceClassification']\n",
      "Splitting data.\n",
      "====================\n",
      "Splitting data into training, testing, and validation sets:\n",
      "Ratios: train_size = 0.75, test size = 0.1, validation size = 0.15\n",
      "DF length: 5928\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "Understand, advocate, and embody the Netflix culture and team goals.\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "They will coach you, inspire you, introduce you to their network and share their knowledge and insights with you.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "The Ideal Candidate:\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.89 (0 = 0.94, 1 = 1.06)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.80 (0 = 0.90, 1 = 1.13)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.87 (0 = 0.93, 1 = 1.08)\n",
      "====================\n",
      "====================\n",
      "Saving Xy df_train_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_train_data - Competence - (Save_protocol=5).pkl\n",
      "Saving Xy df_test_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_test_data - Competence - (Save_protocol=5).pkl\n",
      "Saving Xy df_val_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_val_data - Competence - (Save_protocol=5).pkl\n",
      "Done saving Xy!\n",
      "['df_train_data', 'df_test_data', 'df_val_data']\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[ASome weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with tokenizer.from_pretrained using gpt2.\n",
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "Under stand , Ġadvocate , Ġand Ġembody Ġthe ĠNetflix Ġculture Ġand Ġteam Ġgoals . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "They Ġwill Ġcoach Ġyou , Ġinspire Ġyou , Ġintroduce Ġyou Ġto Ġtheir Ġnetwork Ġand Ġshare Ġtheir Ġknowledge Ġand Ġinsights Ġwith Ġyou . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "The ĠIdeal ĠCandidate : <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "====================\n",
      "--------------------\n",
      "==============================\n",
      "============================== Initializing Trainer using GPT2 + GPT2ForSequenceClassification ==============================\n",
      "++++++++++++++++++++++++++++++\n",
      "--------------------\n",
      "Passing data and arguments to Trainer.\n",
      "--------------------\n",
      "Starting training for Competence using GPT2ForSequenceClassification.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be741b92a4b44df9f42a86c69fbc592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      "\u001b[A                                  \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7647, 'learning_rate': 2.2752043596730245e-05, 'epoch': 1.8}\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249fd387a78f4a22964624662a49e94b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A                                  \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([9, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (9, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([889, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (889,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 889\n",
      "y_labels length: 889\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "{'eval_loss': 0.3990565836429596, 'eval_Explained Variance': 0.32767005107127567, 'eval_Accuracy': 0.8323959505061868, 'eval_Balanced Accuracy': 0.8329119783506622, 'eval_Precision': 0.8069767441860465, 'eval_Average Precision': 0.8725218540938132, 'eval_Recall': 0.8401937046004843, 'eval_F1-score': 0.8232502965599051, 'eval_Matthews Correlation Coefficient': 0.6645036164785327, 'eval_Fowlkes–Mallows Index': 0.7211967350473845, 'eval_R2 Score': 0.32619997151402913, 'eval_ROC': 0.9038013510488941, 'eval_AUC': 0.9038013510488941, 'eval_Log Loss/Cross Entropy': 0.3990565907369381, 'eval_Cohen’s Kappa': 0.664015198593775, 'eval_Geometric Mean': 0.6936893401428367, 'eval_Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.86      0.83      0.84       476\\n           1       0.81      0.84      0.82       413\\n\\n    accuracy                           0.83       889\\n   macro avg       0.83      0.83      0.83       889\\nweighted avg       0.83      0.83      0.83       889\\n', 'eval_Imbalanced Classification Report': '                   pre       rec       spe        f1       geo       iba       sup\\n\\n          0       0.86      0.83      0.84      0.84      0.83      0.69       476\\n          1       0.81      0.84      0.83      0.82      0.83      0.69       413\\n\\navg / total       0.83      0.83      0.83      0.83      0.83      0.69       889\\n', 'eval_Confusion Matrix': '[[393  83]\\n [ 66 347]]', 'eval_Normalized Confusion Matrix': '[[0.82563025 0.17436975]\\n [0.1598063  0.8401937 ]]', 'eval_y_pred': [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1], 'eval_y_pred_prob': [0.0012174119474366307, 0.005041011609137058, 0.0027062867302447557, 0.4982433319091797, 0.23322457075119019, 0.8589008450508118, 0.9267575144767761, 0.9600877165794373, 0.9257948994636536, 0.08870485424995422, 0.4143288731575012, 0.788746178150177, 0.67427659034729, 0.07573334872722626, 0.9515184164047241, 0.06683176755905151, 0.7075896859169006, 0.4788195788860321, 0.6351787447929382, 0.10173770040273666, 0.3888620436191559, 0.9483394026756287, 0.6215506792068481, 0.9510494470596313, 0.8009949922561646, 0.9899112582206726, 0.06417089700698853, 0.09547823667526245, 0.3549973964691162, 0.005479079205542803, 0.17499928176403046, 0.08014654368162155, 0.005268658045679331, 0.7334606051445007, 0.6168219447135925, 0.8147409558296204, 0.1559586375951767, 0.9890170097351074, 0.9004876613616943, 0.010095692239701748, 0.9519298672676086, 0.2747712731361389, 0.007384304888546467, 0.22381724417209625, 0.0933178961277008, 0.04671246558427811, 0.9908946752548218, 0.1904795914888382, 0.860719621181488, 0.22241680324077606, 0.6839304566383362, 0.9527841806411743, 0.3506184220314026, 0.058099254965782166, 0.03720007464289665, 0.9627979397773743, 0.003438984276726842, 0.03078915923833847, 0.9654347896575928, 0.03975832089781761, 0.006290688179433346, 0.024485234171152115, 0.007097634021192789, 0.9743307828903198, 0.7332810759544373, 0.9268169403076172, 0.7959902882575989, 0.1616256684064865, 0.86829674243927, 0.005088768433779478, 0.002461658790707588, 0.7120290398597717, 0.004629768431186676, 0.9208187460899353, 0.020507212728261948, 0.00395023450255394, 0.3195928633213043, 0.5870537161827087, 0.0012056567938998342, 0.944253146648407, 0.9661924839019775, 0.0009129923419095576, 0.003559618489816785, 0.04081597179174423, 0.9643641114234924, 0.9311781525611877, 0.8832263350486755, 0.7117771506309509, 0.14635123312473297, 0.8076114058494568, 0.01767805777490139, 0.9668867588043213, 0.9671045541763306, 0.7269909381866455, 0.6209070086479187, 0.8862590789794922, 0.8661732077598572, 0.10402310639619827, 0.02019619755446911, 0.5141931772232056, 0.7076380252838135, 0.9454470872879028, 0.34220078587532043, 0.6889281272888184, 0.9106590747833252, 0.9032761454582214, 0.7961660623550415, 0.8196009397506714, 0.0036919512785971165, 0.8884884715080261, 0.7103754281997681, 0.8355743288993835, 0.9627093076705933, 0.006240046117454767, 0.00374547578394413, 0.9470285177230835, 0.003475877456367016, 0.31058964133262634, 0.4189120829105377, 0.762529730796814, 0.37291520833969116, 0.04210107401013374, 0.32959049940109253, 0.8187423944473267, 0.2532646059989929, 0.9467722773551941, 0.9482284188270569, 0.7949668765068054, 0.03478327393531799, 0.9336649775505066, 0.5551580786705017, 0.32578060030937195, 0.7669612765312195, 0.09933780878782272, 2.747335202002432e-06, 0.6158085465431213, 0.008858546614646912, 0.0019691898487508297, 0.7876991033554077, 0.8219530582427979, 0.9384108781814575, 0.9811153411865234, 0.2867698073387146, 0.9682304263114929, 0.06555108726024628, 0.9494214057922363, 0.4290057420730591, 0.34313973784446716, 0.040873780846595764, 0.042080581188201904, 0.3045988082885742, 0.029414482414722443, 0.12326642870903015, 0.9259437918663025, 0.554931640625, 0.7614948749542236, 0.9244846105575562, 0.4389466941356659, 0.040106095373630524, 0.007103076670318842, 0.7453662157058716, 0.006469260901212692, 0.45796939730644226, 0.007936769165098667, 0.007318435702472925, 0.008384066633880138, 0.9876229763031006, 0.8011772632598877, 0.014491411857306957, 0.06263163685798645, 0.01457709725946188, 0.34871596097946167, 0.14761142432689667, 0.01880783960223198, 0.006799018010497093, 0.977732241153717, 0.6458194851875305, 0.9669568538665771, 0.01457709725946188, 0.5108134150505066, 0.8927674293518066, 0.5646270513534546, 0.8820775747299194, 0.022717120125889778, 0.9375816583633423, 0.09479543566703796, 0.621496319770813, 0.02234926074743271, 0.015882739797234535, 0.9107755422592163, 0.9962220191955566, 0.6884022355079651, 0.007653822656720877, 0.009481918066740036, 0.5272600054740906, 0.9565286636352539, 0.9655100107192993, 0.897796094417572, 0.5855730175971985, 0.8272964358329773, 0.9095558524131775, 0.3045988082885742, 0.9614713788032532, 0.06750290840864182, 0.05764738842844963, 0.005152529571205378, 0.010044271126389503, 0.29973918199539185, 0.9671603441238403, 0.9426267743110657, 0.20168741047382355, 0.6733048558235168, 0.5824231505393982, 0.016139047220349312, 0.02771763876080513, 0.9514946341514587, 0.09297528862953186, 0.5577776432037354, 0.12701161205768585, 0.3185288906097412, 0.0029001873917877674, 0.1602686196565628, 0.8132486939430237, 0.8536139726638794, 0.9021481871604919, 0.31956756114959717, 0.2983221709728241, 0.34832334518432617, 0.9063884615898132, 0.6850318908691406, 0.007248732727020979, 0.8650521636009216, 0.9866997003555298, 0.7080907225608826, 0.9878838658332825, 0.009832245297729969, 0.9694459438323975, 0.5823725461959839, 0.2986755073070526, 0.025117678567767143, 0.8152499198913574, 0.2698248028755188, 0.09519101679325104, 0.9185341596603394, 0.9556559324264526, 0.9406951069831848, 0.9125854969024658, 0.963355302810669, 0.004364989697933197, 0.7327662706375122, 0.5745207071304321, 0.9633352756500244, 0.6092711091041565, 0.10557617992162704, 0.968897819519043, 0.7042694091796875, 0.016222164034843445, 0.280608594417572, 0.6279902458190918, 0.9327428340911865, 0.003854372538626194, 0.9673501253128052, 0.0034829257056117058, 0.9284362196922302, 0.0026907487772405148, 0.05142434313893318, 0.02925233170390129, 0.30455124378204346, 0.9076349139213562, 0.0007134134066291153, 0.0070576597936451435, 0.4129487872123718, 0.2728545665740967, 0.7570961117744446, 0.8380908370018005, 0.9269193410873413, 0.13807247579097748, 0.026834683492779732, 0.7385897636413574, 0.7114452123641968, 0.3197815418243408, 0.00654867896810174, 0.056108687072992325, 0.022065646946430206, 0.10481413453817368, 0.03723646327853203, 0.008015355095267296, 0.002120152348652482, 0.7699587345123291, 0.03493297100067139, 0.02256150171160698, 0.9325671792030334, 0.008654750883579254, 0.804264485836029, 0.0019898628816008568, 0.9835055470466614, 0.639470100402832, 0.03287167847156525, 0.45848122239112854, 0.9673501253128052, 0.03865852579474449, 0.07060394436120987, 0.012293899431824684, 0.9859185218811035, 0.002053930191323161, 0.8736794590950012, 0.16316597163677216, 0.0006493455730378628, 0.8077228665351868, 0.05763739347457886, 0.059981539845466614, 0.003938647452741861, 0.01278556976467371, 0.22268171608448029, 0.9586662650108337, 0.3391166925430298, 0.8893429040908813, 0.8138332366943359, 0.19670334458351135, 0.997595489025116, 0.5270991921424866, 0.33336973190307617, 0.22920499742031097, 0.8089205622673035, 0.008002574555575848, 0.0015144279459491372, 0.008092396892607212, 0.8693404793739319, 0.911155104637146, 0.981025755405426, 0.625024139881134, 0.4783487319946289, 0.9280783534049988, 0.8649441599845886, 0.5730479955673218, 0.9533458352088928, 0.07909458130598068, 0.9699656367301941, 0.7311906218528748, 0.0016359493602067232, 0.055026888847351074, 0.052029985934495926, 0.7633919715881348, 0.11930343508720398, 0.9287253022193909, 0.8579338788986206, 0.0028972213622182608, 0.02918638102710247, 0.8625470399856567, 0.9238384366035461, 0.005025334656238556, 0.07909458130598068, 0.9637345671653748, 0.937800943851471, 0.9381240010261536, 0.11590375751256943, 0.0004897799808532, 0.952147364616394, 0.063021719455719, 0.7603922486305237, 0.025310151278972626, 0.9787402153015137, 0.9136605262756348, 0.0016520069912075996, 0.6870363354682922, 0.35361382365226746, 0.26422104239463806, 0.8128862977027893, 0.09007416665554047, 0.42039644718170166, 0.006529274862259626, 0.9799768924713135, 0.347137451171875, 0.016190538182854652, 0.23322457075119019, 0.006272463593631983, 0.033896613866090775, 0.22429323196411133, 0.794185996055603, 0.3386557102203369, 0.9452453255653381, 0.22377914190292358, 0.006814863067120314, 0.007689341902732849, 0.09173305332660675, 0.0011069487081840634, 0.5619210004806519, 0.9747280478477478, 0.0004270476056262851, 0.7507752180099487, 0.005656090099364519, 0.22325268387794495, 0.9262645244598389, 0.004523710813373327, 0.9197520017623901, 0.9611339569091797, 0.9715991020202637, 0.7461433410644531, 0.43391814827919006, 0.2093466818332672, 0.8447976112365723, 0.8797740936279297, 0.00338216545060277, 0.2784184217453003, 0.5363371968269348, 0.8522184491157532, 0.8666616678237915, 0.017788061872124672, 0.006697569042444229, 0.17299717664718628, 0.8475314974784851, 0.7584887742996216, 0.8621651530265808, 0.982532262802124, 0.09385854750871658, 0.3660496473312378, 0.0034829257056117058, 0.9707382917404175, 0.033879317343235016, 0.3968384563922882, 0.010156086646020412, 0.6443941593170166, 0.009984931908547878, 0.9613772630691528, 0.13616548478603363, 0.022317873314023018, 0.39456620812416077, 0.07081739604473114, 0.526638388633728, 0.9808583855628967, 0.8771445751190186, 0.554931640625, 0.013335156254470348, 0.4048004448413849, 0.06661441177129745, 0.3776390850543976, 0.09933780878782272, 0.002170288236811757, 0.5043154954910278, 0.2495872527360916, 0.9586662650108337, 0.9388808012008667, 0.1727791428565979, 0.013029525987803936, 0.37225744128227234, 0.005104138515889645, 0.004257310181856155, 0.9304906725883484, 0.8569780588150024, 0.006032443605363369, 0.9137728810310364, 0.007629057392477989, 0.7021716237068176, 0.008384066633880138, 0.8878887891769409, 0.1449904441833496, 0.007613874040544033, 0.021930484101176262, 0.011400814168155193, 0.9763729572296143, 0.8969968557357788, 0.9292410016059875, 0.9228840470314026, 0.8117304444313049, 0.9784949421882629, 0.0004897799808532, 0.009538617916405201, 0.021536948159337044, 0.01971498876810074, 0.9240282773971558, 0.7553223967552185, 0.5477249026298523, 0.8340619206428528, 0.10064392536878586, 0.47068503499031067, 0.9291356205940247, 0.006342544220387936, 0.1397932916879654, 0.3501204550266266, 0.3479534089565277, 0.9509470462799072, 0.3095707595348358, 0.6399508714675903, 0.9844021797180176, 0.6007087230682373, 0.6444580554962158, 0.03827973082661629, 0.49945566058158875, 0.1877775937318802, 0.28349387645721436, 0.11200214177370071, 0.487680584192276, 0.009538617916405201, 0.9114384651184082, 0.8594774007797241, 0.9581576585769653, 0.9528896808624268, 0.7021716237068176, 0.8220797777175903, 0.45717179775238037, 0.974748969078064, 0.9577169418334961, 0.03820020705461502, 0.05822651460766792, 0.8675004839897156, 0.9900957345962524, 0.4822666645050049, 0.9164474010467529, 0.17114078998565674, 0.0030464341398328543, 0.9548081159591675, 0.005073830019682646, 0.9816856384277344, 0.8358286619186401, 0.4889990985393524, 0.8276249766349792, 0.74185711145401, 0.9711332321166992, 0.6303509473800659, 0.06071639433503151, 0.5184455513954163, 0.9475606083869934, 0.01602950505912304, 0.8814218640327454, 0.9497923851013184, 0.982864499092102, 0.061373475939035416, 0.06832679361104965, 0.9593853950500488, 0.02225755713880062, 0.0044631012715399265, 0.8474167585372925, 0.9247039556503296, 0.7945239543914795, 0.7559307217597961, 0.00012072373647242785, 0.2644096910953522, 0.9877592921257019, 0.9441722631454468, 0.18317736685276031, 0.2949252128601074, 0.056165143847465515, 0.0073151178658008575, 0.9208187460899353, 0.6388601660728455, 0.9323327541351318, 0.05856502428650856, 0.06345422565937042, 0.7240229845046997, 0.399243026971817, 0.7864754796028137, 0.09493111073970795, 0.0040940046310424805, 0.8859349489212036, 0.9565860033035278, 0.6268913745880127, 0.03645244613289833, 0.899046003818512, 0.002742326119914651, 0.3859185576438904, 0.8188258409500122, 0.9115148186683655, 0.0037664759438484907, 0.007425440475344658, 0.8157252669334412, 0.1287916600704193, 0.6648956537246704, 0.8553622961044312, 0.9100980758666992, 0.7899913787841797, 0.24242983758449554, 0.015498078428208828, 0.9643310904502869, 0.17187382280826569, 0.00495216203853488, 0.8279326558113098, 0.9435314536094666, 0.025065792724490166, 0.00908085610717535, 0.9480903744697571, 0.053397081792354584, 0.7387746572494507, 0.9686604142189026, 0.9041931629180908, 0.42018288373947144, 0.5786569714546204, 0.020176280289888382, 0.002791905775666237, 0.015993701294064522, 0.0753134936094284, 0.08238363265991211, 0.9677749276161194, 0.9522736668586731, 0.05692018195986748, 0.8988019824028015, 0.9551681876182556, 0.7064639925956726, 0.001394833205267787, 0.8833508491516113, 0.8610775470733643, 0.23563890159130096, 0.3876457214355469, 0.9210522770881653, 0.008524429984390736, 0.3159606456756592, 0.004034498240798712, 0.1550174057483673, 0.5633100271224976, 0.9010384678840637, 0.4097931981086731, 0.9413691163063049, 0.38488346338272095, 0.9606210589408875, 0.9287258982658386, 0.06479683518409729, 0.9070820808410645, 0.0007058825576677918, 0.9297203421592712, 0.8827253580093384, 0.8887080550193787, 0.8856207132339478, 0.6607541441917419, 0.9606936573982239, 0.8927900791168213, 0.8891968131065369, 0.0029208471532911062, 0.9486481547355652, 0.937492847442627, 0.9826164245605469, 0.9339072704315186, 0.5695358514785767, 0.0023218649439513683, 0.6415109038352966, 0.00616477383300662, 0.8983415365219116, 0.15429554879665375, 0.05268397182226181, 0.02839401736855507, 0.9457035660743713, 0.8911575675010681, 0.011830370873212814, 0.29236748814582825, 0.34958818554878235, 0.989316463470459, 0.6821876764297485, 0.1024705097079277, 0.022506123408675194, 0.13308754563331604, 0.0036717774346470833, 0.08037102967500687, 0.11599618196487427, 0.9968570470809937, 0.005530301947146654, 0.6184714436531067, 0.6310797929763794, 0.19595426321029663, 0.013326870277523994, 0.1463451236486435, 0.010936970822513103, 0.31618955731391907, 0.37966302037239075, 0.6139301657676697, 0.3565278947353363, 0.9384374618530273, 0.9735530018806458, 0.8681873083114624, 0.017037833109498024, 0.011250303126871586, 0.5983811616897583, 0.955621063709259, 0.006420660763978958, 0.9397799968719482, 0.7343752980232239, 0.7363311052322388, 0.9957213401794434, 0.8477499485015869, 0.009234011173248291, 0.01720753312110901, 0.37483566999435425, 0.7139818072319031, 0.4768495261669159, 0.5278112888336182, 0.8601202368736267, 0.934683084487915, 0.00851402711123228, 0.525076150894165, 0.6354491114616394, 0.043987393379211426, 0.3870075047016144, 0.023691076785326004, 0.009037190116941929, 0.27636629343032837, 0.6641634702682495, 0.913179337978363, 0.5277072191238403, 0.00276448973454535, 0.594093382358551, 0.1277618259191513, 0.002978809643536806, 0.7503032684326172, 0.008906873874366283, 0.42235615849494934, 0.027079612016677856, 0.4248894453048706, 0.8710847496986389, 0.8421804308891296, 0.9645382165908813, 0.0007505505927838385, 0.5156888365745544, 0.5617136359214783, 0.4820472002029419, 0.14892716705799103, 0.0036866297014057636, 0.19703121483325958, 0.00732165202498436, 0.9829686284065247, 0.013768065720796585, 0.9745413064956665, 0.9560837149620056, 0.005090093705803156, 0.03726533427834511, 0.9654654860496521, 0.8184041380882263, 0.8084994554519653, 0.3262619376182556, 0.7547079920768738, 0.9876225590705872, 0.0037026251666247845, 0.9361950159072876, 0.00615675700828433, 0.802237868309021, 0.07543446123600006, 0.9480856657028198, 0.15375034511089325, 0.3652307689189911, 0.9013315439224243, 0.6138734817504883, 0.9763700366020203, 0.12203563004732132, 0.08549365401268005, 0.5242308378219604, 0.9409862160682678, 0.0007130544981919229, 0.8691656589508057, 0.1422310471534729, 0.49503058195114136, 0.05024110898375511, 0.9640915989875793, 0.41700756549835205, 0.9480621814727783, 0.9538491368293762, 0.9941064119338989, 0.4389466941356659, 0.4060209095478058, 0.8347350358963013, 0.5343331098556519, 0.8536458015441895, 0.3104161322116852, 0.0006997521268203855, 0.10041113942861557, 0.9878029227256775, 0.119514100253582, 0.9444100260734558, 0.9848078489303589, 0.02733834832906723, 0.3905748426914215, 0.9020615816116333, 0.08376509696245193, 0.3319895267486572, 0.7797309756278992, 0.7075607180595398, 0.650840699672699, 0.1522725224494934, 0.9609188437461853, 0.896286129951477, 0.002908014692366123, 0.03907196223735809, 0.03001648373901844, 0.9171575307846069, 0.7066016793251038, 0.0006997521268203855, 0.7879027128219604, 0.03658660873770714, 0.006914820522069931, 0.0571267306804657, 0.012510046362876892, 0.5133895874023438, 0.7810328006744385, 0.004916996229439974, 0.004273972939699888, 0.2776581048965454, 0.8547176122665405, 0.9536082148551941, 0.8965218663215637, 0.944827675819397, 0.03981341794133186, 0.9424313902854919, 0.17499928176403046, 0.0005678990855813026, 0.4101082682609558, 0.15304258465766907, 0.7531859874725342, 0.8883303999900818, 0.005975813139230013, 0.9372934103012085, 0.5205897688865662, 0.950310230255127, 0.4962095618247986, 0.16746483743190765, 0.0016570060979574919, 0.9592409133911133, 0.3471325933933258, 0.11674007028341293, 0.5417840480804443, 0.030346643179655075, 0.2008770853281021, 0.7355766892433167, 0.02444496564567089, 0.017626948654651642, 0.20030973851680756, 0.9747226238250732, 0.9367921948432922, 0.8148051500320435, 0.9609997272491455, 0.8657469153404236, 0.7336590886116028, 0.01852394826710224, 0.03042861633002758, 0.4825861155986786, 0.9911963939666748, 0.8436306715011597, 0.8820775747299194, 0.3260835111141205, 0.10064392536878586, 0.02444496564567089, 0.21103723347187042, 0.4568473994731903, 0.0051450361497700214, 0.24699385464191437, 0.24999013543128967, 0.9958972930908203, 0.2390104979276657, 0.9632431268692017, 0.5823725461959839, 0.23353727161884308, 0.4269208610057831, 0.002791905775666237, 0.9536980390548706, 0.8946826457977295, 0.3503221571445465, 0.004667190834879875, 0.04691063612699509, 0.8820242285728455, 0.005073830019682646, 0.26132574677467346, 0.16205860674381256, 0.8436632752418518, 0.15399335324764252, 0.15993176400661469, 0.2429579347372055, 0.32944706082344055, 0.0024643789511173964, 0.9653074741363525, 0.17489634454250336, 0.9808555245399475, 0.7833375334739685, 0.01254223845899105, 0.32585784792900085, 0.9460709691047668, 0.281941682100296, 0.9141687154769897, 0.11872199177742004, 0.5777023434638977, 0.02038518898189068, 0.3662512004375458, 0.5567908883094788, 0.9888938069343567, 0.7037016153335571, 0.9720603823661804, 0.0027862205170094967, 0.5286475419998169, 0.004503216594457626, 0.04257144033908844, 0.7067950963973999, 0.7323290705680847, 0.33445775508880615, 0.5640690922737122, 0.29108110070228577, 0.7573788166046143], 'eval_runtime': 22.7035, 'eval_samples_per_second': 39.157, 'eval_steps_per_second': 1.982, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
      "\n",
      "\u001b[A                                  \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4070.7394, 'train_samples_per_second': 3.277, 'train_steps_per_second': 0.205, 'train_loss': 0.5621784402312134, 'epoch': 3.0}\n",
      "Done training!\n",
      "--------------------\n",
      "--------------------\n",
      "Evaluating estimator for Competence.\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3c851c43d6429ca15172e95da64f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([9, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (9, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([889, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (889,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 889\n",
      "y_labels length: 889\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "Done evaluating!\n",
      "Getting prediction results for Competence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7feb31ed08574e509611301f07dc519d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([13, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (13, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([593, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (593,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 593\n",
      "y_labels length: 593\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "Done predicting!\n",
      "--------------------\n",
      "--------------------\n",
      "Saving model for Competence.\n",
      "====================\n",
      "Saving Estimator at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n",
      "Saving accelerator at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n",
      "Saving eval_metrics_dict at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving test_metrics_dict at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving df_train_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [1:08:44<00:00, 4124.13s/it]\n",
      "100%|██████████| 1/1 [1:08:44<00:00, 4124.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df_test_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving df_val_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Done saving Xy, labels and estimator!\n",
      "['Estimator', 'accelerator', 'eval_metrics_dict', 'test_metrics_dict', 'df_train_data', 'df_test_data', 'df_val_data']\n",
      "====================\n",
      "Done training!\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/IPython/cor</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">e/magics/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">execution.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1325</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">time</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1322 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1323 │   │   │   </span>st = clock2()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1324 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1325 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exec(code, glob, local_ns)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1326 │   │   │   │   </span>out=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1327 │   │   │   │   # multi-line %%time case</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1328 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> expr_val <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;timed exec&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">194</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1894998644.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span> in             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">assert_all_classifiers_used</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: </span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">'/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/1894998644.py'</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AssertionError: </span>Not all classifiers were used!\n",
       "Avaliable Classifiers:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'GPT2ForSequenceClassification'</span><span style=\"font-weight: bold\">}</span>\n",
       "Used Classifiers:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">set</span><span style=\"font-weight: bold\">()</span>\n",
       "Leftout Classifiers:\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'GPT2ForSequenceClassification'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/homebrew/Caskroom/mambaforge/base/envs/study1_3.10/lib/python3.10/site-packages/IPython/cor\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33me/magics/\u001b[0m\u001b[1;33mexecution.py\u001b[0m:\u001b[94m1325\u001b[0m in \u001b[92mtime\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1322 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1323 \u001b[0m\u001b[2m│   │   │   \u001b[0mst = clock2()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1325 \u001b[2m│   │   │   │   \u001b[0mexec(code, glob, local_ns)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1326 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mout=\u001b[94mNone\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1327 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# multi-line %%time case\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1328 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m expr_val \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33m<timed exec>\u001b[0m:\u001b[94m194\u001b[0m in \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/\u001b[0m\u001b[1;33m1894998644.py\u001b[0m:\u001b[94m14\u001b[0m in             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92massert_all_classifiers_used\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: \u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m'/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_80097/1894998644.py'\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAssertionError: \u001b[0mNot all classifiers were used!\n",
       "Avaliable Classifiers:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'GPT2ForSequenceClassification'\u001b[0m\u001b[1m}\u001b[0m\n",
       "Used Classifiers:\n",
       "\u001b[1;35mset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "Leftout Classifiers:\n",
       "\u001b[1m{\u001b[0m\u001b[32m'GPT2ForSequenceClassification'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "# Define columns to be used\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "estimator_names_list = get_existing_files()\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'classifiers to be used ({len(list(transformers_pipe.keys()))}):\\n{list(transformers_pipe.keys())}'\n",
    "    )\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    if len(glob.glob(f'{results_save_path}{method} df_*_data - {col} - (Save_protocol=*).pkl')) == 3:\n",
    "        # Load previous Xy\n",
    "        print('Loading previous Xy.')\n",
    "        (\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = load_Xy(\n",
    "            col\n",
    "        )\n",
    "    else:\n",
    "        print('Splitting data.')\n",
    "        # Split data\n",
    "        (\n",
    "            train, X_train, y_train,\n",
    "            test, X_test, y_test,\n",
    "            val, X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = split_data(\n",
    "            df_manual, col,\n",
    "        )\n",
    "        # Save Xy data\n",
    "        save_Xy(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            col,\n",
    "        )\n",
    "\n",
    "    # Get model_name, tokenizer, model and optimizer\n",
    "    for transformer_name, transformer_dict in tqdm.tqdm(transformers_pipe.items()):\n",
    "        model_name = transformer_dict['model_name']\n",
    "        config = transformer_dict['config'].from_pretrained(model_name)\n",
    "        tokenizer = transformer_dict['tokenizer'].from_pretrained(model_name, num_labels=2)\n",
    "        model = transformer_dict['model'].from_pretrained(model_name, config=config,).to(device)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        if model.config.pad_token_id is None:\n",
    "            try:\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            except:\n",
    "                model.config.pad_token_id = model.config.eos_token_id\n",
    "        def model_init(trial): return model\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        vectorizer_name = ''.join(model.name_or_path.split('-')).upper()\n",
    "        classifier_name = model.__class__.__name__\n",
    "        output_dir = training_args_dict['output_dir'] = training_args_dict_for_best_trial['output_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).model'\n",
    "        log_dir = training_args_dict['logging_dir'] = training_args_dict_for_best_trial['logging_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).log'\n",
    "\n",
    "        # Encode data\n",
    "        (\n",
    "            X_train_encodings, train_dataset,\n",
    "            X_test_encodings, test_dataset,\n",
    "            X_val_encodings, val_dataset,\n",
    "        ) = encode_data(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            tokenizer,\n",
    "        )\n",
    "\n",
    "        if f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list:\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            continue\n",
    "\n",
    "        # Accelerate model\n",
    "        (\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        ) = accelerator.prepare(\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        )\n",
    "        # model.eval()\n",
    "\n",
    "        # Initialize Trainer\n",
    "        print('-'*20)\n",
    "        print('='*30)\n",
    "        print(f'{\"=\"*30} Initializing Trainer using {vectorizer_name} + {classifier_name} {\"=\"*30}')\n",
    "        print('+'*30)\n",
    "\n",
    "        # Make trainer with fine-tuning arguments\n",
    "        print('-'*20)\n",
    "        print('Passing data and arguments to Trainer.')\n",
    "        estimator = Trainer(\n",
    "            # model_init=model_init,\n",
    "            # args=TrainingArguments(**training_args_dict_for_best_trial),\n",
    "            model=model,\n",
    "            args=TrainingArguments(**training_args_dict),\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "            # data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "        )\n",
    "        if estimator.place_model_on_device:\n",
    "            estimator.model.to(device)\n",
    "\n",
    "        # # Hyperparameter search\n",
    "        # print('-'*20)\n",
    "        # print(f'Starting hyperparameter search for {col}.')\n",
    "        # best_trial = estimator.hyperparameter_search(\n",
    "        #     direction='maximize',\n",
    "        #     backend='optuna',\n",
    "        #     n_trials=10,\n",
    "        #     hp_space=optuna_hp_space,\n",
    "        #     sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        #     pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "        #     compute_objective=compute_objective,\n",
    "        #     n_jobs=n_jobs,\n",
    "        # )\n",
    "        # estimator.save_state()\n",
    "        # estimator.save_metrics('all', metrics_dict)\n",
    "        # estimator.save_model(output_dir)\n",
    "        # accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "        # print('Done hyperparameter search!')\n",
    "        # print('-'*20)\n",
    "\n",
    "        # Train trainer\n",
    "        print('-'*20)\n",
    "        print(f'Starting training for {col} using {classifier_name}.')\n",
    "        estimator.train()#trial=best_trial)\n",
    "        estimator.save_state()\n",
    "        estimator.save_metrics('all', metrics_dict)\n",
    "        estimator.save_model(output_dir)\n",
    "        accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # Evaluate\n",
    "        print('-'*20)\n",
    "        print(f'Evaluating estimator for {col}.')\n",
    "        eval_metrics_dict = estimator.evaluate()\n",
    "        y_val_pred = eval_metrics_dict.pop('eval_y_pred')\n",
    "        y_val_pred_prob = eval_metrics_dict.pop('eval_y_pred_prob')\n",
    "        eval_metrics_dict = clean_metrics_dict(eval_metrics_dict, list(eval_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done evaluating!')\n",
    "\n",
    "        # Get predictions\n",
    "        print(f'Getting prediction results for {col}.')\n",
    "        y_test_pred_logits, y_test_labels, test_metrics_dict = estimator.predict(test_dataset)\n",
    "        y_test_pred = test_metrics_dict.pop('test_y_pred')\n",
    "        y_test_pred_prob = test_metrics_dict.pop('test_y_pred_prob')\n",
    "        test_metrics_dict = clean_metrics_dict(test_metrics_dict, list(test_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done predicting!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # Save model\n",
    "        print('-'*20)\n",
    "        print(f'Saving model for {col}.')\n",
    "        save_Xy_estimator(\n",
    "            X_train, y_train, train_dataset,\n",
    "            X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "            X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "            estimator, accelerator, eval_metrics_dict, test_metrics_dict,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "# Assert that all classifiers were used\n",
    "assert_all_classifiers_used(classifiers_pipe=transformers_pipe)\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
