{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7208bf52b94139ab2419d8546a85d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acfbe17d05e4b4f9a4a27d0449f17ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0678ec2ef21b4f549669bb4c41df81f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Transformer variables\n",
    "method = 'Transformers'\n",
    "results_save_path = f'{models_save_path}{method} Results/'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'w') as f:\n",
    "    f.write(results_save_path)\n",
    "if not os.path.exists(results_save_path):\n",
    "    os.makedirs(results_save_path)\n",
    "done_xy_save_path = f'{results_save_path}Search+Xy/'\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'w') as f:\n",
    "    f.write(done_xy_save_path)\n",
    "if not os.path.exists(done_xy_save_path):\n",
    "    os.makedirs(done_xy_save_path)\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    f'Mean Cross Validation Train - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Cross Validation Test - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "torch.Generator(device_name).manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "accelerator = Accelerator()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ.get('TOKENIZERS_PARALLELISM')\n",
    "best_trial_args = [\n",
    "    'num_train_epochs', 'learning_rate', 'weight_decay', 'warmup_steps',\n",
    "]\n",
    "training_args_dict = {\n",
    "    'seed': random_state,\n",
    "    'resume_from_checkpoint': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'logging_steps': 500,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'eval_steps': 500,\n",
    "    'save_strategy': 'steps',\n",
    "    'save_steps': 500,\n",
    "    # 'metric_for_best_model': 'recall',\n",
    "    # 'torch_compile': bool(transformers.file_utils.is_torch_available()),\n",
    "    'use_mps_device': bool(device_name == 'mps' and torch.backends.mps.is_available()),\n",
    "    'optim': 'adamw_torch',\n",
    "    'load_best_model_at_end': True,\n",
    "    # The below metrics are used by hyperparameter search\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 5e-5,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 20,\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_steps': 100,\n",
    "}\n",
    "training_args_dict_for_best_trial = {\n",
    "    arg_name: arg_\n",
    "    for arg_name, arg_ in training_args_dict.items()\n",
    "    if arg_name not in best_trial_args\n",
    "}\n",
    "\n",
    "# Plotting variables\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "tqdm.tqdm.pandas(desc='progress-bar')\n",
    "tqdm_auto.tqdm.pandas(desc='progress-bar')\n",
    "# tqdm.notebook.tqdm().pandas(desc='progress-bar')\n",
    "tqdm_auto.notebook_tqdm().pandas(desc='progress-bar')\n",
    "# pbar = progressbar.ProgressBar(maxval=10)\n",
    "mpl.style.use(f'{code_dir}/setup_module/apa.mplstyle-main/apa.mplstyle')\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "font = {'family': 'arial', 'weight': 'normal', 'size': 10}\n",
    "mpl.rc('font', **font)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "plt.set_cmap('Blues')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc383",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28d7b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_files(\n",
    "    results_save_path= results_save_path,\n",
    "    estimator_names_list=None,\n",
    "    vectorizer_names_list=None,\n",
    "    classifier_names_list=None,\n",
    "):\n",
    "    if estimator_names_list is None:\n",
    "        estimator_names_list = []\n",
    "\n",
    "    print(f'Searching for existing estimators in directory:\\n{results_save_path}')\n",
    "\n",
    "    for estimators_file in tqdm.tqdm(glob.glob(f'{results_save_path}*.*')):\n",
    "        if f'{method} Estimator - ' in estimators_file:\n",
    "\n",
    "            col=estimators_file.split(f'{method} Estimator - ')[-1].split(' - ')[0]\n",
    "            vectorizer_name=estimators_file.split(f'{col} - ')[-1].split(' + ')[0]\n",
    "            classifier_name=estimators_file.split(f'{vectorizer_name} + ')[-1].split(' (Save_protocol=')[0]\n",
    "\n",
    "            estimator_names_list.append(f'{col} - {vectorizer_name} + {classifier_name}')\n",
    "\n",
    "    return (\n",
    "        list(set(estimator_names_list))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e70db7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy and CV data in df and save\n",
    "def save_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    col,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    compression=None, protocol=None, path_suffix=None, data_dict=None\n",
    "):\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "\n",
    "    # Check data\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    # Make df_train_data\n",
    "    df_train_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_test_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    df_val_data = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Assign dfs to variables\n",
    "    data_dict['df_train_data'] = df_train_data\n",
    "    data_dict['df_test_data'] = df_test_data\n",
    "    data_dict['df_val_data'] = df_val_data\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = f'{models_save_path}{file_name}{path_suffix}'\n",
    "        print(f'Saving Xy {file_name} at {save_path}')\n",
    "        file_.to_pickle(\n",
    "            save_path, protocol=protocol\n",
    "        )\n",
    "    print(f'Done saving Xy!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef9e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "):\n",
    "    # Get train class weights\n",
    "    train_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_train)\n",
    "    train_class_weights_ratio = train_class_weights[0]/train_class_weights[1]\n",
    "    train_class_weights_dict = dict(zip(np.unique(y_train), train_class_weights))\n",
    "\n",
    "    # Get train class weights\n",
    "    test_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_test)\n",
    "    test_class_weights_ratio = test_class_weights[0]/test_class_weights[1]\n",
    "    test_class_weights_dict = dict(zip(np.unique(y_test), test_class_weights))\n",
    "\n",
    "    # Get val class weights\n",
    "    val_class_weights = compute_class_weight(class_weight = class_weight, classes = np.unique(y_train), y = y_val)\n",
    "    val_class_weights_ratio = val_class_weights[0]/val_class_weights[1]\n",
    "    val_class_weights_dict = dict(zip(np.unique(y_val), val_class_weights))\n",
    "\n",
    "    return (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ad68773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "    test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "    val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train)\n",
    "    check_consistent_length(X_test, y_test)\n",
    "    check_consistent_length(X_val, y_val)\n",
    "\n",
    "    print('Done splitting data into training and testing sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set shape: {y_train.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Training set example:\\n{X_train[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set shape: {y_test.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Testing set example:\\n{X_test[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set shape: {y_val.shape}')\n",
    "    print('-'*10)\n",
    "    print(f'Validation set example:\\n{X_val[0]}')\n",
    "    print('~'*10)\n",
    "    print(f'Training data class weights:\\nRatio = {train_class_weights_ratio:.2f} (0 = {train_class_weights[0]:.2f}, 1 = {train_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Testing data class weights:\\nRatio = {test_class_weights_ratio:.2f} (0 = {test_class_weights[0]:.2f}, 1 = {test_class_weights[1]:.2f})')\n",
    "    print('-'*10)\n",
    "    print(f'Validation data class weights:\\nRatio = {val_class_weights_ratio:.2f} (0 = {val_class_weights[0]:.2f}, 1 = {val_class_weights[1]:.2f})')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395dffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, col, analysis_columns=analysis_columns, text_col=text_col):\n",
    "\n",
    "    train_ratio = 0.75\n",
    "    test_ratio = 0.10\n",
    "    validation_ratio = 0.15\n",
    "    test_split = test_size = 1 - train_ratio\n",
    "    validation_split = test_ratio / (test_ratio + validation_ratio)\n",
    "\n",
    "    # Split\n",
    "    print('='*20)\n",
    "    print('Splitting data into training, testing, and validation sets:')\n",
    "    print(f'Ratios: train_size = {train_ratio}, test size = {test_ratio}, validation size = {validation_ratio}')\n",
    "\n",
    "    df = df.dropna(subset=analysis_columns, how='any')\n",
    "    df = df.loc[df[text_col].apply(len) >= 5]\n",
    "    print(f'DF length: {len(df)}')\n",
    "\n",
    "    train, test = train_test_split(\n",
    "        df, train_size=1-test_split, test_size=test_split, random_state=random_state\n",
    "    )\n",
    "    val, test = train_test_split(\n",
    "        test, test_size=validation_split, random_state=random_state\n",
    "    )\n",
    "\n",
    "    X_train = np.array(list(train[text_col].astype('str').values))\n",
    "    y_train = column_or_1d(train[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_test = np.array(list(test[text_col].astype('str').values))\n",
    "    y_test = column_or_1d(test[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    X_val = np.array(list(val[text_col].astype('str').values))\n",
    "    y_val = column_or_1d(val[col].astype('int64').values.tolist(), warn=True)\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train, X_train, y_train,\n",
    "        test, X_test, y_test,\n",
    "        val, X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f07a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], device=device).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Xy_encodings(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, test_dataset,\n",
    "    X_val, y_val, val_dataset,\n",
    "):\n",
    "    # Check for consistent length\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, val_dataset)\n",
    "\n",
    "    # Check encodings\n",
    "    assert all(y_train == train_dataset.labels), 'y_train and train_dataset labels are not the same'\n",
    "    assert all(y_test == test_dataset.labels), 'y_test and test_dataset labels are not the same'\n",
    "\n",
    "    print('Done encoding training, testing, and validation sets.')\n",
    "    print('='*20)\n",
    "    print(f'Training set encodings example:\\n{\" \".join(train_dataset.encodings[0].tokens[:30])}')\n",
    "    print('~'*10)\n",
    "    print(f'Testing set encodings example:\\n{\" \".join(test_dataset.encodings[0].tokens[:30])}')\n",
    "    print('~'*10)\n",
    "    print(f'Validation set encodings example:\\n{\" \".join(val_dataset.encodings[0].tokens[:30])}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3840ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    X_val, y_val,\n",
    "    tokenizer,\n",
    "):\n",
    "    print('='*20)\n",
    "    print(f'Encoding training, testing, and validation sets with {f\"{tokenizer=}\".split(\"=\")[0]}.from_pretrained using {tokenizer.name_or_path}.')\n",
    "\n",
    "    X_train_encodings = tokenizer(\n",
    "        X_train.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    train_dataset = ToDataset(X_train_encodings, y_train)\n",
    "\n",
    "    X_test_encodings = tokenizer(\n",
    "        X_test.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    test_dataset = ToDataset(X_test_encodings, y_test)\n",
    "\n",
    "    X_val_encodings = tokenizer(\n",
    "        X_val.tolist(), truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    val_dataset = ToDataset(X_val_encodings, y_val)\n",
    "\n",
    "    # Print info\n",
    "    print_Xy_encodings(\n",
    "        X_train, y_train, train_dataset,\n",
    "        X_test, y_test, test_dataset,\n",
    "        X_val, y_val, val_dataset,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_encodings, train_dataset,\n",
    "        X_test_encodings, test_dataset,\n",
    "        X_val_encodings, val_dataset,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ddcc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Xy(\n",
    "    col,\n",
    "    results_save_path=results_save_path, method=method,\n",
    "    data_dict=None, protocol=None, path_suffix=None, \n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    print('+'*30)\n",
    "    print(f'{\"=\"*10} Loading Xy from previous for {col} {\"=\"*10}')\n",
    "    print('+'*30)\n",
    "    # Read all dfs\n",
    "    for file_path in glob.glob(f'{models_save_path}*{path_suffix}'):\n",
    "        file_name = file_path.split(f'{models_save_path}')[-1].split(path_suffix)[0]\n",
    "        print(f'Loading {file_name} from {file_path}')\n",
    "        if path_suffix in file_path and 'df_' in file_name and 'cv_results' not in file_name:\n",
    "            data_dict[file_name] = pd.read_pickle(file_path)\n",
    "\n",
    "    # Train data\n",
    "    df_train_data = data_dict['df_train_data']\n",
    "    X_train = df_train_data['X_train'].values\n",
    "    y_train = df_train_data['y_train'].values\n",
    "    # Test data\n",
    "    df_test_data = data_dict['df_test_data']\n",
    "    X_test = df_test_data['X_test'].values\n",
    "    y_test = df_test_data['y_test'].values\n",
    "    # Val data\n",
    "    df_val_data = data_dict['df_val_data']\n",
    "    X_val = df_val_data['X_val'].values\n",
    "    y_val = df_val_data['y_val'].values\n",
    "\n",
    "    print(f'Done loading Xy from previous for {col}!')\n",
    "\n",
    "    # Get class weights\n",
    "    (\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    ) = get_class_weights(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "    )\n",
    "\n",
    "    # Print info\n",
    "    print_Xy(\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        X_val, y_val,\n",
    "        train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "        test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "        val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b78bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna hyperparameter tuning\n",
    "# https://huggingface.co/docs/transformers/v4.27.2/en/hpo_train#hyperparameter-search-using-trainer-api\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        'num_train_epochs': trial.suggest_int('num_train_epochs', 1, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
    "        'per_device_train_batch_size': trial.suggest_categorical('per_device_train_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'per_device_eval_batch_size': trial.suggest_categorical('per_device_eval_batch_size', [4, 8, 16, 32, 64, 128]),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-12, 1e-1, log=True),\n",
    "        'warmup_steps': trial.suggest_int('warmup_steps', 0, 500),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c7abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute objective for hyperparameter tuning\n",
    "# https://github.com/huggingface/transformers/issues/13019\n",
    "def compute_objective(metrics_dict):\n",
    "    return metrics_dict['recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f96c4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImbTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = self._calculate_class_weights(self.train_dataset)\n",
    "\n",
    "    def _calculate_class_weights(self, dataset):\n",
    "        # Count the number of samples in each class\n",
    "        class_counts = torch.zeros(self.model.config.num_labels)\n",
    "        for label in dataset.labels:\n",
    "            class_counts[label] += 1\n",
    "\n",
    "        # Calculate the inverse frequency of each class\n",
    "        inv_frequencies = 1 / class_counts\n",
    "\n",
    "        # Normalize the inverse frequencies so that they sum up to 1\n",
    "        sum_inv_frequencies = torch.sum(inv_frequencies)\n",
    "        return inv_frequencies / sum_inv_frequencies\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(device))\n",
    "        loss = loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7c888f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred(\n",
    "    y_labels, y_pred,\n",
    "    pos_label=None, labels=None, zero_division=None, alpha=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_pred)\n",
    "    if zero_division is None:\n",
    "        zero_division = 0\n",
    "    if alpha is None:\n",
    "        alpha = 0.1\n",
    "\n",
    "    print('Computing metrics using y_pred.')\n",
    "    # Using y_pred\n",
    "    explained_variance = metrics.explained_variance_score(y_labels, y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_labels, y_pred)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(y_labels, y_pred)\n",
    "    precision = metrics.precision_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    recall = metrics.recall_score(y_labels, y_pred, pos_label=pos_label, labels=labels, zero_division=zero_division)\n",
    "    f1 = metrics.f1_score(y_labels, y_pred, pos_label=pos_label,labels=labels, zero_division=zero_division)\n",
    "    mcc = metrics.matthews_corrcoef(y_labels, y_pred)\n",
    "    fm = metrics.fowlkes_mallows_score(y_labels, y_pred)\n",
    "    r2 = metrics.r2_score(y_labels, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y_labels, y_pred, labels=labels)\n",
    "    gmean_iba = imblearn.metrics.make_index_balanced_accuracy(alpha=alpha, squared=True)(geometric_mean_score)\n",
    "    gmean = gmean_iba(y_labels, y_pred)\n",
    "    report = metrics.classification_report(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    imblearn_report = classification_report_imbalanced(y_labels, y_pred, labels=labels, zero_division=zero_division)\n",
    "    cm = metrics.confusion_matrix(y_labels, y_pred, labels=labels)\n",
    "    cm_normalized = metrics.confusion_matrix(y_labels, y_pred, normalize='true', labels=labels)\n",
    "\n",
    "    return (\n",
    "        explained_variance, accuracy, balanced_accuracy, precision,\n",
    "        recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e2dcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_with_y_pred_prob(\n",
    "    y_labels, y_pred_prob,\n",
    "    pos_label=None\n",
    "):\n",
    "    if pos_label is None:\n",
    "        pos_label = 1\n",
    "\n",
    "    print('Computing metrics using y_pred_prob.')\n",
    "    average_precision = metrics.average_precision_score(y_labels, y_pred_prob)\n",
    "    roc_auc = metrics.roc_auc_score(y_labels, y_pred_prob)\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    loss = metrics.log_loss(y_labels, y_pred_prob)\n",
    "    precision_pr, recall_pr, threshold_pr = metrics.precision_recall_curve(y_labels, y_pred_prob, pos_label=1)\n",
    "\n",
    "    return (\n",
    "        average_precision, roc_auc, auc,\n",
    "        fpr, tpr, threshold, loss,\n",
    "        precision_pr, recall_pr, threshold_pr\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea5ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all(\n",
    "    y_labels, y_pred, y_pred_prob\n",
    "):\n",
    "    # Get metrics\n",
    "    print('='*20)\n",
    "    # Using y_pred\n",
    "    if y_pred:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            explained_variance, accuracy, balanced_accuracy, precision,\n",
    "            recall, f1, mcc, fm, r2, kappa, gmean, report, imblearn_report, cm, cm_normalized\n",
    "        ) = compute_metrics_with_y_pred(\n",
    "            y_labels, y_pred\n",
    "        )\n",
    "    # Using y_pred_prob\n",
    "    if y_pred_prob:\n",
    "        print('-'*20)\n",
    "        (\n",
    "            average_precision, roc_auc, auc,\n",
    "            fpr, tpr, threshold, loss,\n",
    "            precision_pr, recall_pr, threshold_pr\n",
    "        ) = compute_metrics_with_y_pred_prob(\n",
    "            y_labels, y_pred_prob\n",
    "        )\n",
    "\n",
    "    # Place metrics into dict\n",
    "    print('-'*20)\n",
    "    print('Appending metrics to dict.')\n",
    "    metrics_dict = {\n",
    "        # f'{scoring.title()} Best Score': float(best_train_score),\n",
    "        # f'{scoring.title()} Best Threshold': threshold,\n",
    "        # 'Train - Mean Cross Validation Score': float(cv_train_scores),\n",
    "        # f'Train - Mean Cross Validation - {scoring.title()}': float(cv_train_recall),\n",
    "        # f'Train - Mean Explained Variance - {scoring.title()}': float(cv_train_explained_variance_recall),\n",
    "        # 'Test - Mean Cross Validation Score': float(cv_test_scores),\n",
    "        # f'Test - Mean Cross Validation - {scoring.title()}': float(cv_test_recall),\n",
    "        # f'Test - Mean Explained Variance - {scoring.title()}': float(cv_test_explained_variance_recall),\n",
    "        'Explained Variance': float(explained_variance),\n",
    "        'Accuracy': float(accuracy),\n",
    "        'Balanced Accuracy': float(balanced_accuracy),\n",
    "        'Precision': float(precision),\n",
    "        'Average Precision': float(average_precision),\n",
    "        'Recall': float(recall),\n",
    "        'F1-score': float(f1),\n",
    "        'Matthews Correlation Coefficient': float(mcc),\n",
    "        'Fowlkes–Mallows Index': float(fm),\n",
    "        'R2 Score': float(r2),\n",
    "        'ROC': float(roc_auc),\n",
    "        'AUC': float(auc),\n",
    "        'Log Loss/Cross Entropy': float(loss),\n",
    "        'Cohen’s Kappa': float(kappa),\n",
    "        'Geometric Mean': float(gmean),\n",
    "        'Classification Report': report,\n",
    "        'Imbalanced Classification Report': str(imblearn_report),\n",
    "        'Confusion Matrix': str(cm),\n",
    "        'Normalized Confusion Matrix': str(cm_normalized),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_prob': y_pred_prob,\n",
    "    }\n",
    "    print('Done appending metrics to dict.')\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28661d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_metrics_dict(metrics_dict, prefix_to_remove):\n",
    "    for metric_name in list(metrics_dict):\n",
    "        if metric_name.startswith(prefix_to_remove):\n",
    "            new_metric_name = ' '.join(metric_name.split(prefix_to_remove)[-1].split('_')).strip()\n",
    "        if not new_metric_name[0].isupper():\n",
    "            new_metric_name = new_metric_name.title()\n",
    "        if new_metric_name == 'Loss':\n",
    "            metrics_dict['Log Loss/Cross Entropy'] = metrics_dict.pop(metric_name)\n",
    "        else:\n",
    "            metrics_dict[new_metric_name] = metrics_dict.pop(metric_name)\n",
    "\n",
    "    return metrics_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f77d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred_prob(y_pred_logits, y_labels):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'Preprocessing y_pred logits and labels for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_logits, tuple):\n",
    "        y_pred_logits = y_pred_logits[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_logits):\n",
    "        y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    else:\n",
    "        y_pred_logits_tensor = y_pred_logits.to(device)\n",
    "\n",
    "    print(f'y_pred_logits shape: {y_pred_logits_tensor.shape}, {y_pred_logits_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    # https://stackoverflow.com/questions/34240703/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits.')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_array.shape}, {y_pred_prob_array.dtype}')\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return torch.tensor(y_pred_prob_array, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "935f15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_y_pred(y_pred_prob_array):\n",
    "\n",
    "    # https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred from y_pred_prob for {col}:')\n",
    "    print('-'*20)\n",
    "\n",
    "    if isinstance(y_pred_prob_array, tuple):\n",
    "        y_pred_prob_array = y_pred_prob_array[0]\n",
    "\n",
    "    if not torch.is_tensor(y_pred_prob_array):\n",
    "        y_pred_prob_tensor = torch.tensor(y_pred_prob_array, device=device)\n",
    "    else:\n",
    "        y_pred_prob_tensor = y_pred_prob_array.to(device)\n",
    "\n",
    "    print(f'y_pred_prob_array shape: {y_pred_prob_tensor.shape}. {y_pred_prob_tensor.dtype}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred through argmax of y_pred_prob.')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_prob_tensor, axis=-1).cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_prob.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "\n",
    "    return y_pred_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d6916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    y_pred_prob_array, y_labels_array = predicted_results_from_eval\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    (\n",
    "        y_pred_array\n",
    "    ) = preprocess_logits_for_metrics_y_pred(y_pred_prob_array)\n",
    "\n",
    "    # Get the the whole of the last column, which is the  probability of 1, and flatten to list\n",
    "    print('-'*20)\n",
    "    print('Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.')\n",
    "    y_labels = y_labels_array.flatten().tolist()\n",
    "    y_pred = y_pred_array.flatten().tolist()\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred_prob length: {len(y_pred_prob)}')\n",
    "    print(f'y_labels length: {len(y_labels)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a625b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get y_pred and y_pred_prob\n",
    "def preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits):\n",
    "\n",
    "    # Get y_pred\n",
    "    print('-'*20)\n",
    "    y_pred_logits_tensor = torch.tensor(y_pred_logits, device=device)\n",
    "    print('Getting y_pred through argmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_array = torch.argmax(y_pred_logits_tensor, axis=-1).cpu().numpy()\n",
    "        print('Using torch.argmax.')\n",
    "    except Exception:\n",
    "        y_pred_array = y_pred_logits.argmax(axis=-1)\n",
    "        print('Using np.argmax.')\n",
    "    print(f'y_pred_array shape: {y_pred_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred...')\n",
    "    y_pred = [bert_label2id[l] for l in y_pred_array.flatten().tolist()]\n",
    "    print(f'y_pred length: {len(y_pred)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_pred_prob\n",
    "    print('-'*20)\n",
    "    print('Getting y_pred_prob through softmax of y_pred_logits...')\n",
    "    try:\n",
    "        y_pred_prob_array = torch.nn.functional.softmax(y_pred_logits_tensor, dim=-1).cpu().numpy()\n",
    "        print('Using torch.nn.functional.softmax.')\n",
    "    except Exception:\n",
    "        y_pred_prob_array = scipy.special.softmax(y_pred_logits, axis=-1)\n",
    "        print('Using scipy.special.softmax.')\n",
    "    # from: https://discuss.huggingface.co/t/different-results-predicting-from-trainer-and-model/12922\n",
    "    assert all(y_pred_prob_array.argmax(axis=-1) == y_pred_array), 'Argmax of y_pred_prob_array does not match y_pred_array.'\n",
    "    print(f'y_pred_prob shape: {y_pred_prob_array.shape}')\n",
    "    print('-'*20)\n",
    "    print('Flattening y_pred_prob and extracting probabilities of 1...')\n",
    "    y_pred_prob = y_pred_prob_array[:, -1].flatten().tolist()\n",
    "    print(f'y_pred length: {len(y_pred_prob)}')\n",
    "    print('-'*20)\n",
    "\n",
    "    y_pred_logits_tensor.detach()\n",
    "\n",
    "    return (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ab4fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_from_logits(\n",
    "    predicted_results_from_eval,\n",
    "):\n",
    "    # Get predictions\n",
    "    print('-'*20)\n",
    "    print(f'Getting y_pred logits and ids for {col}:')\n",
    "    y_pred_logits, y_labels = predicted_results_from_eval\n",
    "    print(f'y_pred_logits shape: {y_pred_logits.shape}')\n",
    "    print(f'y shape: {y_labels.shape}')\n",
    "    print('-'*20)\n",
    "\n",
    "    # Get y_test_pred and y_test_pred_prob\n",
    "    (\n",
    "        y_pred_array, y_pred, y_pred_prob_array, y_pred_prob\n",
    "    ) = preprocess_logits_for_metrics_in_compute_metrics(y_pred_logits)\n",
    "\n",
    "    return compute_metrics_all(y_labels, y_pred, y_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c44994c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(\n",
    "    metrics_dict, df_metrics,\n",
    "    col, vectorizer_name, classifier_name\n",
    "):\n",
    "    # Print metrics\n",
    "    print('=' * 20)\n",
    "    print('~' * 20)\n",
    "    print(' Metrics:')\n",
    "    print('~' * 20)\n",
    "    print(f'Classification Report:\\n {metrics_dict[\"Classification Report\"]}')\n",
    "    print('-' * 20)\n",
    "    for metric_name, metric_value in metrics_dict.items():\n",
    "        if metric_name not in ['Runtime', 'Samples Per Second', 'Steps Per Second']:\n",
    "            with contextlib.suppress(TypeError, ValueError):\n",
    "                metric_value = float(metric_value)\n",
    "            if isinstance(metric_name, (int, float)):\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = metric_value\n",
    "                print(f'{metric_name}: {round(metric_value, 2)}')\n",
    "            else:\n",
    "                df_metrics.loc[\n",
    "                    (classifier_name), (col, vectorizer_name, metric_name)\n",
    "                ] = str(metric_value)\n",
    "                print(f'{metric_name}: {metric_value}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    print('=' * 20)\n",
    "\n",
    "    return df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93960eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to place Xy data in df and save\n",
    "def save_Xy_estimator(\n",
    "    X_train, y_train, train_dataset,\n",
    "    X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "    X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "    estimator, accelerator, eval_metrics_dict, test_metrics_dict,\n",
    "    col, vectorizer_name, classifier_name,\n",
    "    results_save_path=results_save_path,\n",
    "    method=method, done_xy_save_path=done_xy_save_path,\n",
    "    path_suffix=None, data_dict=None,\n",
    "    compression=None, protocol=None,\n",
    "):\n",
    "    if data_dict is None:\n",
    "        data_dict = {}\n",
    "    if compression is None:\n",
    "        compression = False\n",
    "    if protocol is None:\n",
    "        protocol = pickle.HIGHEST_PROTOCOL\n",
    "    if path_suffix is None:\n",
    "        path_suffix = f' - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={protocol}).pkl'\n",
    "\n",
    "    # Check predicted data\n",
    "    check_consistent_length(X_train, y_train, train_dataset)\n",
    "    check_consistent_length(X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset)\n",
    "    check_consistent_length(X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset)\n",
    "\n",
    "    # Make data dict\n",
    "    data_dict['Estimator'] = estimator\n",
    "    data_dict['accelerator'] = accelerator\n",
    "    data_dict['eval_metrics_dict'] = eval_metrics_dict\n",
    "    data_dict['test_metrics_dict'] = test_metrics_dict\n",
    "\n",
    "    # Make df_train_data\n",
    "    data_dict['df_train_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'train_dataset': train_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_test_data\n",
    "    data_dict['df_test_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test,\n",
    "            'y_test_pred': y_test_pred,\n",
    "            'y_test_pred_prob': y_test_pred_prob,\n",
    "            'test_dataset': test_dataset,\n",
    "        },\n",
    "    )\n",
    "    # Make df_val_data\n",
    "    data_dict['df_val_data'] = pd.DataFrame(\n",
    "        {\n",
    "            'X_val': X_val,\n",
    "            'y_val': y_val,\n",
    "            'y_val_pred': y_val_pred,\n",
    "            'y_val_pred_prob': y_val_pred_prob,\n",
    "            'val_dataset': val_dataset,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save files\n",
    "    print('='*20)\n",
    "    saved_files_list = []\n",
    "    for file_name, file_ in data_dict.items():\n",
    "        save_path = (\n",
    "            done_xy_save_path\n",
    "            if file_name not in ['Estimator', 'accelerator']\n",
    "            else results_save_path\n",
    "        )\n",
    "        print(f'Saving {file_name} at {save_path}')\n",
    "        if not isinstance(file_, pd.DataFrame) and file_name == 'Estimator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            # Save as .model\n",
    "            file_.save_model(f'{save_path}{method} {file_name}{path_suffix.replace(\"pkl\", \"model\")}')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif not isinstance(file_, pd.DataFrame) and file_name == 'accelerator' and 'df_' not in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.save(estimator.state, f'{save_path}{method} Estimator{path_suffix.replace(\"pkl\", \"model\")}/accelerator')\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, dict) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' not in file_name and 'metrics_dict' in file_name:\n",
    "            with open(f'{save_path}{method} {file_name}{path_suffix}', 'wb') as f:\n",
    "                pickle.dump(file_, f, protocol=protocol)\n",
    "            saved_files_list.append(file_name)\n",
    "        elif isinstance(file_, pd.DataFrame) and file_name != 'Estimator' and file_name != 'accelerator' and 'df_' in file_name and 'metrics_dict' not in file_name:\n",
    "            file_.to_pickle(\n",
    "                f'{save_path}{method} {file_name}{path_suffix}', protocol=protocol\n",
    "            )\n",
    "            saved_files_list.append(file_name)\n",
    "\n",
    "    assert set(data_dict.keys()) == set(saved_files_list), f'Not all files were saved! Missing: {set(data_dict.keys()) ^ set(saved_files_list)}'\n",
    "    print(f'Done saving Xy, labels and estimator!\\n{list(data_dict.keys())}')\n",
    "    print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fb3f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that all classifiers were used\n",
    "def assert_all_classifiers_used(\n",
    "    estimators_list=None, used_classifiers=None, results_save_path=results_save_path, method=method, classifiers_pipe=transformers_pipe,\n",
    "):\n",
    "    if estimators_list is None:\n",
    "        estimators_list = []\n",
    "    if used_classifiers is None:\n",
    "        used_classifiers = []\n",
    "\n",
    "    for estimator_path in glob.glob(f'{results_save_path}{method} Estimator - *.*'):\n",
    "        if f'{method} Estimator - ' in estimator_path:\n",
    "            classifier_name = estimator_path.split(f'{results_save_path}{method} ')[1].split(' + ')[1].split(' (Save_protocol=')[0]\n",
    "            used_classifiers.append(classifier_name)\n",
    "\n",
    "    assert set(list(classifiers_pipe.keys())) == set(used_classifiers), f'Not all classifiers were used!\\nAvaliable Classifiers:\\n{set(list(classifiers_pipe.keys()))}\\nUsed Classifiers:\\n{set(used_classifiers)}\\nLeftout Classifiers:\\n{set(list(classifiers_pipe.keys())) ^ set(used_classifiers)}'\n",
    "    print('All classifiers were used!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f64ab3",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe8e7a",
   "metadata": {},
   "source": [
    "### READ DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7fbe29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded with shape: (5947, 68)\n"
     ]
    }
   ],
   "source": [
    "with open(f'{data_dir}df_manual_len.txt', 'r') as f:\n",
    "    df_manual_len = int(f.read())\n",
    "\n",
    "df_manual = pd.read_pickle(f'{df_save_dir}df_manual_for_training.pkl')\n",
    "assert len(df_manual) == df_manual_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_manual_len} BUT IS OF LENGTH {len(df_manual)}'\n",
    "print(f'Dataframe loaded with shape: {df_manual.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ea2f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Starting!\n",
      "########################################\n",
      "Searching for existing estimators in directory:\n",
      "/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 79137.81it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5947 ON WARMTH ==============================\n",
      "--------------------\n",
      "classifiers to be used (2):\n",
      "['BertForSequenceClassification', 'GPT2ForSequenceClassification']\n",
      "Loading previous Xy.\n",
      "++++++++++++++++++++++++++++++\n",
      "========== Loading Xy from previous for Warmth ==========\n",
      "++++++++++++++++++++++++++++++\n",
      "Loading df_train_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_train_data - Warmth - (Save_protocol=5).pkl\n",
      "Loading df_test_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_test_data - Warmth - (Save_protocol=5).pkl\n",
      "Loading df_val_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_val_data - Warmth - (Save_protocol=5).pkl\n",
      "Done loading Xy from previous for Warmth!\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "This internship is for you if:\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "General switchboard number +44 (0)207 801 3380.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "Effective ability to prioritize tasks and deliver on deadlines, with high performance standards and a commitment to excellence.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.37 (0 = 0.68, 1 = 1.86)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.39 (0 = 0.69, 1 = 1.79)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.40 (0 = 0.70, 1 = 1.76)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with tokenizer.from_pretrained using bert-base-uncased.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "[CLS] this internship is for you if : [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "[CLS] general switch ##board number + 44 ( 0 ) 207 80 ##1 338 ##0 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "[CLS] effective ability to prior ##iti ##ze tasks and deliver on deadline ##s , with high performance standards and a commitment to excellence . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "====================\n",
      "--------------------\n",
      "Already trained Warmth - BERTBASEUNCASED + BertForSequenceClassification\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with tokenizer.from_pretrained using gpt2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:38<00:00, 19.11s/it]\n",
      " 50%|█████     | 1/2 [00:38<00:38, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "This Ġinternship Ġis Ġfor Ġyou Ġif : <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "General Ġswitch board Ġnumber Ġ+ 44 Ġ( 0 ) 207 Ġ8 01 Ġ33 80 . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "Effective Ġability Ġto Ġprioritize Ġtasks Ġand Ġdeliver Ġon Ġdeadlines , Ġwith Ġhigh Ġperformance Ġstandards Ġand Ġa Ġcommitment Ġto Ġexcellence . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "====================\n",
      "--------------------\n",
      "Already trained Warmth - GPT2 + GPT2ForSequenceClassification\n",
      "--------------------\n",
      "--------------------\n",
      "============================== TRAINING DATASET OF LENGTH 5947 ON COMPETENCE ==============================\n",
      "--------------------\n",
      "classifiers to be used (2):\n",
      "['BertForSequenceClassification', 'GPT2ForSequenceClassification']\n",
      "Loading previous Xy.\n",
      "++++++++++++++++++++++++++++++\n",
      "========== Loading Xy from previous for Competence ==========\n",
      "++++++++++++++++++++++++++++++\n",
      "Loading df_test_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_test_data - Competence - (Save_protocol=5).pkl\n",
      "Loading df_train_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_train_data - Competence - (Save_protocol=5).pkl\n",
      "Loading df_val_data from /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/df_val_data - Competence - (Save_protocol=5).pkl\n",
      "Done loading Xy from previous for Competence!\n",
      "Done splitting data into training and testing sets.\n",
      "====================\n",
      "Training set shape: (4446,)\n",
      "----------\n",
      "Training set example:\n",
      "This internship is for you if:\n",
      "~~~~~~~~~~\n",
      "Testing set shape: (593,)\n",
      "----------\n",
      "Testing set example:\n",
      "General switchboard number +44 (0)207 801 3380.\n",
      "~~~~~~~~~~\n",
      "Validation set shape: (889,)\n",
      "----------\n",
      "Validation set example:\n",
      "Effective ability to prioritize tasks and deliver on deadlines, with high performance standards and a commitment to excellence.\n",
      "~~~~~~~~~~\n",
      "Training data class weights:\n",
      "Ratio = 0.88 (0 = 0.94, 1 = 1.07)\n",
      "----------\n",
      "Testing data class weights:\n",
      "Ratio = 0.86 (0 = 0.93, 1 = 1.08)\n",
      "----------\n",
      "Validation data class weights:\n",
      "Ratio = 0.85 (0 = 0.92, 1 = 1.09)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with tokenizer.from_pretrained using bert-base-uncased.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "[CLS] this internship is for you if : [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "[CLS] general switch ##board number + 44 ( 0 ) 207 80 ##1 338 ##0 . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "[CLS] effective ability to prior ##iti ##ze tasks and deliver on deadline ##s , with high performance standards and a commitment to excellence . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "====================\n",
      "--------------------\n",
      "Already trained Competence - BERTBASEUNCASED + BertForSequenceClassification\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Encoding training, testing, and validation sets with tokenizer.from_pretrained using gpt2.\n",
      "Done encoding training, testing, and validation sets.\n",
      "====================\n",
      "Training set encodings example:\n",
      "This Ġinternship Ġis Ġfor Ġyou Ġif : <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Testing set encodings example:\n",
      "General Ġswitch board Ġnumber Ġ+ 44 Ġ( 0 ) 207 Ġ8 01 Ġ33 80 . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "~~~~~~~~~~\n",
      "Validation set encodings example:\n",
      "Effective Ġability Ġto Ġprioritize Ġtasks Ġand Ġdeliver Ġon Ġdeadlines , Ġwith Ġhigh Ġperformance Ġstandards Ġand Ġa Ġcommitment Ġto Ġexcellence . <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|> <|endoftext|>\n",
      "====================\n",
      "--------------------\n",
      "==============================\n",
      "============================== Initializing Trainer using GPT2 + GPT2ForSequenceClassification ==============================\n",
      "++++++++++++++++++++++++++++++\n",
      "--------------------\n",
      "Passing data and arguments to Trainer.\n",
      "--------------------\n",
      "Starting training for Competence using GPT2ForSequenceClassification.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fe088f74ae43278fc2e34a73732aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/834 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_29345/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "                                             \n",
      " 50%|█████     | 1/2 [38:07<00:38, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6458, 'learning_rate': 2.2752043596730245e-05, 'epoch': 1.8}\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c035caa8688b45d196b37c745fac8ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      " 50%|█████     | 1/2 [38:47<00:38, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([9, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (9, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([889, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (889,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 889\n",
      "y_labels length: 889\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "{'eval_loss': 0.4245269000530243, 'eval_Explained Variance': 0.24461905344258272, 'eval_Accuracy': 0.7997750281214848, 'eval_Balanced Accuracy': 0.8077152378622967, 'eval_Precision': 0.7263779527559056, 'eval_Average Precision': 0.8568961335288551, 'eval_Recall': 0.9044117647058824, 'eval_F1-score': 0.8056768558951967, 'eval_Matthews Correlation Coefficient': 0.6197082447980353, 'eval_Fowlkes–Mallows Index': 0.6833324140201485, 'eval_R2 Score': 0.19366312013370823, 'eval_ROC': 0.8933339448045331, 'eval_AUC': 0.8933339448045331, 'eval_Log Loss/Cross Entropy': 0.43274431435199384, 'eval_Cohen’s Kappa': 0.6041931385006353, 'eval_Geometric Mean': 0.6430536871713344, 'eval_Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.90      0.71      0.79       481\\n           1       0.73      0.90      0.81       408\\n\\n    accuracy                           0.80       889\\n   macro avg       0.81      0.81      0.80       889\\nweighted avg       0.82      0.80      0.80       889\\n', 'eval_Imbalanced Classification Report': '                   pre       rec       spe        f1       geo       iba       sup\\n\\n          0       0.90      0.71      0.90      0.79      0.80      0.63       481\\n          1       0.73      0.90      0.71      0.81      0.80      0.66       408\\n\\navg / total       0.82      0.80      0.82      0.80      0.80      0.64       889\\n', 'eval_Confusion Matrix': '[[342 139]\\n [ 39 369]]', 'eval_Normalized Confusion Matrix': '[[0.71101871 0.28898129]\\n [0.09558824 0.90441176]]', 'eval_y_pred': [1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1], 'eval_y_pred_prob': [0.9715520739555359, 0.619491696357727, 0.002805474679917097, 0.9743720889091492, 0.9628811478614807, 0.0537320040166378, 0.00401957705616951, 0.032465144991874695, 0.04492189735174179, 0.8947418928146362, 0.0017404763493686914, 0.0018518116557970643, 0.8866287469863892, 0.002748437225818634, 0.9408586025238037, 0.9055467247962952, 0.9007279276847839, 0.8847755193710327, 0.8735749125480652, 0.10908448696136475, 0.33299705386161804, 0.3185553252696991, 0.116298146545887, 0.7949313521385193, 0.9309599995613098, 0.982018232345581, 0.011311187408864498, 0.0005023381672799587, 0.4385625720024109, 0.005271177738904953, 0.15048015117645264, 0.8584669828414917, 0.0026286691427230835, 0.5190749168395996, 0.0172442477196455, 0.11912613362073898, 0.010438107885420322, 0.975604236125946, 0.03467034548521042, 0.9635577201843262, 0.7265417575836182, 0.29075440764427185, 0.9163824319839478, 0.16252323985099792, 0.002032562857493758, 0.8058724403381348, 0.6479776501655579, 0.5267285108566284, 0.9606832265853882, 0.597888708114624, 0.8497104048728943, 0.4960636794567108, 0.9273912310600281, 0.44531047344207764, 0.9040186405181885, 0.8089420199394226, 0.0012773009948432446, 0.5941949486732483, 0.9250074625015259, 0.0017596610123291612, 0.8014530539512634, 0.27221307158470154, 0.8880688548088074, 0.8620595932006836, 0.042068447917699814, 0.9563980102539062, 0.714902937412262, 0.6930418014526367, 0.27648621797561646, 0.0008098106482066214, 0.7288575172424316, 0.9847394227981567, 0.6101434230804443, 0.8545913696289062, 0.8929276466369629, 0.9135463237762451, 0.49109792709350586, 0.37669339776039124, 0.4756189286708832, 0.8435414433479309, 0.09779042750597, 0.25351807475090027, 0.8970396518707275, 0.8748995065689087, 0.7582198977470398, 0.0384848415851593, 0.926298975944519, 0.010529682040214539, 0.48511016368865967, 0.32289254665374756, 0.9630683064460754, 0.9392564296722412, 0.4043663740158081, 0.8179682493209839, 0.6052160263061523, 0.9281718134880066, 0.30458545684814453, 0.5640286803245544, 0.9760840535163879, 0.8797200918197632, 0.5554298162460327, 0.8874419331550598, 0.8985171318054199, 0.9271250367164612, 0.8332408666610718, 0.04212388023734093, 0.060676783323287964, 0.017548587173223495, 0.35241201519966125, 0.6192037463188171, 0.9074692130088806, 0.1298573762178421, 0.8891619443893433, 0.43251025676727295, 0.9647431969642639, 0.019581714645028114, 0.7751631140708923, 0.13668276369571686, 0.9758407473564148, 0.7387202978134155, 0.9370231628417969, 0.7258985042572021, 0.0034491049591451883, 0.9028936624526978, 0.6834254264831543, 0.920874834060669, 0.028941716998815536, 0.052444037050008774, 0.2511756122112274, 0.9704803824424744, 0.9677976965904236, 0.7003635168075562, 0.8590667247772217, 0.012651861645281315, 0.10393905639648438, 0.010606850497424603, 0.04552415758371353, 0.6755102872848511, 0.9506392478942871, 0.6171357035636902, 0.6847859621047974, 0.9330989122390747, 0.9604784846305847, 0.04917134344577789, 0.9762442111968994, 0.9716898202896118, 0.9430097341537476, 0.9188061356544495, 0.611906886100769, 0.9165536761283875, 0.9766121506690979, 0.35856619477272034, 0.045397333800792694, 0.021095940843224525, 0.9880643486976624, 0.010102953761816025, 0.062163855880498886, 0.8940554857254028, 0.9847427606582642, 0.553597092628479, 0.35060715675354004, 0.20784133672714233, 0.5255151391029358, 0.9678125381469727, 0.9047497510910034, 0.9443395733833313, 0.001037804875522852, 0.2264399379491806, 0.02761264145374298, 0.275200754404068, 0.7080488801002502, 0.7003160119056702, 0.7656816840171814, 0.9583763480186462, 0.2782968282699585, 0.9122523069381714, 0.08714216947555542, 0.91715008020401, 0.6858444809913635, 0.07039164751768112, 0.01710476167500019, 0.0013645720900967717, 0.9751636981964111, 0.6702094674110413, 0.4736236333847046, 0.8833734393119812, 0.04131898656487465, 0.4576795995235443, 0.6280033588409424, 0.002018721541389823, 0.957391619682312, 0.059479668736457825, 0.7425240278244019, 0.9700760841369629, 0.9469791054725647, 0.8307278156280518, 0.0007953417371027172, 0.965898334980011, 0.2815970480442047, 0.1715719848871231, 0.7562745809555054, 0.35856619477272034, 0.6642898321151733, 0.3714796304702759, 0.104645736515522, 0.7704355716705322, 0.9769095778465271, 0.7180692553520203, 0.30306264758110046, 0.39832937717437744, 0.9305655360221863, 0.031947702169418335, 0.027946501970291138, 0.014996190555393696, 0.13875508308410645, 0.928877592086792, 0.6112427711486816, 0.7577296495437622, 0.8961201906204224, 0.01946185529232025, 0.47417470812797546, 0.9540989995002747, 0.08413022756576538, 0.8570408821105957, 0.03961830213665962, 0.031947702169418335, 0.8781678676605225, 0.6999318599700928, 0.8475834727287292, 0.4130123257637024, 0.011542970314621925, 0.03211810067296028, 0.44129884243011475, 0.9667141437530518, 0.001031053951010108, 0.9334316253662109, 0.02360665425658226, 0.8575595617294312, 0.6877313256263733, 0.006480494048446417, 0.775910496711731, 0.0793120339512825, 0.16757504642009735, 0.9186106324195862, 0.5920486450195312, 0.10426728427410126, 0.9026616215705872, 0.681410551071167, 0.036682114005088806, 0.8278594017028809, 0.7657870054244995, 0.1772981435060501, 0.06014031171798706, 0.7022900581359863, 0.002748437225818634, 0.5772178769111633, 0.00831394549459219, 0.7998314499855042, 0.6353229284286499, 0.8933575749397278, 0.6157981753349304, 0.6580690145492554, 0.5732552409172058, 0.008902152068912983, 0.048656657338142395, 0.9179965257644653, 0.06779659539461136, 0.9797958135604858, 0.2878972589969635, 0.9215443134307861, 0.8880688548088074, 0.000592424301430583, 0.45379137992858887, 0.0003116507432423532, 0.9555315971374512, 0.7284945249557495, 0.023311318829655647, 0.9765547513961792, 0.8364236354827881, 0.0014932515332475305, 0.058140724897384644, 0.8424307703971863, 0.0013697838876396418, 0.2876057028770447, 0.9017085433006287, 0.6571046113967896, 0.044146578758955, 0.12632161378860474, 0.9732057452201843, 0.6236461997032166, 0.1772981435060501, 0.10108151286840439, 0.5252578258514404, 0.24953556060791016, 0.180843785405159, 0.8557873368263245, 0.8382171392440796, 0.7610036134719849, 0.05522914230823517, 0.003192841075360775, 0.8777585029602051, 0.025369763374328613, 0.0038532561156898737, 0.9433444142341614, 0.7148786783218384, 0.9610764980316162, 0.979207456111908, 0.6918556094169617, 0.025815848261117935, 0.264161616563797, 0.37370821833610535, 0.7329296469688416, 0.023876359686255455, 0.10566841065883636, 0.7803924679756165, 0.908460795879364, 0.28089284896850586, 0.8346026539802551, 0.1864520162343979, 0.00410974957048893, 0.0031405433546751738, 0.3846418857574463, 0.9174258708953857, 0.8631933331489563, 0.16574089229106903, 0.7281448245048523, 0.9455350041389465, 0.001514381729066372, 0.8413127064704895, 0.9321456551551819, 0.9197893142700195, 0.8857771158218384, 0.015889722853899002, 0.22091737389564514, 0.9853662252426147, 0.10240344703197479, 0.22089889645576477, 0.49653133749961853, 0.6612719893455505, 0.7901147603988647, 0.9838752746582031, 0.8816916346549988, 0.6544728875160217, 0.10881290584802628, 0.8783398866653442, 0.948540985584259, 0.0008299940382130444, 0.8597645163536072, 0.9615535736083984, 0.35834547877311707, 0.8943740725517273, 0.5005409717559814, 0.07864457368850708, 0.9412854313850403, 0.8734495043754578, 0.9131142497062683, 0.29231083393096924, 0.34582090377807617, 0.4015014171600342, 0.9801163673400879, 0.4735179841518402, 0.05334274470806122, 0.8857157826423645, 0.9706428647041321, 0.1603344827890396, 0.00648145517334342, 0.270016074180603, 0.5846397876739502, 0.23761574923992157, 0.20894378423690796, 0.18265412747859955, 0.14223439991474152, 0.7002358436584473, 0.1351713240146637, 0.7448272705078125, 0.7871658802032471, 0.9163824319839478, 0.8700947761535645, 0.9451523423194885, 0.8050785064697266, 0.9474897384643555, 0.013219715096056461, 0.13449816405773163, 0.023744605481624603, 0.9746296405792236, 0.08899106830358505, 0.9109553694725037, 0.9897834658622742, 0.04437263309955597, 0.1312226504087448, 0.9588618874549866, 0.517324686050415, 0.0017404763493686914, 0.582952082157135, 0.936443567276001, 0.008059977553784847, 0.9497540593147278, 0.008787610568106174, 0.9033811688423157, 0.0013972503365948796, 0.2871337831020355, 0.9290716052055359, 0.628980815410614, 0.8918426632881165, 0.6902441382408142, 0.9707491993904114, 0.8423230051994324, 0.9827911257743835, 0.7017808556556702, 0.8487065434455872, 0.8300204873085022, 0.020642811432480812, 0.18980106711387634, 0.1237233504652977, 0.6724420189857483, 0.02258170396089554, 0.0027488619089126587, 0.18971681594848633, 0.27127721905708313, 0.0013656936353072524, 0.3629680573940277, 0.769523561000824, 0.015177405439317226, 0.8635234832763672, 0.37590840458869934, 0.9711453914642334, 0.008995487354695797, 0.8163004517555237, 0.7424067258834839, 0.9197348356246948, 0.8904926180839539, 0.8995028734207153, 0.0009206819231621921, 0.5274032354354858, 0.8609669804573059, 0.9505302309989929, 0.8928561210632324, 0.8017884492874146, 0.6832064986228943, 0.8914107084274292, 0.751609206199646, 0.938597559928894, 0.08101771771907806, 0.015450800769031048, 0.12137546390295029, 0.6373476386070251, 0.0026210828218609095, 0.9139628410339355, 0.004350100643932819, 0.00104841822758317, 0.8878782391548157, 0.9706318378448486, 0.5987187623977661, 0.9264583587646484, 0.5657303929328918, 0.9874973297119141, 0.7399941682815552, 0.9192962646484375, 0.08129767328500748, 0.8801195025444031, 0.9062110185623169, 0.888813853263855, 0.28553467988967896, 0.008440500125288963, 0.8474841117858887, 0.9615570306777954, 0.8250325322151184, 0.5021469593048096, 0.06003713980317116, 0.0013697838876396418, 0.015813788399100304, 0.9637394547462463, 0.2100611925125122, 0.923373281955719, 0.9437107443809509, 0.021612737327814102, 0.002226329641416669, 0.9392999410629272, 0.9590526223182678, 0.04143275320529938, 0.032679058611392975, 0.0005910584586672485, 0.5830917358398438, 0.9379374980926514, 0.9267374873161316, 0.926406979560852, 0.8113477826118469, 0.8642576932907104, 0.9630975127220154, 0.06198612600564957, 0.8257035613059998, 0.7825089693069458, 0.9841468334197998, 0.10899020731449127, 0.9297373294830322, 0.011198380962014198, 0.720087468624115, 0.09978874027729034, 0.0005362315569072962, 0.9587355256080627, 0.0016266426537185907, 0.15462008118629456, 0.9537742137908936, 0.8246568441390991, 0.7669554948806763, 0.08929120004177094, 0.6697738766670227, 0.7431250214576721, 0.3234742283821106, 0.45935752987861633, 0.991826057434082, 0.4874562621116638, 0.8224272131919861, 0.0015123174525797367, 0.8477813005447388, 0.966339647769928, 0.9456992745399475, 0.04606175795197487, 0.028263868764042854, 0.010550746694207191, 0.9352872371673584, 0.9736338257789612, 0.0008784791571088135, 0.7585514187812805, 0.5472456812858582, 0.9441105127334595, 0.5851635932922363, 0.9669987559318542, 0.022657716646790504, 0.15947426855564117, 0.1943199634552002, 0.63921058177948, 0.7474623918533325, 0.15762847661972046, 0.005805508233606815, 0.9848828911781311, 0.9465001225471497, 0.02817346155643463, 0.9352049827575684, 0.34894928336143494, 0.33479729294776917, 0.002473992295563221, 0.765527606010437, 0.9378648996353149, 0.9118021726608276, 0.028070777654647827, 0.8929201364517212, 0.8093401789665222, 0.0008044514106586576, 0.9603956937789917, 0.9432647228240967, 0.9112125039100647, 0.553597092628479, 0.09683314710855484, 0.7838715314865112, 0.8081638216972351, 0.002203791169449687, 0.938001275062561, 0.8670457005500793, 0.9584677815437317, 0.3029063940048218, 0.12105082720518112, 0.582283079624176, 0.1024903953075409, 0.8654424548149109, 0.947079598903656, 0.8671643733978271, 0.7187352180480957, 0.0185905359685421, 0.03838003799319267, 0.9202935099601746, 0.006280449219048023, 0.8238965272903442, 0.17819087207317352, 0.8111419677734375, 0.001629321021027863, 0.8857647180557251, 0.8219655156135559, 0.876811683177948, 0.8547798991203308, 0.9638680219650269, 0.6938227415084839, 0.6278559565544128, 0.7928277850151062, 0.15084949135780334, 0.9326552152633667, 0.9841468334197998, 0.9417654275894165, 0.9140204191207886, 0.04465756565332413, 0.03193958103656769, 0.9660009145736694, 0.9740089178085327, 0.08645688742399216, 0.008988307788968086, 0.48706677556037903, 0.727257251739502, 0.9661365151405334, 0.6285743713378906, 0.9594697952270508, 0.742863118648529, 0.0026675607077777386, 0.7836322784423828, 0.03276980668306351, 0.213219553232193, 0.9440617561340332, 0.834286093711853, 0.9772447943687439, 0.9352261424064636, 0.9803062677383423, 0.8521581292152405, 0.9834175705909729, 0.8214989304542542, 0.11099360138177872, 0.4529346227645874, 0.12940649688243866, 0.13036061823368073, 0.02056357078254223, 0.7662276029586792, 0.9812183976173401, 0.7603452801704407, 0.041894301772117615, 0.002110388595610857, 0.9513112306594849, 0.14934059977531433, 0.8748995065689087, 0.9310334920883179, 0.8480399250984192, 0.7131591439247131, 0.49189049005508423, 0.8737213015556335, 0.003172358265146613, 0.0009759778622537851, 0.9593921303749084, 0.5845364332199097, 0.15223965048789978, 0.8288589715957642, 0.06344418227672577, 0.9296517968177795, 0.0028223348781466484, 0.5528822541236877, 0.8414386510848999, 0.3658808171749115, 0.8108759522438049, 0.3015817701816559, 0.0019673446658998728, 0.0013645720900967717, 0.7371492981910706, 0.8558923006057739, 0.00040503282798454165, 0.6779979467391968, 0.009290359914302826, 0.128230482339859, 0.12235985696315765, 0.9514940977096558, 0.18321087956428528, 0.9496893286705017, 0.3609212338924408, 0.9662656188011169, 0.0013645720900967717, 0.860000491142273, 0.573039174079895, 0.0019795477855950594, 0.007021536584943533, 0.9513598680496216, 0.6251959800720215, 0.9101964235305786, 0.9661992788314819, 0.0022232173942029476, 0.6284428834915161, 0.6755158305168152, 0.9587355256080627, 0.8610199093818665, 0.037465374916791916, 0.962862491607666, 0.6583178639411926, 0.00025192979956045747, 0.525800883769989, 0.12831760942935944, 0.9078614711761475, 0.9054898023605347, 0.6979047060012817, 0.720198929309845, 0.5987808108329773, 0.21331234276294708, 0.24066734313964844, 0.1469350904226303, 0.08643298596143723, 0.9734463691711426, 0.0005407942226156592, 0.7644121050834656, 0.0016667647287249565, 0.9760459065437317, 0.9654979109764099, 0.9825929403305054, 0.5733829140663147, 0.8802013993263245, 0.007888758555054665, 0.9407896995544434, 0.011395683512091637, 0.9536492824554443, 0.7493910193443298, 0.8232814073562622, 0.16140100359916687, 0.0015545408241450787, 0.902090311050415, 0.8701196908950806, 0.8935409784317017, 0.9530060291290283, 0.016417022794485092, 0.0006353640928864479, 0.885445773601532, 0.0002705789520405233, 0.029788635671138763, 0.8212563395500183, 0.4960636794567108, 0.15546327829360962, 0.6810033917427063, 0.30528029799461365, 0.14568263292312622, 0.40630871057510376, 0.0003870149957947433, 0.6916595697402954, 0.6743851900100708, 0.5216308832168579, 0.584935188293457, 0.054747238755226135, 0.8409539461135864, 0.11209499090909958, 0.001245982595719397, 0.935028076171875, 0.001580429612658918, 0.931016206741333, 0.2120777666568756, 0.9662656188011169, 0.0067707025445997715, 0.926527738571167, 0.7807483673095703, 0.8715007901191711, 0.32441505789756775, 0.9363857507705688, 0.9867339134216309, 0.9545111060142517, 0.7700561881065369, 0.001287265564315021, 0.9879756569862366, 0.994562566280365, 0.36625155806541443, 0.9583920240402222, 0.9471645355224609, 0.0010551356244832277, 0.02336268685758114, 0.9669145345687866, 0.43332377076148987, 0.9539040327072144, 0.8477514982223511, 0.4132761061191559, 0.1986560821533203, 0.9219383001327515, 0.06456433236598969, 0.008299140259623528, 0.7887558937072754, 0.775910496711731, 0.9756189584732056, 0.003416437888517976, 0.9758493900299072, 0.8041956424713135, 0.3836553394794464, 0.9687692523002625, 0.16137737035751343, 0.9443395733833313, 0.8589767217636108, 0.13502128422260284, 0.0019126265542581677, 0.8085867166519165, 0.3471144735813141, 0.10911443829536438, 0.5495976805686951, 0.5044418573379517, 0.702567994594574, 0.9528826475143433, 0.6967519521713257, 0.789118766784668, 0.1630815714597702, 0.9548500776290894, 0.9638382196426392, 0.7560990452766418, 0.9627102017402649, 0.02991117723286152, 0.10224468261003494, 0.35835832357406616, 0.7259217500686646, 0.8900724649429321, 0.7854242920875549, 0.12723006308078766, 0.9237648844718933, 0.00024014365044422448, 0.15048015117645264, 0.25720247626304626, 0.7785717248916626, 0.009224222041666508, 0.98697829246521, 0.575363039970398, 0.001287265564315021, 0.30481570959091187, 0.5017013549804688, 0.9770439267158508, 0.6752851605415344, 0.1689596176147461, 0.9654452800750732, 0.5727971196174622, 0.15671409666538239, 0.9129021167755127, 0.5915282964706421, 0.826809287071228, 0.9120082259178162, 0.7673298716545105, 0.10958833247423172, 0.622783899307251, 0.2578871548175812, 0.9532093405723572, 0.37591713666915894, 0.8751178979873657, 0.9117841124534607, 0.5933013558387756, 0.23797954618930817, 0.3732917904853821, 0.7624131441116333, 0.0006219529896043241, 0.8397876620292664, 0.07274914532899857, 0.14618586003780365, 0.8674867153167725, 0.9158782958984375, 0.9240319728851318, 0.0011218254221603274, 0.6504067182540894, 0.014072469435632229, 0.7928277850151062, 0.15407919883728027, 0.46091848611831665, 0.12186328321695328, 0.02784883603453636, 0.9496933221817017, 0.9502974152565002, 0.013967800885438919, 0.7001437544822693, 0.004644501022994518, 0.14689216017723083, 0.2743728458881378, 0.6408418416976929, 0.026307182386517525, 0.8943847417831421, 0.8535734415054321, 0.005206657573580742, 0.0013645720900967717, 0.4602917432785034, 0.8671643733978271, 0.024799617007374763, 0.9129489064216614, 0.7715044617652893, 0.9517015814781189, 0.0011849839938804507, 0.0036811938043683767, 0.7785797119140625, 0.7095850706100464, 0.1884816437959671, 0.9444220662117004, 0.9194778800010681, 0.540501058101654, 0.8581081628799438, 0.3094565272331238, 0.9538553953170776, 0.6164378523826599, 0.7557454109191895, 0.2038484513759613, 0.9520525932312012, 0.7442927956581116, 0.9295076727867126, 0.1793057769536972, 0.32441505789756775, 0.017813144251704216, 0.0006833560764789581, 0.1237233504652977, 0.004171515814960003, 0.6550195217132568, 0.05057690665125847, 0.427123099565506, 0.9666303992271423], 'eval_runtime': 40.1956, 'eval_samples_per_second': 22.117, 'eval_steps_per_second': 1.12, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_29345/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n",
      "                                             \n",
      " 50%|█████     | 1/2 [1:03:18<00:38, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3721.2679, 'train_samples_per_second': 3.584, 'train_steps_per_second': 0.224, 'train_loss': 0.49925086481108083, 'epoch': 3.0}\n",
      "Done training!\n",
      "--------------------\n",
      "--------------------\n",
      "Evaluating estimator for Competence.\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e98bac847064131967ceb1cbde5d177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([9, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (9, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([889, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (889,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 889\n",
      "y_labels length: 889\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "Done evaluating!\n",
      "Getting prediction results for Competence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_29345/193241225.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx], device=device).clone().detach() for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ba9d842fd249eebc804d213b277a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([20, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (20, 2), float32\n",
      "--------------------\n",
      "Preprocessing y_pred logits and labels for Competence:\n",
      "--------------------\n",
      "y_pred_logits shape: torch.Size([13, 2]), torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred_prob through softmax of y_pred_logits.\n",
      "Using torch.nn.functional.softmax.\n",
      "y_pred_prob_array shape: (13, 2), float32\n",
      "--------------------\n",
      "Getting y_pred from y_pred_prob for Competence:\n",
      "--------------------\n",
      "y_pred_prob_array shape: torch.Size([593, 2]). torch.float32\n",
      "--------------------\n",
      "--------------------\n",
      "Getting y_pred through argmax of y_pred_prob.\n",
      "Using torch.argmax.\n",
      "y_pred_array shape: (593,)\n",
      "--------------------\n",
      "Flattening y_labels , y_pred_array, and y_pred_prob_array, then extracting probabilities of 1.\n",
      "y_pred_prob length: 593\n",
      "y_labels length: 593\n",
      "--------------------\n",
      "====================\n",
      "--------------------\n",
      "Computing metrics using y_pred.\n",
      "--------------------\n",
      "Computing metrics using y_pred_prob.\n",
      "--------------------\n",
      "Appending metrics to dict.\n",
      "Done appending metrics to dict.\n",
      "Done predicting!\n",
      "--------------------\n",
      "--------------------\n",
      "Saving model for Competence.\n",
      "====================\n",
      "Saving Estimator at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n",
      "Saving accelerator at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/\n",
      "Saving eval_metrics_dict at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving test_metrics_dict at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving df_train_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [1:03:49<00:00, 1914.75s/it]\n",
      "100%|██████████| 2/2 [1:04:27<00:00, 1933.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df_test_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Saving df_val_data at /Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/data/classification models/Transformers Results/Search+Xy/\n",
      "Done saving Xy, labels and estimator!\n",
      "['Estimator', 'accelerator', 'eval_metrics_dict', 'test_metrics_dict', 'df_train_data', 'df_test_data', 'df_val_data']\n",
      "====================\n",
      "Done training!\n",
      "--------------------\n",
      "All classifiers were used!\n",
      "########################################\n",
      "DONE!\n",
      "########################################\n",
      "CPU times: user 13min 9s, sys: 7min 54s, total: 21min 3s\n",
      "Wall time: 1h 4min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "# Define columns to be used\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "\n",
    "# Get existing estimators\n",
    "estimator_names_list = get_existing_files()\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "\n",
    "    print('-'*20)\n",
    "    print(f'{\"=\"*30} TRAINING DATASET OF LENGTH {len(df_manual)} ON {col.upper()} {\"=\"*30}')\n",
    "    print('-'*20)\n",
    "    print(\n",
    "        f'classifiers to be used ({len(list(transformers_pipe.keys()))}):\\n{list(transformers_pipe.keys())}'\n",
    "    )\n",
    "    assert len(df_manual[df_manual[str(col)].map(df_manual[str(col)].value_counts() > 1)]) != 0, f'Dataframe has no {col} values!'\n",
    "\n",
    "    if (len(glob.glob(f'{models_save_path}df_*_data - {col} - (Save_protocol=*).pkl')) == 3) or (len(glob.glob(f'{models_save_path}df_*_data - {col} - (Save_protocol=*).pkl')) == 6):\n",
    "        # Load previous Xy\n",
    "        print('Loading previous Xy.')\n",
    "        (\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights_dict, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = load_Xy(\n",
    "            col\n",
    "        )\n",
    "    else:\n",
    "        print('Splitting data.')\n",
    "        # Split data\n",
    "        (\n",
    "            train, X_train, y_train,\n",
    "            test, X_test, y_test,\n",
    "            val, X_val, y_val,\n",
    "            train_class_weights, train_class_weights_ratio, train_class_weights_dict,\n",
    "            test_class_weights, test_class_weights_ratio, test_class_weights_dict,\n",
    "            val_class_weights, val_class_weights_ratio, val_class_weights_dict,\n",
    "        ) = split_data(\n",
    "            df_manual, col,\n",
    "        )\n",
    "        # Save Xy data\n",
    "        save_Xy(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            col,\n",
    "        )\n",
    "\n",
    "    # Get trainer, model_name, tokenizer, model and optimizer\n",
    "    if col == 'Warmth':\n",
    "        Trainer = ImbTrainer\n",
    "    else:\n",
    "        Trainer = Trainer\n",
    "    for transformer_name, transformer_dict in tqdm.tqdm(transformers_pipe.items()):\n",
    "        model_name = transformer_dict['model_name']\n",
    "        config = transformer_dict['config'].from_pretrained(model_name)\n",
    "        tokenizer = transformer_dict['tokenizer'].from_pretrained(model_name, num_labels=2)\n",
    "        model = transformer_dict['model'].from_pretrained(model_name, config=config,).to(device)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        if model.config.pad_token_id is None:\n",
    "            try:\n",
    "                model.config.pad_token_id = tokenizer.pad_token_id\n",
    "            except:\n",
    "                model.config.pad_token_id = model.config.eos_token_id\n",
    "        def model_init(trial): return model\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "        vectorizer_name = ''.join(model.name_or_path.split('-')).upper()\n",
    "        classifier_name = model.__class__.__name__\n",
    "        output_dir = training_args_dict['output_dir'] = training_args_dict_for_best_trial['output_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).model'\n",
    "        log_dir = training_args_dict['logging_dir'] = training_args_dict_for_best_trial['logging_dir'] = f'{results_save_path}{method} Estimator - {col} - {vectorizer_name} + {classifier_name} (Save_protocol={pickle.HIGHEST_PROTOCOL}).log'\n",
    "\n",
    "        # Encode data\n",
    "        (\n",
    "            X_train_encodings, train_dataset,\n",
    "            X_test_encodings, test_dataset,\n",
    "            X_val_encodings, val_dataset,\n",
    "        ) = encode_data(\n",
    "            X_train, y_train,\n",
    "            X_test, y_test,\n",
    "            X_val, y_val,\n",
    "            tokenizer,\n",
    "        )\n",
    "\n",
    "        if f'{col} - {vectorizer_name} + {classifier_name}' in estimator_names_list:\n",
    "            print('-'*20)\n",
    "            print(\n",
    "                f'Already trained {col} - {vectorizer_name} + {classifier_name}'\n",
    "            )\n",
    "            print('-'*20)\n",
    "            continue\n",
    "\n",
    "        # Accelerate model\n",
    "        (\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        ) = accelerator.prepare(\n",
    "            model, tokenizer, optimizer, train_dataset, test_dataset, val_dataset\n",
    "        )\n",
    "        # model.eval()\n",
    "\n",
    "        # Initialize Trainer\n",
    "        print('-'*20)\n",
    "        print('='*30)\n",
    "        print(f'{\"=\"*30} Initializing Trainer using {vectorizer_name} + {classifier_name} {\"=\"*30}')\n",
    "        print('+'*30)\n",
    "\n",
    "        # try:\n",
    "        #     # Make trainer with fine-tuning arguments\n",
    "        #     print('-'*20)\n",
    "        #     print('Passing data and arguments to Trainer.')\n",
    "        #     estimator = Trainer(\n",
    "        #         model_init=model_init,\n",
    "        #         args=TrainingArguments(**training_args_dict_for_best_trial),\n",
    "        #         # model=model,\n",
    "        #         # args=TrainingArguments(**training_args_dict),\n",
    "        #         tokenizer=tokenizer,\n",
    "        #         train_dataset=train_dataset,\n",
    "        #         eval_dataset=val_dataset,\n",
    "        #         preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "        #         compute_metrics=compute_metrics,\n",
    "        #         callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        #         # data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "        #     )\n",
    "        #     if estimator.place_model_on_device:\n",
    "        #         estimator.model.to(device)\n",
    "\n",
    "        #     # Hyperparameter search\n",
    "        #     print('-'*20)\n",
    "        #     print(f'Starting hyperparameter search for {col}.')\n",
    "        #     best_trial = estimator.hyperparameter_search(\n",
    "        #         direction='maximize',\n",
    "        #         backend='optuna',\n",
    "        #         n_trials=10,\n",
    "        #         hp_space=optuna_hp_space,\n",
    "        #         sampler=optuna.samplers.TPESampler(seed=random_state),\n",
    "        #         pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "        #         compute_objective=compute_objective,\n",
    "        #         n_jobs=n_jobs,\n",
    "        #     )\n",
    "        #     print('Done hyperparameter search!')\n",
    "        #     print('-'*20)\n",
    "\n",
    "        #     # Train trainer\n",
    "        #     print('-'*20)\n",
    "        #     print(f'Starting training for {col} using {classifier_name}.')\n",
    "        #     estimator.train(trial=best_trial)\n",
    "        # except:\n",
    "        # Make trainer with fine-tuning arguments\n",
    "        print('-'*20)\n",
    "        print('Passing data and arguments to Trainer.')\n",
    "        estimator = Trainer(\n",
    "            # model_init=model_init,\n",
    "            # args=TrainingArguments(**training_args_dict_for_best_trial),\n",
    "            model=model,\n",
    "            args=TrainingArguments(**training_args_dict),\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "            # data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "        )\n",
    "        if estimator.place_model_on_device:\n",
    "            estimator.model.to(device)\n",
    "\n",
    "        # Train trainer\n",
    "        print('-'*20)\n",
    "        print(f'Starting training for {col} using {classifier_name}.')\n",
    "        estimator.train()\n",
    "        estimator.save_state()\n",
    "        estimator.save_model(output_dir)\n",
    "        accelerator.save(estimator.state, f'{output_dir}/accelerator')\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # Evaluate\n",
    "        print('-'*20)\n",
    "        print(f'Evaluating estimator for {col}.')\n",
    "        eval_metrics_dict = estimator.evaluate()\n",
    "        estimator.save_metrics('all', eval_metrics_dict)\n",
    "        y_val_pred = eval_metrics_dict.pop('eval_y_pred')\n",
    "        y_val_pred_prob = eval_metrics_dict.pop('eval_y_pred_prob')\n",
    "        eval_metrics_dict = clean_metrics_dict(eval_metrics_dict, list(eval_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done evaluating!')\n",
    "\n",
    "        # Get predictions\n",
    "        print(f'Getting prediction results for {col}.')\n",
    "        y_test_pred_logits, y_test_labels, test_metrics_dict = estimator.predict(test_dataset)\n",
    "        estimator.save_metrics('all', test_metrics_dict)\n",
    "        y_test_pred = test_metrics_dict.pop('test_y_pred')\n",
    "        y_test_pred_prob = test_metrics_dict.pop('test_y_pred_prob')\n",
    "        test_metrics_dict = clean_metrics_dict(test_metrics_dict, list(test_metrics_dict.keys())[0].split('_')[0])\n",
    "        print('Done predicting!')\n",
    "        print('-'*20)\n",
    "\n",
    "        # Save model\n",
    "        print('-'*20)\n",
    "        print(f'Saving model for {col}.')\n",
    "        save_Xy_estimator(\n",
    "            X_train, y_train, train_dataset,\n",
    "            X_test, y_test, y_test_pred, y_test_pred_prob, test_dataset,\n",
    "            X_val, y_val, y_val_pred, y_val_pred_prob, val_dataset,\n",
    "            estimator, accelerator, eval_metrics_dict, test_metrics_dict,\n",
    "            col, vectorizer_name, classifier_name,\n",
    "        )\n",
    "        print('Done training!')\n",
    "        print('-'*20)\n",
    "\n",
    "# Assert that all classifiers were used\n",
    "assert_all_classifiers_used(classifiers_pipe=transformers_pipe)\n",
    "print('#'*40)\n",
    "print('DONE!')\n",
    "print('#'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22599f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
