{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "import sys  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from pathlib import Path  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "sys.path.append(code_dir)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_module.imports import *  # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from estimators_get_pipe import * # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n",
    "from setup_module import specification_curve_fork as specy # type:ignore # isort:skip # fmt:skip # noqa # nopep8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e858544b",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65134b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "method = 'Transformers'\n",
    "with open(f'{data_dir}{method}_results_save_path.txt', 'r') as f:\n",
    "    results_save_path = f.read()\n",
    "with open(f'{data_dir}{method}_done_xy_save_path.txt', 'r') as f:\n",
    "    done_xy_save_path = f.read()\n",
    "\n",
    "t = time.time()\n",
    "n_jobs = -1\n",
    "n_splits = 10\n",
    "n_repeats = 3\n",
    "random_state = 42\n",
    "refit = True\n",
    "class_weight = 'balanced'\n",
    "cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "scoring = 'recall'\n",
    "scores = [\n",
    "    'recall', 'accuracy', 'f1', 'roc_auc',\n",
    "    'explained_variance', 'matthews_corrcoef'\n",
    "]\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score),\n",
    "}\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "metrics_dict = {\n",
    "    'Mean Cross Validation Train Score': np.nan,\n",
    "    f'Mean Cross Validation Train - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Train Variance - {scoring.title()}': np.nan,\n",
    "    'Mean Cross Validation Test Score': np.nan,\n",
    "    f'Mean Cross Validation Test - {scoring.title()}': np.nan,\n",
    "    f'Mean Explained Test Variance - {scoring.title()}': np.nan,\n",
    "    'Explained Variance': np.nan,\n",
    "    'Accuracy': np.nan,\n",
    "    'Balanced Accuracy': np.nan,\n",
    "    'Precision': np.nan,\n",
    "    'Recall': np.nan,\n",
    "    'F1-score': np.nan,\n",
    "    'Matthews Correlation Coefficient': np.nan,\n",
    "    'Fowlkes–Mallows Index': np.nan,\n",
    "    'R2 Score': np.nan,\n",
    "    'ROC': np.nan,\n",
    "    'AUC': np.nan,\n",
    "    f'{scoring.title()} Best Threshold': np.nan,\n",
    "    f'{scoring.title()} Best Score': np.nan,\n",
    "    'Log Loss/Cross Entropy': np.nan,\n",
    "    'Cohen’s Kappa': np.nan,\n",
    "    'Geometric Mean': np.nan,\n",
    "    'Classification Report': np.nan,\n",
    "    'Imbalanced Classification Report': np.nan,\n",
    "    'Confusion Matrix': np.nan,\n",
    "    'Normalized Confusion Matrix': np.nan\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Transformer variables\n",
    "max_length = 512\n",
    "returned_tensor = 'pt'\n",
    "cpu_counts = torch.multiprocessing.cpu_count()\n",
    "device = torch.device('mps') if torch.has_mps and torch.backends.mps.is_built() and torch.backends.mps.is_available(\n",
    ") else torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device_name = str(device.type)\n",
    "print(f'Using {device_name.upper()}')\n",
    "# Set random seed\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "torch.Generator(device_name).manual_seed(random_state)\n",
    "cores = multiprocessing.cpu_count()\n",
    "accelerator = Accelerator()\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "os.environ.get('TOKENIZERS_PARALLELISM')\n",
    "best_trial_args = [\n",
    "    'num_train_epochs', 'learning_rate',\n",
    "]\n",
    "training_args_dict = {\n",
    "    'seed': random_state,\n",
    "    'resume_from_checkpoint': True,\n",
    "    'overwrite_output_dir': True,\n",
    "    'logging_steps': 500,\n",
    "    'evaluation_strategy': 'steps',\n",
    "    'eval_steps': 500,\n",
    "    'save_strategy': 'steps',\n",
    "    'save_steps': 500,\n",
    "    # 'metric_for_best_model': 'recall',\n",
    "    # 'torch_compile': bool(transformers.file_utils.is_torch_available()),\n",
    "    'use_mps_device': bool(device_name == 'mps' and torch.backends.mps.is_available()),\n",
    "    'optim': 'adamw_torch',\n",
    "    'load_best_model_at_end': True,\n",
    "    'per_device_train_batch_size': 16,\n",
    "    'per_device_eval_batch_size': 20,\n",
    "    'warmup_steps': 100,\n",
    "    'weight_decay': 0.01,\n",
    "    # The below metrics are used by hyperparameter search\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 5e-5,\n",
    "}\n",
    "training_args_dict_for_best_trial = {\n",
    "    arg_name: arg_\n",
    "    for arg_name, arg_ in training_args_dict.items()\n",
    "    if arg_name not in best_trial_args\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc2e6a3",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c1cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach().to(device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], device=device).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddc61f29",
   "metadata": {},
   "source": [
    "# Classifying"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3594f8c",
   "metadata": {},
   "source": [
    "### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c444486",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_dir}df_jobs_len.txt', 'r') as f:\n",
    "    df_jobs_len = int(f.read())\n",
    "\n",
    "df_jobs = pd.read_pickle(f'{df_save_dir}df_jobs_for_classification.pkl')\n",
    "assert len(df_jobs) == df_jobs_len, f'DATAFRAME MISSING DATA! DF SHOULD BE OF LENGTH {df_jobs_len} BUT IS OF LENGTH {len(df_jobs)}'\n",
    "print(f'Dataframe loaded with shape: {df_jobs.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f621ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('#'*40)\n",
    "print('Starting!')\n",
    "print('#'*40)\n",
    "\n",
    "method = 'Transformers'\n",
    "protocol = pickle.HIGHEST_PROTOCOL\n",
    "analysis_columns = ['Warmth', 'Competence']\n",
    "text_col = 'Job Description spacy_sentencized'\n",
    "classified_columns = ['Warmth_Probability', 'Competence_Probability']\n",
    "final_estimators_dict = {\n",
    "    'Warmth': {\n",
    "        'vectorizer_name': 'TfidfVectorizer',\n",
    "        'classifier_name': 'LogisticRegression',\n",
    "    },\n",
    "    'Competence': {\n",
    "        'vectorizer_name': 'TfidfVectorizer',\n",
    "        'classifier_name': 'LogisticRegression',\n",
    "    },\n",
    "}\n",
    "\n",
    "for col in tqdm.tqdm(analysis_columns):\n",
    "    print('-'*20)\n",
    "    final_estimators_dict[col]['path_suffix'] = path_suffix = f' - {col} - {(vectorizer_name := final_estimators_dict[col][\"vectorizer_name\"])} + {(classifier_name := final_estimators_dict[col][\"classifier_name\"])} (Save_protocol={protocol})'\n",
    "\n",
    "    print('-'*20)\n",
    "    print('Loading Transformer Estimator.')\n",
    "    model = transformers_pipe[classifier_name]['model']\n",
    "    estimator = model.from_pretrained(f'{results_save_path}{method} Fitted Estimator {path_suffix}')\n",
    "    encodings = tokenizer(\n",
    "        df_jobs[text_col].astype('str').values.tolist(),\n",
    "        truncation=True, padding=True, max_length=max_length, return_tensors=returned_tensor\n",
    "    ).to(device)\n",
    "    # dataset = ToDataset(encodings, y)\n",
    "    print('Done loading Transformer Estimator!')\n",
    "    # Accelerate model\n",
    "    (\n",
    "        estimator, tokenizer\n",
    "    ) = accelerator.prepare(\n",
    "        estimator, tokenizer\n",
    "    )\n",
    "    # model.eval()\n",
    "\n",
    "    # Get predictions\n",
    "    print(f'Getting prediction results for {col}.')\n",
    "    estimator = Trainer(\n",
    "        model=estimator,\n",
    "        args=TrainingArguments(**training_args_dict),\n",
    "        tokenizer=tokenizer,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics_y_pred_prob,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        # data_collator=transformers.DataCollatorWithPadding(tokenizer),\n",
    "    )\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        estimator.model.parameters(),\n",
    "        lr=3e-5,\n",
    "    )\n",
    "\n",
    "    print('-'*20)\n",
    "    print('Classifying data.')\n",
    "    y_pred_logits, y_labels, metrics_dict = estimator.predict(test_dataset)\n",
    "    df_jobs[col] = metrics_dict.pop('test_y_pred')\n",
    "    df_jobs[f'{col}_Probability'] = metrics_dict.pop('test_y_pred_prob')[:, -1]\n",
    "    metrics_dict = clean_metrics_dict(test_metrics_dict, list(test_metrics_dict.keys())[0].split('_')[0])\n",
    "\n",
    "    print(f'Done classifying data for {col}!')\n",
    "    print('-'*20)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2bba2b5",
   "metadata": {},
   "source": [
    "## Inspect classified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1736d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c95c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.dropna(subset=dvs_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b88d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a21487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62895d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_info(df_jobs, ivs_all=[analysis_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38502cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df_info(df_jobs, ivs_all=[classified_columns])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "080cd68e",
   "metadata": {},
   "source": [
    "### Plot classified data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831dfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts plot of classifed warmthh and competence\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.countplot(x='Warmth', data=df_jobs, ax=ax[0], palette='colorblind')\n",
    "sns.countplot(x='Competence', data=df_jobs, ax=ax[1], palette='colorblind')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcfa853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of warmth and competence probabilities\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "sns.boxplot(x='Warmth', y='Warmth_Probability', data=df_jobs, ax=ax[0], palette='colorblind')\n",
    "sns.boxplot(x='Competence', y='Competence_Probability', data=df_jobs, ax=ax[1], palette='colorblind')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f479dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification curve analysis\n",
    "print(f'Running specification curve analysis with:\\nDEPENDENT VARIABLES = {dvs_prob}\\nINDEPENDENT VARIABLES = {ivs_perc}\\nCONTROLS = {controls}')\n",
    "sc = specy.SpecificationCurve(df=dj_jobs, y_endog=dvs_prob, x_exog=ivs_perc, controls=controls)\n",
    "sc.fit(estimator=sm.OLS)\n",
    "sc.plot(show_plot=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29e2340e",
   "metadata": {},
   "source": [
    "### Save dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34392218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # assert len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame), f'ERORR: LENGTH OF DF = {len(df_jobs)}'\n",
    "# df_jobs.to_pickle(f'{df_save_dir}df_jobs_for_analysis.pkl')\n",
    "# df_jobs.to_csv(f'{df_save_dir}df_jobs_for_analysis.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f6db7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
