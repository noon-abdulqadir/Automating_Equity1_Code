{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1122d883",
   "metadata": {},
   "source": [
    "# ATTN: This script should be run AFTER all POS tagging, lemmatization, and stemming (spacy, nltk, and BERT) completed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cb736",
   "metadata": {},
   "source": [
    "# Use spacy, NLTK, and gensim to create bi and trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a80dd",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_JOBS_TAGS_LEMMAS_STEMS_SPACY_NLTK_BERT\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93bb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce59f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_matches_ngrams(sentence, matcher, gram_type):\n",
    "    doc = nlp(sentence)\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "            return [doc[start:end].text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk_bert.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Bigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "bigram_patterns = [[{'POS': i} for i in j] for j in bigram_rules]\n",
    "\n",
    "matcher.add('bigram_patterns', bigram_patterns)\n",
    "\n",
    "df_jobs['Job Description 2grams_spacy'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'bigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864716b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n",
    "\n",
    "matcher.add('trigram_patterns', trigram_patterns)\n",
    "\n",
    "df_jobs['Job Description 3grams_spacy'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'trigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e1032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim bi and trigrams\n",
    "# Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description spacy_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2garms'] = list(bigram[df_jobs['Job Description spacy_tokenized']])\n",
    "\n",
    "# Trigrams\n",
    "trigram = Phraser(Phrases(bigram_sentences, connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3garms'] = list(trigram[bigram_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_ngrams_spacy_nltk_gensim.pkl')\n",
    "\n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_ngrams_spacy_nltk_gensim.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
