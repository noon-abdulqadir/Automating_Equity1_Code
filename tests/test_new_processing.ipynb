{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "pattern_numbers = r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21414866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygwalker as pyg\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c22dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "    lambda job_description: ' '.join(job_description.split('/')) if '/' in job_description else job_description\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_dict = {\n",
    "    r'incl\\.': 'including', \n",
    "    r'e\\.g\\.': 'for example', \n",
    "    r'e\\.g': 'for example', \n",
    "    r'etc\\.': 'et cetera', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22cc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_fix_incl = 'Apply appropriate and effective communication methods to senior management and important stakeholders'\n",
    "str_fix_eg = 'Partner with Procurement in order to manage suppliers for the projects & programs in scope'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'] = df_jobs['Job Description'].replace(abb_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'][df_jobs['Job Description'].str.contains(str_fix_incl)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(str_fix_eg, df_jobs['Job Description'][35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952299c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(re.finditer(str_fix_eg, df_jobs['Job Description'][35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35af326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'][35][1461:1580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('incl.', 'including')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('e.g.', 'e.g')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('e.g', 'for example')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('etc.', 'et cetera')\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d45507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs.sample(n=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b23278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_glob_paths_10.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c9ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d53f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_requirement_pattern = r'[Ll]anguage: [Dd]utch|[Dd]utch [Pp]referred|[Dd]utch [Re]quired|[Dd]utch [Ll]anguage|[Pp]roficient in [Dd]utch|[Ss]peak [Dd]utch|[Kk]now [Dd]utch'\n",
    "english_requirement_pattern = r'[Ll]anguage: [Ee]nglish|[Ee]nglish [Pp]referred|[Ee]nglish [Re]quired|[Ee]nglish [Ll]anguage|[Pp]roficient in [Ee]nglish|[Ss]peak [Ee]nglish|[Kk]now [Ee]nglish'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee66305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language requirements\n",
    "# Dutch\n",
    "if 'Dutch Requirement' in df_jobs.columns:\n",
    "    df_jobs.drop(columns=['Dutch Requirement'], inplace=True)\n",
    "df_jobs['Dutch Requirement'] = np.where(\n",
    "    df_jobs['Job Description'].str.contains(dutch_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")\n",
    "\n",
    "# English\n",
    "if 'English Requirement' in df_jobs.columns:\n",
    "    df_jobs.drop(columns=['English Requirement'], inplace=True)\n",
    "df_jobs['English Requirement'] = np.where(\n",
    "    df_jobs['Job Description'].str.contains(english_requirement_pattern),\n",
    "    'Yes',\n",
    "    'No',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Dutch Requirement'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = pd.read_pickle(f'{scraped_data}CBS/Data/Sectors Output from script.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors.columns = [\n",
    "    '_'.join(col) \n",
    "    if 'SBI Sector Titles' not in col \n",
    "    and 'Total Workforce' not in col \n",
    "    else col[-1] \n",
    "    for col in df_sectors.columns\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e87cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors.rename(\n",
    "    columns={\n",
    "        'Keywords': 'Search Keyword', \n",
    "        'Code': 'Sector Code', \n",
    "        'Sector Name': 'Sector', \n",
    "        'Gender_Female_n': 'Female Count (x 1000)', \n",
    "        'Gender_Male_n': 'Male Count (x 1000)', \n",
    "        'Gender_Sectoral Gender Segregation_Dominant Category': 'Gender', \n",
    "        'Age_Sectoral Age Segregation_Dominant Category': 'Age', \n",
    "        'n': 'Sector Count (x 1000)', \n",
    "    },\n",
    "    inplace=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e8ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_sectors['Search Keyword'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ded22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = df_sectors.explode(\n",
    "    'Search Keyword', ignore_index=True\n",
    ").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7518a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.merge(df_sectors, on='Search Keyword', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4188c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ce97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f40d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_jobs['Search Keyword'].loc[df_jobs['Sector'].isna().sum()] != 0:\n",
    "    print('Some search keywords did not match a sector. Fixing')\n",
    "    print(set(df_jobs['Search Keyword'].loc[df_jobs['Sector'].isna()].to_list()))\n",
    "    print(len(df_jobs['Search Keyword'].loc[df_jobs['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n",
    "    df_jobs = fix_keywords(df_jobs)\n",
    "    print(set(df_jobs['Search Keyword'].loc[df_jobs['Sector'].isna()].to_list()))\n",
    "    print(len(df_jobs['Search Keyword'].loc[df_jobs['Search Keyword'].isin(list(keyword_trans_dict.keys()))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884dfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs.loc[(df_jobs['Job Description'].str.contains(str_fix_incl)) | (df_jobs['Job Description'].str.contains(str_fix_eg))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e556d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_descriptions = list(set(df_jobs['Job Description'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da09d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = list(\n",
    "    set(\n",
    "        df_jobs['Job Description'].loc[\n",
    "            (df_jobs['Job Description'].str.contains(str_fix_incl)) |\n",
    "            (df_jobs['Job Description'].str.contains(str_fix_eg))\n",
    "        ].to_list()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f15f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions[0][1500:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.loc[\n",
    "    (df_jobs['Job Description'].str.contains(str_fix_incl)) | \n",
    "    (df_jobs['Job Description'].str.contains(str_fix_eg))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ab475",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a list of punctuations that determine sentence boundry, i.e., split characters\n",
    "def make_custom_punct_chars(main_punct_chars = [':', '|'], repeated_punct_chars = ['\\n', ',']):\n",
    "    custom_punct_chars = []\n",
    "    temp_multi = []\n",
    "    temp_spaced = []\n",
    "\n",
    "    for punct_char in main_punct_chars:\n",
    "        custom_punct_chars+= f'{punct_char}', f'{punct_char} '\n",
    "\n",
    "    for idx in range(4):\n",
    "        for punct_char in repeated_punct_chars:\n",
    "            temp_multi.append(f'{punct_char}'*int(idx+1))\n",
    "            temp_spaced.append(f'{punct_char} '*int(idx+1))\n",
    "\n",
    "    for multi, spaced in zip(temp_multi, temp_spaced):\n",
    "        custom_punct_chars+= multi, spaced\n",
    "\n",
    "    custom_punct_chars.remove(',')\n",
    "    custom_punct_chars.remove(', ')\n",
    "\n",
    "    return custom_punct_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_punct_chars = make_custom_punct_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183846a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_punct_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c845236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentencizer to nlp pip and set custom punctuations\n",
    "sentencizer = nlp.add_pipe('sentencizer')\n",
    "sentencizer.punct_chars.update(custom_punct_chars)\n",
    "\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     with open(f'{data_dir}punctuations.txt', 'wb') as f:\n",
    "#         pickle.dump(sentencizer.punct_chars, f)\n",
    "\n",
    "# with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
    "#     custom_punct_chars = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37292ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     with open(f'{data_dir}punctuations.txt', 'wb') as f:\n",
    "#         pickle.dump(sentencizer.punct_chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e8ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
    "#     custom_punct_char = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6425cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import NORM, ORTH, LEMMA\n",
    "\n",
    "special_cases_dict = {\n",
    "    'incl.': [{65: 'incl', 67: 'including'}],\n",
    "    'incl. ': [{65: 'incl', 67: 'including'}],\n",
    "    '(incl.': [{65: 'incl', 67: 'including'}],\n",
    "    'etc.': [{65: 'etc', 67: 'et cetera'}],\n",
    "    'etc. ': [{65: 'etc', 67: 'et cetera'}],\n",
    "    'e.g.': [{65: 'e.g', 67: 'for example'}],\n",
    "    'e.g. ': [{65: 'e.g', 67: 'for example'}],\n",
    "}\n",
    "\n",
    "nlp.tokenizer.rules.update(special_cases_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer.rules['e.g.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description'].apply(\n",
    "        lambda job_description: [\n",
    "            sent \n",
    "            for sentence in nlp(job_description).sents \n",
    "            for sent in re.split(pattern, sentence.text) \n",
    "            if len(sent) != 0 \n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.explode('Job Description spacy_sentencized', ignore_index=True).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized_lower'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: job_sentence.strip().lower()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c817bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9369d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenize\n",
    "df_jobs['Job Description spacy_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.text.strip().lower()) \n",
    "        for token in nlp.tokenizer(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.text in custom_punct_chars\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf53549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLK\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r'[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?<=[a-z])(?=[A-Z])'\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "\n",
    "pattern_numbers = r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_sentencized = []\n",
    "\n",
    "# for job_description in job_descriptions:\n",
    "#     for sentence in sent_tokenize(job_description):\n",
    "#         for sent in re.split(pattern, sentence):\n",
    "#             nltk_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized = [\n",
    "    sent \n",
    "    for job_description in job_descriptions \n",
    "    for sentence in sent_tokenize(job_description) \n",
    "    for sent in re.split(pattern, sentence)\n",
    "    if len(sent) != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_sentencized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'] = df_jobs['Job Description'].apply(\n",
    "#     lambda job_description: [\n",
    "#         sent \n",
    "#         for sentence in sent_tokenize(job_description) \n",
    "#         for sent in re.split(pattern, sentence)\n",
    "#         if len(sent) != 0\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'] = df_jobs['Job Description'].apply(\n",
    "#     lambda job_description: [\n",
    "#         re.split(pattern, sentence)\n",
    "#         for sentence in sent_tokenize(job_description)\n",
    "#         if len(sentence) != 0\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'][0][-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_jobs['Job Description nltk_sentencized'][0]) + len(df_jobs['Job Description nltk_sentencized'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af60e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs.explode('Job Description nltk_sentencized', ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1126453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized_lower = [\n",
    "    str(sent.strip().lower()) \n",
    "    for sent in nltk_sentencized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d00571",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_sentencized_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2042a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized_lower'] = df_jobs['Job Description nltk_sentencized'].apply(\n",
    "#     lambda job_sentence: job_sentence.strip().lower()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ff470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized_lower']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sentence in df_jobs['Job Description spacy_sentencized'].items():\n",
    "    if str_fix_eg in sentence:\n",
    "        print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized'][17:25].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d85ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sentence in df_jobs['Job Description spacy_sentencized'].items():\n",
    "    if 'Power' in sentence:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d27dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized'][60:70].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if str_fix_eg in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_eg in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_fix = 'Apply appropriate and effective communication methods to senior management and important stakeholders \\(incl.'\n",
    "\n",
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if str_fix_incl.split('\\(')[0] in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized[84:95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1476b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences split on e.g. and incl.\n",
    "nltk_sentencized[\n",
    "    next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_eg in sent):next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_incl.split('\\(')[0] in sent)+2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da72e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if 'Power' in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(idx for idx, sent in enumerate(nltk_sentencized) if 'Power' in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized[next(idx for idx, sent in enumerate(nltk_sentencized) if 'Power' in sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(nltk_sentencized[84:95]):\n",
    "    print(f'Sentence {idx+1}: {sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_tokenized = []\n",
    "\n",
    "# for job_sentence in nltk_sentencized:\n",
    "#     for token in word_tokenize(job_sentence):\n",
    "#         if len(token) != 0 and token != '...' and token.lower() not in set(stopwords.words('english')) and token.lower() not in list(string.punctuation):\n",
    "#             nltk_tokenized.append(str(token.strip().lower())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenized = [\n",
    "    str(token.strip().lower()) \n",
    "    for job_sentence in nltk_sentencized \n",
    "    for token in word_tokenize(job_sentence) \n",
    "    if len(token) != 0 \n",
    "    and token != '...' \n",
    "    and not token.lower() in set(stopwords.words('english')) \n",
    "    and not token.lower() in list(string.punctuation) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad77469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.strip().lower()) \n",
    "        for token in word_tokenize(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and token != '...' \n",
    "        and not token.lower() in set(stopwords.words('english')) \n",
    "        and not token.lower() in list(string.punctuation) \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1365b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'...' in nltk_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fd54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c008f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "gensim_tokenized = []\n",
    "\n",
    "for job_sentence in nltk_sentencized:\n",
    "    gensim_tokenized.append(preprocess_string(re.sub(pattern_numbers, ' ', job_sentence.strip().lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df785608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: preprocess_string(re.sub(pattern_numbers, ' ', job_sentence.strip().lower()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea685cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b222c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_token_tags = pos_tag(nltk_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b74e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_token_tags[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_token_tags'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
    "    lambda token: pos_tag(token)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_token_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda77df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    token_lemma = lemmatizer.lemmatize(token)\n",
    "    token_stem = stemmer.stem(token)\n",
    "    token_pos = get_wordnet_pos(token)\n",
    "    token_pos_lemma = lemmatizer.lemmatize(token, token_pos)\n",
    "    print(f'Token: {token} | Stem: {token_stem} | Lemma: {token_lemma} | POS Lemma: {token_pos_lemma}')\n",
    "    nltk_lemmas.append(token_pos_lemma)\n",
    "    nltk_stems.append(token_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    token_lemma = lemmatizer.lemmatize(token)\n",
    "    token_stem = stemmer.stem(token)\n",
    "    token_pos = get_wordnet_pos(token)\n",
    "    nltk_lemmas.append(lemmatizer.lemmatize(token, get_wordnet_pos(token)))\n",
    "    nltk_stems.append(token_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_lemmas = [\n",
    "    lemmatizer.lemmatize(\n",
    "        token, get_wordnet_pos(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "    )\n",
    "    for token in nltk_tokenized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d82225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_lemmas'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        lemmatizer.lemmatize(\n",
    "            token, get_wordnet_pos(\n",
    "                unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            )\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9723c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8227a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    nltk_stems.append(stemmer.stem(unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45564d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stems = [\n",
    "    stemmer.stem(\n",
    "        unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    )\n",
    "    for token in nltk_tokenized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47888b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_stems'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        stemmer.stem(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_stems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf9454",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc52927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52198bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "# prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "# suffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\n",
    "# infix_re = re.compile(r'''[-~]''')\n",
    "# simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "# def custom_tokenizer(nlp):\n",
    "#     return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "#                                 prefix_search=prefix_re.search,\n",
    "#                                 suffix_search=suffix_re.search,\n",
    "#                                 infix_finditer=infix_re.finditer,\n",
    "#                                 url_match=simple_url_re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65795c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.tokenizer.add_special_case('incl', [{ORTH: u'incl', NORM: u'include', LEMMA: u'include', POS: u'VERB'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c760575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy sentencizer\n",
    "# spacy_sentencized = []\n",
    "\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     for job_description in job_descriptions:\n",
    "#         for sentence in nlp(job_description).sents:\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentencizer\n",
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    spacy_sentencized = [\n",
    "        sent \n",
    "        for job_description in job_descriptions \n",
    "        for sentence in nlp(job_description).sents \n",
    "        for sent in re.split(pattern, sentence.text) \n",
    "        if len(sent) != 0 \n",
    "    ]\n",
    "    \n",
    "    spacy_sentencized_lower = [\n",
    "        str(sent.strip().lower()) \n",
    "        for sent in spacy_sentencized\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy sentencizer\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     spacy_sentencized = [\n",
    "#         re.split(pattern, sentence.text) \n",
    "#         for job_description in job_descriptions \n",
    "#         for sentence in nlp(job_description).sents \n",
    "# #         for sent in re.split(pattern, sentence.text) \n",
    "#         if len(sentence) != 0 \n",
    "#     ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6eff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentencized[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenizer\n",
    "spacy_tokenized = []\n",
    "\n",
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    for job_sentence in spacy_sentencized:\n",
    "        for token in nlp.tokenizer(job_sentence):\n",
    "            if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars:\n",
    "                spacy_tokenized.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f574e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spacy_sentencized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spacy_sentencized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if str_fix_eg in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if str_fix_eg in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str_fix = 'Apply appropriate and effective communication methods to senior management and important stakeholders \\(incl.'\n",
    "\n",
    "\n",
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if str_fix_incl.split('\\(')[0] in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if str_fix_incl.split('\\(')[0] in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c515eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentencized[18:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97282ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if 'Power' in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2147d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if 'Power' in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentencized[63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf1dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate(spacy_sentencized[18:25]):\n",
    "    print(f'Sentence {idx+1}: {sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869b4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy tokenizer\n",
    "# spacy_tokenized = []\n",
    "\n",
    "# for job_sentence in spacy_sentencized:\n",
    "# #         doc = nlp.tokenizer(job_sentence)\n",
    "#     spacy_tokenized.extend(\n",
    "#         [\n",
    "#             token.text for token in nlp.tokenizer(job_sentence) \n",
    "#             if token.text not in custom_punct_chars\n",
    "#             and not token.is_stop \n",
    "\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeddf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spacy_sentencized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d95615",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentencized[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenizer\n",
    "\n",
    "spacy_tokenized = [\n",
    "    str(token.text.strip().lower()) \n",
    "    for job_sentence in spacy_sentencized \n",
    "    for token in nlp.tokenizer(job_sentence) \n",
    "    if len(token) != 0 \n",
    "    and not token.is_stop \n",
    "    and not token.is_punct \n",
    "    and not token.text in custom_punct_chars\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f6eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spacy_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6dcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.get_pipe('attribute_ruler').add([[{\"TEXT\":\"Angeltown\"}]],{\"LEMMA\":\"San Fransisco\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy pos tagging\n",
    "# nlp_stemmer = LancasterStemmer()\n",
    "nlp_stemmer = PorterStemmer()\n",
    "\n",
    "nlp_token_tags = []\n",
    "nlp_lemmas = []\n",
    "nlp_stems = []\n",
    "\n",
    "for job_description in job_descriptions:\n",
    "    for token in nlp(job_description):\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars:\n",
    "            nlp_token_tags.append(tuple([token.text, token.tag_]))\n",
    "            nlp_lemmas.append(token.lemma_)\n",
    "            nlp_stems.append(nlp_stemmer.stem(token.text))\n",
    "#         for sentence in doc.sents:\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7002207",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_token_tags = [\n",
    "    tuple([token.text.strip().lower(), token.tag_])\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_token_tags'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        tuple([token.text.strip().lower(), token.tag_])\n",
    "        for token in nlp(job_sentence)\n",
    "        \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac855d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lemmas = [\n",
    "    token.lemma_.strip().lower()\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_lemmas'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        token.lemma_.strip().lower()\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stems = [\n",
    "    stemmer.stem(token.text.strip().lower())\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ccde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_stems'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        stemmer.stem(token.text.strip().lower())\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9beae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_stems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     nlp_token_tags = [\n",
    "#         tuple([token.text, token.tag_]) \n",
    "#         for job_description in job_descriptions \n",
    "#         for token in nlp(job_description) \n",
    "#         if len(token) != 0 \n",
    "#         and not token.is_stop \n",
    "#         and not token.is_punct \n",
    "#         and not token.text in custom_punct_chars\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_num_and_frequency(row, text_col):\n",
    "\n",
    "    row['Job Description num_words'] = len(str(row[f'{text_col}']).split())\n",
    "    row['Job Description num_unique_words'] = len(set(str(row[f'{text_col}']).split()))\n",
    "    row['Job Description num_chars'] = len(str(row[f'{text_col}']))\n",
    "    row['Job Description num_punctuations'] = len([c for c in str(row[f'{text_col}']) if c in string.punctuation])\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e640ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.apply(lambda row: get_word_num_and_frequency(row=row, text_col='Job Description spacy_sentencized'), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1674f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description bert_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d714d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_jobs['Job Description spacy_tokenized'].str.join(' ').to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_jobs['Job Description bert_tokenized'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c241419",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_jobs['Job Description bert_tokenized'].to_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d609b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c602e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9640b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df_jobs['Job Description spacy_sentencized'].to_list()[1]\n",
    "\n",
    "\n",
    "u = bert_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee503611",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "# bert_pos_tokenizer = AutoTokenizer.from_pretrained(bert_pos_model_name)\n",
    "bert_pos_model = AutoModelForTokenClassification.from_pretrained('QCRI/bert-base-multilingual-cased-pos-english')\n",
    "bert_pos_tag = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens = df_jobs['Job Description spacy_tokenized'].to_list()\n",
    "# x = bert_pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc8f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "u=df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: bert_pos_tag(token)[i] for token in tokens for i in range(len(token))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20351f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "# for token in tokens:\n",
    "#     for i in range(len(tokens)):\n",
    "#         print(i)\n",
    "print(bert_pos_tag(token)[i] for i in range(len(token)) for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf753ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    print(bert_pos_tag(token))\n",
    "#     for i in range(len(token)):\n",
    "#         for pos_data in bert_pos_tag(token)[i]:\n",
    "#             print(token[i], pos_data['word'], pos_data['entity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ca2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = [\n",
    "    (bert_pos_tag['word'], bert_pos_tag['entity'])\n",
    "    for token in tokens\n",
    "    for i in range(len(token))\n",
    "    for bert_pos_tag in bert_pos_tagger(token)[i]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b490e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e4a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "bert_pos_tag = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "df_jobs['Job Description bert_token_tags'] = df_jobs['Job Description bert_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        (pos_data['word'], pos_data['entity'])\n",
    "        for i in range(len(tokens))\n",
    "        for pos_data in bert_pos_tag(tokens)[i]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65befcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"I saw The Who perform. Who did you see?\"\n",
    "doc1 = nlp(text)\n",
    "print(doc1[2], doc1[2].tag_, doc1[2].pos_)  # DT DET\n",
    "print(doc1[3], doc1[3].tag_, doc1[3].pos_)  # WP PRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb94bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I saw The Who \"perform\". Who did you see?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp(text):\n",
    "    print(token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b684c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in sentence:\n",
    "    for token in tokens:\n",
    "        print(bert_pos_tag(token)[0])\n",
    "        \n",
    "#     for idx, token in enumerate(tokens):\n",
    "#         print(tokens[idx],bert_pos_tag(token)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a3c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a89d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in sentence:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15241d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: bert_pos_tag(sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tokens in df_jobs['Job Description spacy_tokenized'].items():\n",
    "    print(bert_pos_tag(tokens)[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ecd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: bert_pos_tag(token)[idx]\n",
    "    for idx, token in enumerate(tokens)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1044aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j.loc[0, '<lambda>'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c834b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ba5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2735ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_tokenized'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05613301",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(bert_pos_tag(sentence)['word'], bert_pos_tag(sentence)(['entit)y'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79894822",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][0]['entity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe33acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][0]['word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5745de",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(bert_tags['word'], outpbert_tagsuts['entity'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[outputs[0][0]['start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in spacy_sentencized:\n",
    "#     num_words = len(sent.split())\n",
    "#     num_unique_words = len(set(sent.split()))\n",
    "#     num_chars = len(sent)\n",
    "#     num_punctuations = len([c for c in sent if c in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(df_jobs_to_be_processed, text_col, algo='vader', sentiment_range=(-1,1)):\n",
    "\n",
    "    ## calculate sentiment\n",
    "    if algo == 'vader':\n",
    "        df_jobs_to_be_processed['Sentiment'] = df_jobs_to_be_processed[text_col].apply(lambda x: sentim_analyzer.polarity_scores(x)['compound'] if isinstance(x, str) else np.nan)\n",
    "    elif algo == 'textblob':\n",
    "        df_jobs_to_be_processed['Sentiment'] = df_jobs_to_be_processed[text_col].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    ## rescaled\n",
    "    if sentiment_range != (-1,1):\n",
    "        df_jobs_to_be_processed['Sentiment'] = preprocessing.MinMaxScaler(feature_range=sentiment_range).fit_transform(df_jobs_to_be_processed[['sentiment']])\n",
    "    # print(df_jobs_to_be_processed[['sentiment']].describe().T)\n",
    "\n",
    "    return df_jobs_to_be_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a05fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# nlp.add_pipe('spacytextblob')\n",
    "sentiment = []\n",
    "for job_sentence in spacy_sentencized:\n",
    "    sentiment.append(nlp(job_sentence)._.blob.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06baee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = [\n",
    "    float(nlp(job_sentence)._.blob.polarity)\n",
    "    for job_sentence in spacy_sentencized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(nlp(sentence)._.blob.polarity)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84363ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df_jobs['Job Description nltk_sentiment'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: float(sentim_analyzer.polarity_scores(sentence)['compound'])\n",
    "    if isinstance(sentence, str) else np.nan\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1031bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_sentiment(df_jobs, text_col='Job Description spacy_sentencized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb854ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "for sent in spacy_sentencized:\n",
    "    print(vader.polarity_scores(sent)['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment\n",
    "# from textblob import TextBlob, Word\n",
    "\n",
    "# for sent in spacy_sentencized:\n",
    "#     print(TextBlob(sent).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Tokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "df_jobs['bert_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda sentence: bert_tokenizer.tokenize(str(sentence)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f80932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['bert_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS, Phraser, Phrases\n",
    "\n",
    "# Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description spacy_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2garms'] = bigram[df_jobs['Job Description spacy_tokenized']]\n",
    "\n",
    "# Trigrams\n",
    "trigram = Phraser(Phrases(bigram_sentences, connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3garms'] = trigram[bigram_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_3garms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85689ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_jobs['Job Description gensim_3garms'].items():\n",
    "    for token in row:\n",
    "        if '_' in token:\n",
    "            if re.search('_([^_]+)_', token):\n",
    "                print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences = ''\n",
    "for token in spacy_tokenized:\n",
    "    spacy_sentences+= token + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_sentences = [spacy_sentences.strip().lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_bigrams = []\n",
    "\n",
    "for job_sentence in spacy_sentences:\n",
    "#     print(job_sentence)\n",
    "    for noun_phrase in list(nlp(job_sentence).noun_chunks):\n",
    "        print(noun_phrase.merge_noun_chunks)\n",
    "#         print(noun_phrase.merge(noun_phrase.root.tag_, noun_phrase.root.lemma_, noun_phrase.root.ent_type_))\n",
    "#         if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars:\n",
    "#             spacy_tokenized.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_similarity(sentences, bert_model):\n",
    "\n",
    "    embeddings = bert_model.encode(sentences, show_progress_bar=True)\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted([[cos_sim[i][j], i, j] for i in range(len(cos_sim)-1) for j in range(i+1, len(cos_sim))], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if args['print_enabled'] is True:\n",
    "        print('Top-5 most similar pairs:')\n",
    "        for score, i, j in all_sentence_combinations[:5]:\n",
    "            print(f'{sentences[i]} \\t {sentences[j]} \\t {cos_sim[i][j]:.4f}')\n",
    "\n",
    "    if args['save_enabled'] is True:\n",
    "        bert_model.to_json_file('bert_config.json')\n",
    "\n",
    "    return all_sentence_combinations, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in spacy_sentencized:\n",
    "    print(bert_tokenizer.encode_plus(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c2e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d6078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d2e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7590d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls(\n",
    "    BertModel.from_pretrained(\n",
    "        bert_model_name,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        output_additional_info=True,\n",
    "    ),\n",
    "    BertAligner.from_pretrained(bert_model_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentencizer\n",
    "# spacy_sentencized = []\n",
    "\n",
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    for job_description in job_descriptions:\n",
    "        for token in nlp(job_description):\n",
    "            print(token.text, token.tag_)\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "sentim_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS, Phraser, Phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96671629",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbabbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_jobs['Job Description spacy_sentencized'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa757140",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20866c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526da0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Bigrams\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "bigram_patterns = [[{'POS': i} for i in j] for j in bigram_rules]\n",
    "trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n",
    "\n",
    "\n",
    "# rules = [\n",
    "#     ['VERB', 'ADJ', 'NOUN'],\n",
    "#     ['NOUN', 'VERB', 'ADV'],\n",
    "#     ['NOUN', 'ADP', 'NOUN'],\n",
    "#     # more rules here...\n",
    "# ]\n",
    "\n",
    "# lst1 = []\n",
    "\n",
    "# for j in trigram_rules:\n",
    "#     lst2 = []\n",
    "#     for i in j:\n",
    "# #         x = {'POS': i}\n",
    "#         lst2.append({'POS': i})\n",
    "# #         print(lst2)\n",
    "#     lst1.append(lst2)\n",
    "# #         lst2.append([{\"POS\": i} ])\n",
    "# print(lst1)\n",
    "# # trigram_patterns = [[{'POS': i} for i in j] for j in patterns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst1 = []\n",
    "\n",
    "# for j in bigram_rules:\n",
    "#     lst2 = []\n",
    "#     for i in j:\n",
    "# #         x = {'POS': i}\n",
    "#         lst2.append({'POS': i})\n",
    "# #         print(lst2)\n",
    "#     lst1.append(lst2)\n",
    "# #         lst2.append([{\"POS\": i} ])\n",
    "# print(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8afdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_patterns = [[{'POS': i} for i in j] for j in bigram_rules]\n",
    "trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2de96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram_patterns = {\n",
    "#     'noun_verb': [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "#     'verb_noun': [{'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "#     'adj_noun': [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "#     'adj_propn': [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e56f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('bigram_patterns', bigram_patterns)\n",
    "matcher.add('trigram_patterns', trigram_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51369ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adba29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for match_id, start, end in matcher(nlp(sentence)):\n",
    "#         sent = nlp(sentence)\n",
    "        bigram.append(nlp(sentence)[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2f46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = [\n",
    "    nlp(sentence)[start:end].text\n",
    "    for sentence in sentences\n",
    "    for match_id, start, end in matcher(nlp(sentence))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce0b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703152e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "gram_type = 'bigram'\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "            matches = doc[start:end].text\n",
    "            print(f\"{doc[:start]} {'_'.join(doc[start:end].text.split())} {doc[end:]}\")\n",
    "#     lst.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8351ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tokens in df_jobs['Job Description bert_tokenized'].items():\n",
    "    for i in range(len(tokens)):\n",
    "        for pos_data in bert_pos_tag(tokens)[i]:\n",
    "            print(pos_data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af95eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "bert_pos_tag = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3235473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_jobs['Job Description bert_token_tags_with_scores'] = df_jobs['Job Description bert_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        (pos_data['word'], pos_data['entity'], pos_data['score'])\n",
    "        for i in range(len(tokens))\n",
    "        for pos_data in bert_pos_tag(tokens)[i]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10b87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description bert_token_tags_with_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff54f20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_jobs['Job Description bert_token_tags_with_scores'].apply(\n",
    "#     lambda tag_list: print(tag_tuple[i])\n",
    "#     for i, tag_tuple in enumerate(tag_list)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd0cb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5f3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_drop_scores(tag_list):\n",
    "\n",
    "    return [\n",
    "        [(tag_list[i][0], tag_list[i][1])]\n",
    "        for tag_tuple in tag_list\n",
    "        for i in range(len(tag_list))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac4df93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_make_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    matches = matcher(doc)\n",
    "    matches_list = []\n",
    "\n",
    "    for idx in range(len(matches)):\n",
    "        for match_id, start, end in matches:\n",
    "            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "                match = doc[matches[idx][1]: matches[idx][2]].text\n",
    "                matches_list.append(match.lower())\n",
    "\n",
    "    return [tuple(ngrams.split()) for ngrams in list(set(matches_list))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gram_type = 'bigram'\n",
    "\n",
    "# def spacy_matches_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "#     doc = nlp(sentence)\n",
    "#     matches = matcher(doc)\n",
    "\n",
    "#     matches_dict = {}\n",
    "\n",
    "#     for idx, (match_id, start, end) in enumerate(matches):\n",
    "#         print(matches[idx], doc[matches[idx][1]:matches[idx][2]].text)\n",
    "#         print('-'*20)\n",
    "# #         if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "# #             matches_dict[doc[matches[idx][start]: matches[idx][end]].text] = matches[idx][start], matches[idx][end]\n",
    "            \n",
    "# #             print(match_id, start, end)\n",
    "# #         matches_dict[doc[matches[idx][start]: matches[idx][end]].text] = matches[idx][start], matches[idx][end]\n",
    "# # #     matches_list.append([matches_dict])\n",
    "    \n",
    "# #     return matches_dict\n",
    "#     print(matches_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_1grams'] = df_jobs['Job Description spacy_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ngrams_list in df_jobs[f'Job Description spacy_2grams']:\n",
    "    for ngram_tuple in ngrams_list:\n",
    "        print('_'.join(ngram_tuple))\n",
    "#             print(f'{ngram_tuple[i]} {ngram_tuple[i+1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "patters_dict = {\n",
    "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
    "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
    "}\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3,\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence:\n",
    "        spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            ' '.join(ngram_tuple): '_'.join(ngram_tuple)\n",
    "            for ngrams_list in df_jobs[f'Job Description spacy_2grams']\n",
    "            for ngram_tuple in ngrams_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if f'{ngram_name}_patterns' in matcher:\n",
    "        matcher.remove(f'{ngram_name}_patterns')\n",
    "    assert f'{ngram_name}_patterns' not in matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "    ' '.join(ngram_tuple): '_'.join(ngram_tuple)\n",
    "    for ngrams_list in df_jobs[f'Job Description spacy_2grams']\n",
    "    for ngram_tuple in ngrams_list\n",
    "}\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for ngrams_list in df_jobs[f'Job Description spacy_2grams']:\n",
    "    for ngram_tuple in ngrams_list:\n",
    "        print(' '.join(ngram_tuple))\n",
    "#         print('_'.join(ngram_tuple))\n",
    "#             print(f'{ngram_tuple[i]} {ngram_tuple[i+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6f8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].replace(\n",
    "    regex = {\n",
    "        ngram: '_'.join(ngram_tuple)\n",
    "        for ngrams_list in df_jobs[f'Job Description spacy_2grams']\n",
    "        for ngram_tuple in ngrams_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1603c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description spacy_bigram_lists'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "#     lambda sentence:\n",
    "#     spacy_make_ngrams(sentence, matcher, 'bigram')\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf70fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'bigram_patterns' in matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0170801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add(f'bigram_patterns', [[{'POS': i} for i in j] for j in bigram_rules])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # INCASE\n",
    "# # Spacy Trigrams\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# trigram_rules = [\n",
    "#     ['VERB', 'ADJ', 'NOUN'],\n",
    "#     ['NOUN', 'VERB', 'ADV'],\n",
    "#     ['NOUN', 'ADP', 'NOUN'],\n",
    "#     # more rules here...\n",
    "# ]\n",
    "\n",
    "# bigram_patterns = [[{'POS': i} for i in j] for j in bigram_rules]\n",
    "\n",
    "# matcher.add('bigram_patterns', bigram_patterns)\n",
    "\n",
    "# df_jobs['Job Description spacy_2grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "#     lambda sentence:\n",
    "#     spacy_make_ngrams(sentence, matcher, 'bigram')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description spacy_2grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].replace(\n",
    "#     regex = {\n",
    "#         bigram: '_'.join(bigram.split())\n",
    "#         for bigrams_list in df_jobs['Job Description spacy_2grams'].to_list()\n",
    "#         for bigram in bigrams_list\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_2grams_in_sent'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1aa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_1grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbae91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "for idx, tokens in df_jobs['Job Description nltk_1grams'].items():\n",
    "#     print(tokens)\n",
    "    print(list(ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54067bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "df_jobs['Job Description nltk_1grams'] = df_jobs['Job Description nltk_tokenized']\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "# NLTK bi and trigrams\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams'] = df_jobs['Job Description nltk_1grams'].apply(\n",
    "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
    "    )\n",
    "    \n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            ' '.join(ngram_tuple): '_'.join(ngram_tuple)\n",
    "            for ngrams_list in df_jobs[f'Job Description nltk_{str(ngram_num)}grams']\n",
    "            for ngram_tuple in ngrams_list\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "\n",
    "for ngrams_list in df_jobs[f'Job Description nltk_2grams']:\n",
    "    for ngram_tuple in ngrams_list:\n",
    "#         print(ngram_tuple)\n",
    "        print('_'.join(ngram_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description nltk_2grams'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb49e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description nltk_2grams_in_sent'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5be905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32c373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ngram_name, ngram_num in ngram_dict.items():\n",
    "df_jobs[f'Job Description nltk_2grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "    regex = {\n",
    "        ' '.join(ngram_tuple): '_'.join(ngram_tuple)\n",
    "        for ngrams_list in df_jobs[f'Job Description nltk_2grams']\n",
    "        for ngram_tuple in ngrams_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description nltk_2grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d31206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_1grams'] = df_jobs['Job Description gensim_tokenized']\n",
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description gensim_1grams'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2grams'] = bigram[df_jobs['Job Description gensim_1grams']]\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2grams'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3grams'] = trigram[df_jobs['Job Description gensim_2grams']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335d6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description gensim_2grams'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for ngrams_list in df_jobs[f'Job Description gensim_2grams']:\n",
    "    for ngrams in ngrams_list:\n",
    "        if '_' in ngrams:\n",
    "            print(ngrams)\n",
    "            print(' '.join(ngrams.split('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_jobs[f'Job Description gensim_2grams_in_sent'] = df_jobs['Job Description gensim_1grams'].str.lower().replace(\n",
    "        regex = {\n",
    "            ' '.join(ngrams.split('_')): ngrams\n",
    "            for ngrams_list in df_jobs[f'Job Description gensim_2grams']\n",
    "            for ngrams in ngrams_list\n",
    "            if '_' in ngrams\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_1grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description gensim_2grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e96190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description gensim_2grams_in_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description spacy_2grams_in_sent'] = df_jobs['Job Description spacy_2grams_in_sent'].apply(\n",
    "#     lambda sentence:\n",
    "#     sentence.split()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fed315",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lists = df_jobs['Job Description spacy_bigram_dicts'].to_list()\n",
    "bigram_dicts = {}\n",
    "\n",
    "for bigrams_list in bigram_lists:\n",
    "    for bigram in bigrams_list:\n",
    "        bigram_dicts[bigram] = '_'.join(bigram.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8fe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dicts = {\n",
    "    bigram: '_'.join(bigram.split())\n",
    "    for bigrams_list in df_jobs['Job Description spacy_bigram_lists'].to_list()\n",
    "    for bigram in bigrams_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89604c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_2grams'] = df_jobs['Job Description spacy_sentencized'].replace(\n",
    "    regex = {\n",
    "        bigram: '_'.join(bigram.split())\n",
    "        for bigrams_list in df_jobs['Job Description spacy_bigram_lists'].to_list()\n",
    "        for bigram in bigrams_list\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56653a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_2grams'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308df63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
    "    )\n",
    "    \n",
    "    print(df_jobs[f'Job Description nltk_{str(ngram_num)}grams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigrams'] = df_jobs['Job Description spacy_bigrams'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f072150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bigram_dicts.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48daed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "for d in bigram_dicts.to_list():\n",
    "    for k, v in d.items():\n",
    "        x[k] = v\n",
    "    x.update(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {\n",
    "    key: value for bigram_dict in bigram_dicts.to_list() for key, value in bigram_dict.items() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eecbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b7b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigrams'] = df_jobs['Job Description spacy_sentencized'].replace(\n",
    "    regex = {\n",
    "        key: value for bigram_dict in bigram_dicts.to_list() for key, value in bigram_dict.items()\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized'].replace(regex=x)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b539884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram_dicts'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'bigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c25c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram_dicts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511abe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description spacy_sentencized'].replace(bigram_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab92445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dicts = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'bigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dicts#.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for dic in bigram_dicts:\n",
    "    d.update(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af783573",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce9b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_sentencized'].replace(d)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram_dicts'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spacy_matches_ngrams(sentence, matcher, gram_type):\n",
    "\n",
    "#     doc = nlp(sentence)\n",
    "#     matches = {}\n",
    "\n",
    "#     for match_id, start, end in matcher(doc):\n",
    "#         if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "#             match = doc[start:end].text\n",
    "#             matches[match] = '_'.join(match.split())\n",
    "\n",
    "#     for idx, (key, value) in enumerate(matches.items()):\n",
    "#         if key in doc.text:\n",
    "#             print(' '.join(doc.text.split()).replace(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'bigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c082025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04056bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_matches_ngrams(sentence, matcher, gram_type):\n",
    "    starts = []\n",
    "    ends = []\n",
    "    matches = []\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "            starts.append(start)\n",
    "            ends.append(end)\n",
    "    for start, end in zip(starts, ends):\n",
    "        matches.append(doc[start:end].text)\n",
    "    new_doc = [doc.text if doc.text.split() not in matches else doc.text.replace(doc.text, '_'.join(match.split()))]\n",
    "    return new_doc\n",
    "\n",
    "#         print(f'{doc[:start].text} {matches} {doc[end:].text}')\n",
    "#     if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
    "#         matches = '_'.join(doc[start:end].text.split())\n",
    "#         x.append(matches)\n",
    "#     print(f'{doc[:start].text} {matches} {doc[end:].text}')\n",
    "#             return f'{doc[start-start:].text} {matches} {doc[:end].text}'\n",
    "        \n",
    "#         new_doc = f'{doc[:start].text} {matches} {doc[end:].text}'\n",
    "#         x.append(new_doc)\n",
    "#         print(x)\n",
    "#             print(f'{doc.text[:start]} {doc.text[:end]}', matches)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[['Job Description spacy_bigram', 'Job Description spacy_sentencized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n",
    "\n",
    "matcher.add('trigram_patterns', trigram_patterns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f662a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence:\n",
    "    spacy_matches_ngrams(sentence, matcher, 'bigram')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e30dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_bigram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613f737",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentences[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)\n",
    "\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96abae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = [\n",
    "    nlp(sentence)[start:end].text\n",
    "    for sentence in sentences\n",
    "    for match_id, start, end in matcher(nlp(sentence))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfac3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    \n",
    "    # Get string representation\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "\n",
    "    # The matched span\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    print(repr(doc[start:end].text))\n",
    "    print(match_id, string_id, start, end)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns = {\n",
    "#     \"noun_verb\": [{\"POS\": \"NOUN\"}, {\"POS\": \"VERB\"}],\n",
    "#     \"verb_noun\": [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}],\n",
    "#     \"adj_noun\": [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "#     \"adj_propn\": [{\"POS\": \"ADJ\"}, {\"POS\": \"PROPN\"}],\n",
    "# }\n",
    "\n",
    "# for pattern_name, pattern in bigram_patterns.items():\n",
    "#     print\n",
    "# #     matcher.add(pattern_name, [pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern_name, pattern in trigram_patterns.items():\n",
    "    matcher.add(pattern_name, [pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e002bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_description in job_descriptions:\n",
    "    doc = nlp(job_description)\n",
    "    matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_description in job_descriptions:\n",
    "    spacy_bigrams = [\n",
    "        nlp(job_description)[start:end].text\n",
    "        for _, start, end in matcher(nlp(job_description))\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    \n",
    "    # Get string representation\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "\n",
    "    # The matched span\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    print(repr(span.text))\n",
    "    print(match_id, string_id, start, end)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e73cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc44cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_1grams'] = df_jobs['Job Description spacy_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "bigram_patterns = [[{'POS': i} for i in j] for j in bigram_rules]\n",
    "trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', bigram_patterns)\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_sentencized_cleaned'].apply(\n",
    "        lambda sentence:\n",
    "        spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized_cleaned'].replace(\n",
    "        regex = {\n",
    "            ngram: '_'.join(ngram.split())\n",
    "            for ngrams_list in df_jobs[f'Job Description spacy_{str(ngram_num)}grams'].to_list()\n",
    "            for ngram in ngrams_list\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6595ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in spacy_sentencized:\n",
    "#     print(model.encode(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.electra.modeling_tf_electra import TFElectraMainLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pavanchhatpar/electra-base-sentence-splitter\")\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained(\"pavanchhatpar/electra-base-sentence-splitter\", from_tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ee3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('sentence-splitter', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca787e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "# NLTK Bigrams\n",
    "df_jobs[f'Job Description nltk_3grams'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: list(trigrams(tokens))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52d9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_1grams'] = df_jobs['Job Description gensim_tokenized']\n",
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description gensim_1grams'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2garms'] = bigram[df_jobs['Job Description gensim_1grams']]\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2garms'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3garms'] = trigram[df_jobs['Job Description gensim_2garms']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_3garms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description nltk_3grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCASE BIGRAMS\n",
    "# # Spacy Trigrams\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# trigram_rules = [\n",
    "#     ['VERB', 'ADJ', 'NOUN'],\n",
    "#     ['NOUN', 'VERB', 'ADV'],\n",
    "#     ['NOUN', 'ADP', 'NOUN'],\n",
    "#     # more rules here...\n",
    "# ]\n",
    "\n",
    "# trigram_patterns = [[{'POS': i} for i in j] for j in trigram_rules]\n",
    "\n",
    "# matcher.add('trigram_patterns', trigram_patterns)\n",
    "\n",
    "# df_jobs['Job Description spacy_3grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "#     lambda sentence:\n",
    "#     spacy_make_ngrams(sentence, matcher, 'trigram')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description spacy_3grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].replace(\n",
    "#     regex = {\n",
    "#         trigram: '_'.join(trigram.split())\n",
    "#         for trigrams_list in df_jobs['Job Description spacy_3grams'].to_list()\n",
    "#         for trigram in trigrams_list\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# # NLTK Trigrams\n",
    "# df_jobs[f'Job Description nltk_3grams'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "#     lambda tokens: list(trigrams(tokens))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455414f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description spacy_sentencized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2garms'] = bigram[df_jobs['Job Description spacy_tokenized']]\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2garms'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3garms'] = trigram[df_jobs['Job Description gensim_2garms']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e299eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_2garms'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46143416",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag, pos_tag_sents\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ngrams, bigrams, trigrams\n\u001b[0;32m---> 33\u001b[0m nltk_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mllm_path\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/nltk\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(nltk_path)\n\u001b[1;32m     36\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m, download_dir \u001b[38;5;241m=\u001b[39m nltk_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_path' is not defined"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "random.seed(42)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ee1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_1grams'] = df_jobs['Job Description spacy_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_2grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d92375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abs_frequency(row, text_col, ngram_number, embedding_library):\n",
    "\n",
    "    abs_word_freq = defaultdict(int)\n",
    "    for word in row[f'Job Description {embedding_library}_{ngram_number}grams']:\n",
    "        abs_word_freq[word] += 1\n",
    "\n",
    "        abs_wtd_df = (\n",
    "            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
    "            .rename(columns={0: 'abs_word_freq'})\n",
    "            .sort_values(by=['abs_word_freq'], ascending=False)\n",
    "            )\n",
    "        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
    "        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
    "\n",
    "        row[f'Job Description {embedding_library}_{ngram_number}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_number}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
    "        row[f'Job Description {embedding_library}_{ngram_number}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
    "\n",
    "    print(row)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_1grams'] = df_jobs['Job Description nltk_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d8177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK bi and trigrams\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "\n",
    "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams'] = df_jobs['Job Description nltk_1grams'].apply(\n",
    "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cfc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs[f'Job Description nltk_2grams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b041d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Allgrams\n",
    "df_jobs[f'Job Description nltk_123grams'] = df_jobs['Job Description nltk_1grams'] + df_jobs['Job Description nltk_2grams'] + df_jobs['Job Description nltk_3grams']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy bi and trigrams\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['NOUN', 'VERB'],\n",
    "    ['VERB', 'NOUN'],\n",
    "    ['ADJ', 'NOUN'],\n",
    "    ['ADJ', 'PROPN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "patters_dict = {\n",
    "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
    "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
    "}\n",
    "\n",
    "ngram_dict = {\n",
    "    'bigram': 2,\n",
    "    'trigram': 3,\n",
    "}\n",
    "\n",
    "for ngram_name, ngram_num in ngram_dict.items():\n",
    "    \n",
    "    \n",
    "    matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "        lambda sentence:\n",
    "        spacy_make_ngrams(sentence, matcher, ngram_name)\n",
    "    )\n",
    "\n",
    "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
    "        regex = {\n",
    "            ' '.join(ngram_tuple): '_'.join(ngram_tuple)\n",
    "            for ngrams_list in df_jobs[f'Job Description spacy_{str(ngram_num)}grams']\n",
    "            for ngram_tuple in ngrams_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if f'{ngram_name}_patterns' in matcher:\n",
    "        matcher.remove(f'{ngram_name}_patterns')\n",
    "    assert f'{ngram_name}_patterns' not in matcher\n",
    "df_jobs['Job Description spacy_123grams'] = df_jobs['Job Description spacy_1grams'] + df_jobs['Job Description spacy_2grams'] + df_jobs['Job Description spacy_3grams']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim bi and trigrams\n",
    "# Gensim Bigrams\n",
    "bigram = Phraser(Phrases(df_jobs['Job Description gensim_1grams'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_2grams'] = list(bigram[df_jobs['Job Description gensim_1grams']])\n",
    "\n",
    "# Gensim Trigrams\n",
    "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2grams'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
    "df_jobs['Job Description gensim_3grams'] = trigram[df_jobs['Job Description gensim_2grams']]\n",
    "df_jobs[f'Job Description gensim_123grams'] = df_jobs['Job Description gensim_1grams'] + df_jobs['Job Description gensim_2grams'] + df_jobs['Job Description gensim_3grams']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
    "\n",
    "for embedding_library, ngram_number in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_jobs = df_jobs.apply(lambda row: get_abs_frequency(row=row, text_col='Job Description spacy_tokenized', ngram_number=ngram_number, embedding_library=embedding_library), axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820eca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_123grams_abs_word_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e9159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07590228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_3grams_abs_word_perc_cum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import (CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec)\n",
    "\n",
    "ngrams_list=[1, 2, 3, 123]\n",
    "embedding_libraries_list = ['spacy', 'nltk']\n",
    "\n",
    "def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
    "\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = corpora.Dictionary(\n",
    "        [\n",
    "            list(ngram_)\n",
    "            if isinstance(ngram_, tuple)\n",
    "            else [ngram_]\n",
    "            if isinstance(ngram_, list)\n",
    "            else [ngram_]\n",
    "            for ngram_ in row[f'Job Description {embedding_library}_{ngram_num}grams']\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    row[f'Job Description {embedding_library}_{ngram_num}grams_corpus'] = [\n",
    "        row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'].doc2bow(\n",
    "            [\n",
    "                list(ngram_)\n",
    "                if isinstance(ngram_, tuple)\n",
    "                else [ngram_]\n",
    "                if isinstance(ngram_, list)\n",
    "                else ngram_\n",
    "                for ngram_ in row[f'Job Description {embedding_library}_{ngram_num}grams']\n",
    "\n",
    "            ],\n",
    "            return_missing=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "#     row[f'{ngram_num}grams_{embedding_library}_tfidf'] = TfidfModel(\n",
    "#         row[f'{ngram_num}grams_{embedding_library}_corpus']\n",
    "#     )\n",
    "\n",
    "    # row[f'{ngram_num}grams_{embedding_library}_tfidf_matrix'] = [\n",
    "    #     row[f'{ngram_num}grams_{embedding_library}_tfidf'][doc]\n",
    "    #     for doc in row[f'{ngram_num}grams_{embedding_library}_corpus']\n",
    "    # ]\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7424798",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_jobs.iterrows():\n",
    "#     print(row['Job Description spacy_2grams'])\n",
    "#     print(type(row['Job Description spacy_2grams']))\n",
    "#     print(corpora.Dictionary([row['Job Description spacy_1grams']]))\n",
    "    print(corpora.Dictionary(list(row['Job Description spacy_2grams'])))\n",
    "#     print(corpora.Dictionary(row['Job Description spacy_3grams']))\n",
    "#     print(corpora.Dictionary(row['Job Description spacy_123grams']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95db1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_3grams_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc505367",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_library, ngram_num in itertools.product(embedding_libraries_list, ngrams_list):\n",
    "    df_jobs = df_jobs.apply(lambda row: get_corpus_and_dictionary(row=row, ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d47205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_123grams_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0d68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_123grams_dictionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_jobs.iterrows():\n",
    "    for ngram_tuple in row['Job Description spacy_3grams']:\n",
    "        if isinstance(ngram_tuple, tuple):\n",
    "            ngram_tuple = list(ngram_tuple)\n",
    "#             print(ngram_tuple)\n",
    "\n",
    "    \n",
    "    \n",
    "    x = [\n",
    "        list(ngram_)\n",
    "        if isinstance(ngram_, tuple) or isinstance(ngram_, list)\n",
    "        else ngram_\n",
    "        for idx, row in df_jobs.iterrows()\n",
    "        for ngram_ in row['Job Description spacy_3grams']\n",
    "        \n",
    "        \n",
    "    ]\n",
    "\n",
    "#     for ngram_list in row['Job Description spacy_3grams']:\n",
    "#         if isinstance(ngram_list, tuple):\n",
    "#             print(list(ngram))\n",
    "#             print(type(list(ngram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb6be63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_3grams'].apply(\n",
    "    lambda ngram_list: [\n",
    "        list(ngram_tuple)\n",
    "        for ngram_tuple in ngram_list\n",
    "        if isinstance(ngram_tuple, tuple)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2087028c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
