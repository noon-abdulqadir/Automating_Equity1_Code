{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7d3efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "189dea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "pattern_numbers = r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21414866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygwalker as pyg\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "8c22dfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8818717b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62577"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "523b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "    lambda job_description: ' '.join(job_description.split('/')) if '/' in job_description else job_description\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "3727334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abb_dict = {\n",
    "    r'incl\\.': 'including', \n",
    "    r'e\\.g\\.': 'for example', \n",
    "    r'e\\.g': 'for example', \n",
    "    r'etc\\.': 'et cetera', \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a22cc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_fix_incl = 'Apply appropriate and effective communication methods to senior management and important stakeholders'\n",
    "str_fix_eg = 'Partner with Procurement in order to manage suppliers for the projects & programs in scope'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "f2fd4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description'] = df_jobs['Job Description'].replace(abb_dict, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7159677a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35      Purpose\\n\\nAs a Studio A Director\\n\\nSpecial P...\n",
       "3423    Purpose:\\nAs a Manager Digital Strategy and Pr...\n",
       "Name: Job Description, dtype: object"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description'][df_jobs['Job Description'].str.contains(str_fix_incl)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "db1a3934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Partner with Procurement in order to manage suppliers for the projects & programs in scope']"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(str_fix_eg, df_jobs['Job Description'][35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "952299c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(1461, 1551), match='Partner with Procurement in order to manage suppl>]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(re.finditer(str_fix_eg, df_jobs['Job Description'][35]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d35af326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Partner with Procurement in order to manage suppliers for the projects & programs in scope, for example interviews, sta'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description'][35][1461:1580]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "bd40a78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('incl.', 'including')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('e.g.', 'e.g')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('e.g', 'for example')\n",
    "# )\n",
    "\n",
    "# df_jobs['Job Description'] = df_jobs.loc[df_jobs['Job Description'].notnull(), 'Job Description'].apply(\n",
    "#     lambda job_description: job_description.replace('etc.', 'et cetera')\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "15d45507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "1039d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs.sample(n=100, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f7b23278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_glob_paths_10.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "c9c9ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62577 entries, 0 to 62576\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Search Keyword     62577 non-null  object \n",
      " 1   Platform           62577 non-null  object \n",
      " 2   Job ID             62577 non-null  object \n",
      " 3   Job Title          62577 non-null  object \n",
      " 4   Company Name       62574 non-null  object \n",
      " 5   Location           62577 non-null  object \n",
      " 6   Job Description    62577 non-null  object \n",
      " 7   Rating             3975 non-null   float64\n",
      " 8   Employment Type    61995 non-null  object \n",
      " 9   Company URL        59263 non-null  object \n",
      " 10  Job URL            62577 non-null  object \n",
      " 11  Job Age            62577 non-null  object \n",
      " 12  Job Age Number     62577 non-null  object \n",
      " 13  Collection Date    62577 non-null  object \n",
      " 14  Data Row           58599 non-null  float64\n",
      " 15  Tracking ID        58599 non-null  object \n",
      " 16  Industry           59184 non-null  object \n",
      " 17  Job Date           58602 non-null  object \n",
      " 18  Type of ownership  582 non-null    object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1fee089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name',\n",
       "       'Location', 'Job Description', 'Rating', 'Employment Type',\n",
       "       'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
       "       'Collection Date', 'Data Row', 'Tracking ID', 'Industry', 'Job Date',\n",
       "       'Type of ownership'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "e79fca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = pd.read_pickle(f'{scraped_data}CBS/Data/Sectors Output from script.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "fa8d54fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Sector Name</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Keywords Count</th>\n",
       "      <th>% per Sector</th>\n",
       "      <th>% per Social Category</th>\n",
       "      <th>% per Workforce</th>\n",
       "      <th>Gender_Female_n</th>\n",
       "      <th>Gender_Female_% per Sector</th>\n",
       "      <th>Gender_Female_% per Social Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Age_Older (&gt;= 45 years)_% per Sector</th>\n",
       "      <th>Age_Older (&gt;= 45 years)_% per Social Category</th>\n",
       "      <th>Age_Older (&gt;= 45 years)_% per Workforce</th>\n",
       "      <th>Age_Younger (&lt; 45 years)_n</th>\n",
       "      <th>Age_Younger (&lt; 45 years)_% per Sector</th>\n",
       "      <th>Age_Younger (&lt; 45 years)_% per Social Category</th>\n",
       "      <th>Age_Younger (&lt; 45 years)_% per Workforce</th>\n",
       "      <th>Age_Sectoral Age Segregation_Dominant Category</th>\n",
       "      <th>n</th>\n",
       "      <th>% Sector per Workforce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-U</td>\n",
       "      <td>All economic activities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4029.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4892.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8391.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture and industry</td>\n",
       "      <td>[forestry, agriculture, fishing]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.221587</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493209</td>\n",
       "      <td>0.064905</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>708.0</td>\n",
       "      <td>0.506076</td>\n",
       "      <td>0.048394</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>1399.0</td>\n",
       "      <td>0.055362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>Industry (no construction), energy</td>\n",
       "      <td>[mining, quarry, quarrying]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>423.0</td>\n",
       "      <td>0.209199</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527201</td>\n",
       "      <td>0.100273</td>\n",
       "      <td>0.042184</td>\n",
       "      <td>954.0</td>\n",
       "      <td>0.471810</td>\n",
       "      <td>0.065208</td>\n",
       "      <td>0.037752</td>\n",
       "      <td>Older</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>0.080016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>[gas supply, industry, managing director, prod...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>174.0</td>\n",
       "      <td>0.227154</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.038849</td>\n",
       "      <td>0.016343</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.462141</td>\n",
       "      <td>0.024197</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>Older</td>\n",
       "      <td>766.0</td>\n",
       "      <td>0.030313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D</td>\n",
       "      <td>Energy supply</td>\n",
       "      <td>[energy, energy supply, electricity]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.001148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E</td>\n",
       "      <td>Water supply and waste management</td>\n",
       "      <td>[waste management, water, water stock, water s...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>Older</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.001425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "      <td>Construction</td>\n",
       "      <td>[construction, build]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.529762</td>\n",
       "      <td>0.012167</td>\n",
       "      <td>0.007044</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>336.0</td>\n",
       "      <td>0.013296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>G</td>\n",
       "      <td>Commercial services</td>\n",
       "      <td>[retail, selling, retail trade, wholesale, man...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>3421.0</td>\n",
       "      <td>0.431345</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340941</td>\n",
       "      <td>0.254350</td>\n",
       "      <td>0.107004</td>\n",
       "      <td>5228.0</td>\n",
       "      <td>0.659185</td>\n",
       "      <td>0.357348</td>\n",
       "      <td>0.206886</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>7931.0</td>\n",
       "      <td>0.313850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H</td>\n",
       "      <td>Transportation and storage</td>\n",
       "      <td>[transportation, logistics, transport, storage...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.012953</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.246114</td>\n",
       "      <td>0.007905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531088</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.468912</td>\n",
       "      <td>0.012372</td>\n",
       "      <td>0.007163</td>\n",
       "      <td>Older</td>\n",
       "      <td>386.0</td>\n",
       "      <td>0.015275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I</td>\n",
       "      <td>Accommodation and food serving</td>\n",
       "      <td>[food, hotel, serving, service, accommodation,...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.007055</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>320.0</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.021873</td>\n",
       "      <td>0.012663</td>\n",
       "      <td>Younger</td>\n",
       "      <td>396.0</td>\n",
       "      <td>0.015671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>J</td>\n",
       "      <td>Information and communication</td>\n",
       "      <td>[database, network specialist, director, commu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.013329</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>290.0</td>\n",
       "      <td>0.011476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>K</td>\n",
       "      <td>Financial institutions</td>\n",
       "      <td>[commercial service, commercial, financial ins...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.010949</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.394161</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532847</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.005778</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.467153</td>\n",
       "      <td>0.008749</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>Older</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.010843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>L</td>\n",
       "      <td>Renting, buying, selling real estate</td>\n",
       "      <td>[selling real estate, renting, rental, sales r...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>Older</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.002691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>M</td>\n",
       "      <td>Business services</td>\n",
       "      <td>[sociologist, electrotechnical engineer, lingu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>857.0</td>\n",
       "      <td>0.421337</td>\n",
       "      <td>0.071316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356932</td>\n",
       "      <td>0.068291</td>\n",
       "      <td>0.028730</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>0.642576</td>\n",
       "      <td>0.089337</td>\n",
       "      <td>0.051721</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>2034.0</td>\n",
       "      <td>0.080491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>N</td>\n",
       "      <td>Renting and other business support</td>\n",
       "      <td>[honorary service activity, specialised busine...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>416.0</td>\n",
       "      <td>0.427544</td>\n",
       "      <td>0.034618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319630</td>\n",
       "      <td>0.029254</td>\n",
       "      <td>0.012307</td>\n",
       "      <td>660.0</td>\n",
       "      <td>0.678314</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.026118</td>\n",
       "      <td>Younger</td>\n",
       "      <td>973.0</td>\n",
       "      <td>0.038504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O</td>\n",
       "      <td>Government and care</td>\n",
       "      <td>[government, public administration]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>3970.0</td>\n",
       "      <td>0.681545</td>\n",
       "      <td>0.330365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488240</td>\n",
       "      <td>0.267520</td>\n",
       "      <td>0.112545</td>\n",
       "      <td>2981.0</td>\n",
       "      <td>0.511760</td>\n",
       "      <td>0.203759</td>\n",
       "      <td>0.117966</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>5825.0</td>\n",
       "      <td>0.230510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P</td>\n",
       "      <td>Education</td>\n",
       "      <td>[primary school teacher, education, teacher, h...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.011070</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>353.0</td>\n",
       "      <td>0.651292</td>\n",
       "      <td>0.029375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466790</td>\n",
       "      <td>0.023798</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.531365</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>542.0</td>\n",
       "      <td>0.021448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Q</td>\n",
       "      <td>Health and social work activities</td>\n",
       "      <td>[psychologist, social work, doctor, care, soci...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.007676</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>1208.0</td>\n",
       "      <td>0.842987</td>\n",
       "      <td>0.100524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461270</td>\n",
       "      <td>0.062177</td>\n",
       "      <td>0.026157</td>\n",
       "      <td>770.0</td>\n",
       "      <td>0.537334</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>1433.0</td>\n",
       "      <td>0.056708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>R</td>\n",
       "      <td>Culture, recreation, other services</td>\n",
       "      <td>[culture, catering, sport, recreation]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.567839</td>\n",
       "      <td>0.018807</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389447</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>0.006134</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.605528</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.009537</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>398.0</td>\n",
       "      <td>0.015750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>S</td>\n",
       "      <td>Other service activities</td>\n",
       "      <td>[staff]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.007240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>Mixed Age</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Total (excluding A-U)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>12017.0</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420696</td>\n",
       "      <td>14630.0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25270.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Code                           Sector Name  \\\n",
       "0   A-U               All economic activities   \n",
       "1     A              Agriculture and industry   \n",
       "2     B    Industry (no construction), energy   \n",
       "3     C                         Manufacturing   \n",
       "4     D                         Energy supply   \n",
       "5     E     Water supply and waste management   \n",
       "6     F                          Construction   \n",
       "7     G                   Commercial services   \n",
       "8     H            Transportation and storage   \n",
       "9     I        Accommodation and food serving   \n",
       "10    J         Information and communication   \n",
       "11    K                Financial institutions   \n",
       "12    L  Renting, buying, selling real estate   \n",
       "13    M                     Business services   \n",
       "14    N    Renting and other business support   \n",
       "15    O                   Government and care   \n",
       "16    P                             Education   \n",
       "17    Q     Health and social work activities   \n",
       "18    R   Culture, recreation, other services   \n",
       "19    S              Other service activities   \n",
       "20  NaN                 Total (excluding A-U)   \n",
       "\n",
       "                                             Keywords  Keywords Count  \\\n",
       "0                                                 NaN             NaN   \n",
       "1                    [forestry, agriculture, fishing]             3.0   \n",
       "2                         [mining, quarry, quarrying]             3.0   \n",
       "3   [gas supply, industry, managing director, prod...             7.0   \n",
       "4                [energy, energy supply, electricity]             3.0   \n",
       "5   [waste management, water, water stock, water s...             4.0   \n",
       "6                               [construction, build]             2.0   \n",
       "7   [retail, selling, retail trade, wholesale, man...            11.0   \n",
       "8   [transportation, logistics, transport, storage...             5.0   \n",
       "9   [food, hotel, serving, service, accommodation,...             7.0   \n",
       "10  [database, network specialist, director, commu...             9.0   \n",
       "11  [commercial service, commercial, financial ins...             3.0   \n",
       "12  [selling real estate, renting, rental, sales r...             5.0   \n",
       "13  [sociologist, electrotechnical engineer, lingu...             5.0   \n",
       "14  [honorary service activity, specialised busine...             8.0   \n",
       "15                [government, public administration]             2.0   \n",
       "16  [primary school teacher, education, teacher, h...             6.0   \n",
       "17  [psychologist, social work, doctor, care, soci...            11.0   \n",
       "18             [culture, catering, sport, recreation]             4.0   \n",
       "19                                            [staff]             1.0   \n",
       "20                                                NaN            99.0   \n",
       "\n",
       "    % per Sector  % per Social Category  % per Workforce  Gender_Female_n  \\\n",
       "0            NaN                    NaN              NaN           4029.0   \n",
       "1       0.002144               0.030303         0.000119            310.0   \n",
       "2       0.001484               0.030303         0.000119            423.0   \n",
       "3       0.009138               0.070707         0.000277            174.0   \n",
       "4       0.103448               0.030303         0.000119              8.0   \n",
       "5       0.111111               0.040404         0.000158              7.0   \n",
       "6       0.005952               0.020202         0.000079             42.0   \n",
       "7       0.001387               0.111111         0.000435           3421.0   \n",
       "8       0.012953               0.050505         0.000198             95.0   \n",
       "9       0.017677               0.070707         0.000277            199.0   \n",
       "10      0.031034               0.090909         0.000356             80.0   \n",
       "11      0.010949               0.030303         0.000119            108.0   \n",
       "12      0.073529               0.050505         0.000198             33.0   \n",
       "13      0.002458               0.050505         0.000198            857.0   \n",
       "14      0.008222               0.080808         0.000317            416.0   \n",
       "15      0.000343               0.020202         0.000079           3970.0   \n",
       "16      0.011070               0.060606         0.000237            353.0   \n",
       "17      0.007676               0.111111         0.000435           1208.0   \n",
       "18      0.010050               0.040404         0.000158            226.0   \n",
       "19      0.007576               0.010101         0.000040             87.0   \n",
       "20      0.003918               1.000000         0.003918          12017.0   \n",
       "\n",
       "    Gender_Female_% per Sector  Gender_Female_% per Social Category  ...  \\\n",
       "0                          NaN                                  NaN  ...   \n",
       "1                     0.221587                             0.025797  ...   \n",
       "2                     0.209199                             0.035200  ...   \n",
       "3                     0.227154                             0.014479  ...   \n",
       "4                     0.275862                             0.000666  ...   \n",
       "5                     0.194444                             0.000583  ...   \n",
       "6                     0.125000                             0.003495  ...   \n",
       "7                     0.431345                             0.284680  ...   \n",
       "8                     0.246114                             0.007905  ...   \n",
       "9                     0.502525                             0.016560  ...   \n",
       "10                    0.275862                             0.006657  ...   \n",
       "11                    0.394161                             0.008987  ...   \n",
       "12                    0.485294                             0.002746  ...   \n",
       "13                    0.421337                             0.071316  ...   \n",
       "14                    0.427544                             0.034618  ...   \n",
       "15                    0.681545                             0.330365  ...   \n",
       "16                    0.651292                             0.029375  ...   \n",
       "17                    0.842987                             0.100524  ...   \n",
       "18                    0.567839                             0.018807  ...   \n",
       "19                    0.659091                             0.007240  ...   \n",
       "20                    0.475544                             1.000000  ...   \n",
       "\n",
       "    Age_Older (>= 45 years)_% per Sector  \\\n",
       "0                                    NaN   \n",
       "1                               0.493209   \n",
       "2                               0.527201   \n",
       "3                               0.539164   \n",
       "4                               0.517241   \n",
       "5                               0.583333   \n",
       "6                               0.476190   \n",
       "7                               0.340941   \n",
       "8                               0.531088   \n",
       "9                               0.189394   \n",
       "10                              0.327586   \n",
       "11                              0.532847   \n",
       "12                              0.529412   \n",
       "13                              0.356932   \n",
       "14                              0.319630   \n",
       "15                              0.488240   \n",
       "16                              0.466790   \n",
       "17                              0.461270   \n",
       "18                              0.389447   \n",
       "19                              0.416667   \n",
       "20                              0.420696   \n",
       "\n",
       "    Age_Older (>= 45 years)_% per Social Category  \\\n",
       "0                                             NaN   \n",
       "1                                        0.064905   \n",
       "2                                        0.100273   \n",
       "3                                        0.038849   \n",
       "4                                        0.001411   \n",
       "5                                        0.001975   \n",
       "6                                        0.015050   \n",
       "7                                        0.254350   \n",
       "8                                        0.019283   \n",
       "9                                        0.007055   \n",
       "10                                       0.008936   \n",
       "11                                       0.013733   \n",
       "12                                       0.003386   \n",
       "13                                       0.068291   \n",
       "14                                       0.029254   \n",
       "15                                       0.267520   \n",
       "16                                       0.023798   \n",
       "17                                       0.062177   \n",
       "18                                       0.014580   \n",
       "19                                       0.005174   \n",
       "20                                       1.000000   \n",
       "\n",
       "    Age_Older (>= 45 years)_% per Workforce  Age_Younger (< 45 years)_n  \\\n",
       "0                                       NaN                      4892.0   \n",
       "1                                  0.027305                       708.0   \n",
       "2                                  0.042184                       954.0   \n",
       "3                                  0.016343                       354.0   \n",
       "4                                  0.000594                        13.0   \n",
       "5                                  0.000831                        16.0   \n",
       "6                                  0.006332                       178.0   \n",
       "7                                  0.107004                      5228.0   \n",
       "8                                  0.008112                       181.0   \n",
       "9                                  0.002968                       320.0   \n",
       "10                                 0.003759                       195.0   \n",
       "11                                 0.005778                       128.0   \n",
       "12                                 0.001425                        33.0   \n",
       "13                                 0.028730                      1307.0   \n",
       "14                                 0.012307                       660.0   \n",
       "15                                 0.112545                      2981.0   \n",
       "16                                 0.010012                       288.0   \n",
       "17                                 0.026157                       770.0   \n",
       "18                                 0.006134                       241.0   \n",
       "19                                 0.002176                        75.0   \n",
       "20                                 0.420696                     14630.0   \n",
       "\n",
       "    Age_Younger (< 45 years)_% per Sector  \\\n",
       "0                                     NaN   \n",
       "1                                0.506076   \n",
       "2                                0.471810   \n",
       "3                                0.462141   \n",
       "4                                0.448276   \n",
       "5                                0.444444   \n",
       "6                                0.529762   \n",
       "7                                0.659185   \n",
       "8                                0.468912   \n",
       "9                                0.808081   \n",
       "10                               0.672414   \n",
       "11                               0.467153   \n",
       "12                               0.485294   \n",
       "13                               0.642576   \n",
       "14                               0.678314   \n",
       "15                               0.511760   \n",
       "16                               0.531365   \n",
       "17                               0.537334   \n",
       "18                               0.605528   \n",
       "19                               0.568182   \n",
       "20                               0.578947   \n",
       "\n",
       "   Age_Younger (< 45 years)_% per Social Category  \\\n",
       "0                                             NaN   \n",
       "1                                        0.048394   \n",
       "2                                        0.065208   \n",
       "3                                        0.024197   \n",
       "4                                        0.000889   \n",
       "5                                        0.001094   \n",
       "6                                        0.012167   \n",
       "7                                        0.357348   \n",
       "8                                        0.012372   \n",
       "9                                        0.021873   \n",
       "10                                       0.013329   \n",
       "11                                       0.008749   \n",
       "12                                       0.002256   \n",
       "13                                       0.089337   \n",
       "14                                       0.045113   \n",
       "15                                       0.203759   \n",
       "16                                       0.019686   \n",
       "17                                       0.052632   \n",
       "18                                       0.016473   \n",
       "19                                       0.005126   \n",
       "20                                       1.000000   \n",
       "\n",
       "    Age_Younger (< 45 years)_% per Workforce  \\\n",
       "0                                        NaN   \n",
       "1                                   0.028017   \n",
       "2                                   0.037752   \n",
       "3                                   0.014009   \n",
       "4                                   0.000514   \n",
       "5                                   0.000633   \n",
       "6                                   0.007044   \n",
       "7                                   0.206886   \n",
       "8                                   0.007163   \n",
       "9                                   0.012663   \n",
       "10                                  0.007717   \n",
       "11                                  0.005065   \n",
       "12                                  0.001306   \n",
       "13                                  0.051721   \n",
       "14                                  0.026118   \n",
       "15                                  0.117966   \n",
       "16                                  0.011397   \n",
       "17                                  0.030471   \n",
       "18                                  0.009537   \n",
       "19                                  0.002968   \n",
       "20                                  0.578947   \n",
       "\n",
       "    Age_Sectoral Age Segregation_Dominant Category        n  \\\n",
       "0                                              NaN   8391.0   \n",
       "1                                        Mixed Age   1399.0   \n",
       "2                                            Older   2022.0   \n",
       "3                                            Older    766.0   \n",
       "4                                        Mixed Age     29.0   \n",
       "5                                            Older     36.0   \n",
       "6                                        Mixed Age    336.0   \n",
       "7                                        Mixed Age   7931.0   \n",
       "8                                            Older    386.0   \n",
       "9                                          Younger    396.0   \n",
       "10                                       Mixed Age    290.0   \n",
       "11                                           Older    274.0   \n",
       "12                                           Older     68.0   \n",
       "13                                       Mixed Age   2034.0   \n",
       "14                                         Younger    973.0   \n",
       "15                                       Mixed Age   5825.0   \n",
       "16                                       Mixed Age    542.0   \n",
       "17                                       Mixed Age   1433.0   \n",
       "18                                       Mixed Age    398.0   \n",
       "19                                       Mixed Age    132.0   \n",
       "20                                             NaN  25270.0   \n",
       "\n",
       "    % Sector per Workforce  \n",
       "0                      NaN  \n",
       "1                 0.055362  \n",
       "2                 0.080016  \n",
       "3                 0.030313  \n",
       "4                 0.001148  \n",
       "5                 0.001425  \n",
       "6                 0.013296  \n",
       "7                 0.313850  \n",
       "8                 0.015275  \n",
       "9                 0.015671  \n",
       "10                0.011476  \n",
       "11                0.010843  \n",
       "12                0.002691  \n",
       "13                0.080491  \n",
       "14                0.038504  \n",
       "15                0.230510  \n",
       "16                0.021448  \n",
       "17                0.056708  \n",
       "18                0.015750  \n",
       "19                0.005224  \n",
       "20                1.000000  \n",
       "\n",
       "[21 rows x 27 columns]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "3bdc3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors.columns = [\n",
    "    '_'.join(col) \n",
    "    if 'SBI Sector Titles' not in col \n",
    "    and 'Total Workforce' not in col \n",
    "    else col[-1] \n",
    "    for col in df_sectors.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "093a5642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_sectors['Keywords'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "60250946",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sectors = df_sectors.explode(\n",
    "    'Keywords', ignore_index=True\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "dad95a5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[652], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_sectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSearch Keyword\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mKeywords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:707\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m    703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/study1_3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1340\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1336\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1337\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1339\u001b[0m     ):\n\u001b[0;32m-> 1340\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "pd.merge(\n",
    "    df_jobs, df_sectors, left_on='Search Keyword', right_on='Keywords', right_index=True, how='left', sort=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "1ec54be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/46/q15p556n1dd63z6gkwyh896c0000gn/T/ipykernel_35937/3937353499.py:3: FutureWarning: merging between different levels is deprecated and will be removed in a future version. (3 levels on the left, 1 on the right)\n",
      "  df3 = df_sectors.join(df_jobs, how='left')\n"
     ]
    }
   ],
   "source": [
    "df_jobs.set_index('Search Keyword', inplace=True)\n",
    "df_sectors.set_index('Keywords', inplace=True)\n",
    "df3 = df_sectors.join(df_jobs, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ef95c5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), Code)</th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), Sector Name)</th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), Keywords Count)</th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), % per Sector)</th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), % per Social Category)</th>\n",
       "      <th>(SBI Sector Titles, Industry class / branch (SIC2008), % per Workforce)</th>\n",
       "      <th>(Gender, Female, n)</th>\n",
       "      <th>(Gender, Female, % per Sector)</th>\n",
       "      <th>(Gender, Female, % per Social Category)</th>\n",
       "      <th>(Gender, Female, % per Workforce)</th>\n",
       "      <th>...</th>\n",
       "      <th>Job Description nlp_token_tags</th>\n",
       "      <th>Job Description nlp_lemmas</th>\n",
       "      <th>Job Description nlp_stems</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>1grams_bert</th>\n",
       "      <th>bert_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accommodation</th>\n",
       "      <td>I</td>\n",
       "      <td>Accommodation and food serving</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>199.0</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>0.016560</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agriculture</th>\n",
       "      <td>A</td>\n",
       "      <td>Agriculture and industry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.221587</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>0.012268</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application developer</th>\n",
       "      <td>J</td>\n",
       "      <td>Information and communication</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author</th>\n",
       "      <td>J</td>\n",
       "      <td>Information and communication</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.006657</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>build</th>\n",
       "      <td>F</td>\n",
       "      <td>Construction</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.005952</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesale</th>\n",
       "      <td>G</td>\n",
       "      <td>Commercial services</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>3421.0</td>\n",
       "      <td>0.431345</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.135378</td>\n",
       "      <td>...</td>\n",
       "      <td>[(ability, NN), (to, TO), (travel, VB), (,, ,)...</td>\n",
       "      <td>[ability, travel, domestic, international, req...</td>\n",
       "      <td>[abil, travel, domest, intern, requir]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>[ability, to, travel, ,, domestic, or, interna...</td>\n",
       "      <td>[ability, to, travel, ,, domestic, or, interna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesale</th>\n",
       "      <td>G</td>\n",
       "      <td>Commercial services</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>3421.0</td>\n",
       "      <td>0.431345</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.135378</td>\n",
       "      <td>...</td>\n",
       "      <td>[(in, IN), (-, HYPH), (depth, NN), (understand...</td>\n",
       "      <td>[depth, understanding]</td>\n",
       "      <td>[depth, understand]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[in, -, depth, understanding, of, it]</td>\n",
       "      <td>[in, -, depth, understanding, of, it]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesale</th>\n",
       "      <td>G</td>\n",
       "      <td>Commercial services</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>3421.0</td>\n",
       "      <td>0.431345</td>\n",
       "      <td>0.284680</td>\n",
       "      <td>0.135378</td>\n",
       "      <td>...</td>\n",
       "      <td>[(fluent, NNP), (english, NNP), (both, CC), (v...</td>\n",
       "      <td>[fluent, english, verbally, write]</td>\n",
       "      <td>[fluent, english, verbal, written]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[fluent, english, both, verbal, ##ly, and, wri...</td>\n",
       "      <td>[fluent, english, both, verbal, ##ly, and, wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>A-U</td>\n",
       "      <td>All economic activities</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4029.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Total (excluding A-U)</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>12017.0</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475544</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>234 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      (SBI Sector Titles, Industry class / branch (SIC2008), Code)  \\\n",
       "accommodation                                                          I             \n",
       "agriculture                                                            A             \n",
       "application developer                                                  J             \n",
       "author                                                                 J             \n",
       "build                                                                  F             \n",
       "...                                                                  ...             \n",
       "wholesale                                                              G             \n",
       "wholesale                                                              G             \n",
       "wholesale                                                              G             \n",
       "NaN                                                                  A-U             \n",
       "NaN                                                                  NaN             \n",
       "\n",
       "                      (SBI Sector Titles, Industry class / branch (SIC2008), Sector Name)  \\\n",
       "accommodation                             Accommodation and food serving                    \n",
       "agriculture                                     Agriculture and industry                    \n",
       "application developer                      Information and communication                    \n",
       "author                                     Information and communication                    \n",
       "build                                                       Construction                    \n",
       "...                                                                  ...                    \n",
       "wholesale                                            Commercial services                    \n",
       "wholesale                                            Commercial services                    \n",
       "wholesale                                            Commercial services                    \n",
       "NaN                                              All economic activities                    \n",
       "NaN                                                Total (excluding A-U)                    \n",
       "\n",
       "                       (SBI Sector Titles, Industry class / branch (SIC2008), Keywords Count)  \\\n",
       "accommodation                                                        7.0                        \n",
       "agriculture                                                          3.0                        \n",
       "application developer                                                9.0                        \n",
       "author                                                               9.0                        \n",
       "build                                                                2.0                        \n",
       "...                                                                  ...                        \n",
       "wholesale                                                           11.0                        \n",
       "wholesale                                                           11.0                        \n",
       "wholesale                                                           11.0                        \n",
       "NaN                                                                  NaN                        \n",
       "NaN                                                                 99.0                        \n",
       "\n",
       "                       (SBI Sector Titles, Industry class / branch (SIC2008), % per Sector)  \\\n",
       "accommodation                                                   0.017677                      \n",
       "agriculture                                                     0.002144                      \n",
       "application developer                                           0.031034                      \n",
       "author                                                          0.031034                      \n",
       "build                                                           0.005952                      \n",
       "...                                                                  ...                      \n",
       "wholesale                                                       0.001387                      \n",
       "wholesale                                                       0.001387                      \n",
       "wholesale                                                       0.001387                      \n",
       "NaN                                                                  NaN                      \n",
       "NaN                                                             0.003918                      \n",
       "\n",
       "                       (SBI Sector Titles, Industry class / branch (SIC2008), % per Social Category)  \\\n",
       "accommodation                                                   0.070707                               \n",
       "agriculture                                                     0.030303                               \n",
       "application developer                                           0.090909                               \n",
       "author                                                          0.090909                               \n",
       "build                                                           0.020202                               \n",
       "...                                                                  ...                               \n",
       "wholesale                                                       0.111111                               \n",
       "wholesale                                                       0.111111                               \n",
       "wholesale                                                       0.111111                               \n",
       "NaN                                                                  NaN                               \n",
       "NaN                                                             1.000000                               \n",
       "\n",
       "                       (SBI Sector Titles, Industry class / branch (SIC2008), % per Workforce)  \\\n",
       "accommodation                                                   0.000277                         \n",
       "agriculture                                                     0.000119                         \n",
       "application developer                                           0.000356                         \n",
       "author                                                          0.000356                         \n",
       "build                                                           0.000079                         \n",
       "...                                                                  ...                         \n",
       "wholesale                                                       0.000435                         \n",
       "wholesale                                                       0.000435                         \n",
       "wholesale                                                       0.000435                         \n",
       "NaN                                                                  NaN                         \n",
       "NaN                                                             0.003918                         \n",
       "\n",
       "                       (Gender, Female, n)  (Gender, Female, % per Sector)  \\\n",
       "accommodation                        199.0                        0.502525   \n",
       "agriculture                          310.0                        0.221587   \n",
       "application developer                 80.0                        0.275862   \n",
       "author                                80.0                        0.275862   \n",
       "build                                 42.0                        0.125000   \n",
       "...                                    ...                             ...   \n",
       "wholesale                           3421.0                        0.431345   \n",
       "wholesale                           3421.0                        0.431345   \n",
       "wholesale                           3421.0                        0.431345   \n",
       "NaN                                 4029.0                             NaN   \n",
       "NaN                                12017.0                        0.475544   \n",
       "\n",
       "                       (Gender, Female, % per Social Category)  \\\n",
       "accommodation                                         0.016560   \n",
       "agriculture                                           0.025797   \n",
       "application developer                                 0.006657   \n",
       "author                                                0.006657   \n",
       "build                                                 0.003495   \n",
       "...                                                        ...   \n",
       "wholesale                                             0.284680   \n",
       "wholesale                                             0.284680   \n",
       "wholesale                                             0.284680   \n",
       "NaN                                                        NaN   \n",
       "NaN                                                   1.000000   \n",
       "\n",
       "                       (Gender, Female, % per Workforce)  ...  \\\n",
       "accommodation                                   0.007875  ...   \n",
       "agriculture                                     0.012268  ...   \n",
       "application developer                           0.003166  ...   \n",
       "author                                          0.003166  ...   \n",
       "build                                           0.001662  ...   \n",
       "...                                                  ...  ...   \n",
       "wholesale                                       0.135378  ...   \n",
       "wholesale                                       0.135378  ...   \n",
       "wholesale                                       0.135378  ...   \n",
       "NaN                                                  NaN  ...   \n",
       "NaN                                             0.475544  ...   \n",
       "\n",
       "                                          Job Description nlp_token_tags  \\\n",
       "accommodation                                                        NaN   \n",
       "agriculture                                                          NaN   \n",
       "application developer                                                NaN   \n",
       "author                                                               NaN   \n",
       "build                                                                NaN   \n",
       "...                                                                  ...   \n",
       "wholesale              [(ability, NN), (to, TO), (travel, VB), (,, ,)...   \n",
       "wholesale              [(in, IN), (-, HYPH), (depth, NN), (understand...   \n",
       "wholesale              [(fluent, NNP), (english, NNP), (both, CC), (v...   \n",
       "NaN                                                                  NaN   \n",
       "NaN                                                                  NaN   \n",
       "\n",
       "                                              Job Description nlp_lemmas  \\\n",
       "accommodation                                                        NaN   \n",
       "agriculture                                                          NaN   \n",
       "application developer                                                NaN   \n",
       "author                                                               NaN   \n",
       "build                                                                NaN   \n",
       "...                                                                  ...   \n",
       "wholesale              [ability, travel, domestic, international, req...   \n",
       "wholesale                                         [depth, understanding]   \n",
       "wholesale                             [fluent, english, verbally, write]   \n",
       "NaN                                                                  NaN   \n",
       "NaN                                                                  NaN   \n",
       "\n",
       "                                    Job Description nlp_stems  num_words  \\\n",
       "accommodation                                             NaN        NaN   \n",
       "agriculture                                               NaN        NaN   \n",
       "application developer                                     NaN        NaN   \n",
       "author                                                    NaN        NaN   \n",
       "build                                                     NaN        NaN   \n",
       "...                                                       ...        ...   \n",
       "wholesale              [abil, travel, domest, intern, requir]        8.0   \n",
       "wholesale                                 [depth, understand]        4.0   \n",
       "wholesale                  [fluent, english, verbal, written]        6.0   \n",
       "NaN                                                       NaN        NaN   \n",
       "NaN                                                       NaN        NaN   \n",
       "\n",
       "                      num_unique_words  num_chars  num_punctuations  \\\n",
       "accommodation                      NaN        NaN               NaN   \n",
       "agriculture                        NaN        NaN               NaN   \n",
       "application developer              NaN        NaN               NaN   \n",
       "author                             NaN        NaN               NaN   \n",
       "build                              NaN        NaN               NaN   \n",
       "...                                ...        ...               ...   \n",
       "wholesale                          8.0       57.0               2.0   \n",
       "wholesale                          4.0       28.0               1.0   \n",
       "wholesale                          6.0       40.0               0.0   \n",
       "NaN                                NaN        NaN               NaN   \n",
       "NaN                                NaN        NaN               NaN   \n",
       "\n",
       "                       sentiment  \\\n",
       "accommodation                NaN   \n",
       "agriculture                  NaN   \n",
       "application developer        NaN   \n",
       "author                       NaN   \n",
       "build                        NaN   \n",
       "...                          ...   \n",
       "wholesale                 0.3182   \n",
       "wholesale                 0.0000   \n",
       "wholesale                 0.0000   \n",
       "NaN                          NaN   \n",
       "NaN                          NaN   \n",
       "\n",
       "                                                             1grams_bert  \\\n",
       "accommodation                                                        NaN   \n",
       "agriculture                                                          NaN   \n",
       "application developer                                                NaN   \n",
       "author                                                               NaN   \n",
       "build                                                                NaN   \n",
       "...                                                                  ...   \n",
       "wholesale              [ability, to, travel, ,, domestic, or, interna...   \n",
       "wholesale                          [in, -, depth, understanding, of, it]   \n",
       "wholesale              [fluent, english, both, verbal, ##ly, and, wri...   \n",
       "NaN                                                                  NaN   \n",
       "NaN                                                                  NaN   \n",
       "\n",
       "                                                          bert_tokenized  \n",
       "accommodation                                                        NaN  \n",
       "agriculture                                                          NaN  \n",
       "application developer                                                NaN  \n",
       "author                                                               NaN  \n",
       "build                                                                NaN  \n",
       "...                                                                  ...  \n",
       "wholesale              [ability, to, travel, ,, domestic, or, interna...  \n",
       "wholesale                          [in, -, depth, understanding, of, it]  \n",
       "wholesale              [fluent, english, both, verbal, ##ly, and, wri...  \n",
       "NaN                                                                  NaN  \n",
       "NaN                                                                  NaN  \n",
       "\n",
       "[234 rows x 62 columns]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "9884dfe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_jobs.loc[(df_jobs['Job Description'].str.contains(str_fix_incl)) | (df_jobs['Job Description'].str.contains(str_fix_eg))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8e556d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_descriptions = list(set(df_jobs['Job Description'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "9da09d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = list(\n",
    "    set(\n",
    "        df_jobs['Job Description'].loc[\n",
    "            (df_jobs['Job Description'].str.contains(str_fix_incl)) |\n",
    "            (df_jobs['Job Description'].str.contains(str_fix_eg))\n",
    "        ].to_list()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0de4de75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "87f15f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nage suppliers for the projects & programs in scope, for example interviews, staffing, supplier costs.\\n\\nCommunication   stakeholder management\\nAct as the key contact person for strategic stakeholders.\\nApply appropriate and effective communication methods to senior management and important stakeholders (including vendor) throughout the project lifecycle.\\nAs conflicts and escalations arise within projects, identify solutions and manage the resolution in a timely and appropriate manner.\\nDrive chang'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_descriptions[0][1500:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "6e5aaa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.loc[\n",
    "    (df_jobs['Job Description'].str.contains(str_fix_incl)) | \n",
    "    (df_jobs['Job Description'].str.contains(str_fix_eg))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "a52f466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "8b0ab475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name',\n",
       "       'Location', 'Job Description', 'Rating', 'Employment Type',\n",
       "       'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
       "       'Collection Date', 'Data Row', 'Tracking ID', 'Industry', 'Job Date',\n",
       "       'Type of ownership'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "8d5e14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a list of punctuations that determine sentence boundry, i.e., split characters\n",
    "def make_custom_punct_chars(main_punct_chars = [':', '|'], repeated_punct_chars = ['\\n', ',']):\n",
    "    custom_punct_chars = []\n",
    "    temp_multi = []\n",
    "    temp_spaced = []\n",
    "\n",
    "    for punct_char in main_punct_chars:\n",
    "        custom_punct_chars+= f'{punct_char}', f'{punct_char} '\n",
    "\n",
    "    for idx in range(4):\n",
    "        for punct_char in repeated_punct_chars:\n",
    "            temp_multi.append(f'{punct_char}'*int(idx+1))\n",
    "            temp_spaced.append(f'{punct_char} '*int(idx+1))\n",
    "\n",
    "    for multi, spaced in zip(temp_multi, temp_spaced):\n",
    "        custom_punct_chars+= multi, spaced\n",
    "\n",
    "    custom_punct_chars.remove(',')\n",
    "    custom_punct_chars.remove(', ')\n",
    "\n",
    "    return custom_punct_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0c7f61ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_punct_chars = make_custom_punct_chars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "183846a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':',\n",
       " ': ',\n",
       " '|',\n",
       " '| ',\n",
       " '\\n',\n",
       " '\\n ',\n",
       " '\\n\\n',\n",
       " '\\n \\n ',\n",
       " ',,',\n",
       " ', , ',\n",
       " '\\n\\n\\n',\n",
       " '\\n \\n \\n ',\n",
       " ',,,',\n",
       " ', , , ',\n",
       " '\\n\\n\\n\\n',\n",
       " '\\n \\n \\n \\n ',\n",
       " ',,,,',\n",
       " ', , , , ']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_punct_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "5c845236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "cd2f4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = nlp.add_pipe('sentencizer')\n",
    "sentencizer.punct_chars.update(custom_punct_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "37292ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    with open(f'{data_dir}punctuations.txt', 'wb') as f:\n",
    "        pickle.dump(sentencizer.punct_chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "08e8ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data_dir}punctuations.txt', 'rb') as f:\n",
    "    custom_punct_char = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "6425cf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import NORM, ORTH, LEMMA\n",
    "\n",
    "special_cases_dict = {\n",
    "    'incl.': [{65: 'incl', 67: 'including'}],\n",
    "    'incl. ': [{65: 'incl', 67: 'including'}],\n",
    "    '(incl.': [{65: 'incl', 67: 'including'}],\n",
    "    'etc.': [{65: 'etc', 67: 'et cetera'}],\n",
    "    'etc. ': [{65: 'etc', 67: 'et cetera'}],\n",
    "    'e.g.': [{65: 'e.g', 67: 'for example'}],\n",
    "    'e.g. ': [{65: 'e.g', 67: 'for example'}],\n",
    "}\n",
    "\n",
    "nlp.tokenizer.rules.update(special_cases_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "4987aa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{65: 'e.g', 67: 'for example'}]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.rules['e.g.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f5f7ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    df_jobs['Job Description spacy_sentencized'] = df_jobs['Job Description'].apply(\n",
    "        lambda job_description: [\n",
    "            sent \n",
    "            for sentence in nlp(job_description).sents \n",
    "            for sent in re.split(pattern, sentence.text) \n",
    "            if len(sent) != 0 \n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "4fcf739c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                Purpose\n",
       "1                                 As a Studio A Director\n",
       "2                                       Special Projects\n",
       "3      in adidas Digital your task is to manage and d...\n",
       "4      The program scope may comprise multiple countr...\n",
       "                             ...                        \n",
       "130    Strong MS-Office skills (Word, Excel, PowerPoint)\n",
       "131     Basic Experience and a broad understanding of IT\n",
       "132    Working knowledge of Agile working methods for...\n",
       "133    Working knowledge of PMI methods ideally with ...\n",
       "134             Fluent English both verbally and written\n",
       "Name: Job Description spacy_sentencized, Length: 135, dtype: object"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description spacy_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "01bc9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.explode('Job Description spacy_sentencized', ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "68d1cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description Job Description spacy_sentencized_lower'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: job_sentence.strip().lower()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "fb9369d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "b37e3f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description spacy_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.text.strip().lower()) \n",
    "        for token in nlp.tokenizer(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and not token.is_stop \n",
    "        and not token.is_punct \n",
    "        and not token.text in custom_punct_chars\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "159a4126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                              [purpose]\n",
       "1                                     [studio, director]\n",
       "2                                    [special, projects]\n",
       "3      [adidas, digital, task, manage, deliver, studi...\n",
       "4      [program, scope, comprise, multiple, countries...\n",
       "                             ...                        \n",
       "130    [strong, ms, office, skills, word, excel, powe...\n",
       "131            [basic, experience, broad, understanding]\n",
       "132    [working, knowledge, agile, working, methods, ...\n",
       "133    [working, knowledge, pmi, methods, ideally, ce...\n",
       "134                 [fluent, english, verbally, written]\n",
       "Name: Job Description spacy_tokenized, Length: 135, dtype: object"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description spacy_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "eaf53549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLK\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7771bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r'[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?<=[a-z])(?=[A-Z])'\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "\n",
    "pattern_numbers = r'[^a-zA-Z0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "483e577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_sentencized = []\n",
    "\n",
    "# for job_description in job_descriptions:\n",
    "#     for sentence in sent_tokenize(job_description):\n",
    "#         for sent in re.split(pattern, sentence):\n",
    "#             nltk_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3defa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized = [\n",
    "    sent \n",
    "    for job_description in job_descriptions \n",
    "    for sentence in sent_tokenize(job_description) \n",
    "    for sent in re.split(pattern, sentence)\n",
    "    if len(sent) != 0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "6151cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_sentencized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1b85716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'] = df_jobs['Job Description'].apply(\n",
    "#     lambda job_description: [\n",
    "#         sent \n",
    "#         for sentence in sent_tokenize(job_description) \n",
    "#         for sent in re.split(pattern, sentence)\n",
    "#         if len(sent) != 0\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "73db85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'] = df_jobs['Job Description'].apply(\n",
    "#     lambda job_description: [\n",
    "#         re.split(pattern, sentence)\n",
    "#         for sentence in sent_tokenize(job_description)\n",
    "#         if len(sentence) != 0\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "3808eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized'][0][-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "22ea45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_jobs['Job Description nltk_sentencized'][0]) + len(df_jobs['Job Description nltk_sentencized'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "0af60e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = df_jobs.explode('Job Description nltk_sentencized', ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "98e9b630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "e1a7fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                Purpose\n",
       "1                                 As a Studio A Director\n",
       "2                                       Special Projects\n",
       "3      in adidas Digital your task is to manage and d...\n",
       "4      The program scope may comprise multiple countr...\n",
       "                             ...                        \n",
       "128    Strong MS-Office skills (Word, Excel, PowerPoint)\n",
       "129     Basic Experience and a broad understanding of IT\n",
       "130    Working knowledge of Agile working methods for...\n",
       "131    Working knowledge of PMI methods ideally with ...\n",
       "132             Fluent English both verbally and written\n",
       "Name: Job Description nltk_sentencized, Length: 133, dtype: object"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_jobs['Job Description nltk_sentencized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1126453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_sentencized_lower = [\n",
    "    str(sent.strip().lower()) \n",
    "    for sent in nltk_sentencized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0d00571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_sentencized_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "fc2042a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized_lower'] = df_jobs['Job Description nltk_sentencized'].apply(\n",
    "#     lambda job_sentence: job_sentence.strip().lower()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "246ff470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Job Description nltk_sentencized_lower']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "bde6bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in df_jobs['Job Description spacy_sentencized'].items():\n",
    "    if str_fix_eg in sentence:\n",
    "        print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "feb5d856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Form, lead, manage and monitor multi-functional team resources (internals and externals) to deliver and support projects & programs.',\n",
       " 'Partner with Procurement in order to manage suppliers for the projects & programs in scope, for example interviews, staffing, supplier costs.',\n",
       " 'Communication   stakeholder management',\n",
       " 'Act as the key contact person for strategic stakeholders.',\n",
       " 'Apply appropriate and effective communication methods to senior management and important stakeholders (including vendor) throughout the project lifecycle.',\n",
       " 'As conflicts and escalations arise within projects, identify solutions and manage the resolution in a timely and appropriate manner.',\n",
       " 'Drive change management activities for respective projects & programs and ensure changes are smoothly and successfully implemented to achieve lasting benefits.',\n",
       " 'Project Controlling']"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description spacy_sentencized'][17:25].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "84d85ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "for index, sentence in df_jobs['Job Description spacy_sentencized'].items():\n",
    "    if 'Power' in sentence:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "92d27dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Highly developed leadership skills are required',\n",
       " 'Hard-Skills',\n",
       " 'Knowledge of digital technologies and communications platforms and culture',\n",
       " 'Strong MS-Office skills (Word, Excel, PowerPoint)',\n",
       " 'Ability to travel, domestic or international, as required',\n",
       " 'In-depth understanding of IT',\n",
       " 'Fluent English both verbally and written',\n",
       " 'Purpose:',\n",
       " 'As a Manager Digital Strategy and Programs in adidas market ecom organization your task is to manage and deliver small digital projects, significant RFCs or sub-elements of mid-scale digital projects.',\n",
       " 'The project scope is on a local (market) level across brands and functions.']"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description spacy_sentencized'][60:70].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0bde1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if str_fix_eg in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1548e80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_eg in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fdc134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# str_fix = 'Apply appropriate and effective communication methods to senior management and important stakeholders \\(incl.'\n",
    "\n",
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if str_fix_incl.split('\\(')[0] in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "808b22b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apply appropriate and effective communication methods to senior management and important stakeholders throughout the project lifecycle.',\n",
       " 'As conflicts and escalations arise within projects, identify solutions and drive their resolution in a timely and appropriate manner.',\n",
       " 'Manage change within projects and ensure changes are smoothly and successfully implemented to achieve lasting benefits.',\n",
       " 'Project Controlling',\n",
       " 'Manage project controlling as an independent element to ensure that actual project costs are in line the committed budget.',\n",
       " 'Conduct engagement reviews.',\n",
       " 'Verify compliance with quality assurance procedures.',\n",
       " 'Validate if project are delivering against program KPIs.',\n",
       " 'Monitor project variables (cost, effort, scope, et cetera) against plan in order to implement corrective or preventive actions.',\n",
       " '\"If required\" Responsibilities',\n",
       " 'Agile Transformation']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_sentencized[84:95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1476b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Partner with Procurement in order to manage suppliers for the projects & programs in scope, for example interviews, staffing, supplier costs.',\n",
       " 'Communication   stakeholder management',\n",
       " 'Act as the key contact person for strategic stakeholders.',\n",
       " 'Apply appropriate and effective communication methods to senior management and important stakeholders (including vendor) throughout the project lifecycle.',\n",
       " 'As conflicts and escalations arise within projects, identify solutions and manage the resolution in a timely and appropriate manner.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences split on e.g. and incl.\n",
    "nltk_sentencized[\n",
    "    next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_eg in sent):next(idx for idx, sent in enumerate(nltk_sentencized) if str_fix_incl.split('\\(')[0] in sent)+2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4da72e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(nltk_sentencized):\n",
    "    if 'Power' in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3013d2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idx for idx, sent in enumerate(nltk_sentencized) if 'Power' in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ece8b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong MS-Office skills (Word, Excel, PowerPoint)'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_sentencized[next(idx for idx, sent in enumerate(nltk_sentencized) if 'Power' in sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47c2a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Apply appropriate and effective communication methods to senior management and important stakeholders throughout the project lifecycle.\n",
      "\n",
      "Sentence 2: As conflicts and escalations arise within projects, identify solutions and drive their resolution in a timely and appropriate manner.\n",
      "\n",
      "Sentence 3: Manage change within projects and ensure changes are smoothly and successfully implemented to achieve lasting benefits.\n",
      "\n",
      "Sentence 4: Project Controlling\n",
      "\n",
      "Sentence 5: Manage project controlling as an independent element to ensure that actual project costs are in line the committed budget.\n",
      "\n",
      "Sentence 6: Conduct engagement reviews.\n",
      "\n",
      "Sentence 7: Verify compliance with quality assurance procedures.\n",
      "\n",
      "Sentence 8: Validate if project are delivering against program KPIs.\n",
      "\n",
      "Sentence 9: Monitor project variables (cost, effort, scope, et cetera) against plan in order to implement corrective or preventive actions.\n",
      "\n",
      "Sentence 10: \"If required\" Responsibilities\n",
      "\n",
      "Sentence 11: Agile Transformation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(nltk_sentencized[84:95]):\n",
    "    print(f'Sentence {idx+1}: {sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb93bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_tokenized = []\n",
    "\n",
    "# for job_sentence in nltk_sentencized:\n",
    "#     for token in word_tokenize(job_sentence):\n",
    "#         if len(token) != 0 and token != '...' and token.lower() not in set(stopwords.words('english')) and token.lower() not in list(string.punctuation):\n",
    "#             nltk_tokenized.append(str(token.strip().lower())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3eb3ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenized = [\n",
    "    str(token.strip().lower()) \n",
    "    for job_sentence in nltk_sentencized \n",
    "    for token in word_tokenize(job_sentence) \n",
    "    if len(token) != 0 \n",
    "    and token != '...' \n",
    "    and not token.lower() in set(stopwords.words('english')) \n",
    "    and not token.lower() in list(string.punctuation) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "4ad77469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        str(token.strip().lower()) \n",
    "        for token in word_tokenize(job_sentence) \n",
    "        if len(token) != 0 \n",
    "        and token != '...' \n",
    "        and not token.lower() in set(stopwords.words('english')) \n",
    "        and not token.lower() in list(string.punctuation) \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "9c1365b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                              [purpose]\n",
       "1                                     [studio, director]\n",
       "2                                    [special, projects]\n",
       "3      [adidas, digital, task, manage, deliver, studi...\n",
       "4      [program, scope, may, comprise, multiple, coun...\n",
       "                             ...                        \n",
       "130    [strong, ms-office, skills, word, excel, power...\n",
       "131            [basic, experience, broad, understanding]\n",
       "132    [working, knowledge, agile, working, methods, ...\n",
       "133    [working, knowledge, pmi, methods, ideally, ce...\n",
       "134                 [fluent, english, verbally, written]\n",
       "Name: Job Description nltk_tokenized, Length: 135, dtype: object"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description nltk_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d604e3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'...' in nltk_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c90fd54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5b74e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_token_tags[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82b222c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_token_tags = pos_tag(nltk_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "fee8b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_token_tags'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
    "    lambda token: pos_tag(token)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "135b1a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        [(purpose, NN)]\n",
       "1                         [(studio, NN), (director, NN)]\n",
       "2                       [(special, JJ), (projects, NNS)]\n",
       "3      [(adidas, JJ), (digital, JJ), (task, NN), (man...\n",
       "4      [(program, NN), (scope, NN), (may, MD), (compr...\n",
       "                             ...                        \n",
       "130    [(strong, JJ), (ms-office, NN), (skills, NNS),...\n",
       "131    [(basic, JJ), (experience, NN), (broad, JJ), (...\n",
       "132    [(working, VBG), (knowledge, NN), (agile, IN),...\n",
       "133    [(working, VBG), (knowledge, NN), (pmi, NN), (...\n",
       "134    [(fluent, JJ), (english, JJ), (verbally, RB), ...\n",
       "Name: Job Description nltk_token_tags, Length: 135, dtype: object"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description nltk_token_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "297d9738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "50db3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "dda77df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: purpose | Stem: purpos | Lemma: purpose | POS Lemma: purpose\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: director | Stem: director | Lemma: director | POS Lemma: director\n",
      "Token: special | Stem: special | Lemma: special | POS Lemma: special\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: adidas | Stem: adida | Lemma: adidas | POS Lemma: adidas\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: task | Stem: task | Lemma: task | POS Lemma: task\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: including | Stem: includ | Lemma: including | POS Lemma: include\n",
      "Token: full | Stem: full | Lemma: full | POS Lemma: full\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: roadmap | Stem: roadmap | Lemma: roadmap | POS Lemma: roadmap\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: may | Stem: may | Lemma: may | POS Lemma: may\n",
      "Token: comprise | Stem: compris | Lemma: comprise | POS Lemma: comprise\n",
      "Token: multiple | Stem: multipl | Lemma: multiple | POS Lemma: multiple\n",
      "Token: countries | Stem: countri | Lemma: country | POS Lemma: country\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: objective | Stem: object | Lemma: objective | POS Lemma: objective\n",
      "Token: conduct | Stem: conduct | Lemma: conduct | POS Lemma: conduct\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: boundaries | Stem: boundari | Lemma: boundary | POS Lemma: boundary\n",
      "Token: time | Stem: time | Lemma: time | POS Lemma: time\n",
      "Token: cost | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: quality | Stem: qualiti | Lemma: quality | POS Lemma: quality\n",
      "Token: focusing | Stem: focus | Lemma: focusing | POS Lemma: focus\n",
      "Token: consumers | Stem: consum | Lemma: consumer | POS Lemma: consumer\n",
      "Token: expectations | Stem: expect | Lemma: expectation | POS Lemma: expectation\n",
      "Token: requirements | Stem: requir | Lemma: requirement | POS Lemma: requirement\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: executing | Stem: execut | Lemma: executing | POS Lemma: execute\n",
      "Token: tracking | Stem: track | Lemma: tracking | POS Lemma: track\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: cooperation | Stem: cooper | Lemma: cooperation | POS Lemma: cooperation\n",
      "Token: respective | Stem: respect | Lemma: respective | POS Lemma: respective\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: members | Stem: member | Lemma: member | POS Lemma: member\n",
      "Token: examples | Stem: exampl | Lemma: example | POS Lemma: example\n",
      "Token: expected | Stem: expect | Lemma: expected | POS Lemma: expect\n",
      "Token: role | Stem: role | Lemma: role | POS Lemma: role\n",
      "Token: additionally | Stem: addit | Lemma: additionally | POS Lemma: additionally\n",
      "Token: focusing | Stem: focus | Lemma: focusing | POS Lemma: focus\n",
      "Token: resource | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: vendor | Stem: vendor | Lemma: vendor | POS Lemma: vendor\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: coordination | Stem: coordin | Lemma: coordination | POS Lemma: coordination\n",
      "Token: activities | Stem: activ | Lemma: activity | POS Lemma: activity\n",
      "Token: acting | Stem: act | Lemma: acting | POS Lemma: act\n",
      "Token: central | Stem: central | Lemma: central | POS Lemma: central\n",
      "Token: interface | Stem: interfac | Lemma: interface | POS Lemma: interface\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: including | Stem: includ | Lemma: including | POS Lemma: include\n",
      "Token: full | Stem: full | Lemma: full | POS Lemma: full\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: roadmap | Stem: roadmap | Lemma: roadmap | POS Lemma: roadmap\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: delivery | Stem: deliveri | Lemma: delivery | POS Lemma: delivery\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: set | Stem: set | Lemma: set | POS Lemma: set\n",
      "Token: roll | Stem: roll | Lemma: roll | POS Lemma: roll\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: hubs | Stem: hub | Lemma: hub | POS Lemma: hub\n",
      "Token: portland | Stem: portland | Lemma: portland | POS Lemma: portland\n",
      "Token: shanghai | Stem: shanghai | Lemma: shanghai | POS Lemma: shanghai\n",
      "Token: locations | Stem: locat | Lemma: location | POS Lemma: location\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: clear | Stem: clear | Lemma: clear | POS Lemma: clear\n",
      "Token: operating | Stem: oper | Lemma: operating | POS Lemma: operating\n",
      "Token: interaction | Stem: interact | Lemma: interaction | POS Lemma: interaction\n",
      "Token: models | Stem: model | Lemma: model | POS Lemma: model\n",
      "Token: processes | Stem: process | Lemma: process | POS Lemma: process\n",
      "Token: reporting | Stem: report | Lemma: reporting | POS Lemma: reporting\n",
      "Token: frameworks | Stem: framework | Lemma: framework | POS Lemma: framework\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: strategic | Stem: strateg | Lemma: strategic | POS Lemma: strategic\n",
      "Token: partnership | Stem: partnership | Lemma: partnership | POS Lemma: partnership\n",
      "Token: oliver | Stem: oliv | Lemma: oliver | POS Lemma: oliver\n",
      "Token: globally | Stem: global | Lemma: globally | POS Lemma: globally\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: set | Stem: set | Lemma: set | POS Lemma: set\n",
      "Token: pilots | Stem: pilot | Lemma: pilot | POS Lemma: pilot\n",
      "Token: new | Stem: new | Lemma: new | POS Lemma: new\n",
      "Token: production | Stem: product | Lemma: production | POS Lemma: production\n",
      "Token: topics | Stem: topic | Lemma: topic | POS Lemma: topic\n",
      "Token: ie | Stem: ie | Lemma: ie | POS Lemma: ie\n",
      "Token: tier | Stem: tier | Lemma: tier | POS Lemma: tier\n",
      "Token: 1 | Stem: 1 | Lemma: 1 | POS Lemma: 1\n",
      "Token: elevated | Stem: elev | Lemma: elevated | POS Lemma: elevate\n",
      "Token: content | Stem: content | Lemma: content | POS Lemma: content\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: point | Stem: point | Lemma: point | POS Lemma: point\n",
      "Token: moved | Stem: move | Lemma: moved | POS Lemma: move\n",
      "Token: mass | Stem: mass | Lemma: mass | POS Lemma: mass\n",
      "Token: production | Stem: product | Lemma: production | POS Lemma: production\n",
      "Token: scale | Stem: scale | Lemma: scale | POS Lemma: scale\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: top | Stem: top | Lemma: top | POS Lemma: top\n",
      "Token: creator | Stem: creator | Lemma: creator | POS Lemma: creator\n",
      "Token: production | Stem: product | Lemma: production | POS Lemma: production\n",
      "Token: requests | Stem: request | Lemma: request | POS Lemma: request\n",
      "Token: ie | Stem: ie | Lemma: ie | POS Lemma: ie\n",
      "Token: beyonce | Stem: beyonc | Lemma: beyonce | POS Lemma: beyonce\n",
      "Token: pharrell | Stem: pharrel | Lemma: pharrell | POS Lemma: pharrell\n",
      "Token: et | Stem: et | Lemma: et | POS Lemma: et\n",
      "Token: ceteramanage | Stem: ceteramanag | Lemma: ceteramanage | POS Lemma: ceteramanage\n",
      "Token: supporting | Stem: support | Lemma: supporting | POS Lemma: support\n",
      "Token: system | Stem: system | Lemma: system | POS Lemma: system\n",
      "Token: platforms | Stem: platform | Lemma: platform | POS Lemma: platform\n",
      "Token: resource | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: form | Stem: form | Lemma: form | POS Lemma: form\n",
      "Token: lead | Stem: lead | Lemma: lead | POS Lemma: lead\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: monitor | Stem: monitor | Lemma: monitor | POS Lemma: monitor\n",
      "Token: multi-functional | Stem: multi-funct | Lemma: multi-functional | POS Lemma: multi-functional\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: resources | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: internals | Stem: intern | Lemma: internals | POS Lemma: internals\n",
      "Token: externals | Stem: extern | Lemma: external | POS Lemma: external\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: support | Stem: support | Lemma: support | POS Lemma: support\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: partner | Stem: partner | Lemma: partner | POS Lemma: partner\n",
      "Token: procurement | Stem: procur | Lemma: procurement | POS Lemma: procurement\n",
      "Token: order | Stem: order | Lemma: order | POS Lemma: order\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: suppliers | Stem: supplier | Lemma: supplier | POS Lemma: supplier\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: example | Stem: exampl | Lemma: example | POS Lemma: example\n",
      "Token: interviews | Stem: interview | Lemma: interview | POS Lemma: interview\n",
      "Token: staffing | Stem: staf | Lemma: staffing | POS Lemma: staff\n",
      "Token: supplier | Stem: supplier | Lemma: supplier | POS Lemma: supplier\n",
      "Token: costs | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: stakeholder | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: act | Stem: act | Lemma: act | POS Lemma: act\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: contact | Stem: contact | Lemma: contact | POS Lemma: contact\n",
      "Token: person | Stem: person | Lemma: person | POS Lemma: person\n",
      "Token: strategic | Stem: strateg | Lemma: strategic | POS Lemma: strategic\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: apply | Stem: appli | Lemma: apply | POS Lemma: apply\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: effective | Stem: effect | Lemma: effective | POS Lemma: effective\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: methods | Stem: method | Lemma: method | POS Lemma: method\n",
      "Token: senior | Stem: senior | Lemma: senior | POS Lemma: senior\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: important | Stem: import | Lemma: important | POS Lemma: important\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: including | Stem: includ | Lemma: including | POS Lemma: include\n",
      "Token: vendor | Stem: vendor | Lemma: vendor | POS Lemma: vendor\n",
      "Token: throughout | Stem: throughout | Lemma: throughout | POS Lemma: throughout\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: lifecycle | Stem: lifecycl | Lemma: lifecycle | POS Lemma: lifecycle\n",
      "Token: conflicts | Stem: conflict | Lemma: conflict | POS Lemma: conflict\n",
      "Token: escalations | Stem: escal | Lemma: escalation | POS Lemma: escalation\n",
      "Token: arise | Stem: aris | Lemma: arise | POS Lemma: arise\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: identify | Stem: identifi | Lemma: identify | POS Lemma: identify\n",
      "Token: solutions | Stem: solut | Lemma: solution | POS Lemma: solution\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: resolution | Stem: resolut | Lemma: resolution | POS Lemma: resolution\n",
      "Token: timely | Stem: time | Lemma: timely | POS Lemma: timely\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: manner | Stem: manner | Lemma: manner | POS Lemma: manner\n",
      "Token: drive | Stem: drive | Lemma: drive | POS Lemma: drive\n",
      "Token: change | Stem: chang | Lemma: change | POS Lemma: change\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: activities | Stem: activ | Lemma: activity | POS Lemma: activity\n",
      "Token: respective | Stem: respect | Lemma: respective | POS Lemma: respective\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: changes | Stem: chang | Lemma: change | POS Lemma: change\n",
      "Token: smoothly | Stem: smooth | Lemma: smoothly | POS Lemma: smoothly\n",
      "Token: successfully | Stem: success | Lemma: successfully | POS Lemma: successfully\n",
      "Token: implemented | Stem: implement | Lemma: implemented | POS Lemma: implement\n",
      "Token: achieve | Stem: achiev | Lemma: achieve | POS Lemma: achieve\n",
      "Token: lasting | Stem: last | Lemma: lasting | POS Lemma: last\n",
      "Token: benefits | Stem: benefit | Lemma: benefit | POS Lemma: benefit\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: controlling | Stem: control | Lemma: controlling | POS Lemma: control\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: controlling | Stem: control | Lemma: controlling | POS Lemma: control\n",
      "Token: independent | Stem: independ | Lemma: independent | POS Lemma: independent\n",
      "Token: element | Stem: element | Lemma: element | POS Lemma: element\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: actual | Stem: actual | Lemma: actual | POS Lemma: actual\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: costs | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: line | Stem: line | Lemma: line | POS Lemma: line\n",
      "Token: committed | Stem: commit | Lemma: committed | POS Lemma: commit\n",
      "Token: budget | Stem: budget | Lemma: budget | POS Lemma: budget\n",
      "Token: lead | Stem: lead | Lemma: lead | POS Lemma: lead\n",
      "Token: conduct | Stem: conduct | Lemma: conduct | POS Lemma: conduct\n",
      "Token: engagement | Stem: engag | Lemma: engagement | POS Lemma: engagement\n",
      "Token: reviews | Stem: review | Lemma: review | POS Lemma: review\n",
      "Token: verify | Stem: verifi | Lemma: verify | POS Lemma: verify\n",
      "Token: implementation | Stem: implement | Lemma: implementation | POS Lemma: implementation\n",
      "Token: quality | Stem: qualiti | Lemma: quality | POS Lemma: quality\n",
      "Token: assurance | Stem: assur | Lemma: assurance | POS Lemma: assurance\n",
      "Token: procedures | Stem: procedur | Lemma: procedure | POS Lemma: procedure\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: delivery | Stem: deliveri | Lemma: delivery | POS Lemma: delivery\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: kpis | Stem: kpis | Lemma: kpis | POS Lemma: kpis\n",
      "Token: monitor | Stem: monitor | Lemma: monitor | POS Lemma: monitor\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: variables | Stem: variabl | Lemma: variable | POS Lemma: variable\n",
      "Token: cost | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: effort | Stem: effort | Lemma: effort | POS Lemma: effort\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: et | Stem: et | Lemma: et | POS Lemma: et\n",
      "Token: cetera | Stem: cetera | Lemma: cetera | POS Lemma: cetera\n",
      "Token: plan | Stem: plan | Lemma: plan | POS Lemma: plan\n",
      "Token: order | Stem: order | Lemma: order | POS Lemma: order\n",
      "Token: implement | Stem: implement | Lemma: implement | POS Lemma: implement\n",
      "Token: corrective | Stem: correct | Lemma: corrective | POS Lemma: corrective\n",
      "Token: preventive | Stem: prevent | Lemma: preventive | POS Lemma: preventive\n",
      "Token: actions | Stem: action | Lemma: action | POS Lemma: action\n",
      "Token: people | Stem: peopl | Lemma: people | POS Lemma: people\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: leadership | Stem: leadership | Lemma: leadership | POS Lemma: leadership\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: present | Stem: present | Lemma: present | POS Lemma: present\n",
      "Token: every | Stem: everi | Lemma: every | POS Lemma: every\n",
      "Token: level | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: creating | Stem: creat | Lemma: creating | POS Lemma: create\n",
      "Token: motivational | Stem: motiv | Lemma: motivational | POS Lemma: motivational\n",
      "Token: supportive | Stem: support | Lemma: supportive | POS Lemma: supportive\n",
      "Token: work | Stem: work | Lemma: work | POS Lemma: work\n",
      "Token: environment | Stem: environ | Lemma: environment | POS Lemma: environment\n",
      "Token: employees | Stem: employe | Lemma: employee | POS Lemma: employee\n",
      "Token: coached | Stem: coach | Lemma: coached | POS Lemma: coached\n",
      "Token: trained | Stem: train | Lemma: trained | POS Lemma: train\n",
      "Token: provided | Stem: provid | Lemma: provided | POS Lemma: provide\n",
      "Token: career | Stem: career | Lemma: career | POS Lemma: career\n",
      "Token: opportunities | Stem: opportun | Lemma: opportunity | POS Lemma: opportunity\n",
      "Token: development | Stem: develop | Lemma: development | POS Lemma: development\n",
      "Token: continuously | Stem: continu | Lemma: continuously | POS Lemma: continuously\n",
      "Token: monitor | Stem: monitor | Lemma: monitor | POS Lemma: monitor\n",
      "Token: evaluate | Stem: evalu | Lemma: evaluate | POS Lemma: evaluate\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: workload | Stem: workload | Lemma: workload | POS Lemma: workload\n",
      "Token: organizational | Stem: organiz | Lemma: organizational | POS Lemma: organizational\n",
      "Token: efficiency | Stem: effici | Lemma: efficiency | POS Lemma: efficiency\n",
      "Token: support | Stem: support | Lemma: support | POS Lemma: support\n",
      "Token: systems | Stem: system | Lemma: system | POS Lemma: system\n",
      "Token: data | Stem: data | Lemma: data | POS Lemma: data\n",
      "Token: analysis | Stem: analysi | Lemma: analysis | POS Lemma: analysis\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: feedback | Stem: feedback | Lemma: feedback | POS Lemma: feedback\n",
      "Token: make | Stem: make | Lemma: make | POS Lemma: make\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: changes | Stem: chang | Lemma: change | POS Lemma: change\n",
      "Token: order | Stem: order | Lemma: order | POS Lemma: order\n",
      "Token: meet | Stem: meet | Lemma: meet | POS Lemma: meet\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: needs | Stem: need | Lemma: need | POS Lemma: need\n",
      "Token: provide | Stem: provid | Lemma: provide | POS Lemma: provide\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: members | Stem: member | Lemma: member | POS Lemma: member\n",
      "Token: direct | Stem: direct | Lemma: direct | POS Lemma: direct\n",
      "Token: reports | Stem: report | Lemma: report | POS Lemma: report\n",
      "Token: clear | Stem: clear | Lemma: clear | POS Lemma: clear\n",
      "Token: direction | Stem: direct | Lemma: direction | POS Lemma: direction\n",
      "Token: targets | Stem: target | Lemma: target | POS Lemma: target\n",
      "Token: aligned | Stem: align | Lemma: aligned | POS Lemma: align\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: needs | Stem: need | Lemma: need | POS Lemma: need\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: objectives | Stem: object | Lemma: objective | POS Lemma: objective\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: relationships | Stem: relationship | Lemma: relationship | POS Lemma: relationship\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: creative | Stem: creativ | Lemma: creative | POS Lemma: creative\n",
      "Token: operations | Stem: oper | Lemma: operation | POS Lemma: operation\n",
      "Token: production | Stem: product | Lemma: production | POS Lemma: production\n",
      "Token: content | Stem: content | Lemma: content | POS Lemma: content\n",
      "Token: strategy | Stem: strategi | Lemma: strategy | POS Lemma: strategy\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: teams | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: design | Stem: design | Lemma: design | POS Lemma: design\n",
      "Token: activation | Stem: activ | Lemma: activation | POS Lemma: activation\n",
      "Token: major | Stem: major | Lemma: major | POS Lemma: major\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: comms | Stem: comm | Lemma: comms | POS Lemma: comms\n",
      "Token: teams | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: operations | Stem: oper | Lemma: operation | POS Lemma: operation\n",
      "Token: brand | Stem: brand | Lemma: brand | POS Lemma: brand\n",
      "Token: design | Stem: design | Lemma: design | POS Lemma: design\n",
      "Token: major | Stem: major | Lemma: major | POS Lemma: major\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: teams | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: growth | Stem: growth | Lemma: growth | POS Lemma: growth\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: products | Stem: product | Lemma: product | POS Lemma: product\n",
      "Token: dto | Stem: dto | Lemma: dto | POS Lemma: dto\n",
      "Token: respective | Stem: respect | Lemma: respective | POS Lemma: respective\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: function | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: gops | Stem: gop | Lemma: gop | POS Lemma: gop\n",
      "Token: finance | Stem: financ | Lemma: finance | POS Lemma: finance\n",
      "Token: hr | Stem: hr | Lemma: hr | POS Lemma: hr\n",
      "Token: brand | Stem: brand | Lemma: brand | POS Lemma: brand\n",
      "Token: marketing | Stem: market | Lemma: marketing | POS Lemma: marketing\n",
      "Token: wholesale | Stem: wholesal | Lemma: wholesale | POS Lemma: wholesale\n",
      "Token: retail | Stem: retail | Lemma: retail | POS Lemma: retail\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: external | Stem: extern | Lemma: external | POS Lemma: external\n",
      "Token: studios | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: agencies | Stem: agenc | Lemma: agency | POS Lemma: agency\n",
      "Token: procurement | Stem: procur | Lemma: procurement | POS Lemma: procurement\n",
      "Token: requirements | Stem: requir | Lemma: requirement | POS Lemma: requirement\n",
      "Token: education | Stem: educ | Lemma: education | POS Lemma: education\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: ideally | Stem: ideal | Lemma: ideally | POS Lemma: ideally\n",
      "Token: master | Stem: master | Lemma: master | POS Lemma: master\n",
      "Token: degree | Stem: degre | Lemma: degree | POS Lemma: degree\n",
      "Token: focus | Stem: focus | Lemma: focus | POS Lemma: focus\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: administration | Stem: administr | Lemma: administration | POS Lemma: administration\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: related | Stem: relat | Lemma: related | POS Lemma: related\n",
      "Token: areas | Stem: area | Lemma: area | POS Lemma: area\n",
      "Token: equivalent | Stem: equival | Lemma: equivalent | POS Lemma: equivalent\n",
      "Token: combination | Stem: combin | Lemma: combination | POS Lemma: combination\n",
      "Token: education | Stem: educ | Lemma: education | POS Lemma: education\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: 10+ | Stem: 10+ | Lemma: 10+ | POS Lemma: 10+\n",
      "Token: years | Stem: year | Lemma: year | POS Lemma: year\n",
      "Token: in-depth | Stem: in-depth | Lemma: in-depth | POS Lemma: in-depth\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: related | Stem: relat | Lemma: related | POS Lemma: related\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: similar | Stem: similar | Lemma: similar | POS Lemma: similar\n",
      "Token: topics | Stem: topic | Lemma: topic | POS Lemma: topic\n",
      "Token: 6-8 | Stem: 6-8 | Lemma: 6-8 | POS Lemma: 6-8\n",
      "Token: years | Stem: year | Lemma: year | POS Lemma: year\n",
      "Token: professional | Stem: profession | Lemma: professional | POS Lemma: professional\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: ecosystem | Stem: ecosystem | Lemma: ecosystem | POS Lemma: ecosystem\n",
      "Token: environment | Stem: environ | Lemma: environment | POS Lemma: environment\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: in-house | Stem: in-hous | Lemma: in-house | POS Lemma: in-house\n",
      "Token: studio | Stem: studio | Lemma: studio | POS Lemma: studio\n",
      "Token: roll-out | Stem: roll-out | Lemma: roll-out | POS Lemma: roll-out\n",
      "Token: understanding | Stem: understand | Lemma: understanding | POS Lemma: understand\n",
      "Token: ecom | Stem: ecom | Lemma: ecom | POS Lemma: ecom\n",
      "Token: image | Stem: imag | Lemma: image | POS Lemma: image\n",
      "Token: video | Stem: video | Lemma: video | POS Lemma: video\n",
      "Token: copy | Stem: copi | Lemma: copy | POS Lemma: copy\n",
      "Token: mass | Stem: mass | Lemma: mass | POS Lemma: mass\n",
      "Token: production | Stem: product | Lemma: production | POS Lemma: production\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: agencies | Stem: agenc | Lemma: agency | POS Lemma: agency\n",
      "Token: consultancy | Stem: consult | Lemma: consultancy | POS Lemma: consultancy\n",
      "Token: plus | Stem: plus | Lemma: plus | POS Lemma: plus\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: various | Stem: various | Lemma: various | POS Lemma: various\n",
      "Token: seniority | Stem: senior | Lemma: seniority | POS Lemma: seniority\n",
      "Token: levels | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: well | Stem: well | Lemma: well | POS Lemma: well\n",
      "Token: subject | Stem: subject | Lemma: subject | POS Lemma: subject\n",
      "Token: matter | Stem: matter | Lemma: matter | POS Lemma: matter\n",
      "Token: experts | Stem: expert | Lemma: expert | POS Lemma: expert\n",
      "Token: across | Stem: across | Lemma: across | POS Lemma: across\n",
      "Token: various | Stem: various | Lemma: various | POS Lemma: various\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: developers | Stem: develop | Lemma: developer | POS Lemma: developer\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: product | Stem: product | Lemma: product | POS Lemma: product\n",
      "Token: marketing | Stem: market | Lemma: marketing | POS Lemma: marketing\n",
      "Token: teams | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: et | Stem: et | Lemma: et | POS Lemma: et\n",
      "Token: cetera | Stem: cetera | Lemma: cetera | POS Lemma: cetera\n",
      "Token: 3+ | Stem: 3+ | Lemma: 3+ | POS Lemma: 3+\n",
      "Token: years | Stem: year | Lemma: year | POS Lemma: year\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: leading | Stem: lead | Lemma: leading | POS Lemma: lead\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: soft-skills | Stem: soft-skil | Lemma: soft-skills | POS Lemma: soft-skills\n",
      "Token: good | Stem: good | Lemma: good | POS Lemma: good\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: especially | Stem: especi | Lemma: especially | POS Lemma: especially\n",
      "Token: interacting | Stem: interact | Lemma: interacting | POS Lemma: interact\n",
      "Token: different | Stem: differ | Lemma: different | POS Lemma: different\n",
      "Token: levels | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: ability | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: work | Stem: work | Lemma: work | POS Lemma: work\n",
      "Token: fast-paced | Stem: fast-pac | Lemma: fast-paced | POS Lemma: fast-paced\n",
      "Token: environment | Stem: environ | Lemma: environment | POS Lemma: environment\n",
      "Token: different | Stem: differ | Lemma: different | POS Lemma: different\n",
      "Token: international | Stem: intern | Lemma: international | POS Lemma: international\n",
      "Token: cultures | Stem: cultur | Lemma: culture | POS Lemma: culture\n",
      "Token: ability | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: handle | Stem: handl | Lemma: handle | POS Lemma: handle\n",
      "Token: ambiguity | Stem: ambigu | Lemma: ambiguity | POS Lemma: ambiguity\n",
      "Token: untangle | Stem: untangl | Lemma: untangle | POS Lemma: untangle\n",
      "Token: complex | Stem: complex | Lemma: complex | POS Lemma: complex\n",
      "Token: situations | Stem: situat | Lemma: situation | POS Lemma: situation\n",
      "Token: actionable | Stem: action | Lemma: actionable | POS Lemma: actionable\n",
      "Token: activities | Stem: activ | Lemma: activity | POS Lemma: activity\n",
      "Token: distinctive | Stem: distinct | Lemma: distinctive | POS Lemma: distinctive\n",
      "Token: strategic | Stem: strateg | Lemma: strategic | POS Lemma: strategic\n",
      "Token: mindset | Stem: mindset | Lemma: mindset | POS Lemma: mindset\n",
      "Token: ability | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: prioritize | Stem: priorit | Lemma: prioritize | POS Lemma: prioritize\n",
      "Token: delegate | Stem: deleg | Lemma: delegate | POS Lemma: delegate\n",
      "Token: high | Stem: high | Lemma: high | POS Lemma: high\n",
      "Token: numbers | Stem: number | Lemma: number | POS Lemma: number\n",
      "Token: tasks | Stem: task | Lemma: task | POS Lemma: task\n",
      "Token: varying | Stem: vari | Lemma: varying | POS Lemma: vary\n",
      "Token: workload | Stem: workload | Lemma: workload | POS Lemma: workload\n",
      "Token: importance | Stem: import | Lemma: importance | POS Lemma: importance\n",
      "Token: solutions-oriented | Stem: solutions-ori | Lemma: solutions-oriented | POS Lemma: solutions-oriented\n",
      "Token: approach | Stem: approach | Lemma: approach | POS Lemma: approach\n",
      "Token: entrepreneurial | Stem: entrepreneuri | Lemma: entrepreneurial | POS Lemma: entrepreneurial\n",
      "Token: mindset | Stem: mindset | Lemma: mindset | POS Lemma: mindset\n",
      "Token: good | Stem: good | Lemma: good | POS Lemma: good\n",
      "Token: numerical | Stem: numer | Lemma: numerical | POS Lemma: numerical\n",
      "Token: analytical | Stem: analyt | Lemma: analytical | POS Lemma: analytical\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: highly | Stem: high | Lemma: highly | POS Lemma: highly\n",
      "Token: developed | Stem: develop | Lemma: developed | POS Lemma: developed\n",
      "Token: leadership | Stem: leadership | Lemma: leadership | POS Lemma: leadership\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: required | Stem: requir | Lemma: required | POS Lemma: require\n",
      "Token: hard-skills | Stem: hard-skil | Lemma: hard-skills | POS Lemma: hard-skills\n",
      "Token: knowledge | Stem: knowledg | Lemma: knowledge | POS Lemma: knowledge\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: technologies | Stem: technolog | Lemma: technology | POS Lemma: technology\n",
      "Token: communications | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: platforms | Stem: platform | Lemma: platform | POS Lemma: platform\n",
      "Token: culture | Stem: cultur | Lemma: culture | POS Lemma: culture\n",
      "Token: strong | Stem: strong | Lemma: strong | POS Lemma: strong\n",
      "Token: ms-office | Stem: ms-offic | Lemma: ms-office | POS Lemma: ms-office\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: word | Stem: word | Lemma: word | POS Lemma: word\n",
      "Token: excel | Stem: excel | Lemma: excel | POS Lemma: excel\n",
      "Token: powerpoint | Stem: powerpoint | Lemma: powerpoint | POS Lemma: powerpoint\n",
      "Token: ability | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: travel | Stem: travel | Lemma: travel | POS Lemma: travel\n",
      "Token: domestic | Stem: domest | Lemma: domestic | POS Lemma: domestic\n",
      "Token: international | Stem: intern | Lemma: international | POS Lemma: international\n",
      "Token: required | Stem: requir | Lemma: required | POS Lemma: require\n",
      "Token: in-depth | Stem: in-depth | Lemma: in-depth | POS Lemma: in-depth\n",
      "Token: understanding | Stem: understand | Lemma: understanding | POS Lemma: understand\n",
      "Token: fluent | Stem: fluent | Lemma: fluent | POS Lemma: fluent\n",
      "Token: english | Stem: english | Lemma: english | POS Lemma: english\n",
      "Token: verbally | Stem: verbal | Lemma: verbally | POS Lemma: verbally\n",
      "Token: written | Stem: written | Lemma: written | POS Lemma: write\n",
      "Token: purpose | Stem: purpos | Lemma: purpose | POS Lemma: purpose\n",
      "Token: manager | Stem: manag | Lemma: manager | POS Lemma: manager\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: strategy | Stem: strategi | Lemma: strategy | POS Lemma: strategy\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: adidas | Stem: adida | Lemma: adidas | POS Lemma: adidas\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: ecom | Stem: ecom | Lemma: ecom | POS Lemma: ecom\n",
      "Token: organization | Stem: organ | Lemma: organization | POS Lemma: organization\n",
      "Token: task | Stem: task | Lemma: task | POS Lemma: task\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: small | Stem: small | Lemma: small | POS Lemma: small\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: significant | Stem: signific | Lemma: significant | POS Lemma: significant\n",
      "Token: rfcs | Stem: rfcs | Lemma: rfcs | POS Lemma: rfcs\n",
      "Token: sub-elements | Stem: sub-el | Lemma: sub-elements | POS Lemma: sub-elements\n",
      "Token: mid-scale | Stem: mid-scal | Lemma: mid-scale | POS Lemma: mid-scale\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: level | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: across | Stem: across | Lemma: across | POS Lemma: across\n",
      "Token: brands | Stem: brand | Lemma: brand | POS Lemma: brand\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: objective | Stem: object | Lemma: objective | POS Lemma: objective\n",
      "Token: conduct | Stem: conduct | Lemma: conduct | POS Lemma: conduct\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: boundaries | Stem: boundari | Lemma: boundary | POS Lemma: boundary\n",
      "Token: time | Stem: time | Lemma: time | POS Lemma: time\n",
      "Token: cost | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: quality | Stem: qualiti | Lemma: quality | POS Lemma: quality\n",
      "Token: focusing | Stem: focus | Lemma: focusing | POS Lemma: focus\n",
      "Token: consumers | Stem: consum | Lemma: consumer | POS Lemma: consumer\n",
      "Token: expectations | Stem: expect | Lemma: expectation | POS Lemma: expectation\n",
      "Token: requirements | Stem: requir | Lemma: requirement | POS Lemma: requirement\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: executing | Stem: execut | Lemma: executing | POS Lemma: execute\n",
      "Token: tracking | Stem: track | Lemma: tracking | POS Lemma: track\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: cooperation | Stem: cooper | Lemma: cooperation | POS Lemma: cooperation\n",
      "Token: respective | Stem: respect | Lemma: respective | POS Lemma: respective\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: members | Stem: member | Lemma: member | POS Lemma: member\n",
      "Token: examples | Stem: exampl | Lemma: example | POS Lemma: example\n",
      "Token: expected | Stem: expect | Lemma: expected | POS Lemma: expect\n",
      "Token: role | Stem: role | Lemma: role | POS Lemma: role\n",
      "Token: additionally | Stem: addit | Lemma: additionally | POS Lemma: additionally\n",
      "Token: focusing | Stem: focus | Lemma: focusing | POS Lemma: focus\n",
      "Token: resource | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: coordination | Stem: coordin | Lemma: coordination | POS Lemma: coordination\n",
      "Token: activities | Stem: activ | Lemma: activity | POS Lemma: activity\n",
      "Token: acting | Stem: act | Lemma: acting | POS Lemma: act\n",
      "Token: central | Stem: central | Lemma: central | POS Lemma: central\n",
      "Token: interface | Stem: interfac | Lemma: interface | POS Lemma: interface\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: responsibilities | Stem: respons | Lemma: responsibility | POS Lemma: responsibility\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: small | Stem: small | Lemma: small | POS Lemma: small\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: level | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: delivery | Stem: deliveri | Lemma: delivery | POS Lemma: delivery\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: end-to-end | Stem: end-to-end | Lemma: end-to-end | POS Lemma: end-to-end\n",
      "Token: successfully | Stem: success | Lemma: successfully | POS Lemma: successfully\n",
      "Token: ideally | Stem: ideal | Lemma: ideally | POS Lemma: ideally\n",
      "Token: effective | Stem: effect | Lemma: effective | POS Lemma: effective\n",
      "Token: application | Stem: applic | Lemma: application | POS Lemma: application\n",
      "Token: pmi | Stem: pmi | Lemma: pmi | POS Lemma: pmi\n",
      "Token: agile | Stem: agil | Lemma: agile | POS Lemma: agile\n",
      "Token: methodology | Stem: methodolog | Lemma: methodology | POS Lemma: methodology\n",
      "Token: develop | Stem: develop | Lemma: develop | POS Lemma: develop\n",
      "Token: detailed | Stem: detail | Lemma: detailed | POS Lemma: detailed\n",
      "Token: plans | Stem: plan | Lemma: plan | POS Lemma: plan\n",
      "Token: allow | Stem: allow | Lemma: allow | POS Lemma: allow\n",
      "Token: providing | Stem: provid | Lemma: providing | POS Lemma: provide\n",
      "Token: transparency | Stem: transpar | Lemma: transparency | POS Lemma: transparency\n",
      "Token: progress | Stem: progress | Lemma: progress | POS Lemma: progress\n",
      "Token: identifying | Stem: identifi | Lemma: identifying | POS Lemma: identify\n",
      "Token: risks | Stem: risk | Lemma: risk | POS Lemma: risk\n",
      "Token: time | Stem: time | Lemma: time | POS Lemma: time\n",
      "Token: identify | Stem: identifi | Lemma: identify | POS Lemma: identify\n",
      "Token: interdependencies | Stem: interdepend | Lemma: interdependency | POS Lemma: interdependency\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: solve | Stem: solv | Lemma: solve | POS Lemma: solve\n",
      "Token: issues | Stem: issu | Lemma: issue | POS Lemma: issue\n",
      "Token: proactively | Stem: proactiv | Lemma: proactively | POS Lemma: proactively\n",
      "Token: drive | Stem: drive | Lemma: drive | POS Lemma: drive\n",
      "Token: continuous | Stem: continu | Lemma: continuous | POS Lemma: continuous\n",
      "Token: improvement | Stem: improv | Lemma: improvement | POS Lemma: improvement\n",
      "Token: products | Stem: product | Lemma: product | POS Lemma: product\n",
      "Token: processes | Stem: process | Lemma: process | POS Lemma: process\n",
      "Token: systems | Stem: system | Lemma: system | POS Lemma: system\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: resource | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: planning | Stem: plan | Lemma: planning | POS Lemma: planning\n",
      "Token: lead | Stem: lead | Lemma: lead | POS Lemma: lead\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: resources | Stem: resourc | Lemma: resource | POS Lemma: resource\n",
      "Token: internals | Stem: intern | Lemma: internals | POS Lemma: internals\n",
      "Token: externals | Stem: extern | Lemma: external | POS Lemma: external\n",
      "Token: deliver | Stem: deliv | Lemma: deliver | POS Lemma: deliver\n",
      "Token: support | Stem: support | Lemma: support | POS Lemma: support\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: act | Stem: act | Lemma: act | POS Lemma: act\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: contact | Stem: contact | Lemma: contact | POS Lemma: contact\n",
      "Token: person | Stem: person | Lemma: person | POS Lemma: person\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: apply | Stem: appli | Lemma: apply | POS Lemma: apply\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: effective | Stem: effect | Lemma: effective | POS Lemma: effective\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: methods | Stem: method | Lemma: method | POS Lemma: method\n",
      "Token: senior | Stem: senior | Lemma: senior | POS Lemma: senior\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: important | Stem: import | Lemma: important | POS Lemma: important\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: throughout | Stem: throughout | Lemma: throughout | POS Lemma: throughout\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: lifecycle | Stem: lifecycl | Lemma: lifecycle | POS Lemma: lifecycle\n",
      "Token: conflicts | Stem: conflict | Lemma: conflict | POS Lemma: conflict\n",
      "Token: escalations | Stem: escal | Lemma: escalation | POS Lemma: escalation\n",
      "Token: arise | Stem: aris | Lemma: arise | POS Lemma: arise\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: identify | Stem: identifi | Lemma: identify | POS Lemma: identify\n",
      "Token: solutions | Stem: solut | Lemma: solution | POS Lemma: solution\n",
      "Token: drive | Stem: drive | Lemma: drive | POS Lemma: drive\n",
      "Token: resolution | Stem: resolut | Lemma: resolution | POS Lemma: resolution\n",
      "Token: timely | Stem: time | Lemma: timely | POS Lemma: timely\n",
      "Token: appropriate | Stem: appropri | Lemma: appropriate | POS Lemma: appropriate\n",
      "Token: manner | Stem: manner | Lemma: manner | POS Lemma: manner\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: change | Stem: chang | Lemma: change | POS Lemma: change\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: projects | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: changes | Stem: chang | Lemma: change | POS Lemma: change\n",
      "Token: smoothly | Stem: smooth | Lemma: smoothly | POS Lemma: smoothly\n",
      "Token: successfully | Stem: success | Lemma: successfully | POS Lemma: successfully\n",
      "Token: implemented | Stem: implement | Lemma: implemented | POS Lemma: implement\n",
      "Token: achieve | Stem: achiev | Lemma: achieve | POS Lemma: achieve\n",
      "Token: lasting | Stem: last | Lemma: lasting | POS Lemma: last\n",
      "Token: benefits | Stem: benefit | Lemma: benefit | POS Lemma: benefit\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: controlling | Stem: control | Lemma: controlling | POS Lemma: control\n",
      "Token: manage | Stem: manag | Lemma: manage | POS Lemma: manage\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: controlling | Stem: control | Lemma: controlling | POS Lemma: control\n",
      "Token: independent | Stem: independ | Lemma: independent | POS Lemma: independent\n",
      "Token: element | Stem: element | Lemma: element | POS Lemma: element\n",
      "Token: ensure | Stem: ensur | Lemma: ensure | POS Lemma: ensure\n",
      "Token: actual | Stem: actual | Lemma: actual | POS Lemma: actual\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: costs | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: line | Stem: line | Lemma: line | POS Lemma: line\n",
      "Token: committed | Stem: commit | Lemma: committed | POS Lemma: commit\n",
      "Token: budget | Stem: budget | Lemma: budget | POS Lemma: budget\n",
      "Token: conduct | Stem: conduct | Lemma: conduct | POS Lemma: conduct\n",
      "Token: engagement | Stem: engag | Lemma: engagement | POS Lemma: engagement\n",
      "Token: reviews | Stem: review | Lemma: review | POS Lemma: review\n",
      "Token: verify | Stem: verifi | Lemma: verify | POS Lemma: verify\n",
      "Token: compliance | Stem: complianc | Lemma: compliance | POS Lemma: compliance\n",
      "Token: quality | Stem: qualiti | Lemma: quality | POS Lemma: quality\n",
      "Token: assurance | Stem: assur | Lemma: assurance | POS Lemma: assurance\n",
      "Token: procedures | Stem: procedur | Lemma: procedure | POS Lemma: procedure\n",
      "Token: validate | Stem: valid | Lemma: validate | POS Lemma: validate\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: delivering | Stem: deliv | Lemma: delivering | POS Lemma: deliver\n",
      "Token: program | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: kpis | Stem: kpis | Lemma: kpis | POS Lemma: kpis\n",
      "Token: monitor | Stem: monitor | Lemma: monitor | POS Lemma: monitor\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: variables | Stem: variabl | Lemma: variable | POS Lemma: variable\n",
      "Token: cost | Stem: cost | Lemma: cost | POS Lemma: cost\n",
      "Token: effort | Stem: effort | Lemma: effort | POS Lemma: effort\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: et | Stem: et | Lemma: et | POS Lemma: et\n",
      "Token: cetera | Stem: cetera | Lemma: cetera | POS Lemma: cetera\n",
      "Token: plan | Stem: plan | Lemma: plan | POS Lemma: plan\n",
      "Token: order | Stem: order | Lemma: order | POS Lemma: order\n",
      "Token: implement | Stem: implement | Lemma: implement | POS Lemma: implement\n",
      "Token: corrective | Stem: correct | Lemma: corrective | POS Lemma: corrective\n",
      "Token: preventive | Stem: prevent | Lemma: preventive | POS Lemma: preventive\n",
      "Token: actions | Stem: action | Lemma: action | POS Lemma: action\n",
      "Token: `` | Stem: `` | Lemma: `` | POS Lemma: ``\n",
      "Token: required | Stem: requir | Lemma: required | POS Lemma: require\n",
      "Token: '' | Stem: '' | Lemma: '' | POS Lemma: ''\n",
      "Token: responsibilities | Stem: respons | Lemma: responsibility | POS Lemma: responsibility\n",
      "Token: agile | Stem: agil | Lemma: agile | POS Lemma: agile\n",
      "Token: transformation | Stem: transform | Lemma: transformation | POS Lemma: transformation\n",
      "Token: drive | Stem: drive | Lemma: drive | POS Lemma: drive\n",
      "Token: implementation | Stem: implement | Lemma: implementation | POS Lemma: implementation\n",
      "Token: elements | Stem: element | Lemma: element | POS Lemma: element\n",
      "Token: transformation | Stem: transform | Lemma: transformation | POS Lemma: transformation\n",
      "Token: full | Stem: full | Lemma: full | POS Lemma: full\n",
      "Token: agile | Stem: agil | Lemma: agile | POS Lemma: agile\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: mode | Stem: mode | Lemma: mode | POS Lemma: mode\n",
      "Token: facilitate | Stem: facilit | Lemma: facilitate | POS Lemma: facilitate\n",
      "Token: agile | Stem: agil | Lemma: agile | POS Lemma: agile\n",
      "Token: adoption | Stem: adopt | Lemma: adoption | POS Lemma: adoption\n",
      "Token: contribute | Stem: contribut | Lemma: contribute | POS Lemma: contribute\n",
      "Token: developing | Stem: develop | Lemma: developing | POS Lemma: develop\n",
      "Token: orchestrating | Stem: orchestr | Lemma: orchestrating | POS Lemma: orchestrate\n",
      "Token: demand | Stem: demand | Lemma: demand | POS Lemma: demand\n",
      "Token: delivery | Stem: deliveri | Lemma: delivery | POS Lemma: delivery\n",
      "Token: process | Stem: process | Lemma: process | POS Lemma: process\n",
      "Token: within | Stem: within | Lemma: within | POS Lemma: within\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: facilitate | Stem: facilit | Lemma: facilitate | POS Lemma: facilitate\n",
      "Token: initial | Stem: initi | Lemma: initial | POS Lemma: initial\n",
      "Token: demand | Stem: demand | Lemma: demand | POS Lemma: demand\n",
      "Token: discussions | Stem: discuss | Lemma: discussion | POS Lemma: discussion\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: development | Stem: develop | Lemma: development | POS Lemma: development\n",
      "Token: create | Stem: creat | Lemma: create | POS Lemma: create\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: cases | Stem: case | Lemma: case | POS Lemma: case\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: support | Stem: support | Lemma: support | POS Lemma: support\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: analytics | Stem: analyt | Lemma: analytics | POS Lemma: analytics\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: evaluate | Stem: evalu | Lemma: evaluate | POS Lemma: evaluate\n",
      "Token: growth | Stem: growth | Lemma: growth | POS Lemma: growth\n",
      "Token: opportunities | Stem: opportun | Lemma: opportunity | POS Lemma: opportunity\n",
      "Token: implement | Stem: implement | Lemma: implement | POS Lemma: implement\n",
      "Token: base | Stem: base | Lemma: base | POS Lemma: base\n",
      "Token: platform | Stem: platform | Lemma: platform | POS Lemma: platform\n",
      "Token: capabilities | Stem: capabl | Lemma: capability | POS Lemma: capability\n",
      "Token: build | Stem: build | Lemma: build | POS Lemma: build\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: required | Stem: requir | Lemma: required | POS Lemma: require\n",
      "Token: fulfill | Stem: fulfil | Lemma: fulfill | POS Lemma: fulfill\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: cases | Stem: case | Lemma: case | POS Lemma: case\n",
      "Token: together | Stem: togeth | Lemma: together | POS Lemma: together\n",
      "Token: key | Stem: key | Lemma: key | POS Lemma: key\n",
      "Token: relationships | Stem: relationship | Lemma: relationship | POS Lemma: relationship\n",
      "Token: leading | Stem: lead | Lemma: leading | POS Lemma: lead\n",
      "Token: sr | Stem: sr | Lemma: sr | POS Lemma: sr\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: manager | Stem: manag | Lemma: manager | POS Lemma: manager\n",
      "Token: sr. | Stem: sr. | Lemma: sr. | POS Lemma: sr.\n",
      "Token: dir | Stem: dir | Lemma: dir | POS Lemma: dir\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: strategy | Stem: strategi | Lemma: strategy | POS Lemma: strategy\n",
      "Token: programs | Stem: program | Lemma: program | POS Lemma: program\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: team | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: local | Stem: local | Lemma: local | POS Lemma: local\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: global | Stem: global | Lemma: global | POS Lemma: global\n",
      "Token: digital | Stem: digit | Lemma: digital | POS Lemma: digital\n",
      "Token: sales | Stem: sale | Lemma: sale | POS Lemma: sale\n",
      "Token: solutions | Stem: solut | Lemma: solution | POS Lemma: solution\n",
      "Token: respective | Stem: respect | Lemma: respective | POS Lemma: respective\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: ops | Stem: op | Lemma: ops | POS Lemma: ops\n",
      "Token: finance | Stem: financ | Lemma: finance | POS Lemma: finance\n",
      "Token: hr | Stem: hr | Lemma: hr | POS Lemma: hr\n",
      "Token: brand | Stem: brand | Lemma: brand | POS Lemma: brand\n",
      "Token: marketing | Stem: market | Lemma: marketing | POS Lemma: marketing\n",
      "Token: wholesale | Stem: wholesal | Lemma: wholesale | POS Lemma: wholesale\n",
      "Token: retail | Stem: retail | Lemma: retail | POS Lemma: retail\n",
      "Token: hr | Stem: hr | Lemma: hr | POS Lemma: hr\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: controlling | Stem: control | Lemma: controlling | POS Lemma: control\n",
      "Token: education | Stem: educ | Lemma: education | POS Lemma: education\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: university | Stem: univers | Lemma: university | POS Lemma: university\n",
      "Token: degree | Stem: degre | Lemma: degree | POS Lemma: degree\n",
      "Token: focus | Stem: focus | Lemma: focus | POS Lemma: focus\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: administration | Stem: administr | Lemma: administration | POS Lemma: administration\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: related | Stem: relat | Lemma: related | POS Lemma: related\n",
      "Token: areas | Stem: area | Lemma: area | POS Lemma: area\n",
      "Token: equivalent | Stem: equival | Lemma: equivalent | POS Lemma: equivalent\n",
      "Token: combination | Stem: combin | Lemma: combination | POS Lemma: combination\n",
      "Token: education | Stem: educ | Lemma: education | POS Lemma: education\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: 5+ | Stem: 5+ | Lemma: 5+ | POS Lemma: 5+\n",
      "Token: years | Stem: year | Lemma: year | POS Lemma: year\n",
      "Token: in-depth | Stem: in-depth | Lemma: in-depth | POS Lemma: in-depth\n",
      "Token: professional | Stem: profession | Lemma: professional | POS Lemma: professional\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: related | Stem: relat | Lemma: related | POS Lemma: related\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: management | Stem: manag | Lemma: management | POS Lemma: management\n",
      "Token: similar | Stem: similar | Lemma: similar | POS Lemma: similar\n",
      "Token: topics | Stem: topic | Lemma: topic | POS Lemma: topic\n",
      "Token: 1-3 | Stem: 1-3 | Lemma: 1-3 | POS Lemma: 1-3\n",
      "Token: years | Stem: year | Lemma: year | POS Lemma: year\n",
      "Token: professional | Stem: profession | Lemma: professional | POS Lemma: professional\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: ecosystem | Stem: ecosystem | Lemma: ecosystem | POS Lemma: ecosystem\n",
      "Token: environment | Stem: environ | Lemma: environment | POS Lemma: environment\n",
      "Token: project | Stem: project | Lemma: project | POS Lemma: project\n",
      "Token: area | Stem: area | Lemma: area | POS Lemma: area\n",
      "Token: scope | Stem: scope | Lemma: scope | POS Lemma: scope\n",
      "Token: proven | Stem: proven | Lemma: proven | POS Lemma: proven\n",
      "Token: market | Stem: market | Lemma: market | POS Lemma: market\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: acumen | Stem: acumen | Lemma: acumen | POS Lemma: acumen\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: ecommerce | Stem: ecommerc | Lemma: ecommerce | POS Lemma: ecommerce\n",
      "Token: plus | Stem: plus | Lemma: plus | POS Lemma: plus\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: finance | Stem: financ | Lemma: finance | POS Lemma: finance\n",
      "Token: plus | Stem: plus | Lemma: plus | POS Lemma: plus\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: various | Stem: various | Lemma: various | POS Lemma: various\n",
      "Token: seniority | Stem: senior | Lemma: seniority | POS Lemma: seniority\n",
      "Token: levels | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: well | Stem: well | Lemma: well | POS Lemma: well\n",
      "Token: subject | Stem: subject | Lemma: subject | POS Lemma: subject\n",
      "Token: matter | Stem: matter | Lemma: matter | POS Lemma: matter\n",
      "Token: experts | Stem: expert | Lemma: expert | POS Lemma: expert\n",
      "Token: across | Stem: across | Lemma: across | POS Lemma: across\n",
      "Token: various | Stem: various | Lemma: various | POS Lemma: various\n",
      "Token: functions | Stem: function | Lemma: function | POS Lemma: function\n",
      "Token: developers | Stem: develop | Lemma: developer | POS Lemma: developer\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: stakeholders | Stem: stakehold | Lemma: stakeholder | POS Lemma: stakeholder\n",
      "Token: product | Stem: product | Lemma: product | POS Lemma: product\n",
      "Token: marketing | Stem: market | Lemma: marketing | POS Lemma: marketing\n",
      "Token: teams | Stem: team | Lemma: team | POS Lemma: team\n",
      "Token: et | Stem: et | Lemma: et | POS Lemma: et\n",
      "Token: cetera | Stem: cetera | Lemma: cetera | POS Lemma: cetera\n",
      "Token: knowledge | Stem: knowledg | Lemma: knowledge | POS Lemma: knowledge\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: abilities | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: soft-skills | Stem: soft-skil | Lemma: soft-skills | POS Lemma: soft-skills\n",
      "Token: good | Stem: good | Lemma: good | POS Lemma: good\n",
      "Token: communication | Stem: communic | Lemma: communication | POS Lemma: communication\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: especially | Stem: especi | Lemma: especially | POS Lemma: especially\n",
      "Token: interacting | Stem: interact | Lemma: interacting | POS Lemma: interact\n",
      "Token: different | Stem: differ | Lemma: different | POS Lemma: different\n",
      "Token: levels | Stem: level | Lemma: level | POS Lemma: level\n",
      "Token: business | Stem: busi | Lemma: business | POS Lemma: business\n",
      "Token: ability | Stem: abil | Lemma: ability | POS Lemma: ability\n",
      "Token: work | Stem: work | Lemma: work | POS Lemma: work\n",
      "Token: fast-paced | Stem: fast-pac | Lemma: fast-paced | POS Lemma: fast-paced\n",
      "Token: environment | Stem: environ | Lemma: environment | POS Lemma: environment\n",
      "Token: different | Stem: differ | Lemma: different | POS Lemma: different\n",
      "Token: international | Stem: intern | Lemma: international | POS Lemma: international\n",
      "Token: cultures | Stem: cultur | Lemma: culture | POS Lemma: culture\n",
      "Token: solutions-oriented | Stem: solutions-ori | Lemma: solutions-oriented | POS Lemma: solutions-oriented\n",
      "Token: approach | Stem: approach | Lemma: approach | POS Lemma: approach\n",
      "Token: entrepreneurial | Stem: entrepreneuri | Lemma: entrepreneurial | POS Lemma: entrepreneurial\n",
      "Token: mindset | Stem: mindset | Lemma: mindset | POS Lemma: mindset\n",
      "Token: good | Stem: good | Lemma: good | POS Lemma: good\n",
      "Token: numerical | Stem: numer | Lemma: numerical | POS Lemma: numerical\n",
      "Token: analytical | Stem: analyt | Lemma: analytical | POS Lemma: analytical\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: hard-skills | Stem: hard-skil | Lemma: hard-skills | POS Lemma: hard-skills\n",
      "Token: strong | Stem: strong | Lemma: strong | POS Lemma: strong\n",
      "Token: ms-office | Stem: ms-offic | Lemma: ms-office | POS Lemma: ms-office\n",
      "Token: skills | Stem: skill | Lemma: skill | POS Lemma: skill\n",
      "Token: word | Stem: word | Lemma: word | POS Lemma: word\n",
      "Token: excel | Stem: excel | Lemma: excel | POS Lemma: excel\n",
      "Token: powerpoint | Stem: powerpoint | Lemma: powerpoint | POS Lemma: powerpoint\n",
      "Token: basic | Stem: basic | Lemma: basic | POS Lemma: basic\n",
      "Token: experience | Stem: experi | Lemma: experience | POS Lemma: experience\n",
      "Token: broad | Stem: broad | Lemma: broad | POS Lemma: broad\n",
      "Token: understanding | Stem: understand | Lemma: understanding | POS Lemma: understand\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: knowledge | Stem: knowledg | Lemma: knowledge | POS Lemma: knowledge\n",
      "Token: agile | Stem: agil | Lemma: agile | POS Lemma: agile\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: methods | Stem: method | Lemma: method | POS Lemma: method\n",
      "Token: example | Stem: exampl | Lemma: example | POS Lemma: example\n",
      "Token: scrum | Stem: scrum | Lemma: scrum | POS Lemma: scrum\n",
      "Token: kanban | Stem: kanban | Lemma: kanban | POS Lemma: kanban\n",
      "Token: working | Stem: work | Lemma: working | POS Lemma: work\n",
      "Token: knowledge | Stem: knowledg | Lemma: knowledge | POS Lemma: knowledge\n",
      "Token: pmi | Stem: pmi | Lemma: pmi | POS Lemma: pmi\n",
      "Token: methods | Stem: method | Lemma: method | POS Lemma: method\n",
      "Token: ideally | Stem: ideal | Lemma: ideally | POS Lemma: ideally\n",
      "Token: certification | Stem: certif | Lemma: certification | POS Lemma: certification\n",
      "Token: fluent | Stem: fluent | Lemma: fluent | POS Lemma: fluent\n",
      "Token: english | Stem: english | Lemma: english | POS Lemma: english\n",
      "Token: verbally | Stem: verbal | Lemma: verbally | POS Lemma: verbally\n",
      "Token: written | Stem: written | Lemma: written | POS Lemma: write\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    token_lemma = lemmatizer.lemmatize(token)\n",
    "    token_stem = stemmer.stem(token)\n",
    "    token_pos = get_wordnet_pos(token)\n",
    "    token_pos_lemma = lemmatizer.lemmatize(token, token_pos)\n",
    "    print(f'Token: {token} | Stem: {token_stem} | Lemma: {token_lemma} | POS Lemma: {token_pos_lemma}')\n",
    "    nltk_lemmas.append(token_pos_lemma)\n",
    "    nltk_stems.append(token_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    token_lemma = lemmatizer.lemmatize(token)\n",
    "    token_stem = stemmer.stem(token)\n",
    "    token_pos = get_wordnet_pos(token)\n",
    "    nltk_lemmas.append(lemmatizer.lemmatize(token, get_wordnet_pos(token)))\n",
    "    nltk_stems.append(token_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "0e4b147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_lemmas = [\n",
    "    lemmatizer.lemmatize(\n",
    "        token, get_wordnet_pos(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "    )\n",
    "    for token in nltk_tokenized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "98d82225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_lemmas'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        lemmatizer.lemmatize(\n",
    "            token, get_wordnet_pos(\n",
    "                unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            )\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ae9723c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                              [purpose]\n",
       "1                                     [studio, director]\n",
       "2                                     [special, project]\n",
       "3      [adidas, digital, task, manage, deliver, studi...\n",
       "4      [program, scope, comprise, multiple, countries...\n",
       "                             ...                        \n",
       "130    [strong, ms, office, skills, word, excel, powe...\n",
       "131               [basic, experience, broad, understand]\n",
       "132    [work, knowledge, agile, work, methods, exampl...\n",
       "133    [work, knowledge, pmi, methods, ideally, certi...\n",
       "134                   [fluent, english, verbally, write]\n",
       "Name: Job Description nltk_lemmas, Length: 135, dtype: object"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description nltk_lemmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d8227a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk_lemmas = []\n",
    "nltk_stems = []\n",
    "\n",
    "for token in nltk_tokenized:\n",
    "    token = unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "    nltk_stems.append(stemmer.stem(unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "45564d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stems = [\n",
    "    stemmer.stem(\n",
    "        unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    )\n",
    "    for token in nltk_tokenized\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "47888b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nltk_stems'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
    "    lambda tokens: [\n",
    "        stemmer.stem(\n",
    "            unicodedata.normalize('NFKD', str(token.strip().lower())).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        )\n",
    "        for token in tokens\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "aede5700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               [purpos]\n",
       "1                                     [studio, director]\n",
       "2                                     [special, project]\n",
       "3      [adida, digit, task, manag, deliv, studio, pro...\n",
       "4      [program, scope, compris, multipl, countri, fu...\n",
       "                             ...                        \n",
       "130    [strong, ms, offic, skill, word, excel, powerp...\n",
       "131                   [basic, experi, broad, understand]\n",
       "132    [work, knowledg, agil, work, method, exampl, s...\n",
       "133         [work, knowledg, pmi, method, ideal, certif]\n",
       "134                   [fluent, english, verbal, written]\n",
       "Name: Job Description nltk_stems, Length: 135, dtype: object"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description nltk_stems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "b6cf9454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "7c008f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "\n",
    "gensim_tokenized = []\n",
    "\n",
    "for job_sentence in nltk_sentencized:\n",
    "    gensim_tokenized.append(preprocess_string(re.sub(pattern_numbers, ' ', job_sentence.strip().lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "2bcf8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purpos']"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "df785608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description gensim_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: preprocess_string(re.sub(pattern_numbers, ' ', job_sentence.strip().lower()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "fea685cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               [purpos]\n",
       "1                                     [studio, director]\n",
       "2                                     [special, project]\n",
       "3      [adida, digit, task, manag, deliv, studio, pro...\n",
       "4      [program, scope, compris, multipl, countri, fu...\n",
       "                             ...                        \n",
       "130      [strong, offic, skill, word, excel, powerpoint]\n",
       "131                   [basic, experi, broad, understand]\n",
       "132    [work, knowledg, agil, work, method, exampl, s...\n",
       "133         [work, knowledg, pmi, method, ideal, certif]\n",
       "134                   [fluent, english, verbal, written]\n",
       "Name: Job Description gensim_tokenized, Length: 135, dtype: object"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description gensim_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "52198bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_cases = {\":)\": [{\"ORTH\": \":)\"}]}\n",
    "# prefix_re = re.compile(r'''^[\\\\[\\\\(\"']''')\n",
    "# suffix_re = re.compile(r'''[\\\\]\\\\)\"']$''')\n",
    "# infix_re = re.compile(r'''[-~]''')\n",
    "# simple_url_re = re.compile(r'''^https?://''')\n",
    "\n",
    "# def custom_tokenizer(nlp):\n",
    "#     return Tokenizer(nlp.vocab, rules=special_cases,\n",
    "#                                 prefix_search=prefix_re.search,\n",
    "#                                 suffix_search=suffix_re.search,\n",
    "#                                 infix_finditer=infix_re.finditer,\n",
    "#                                 url_match=simple_url_re.match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "a65795c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.tokenizer.add_special_case('incl', [{ORTH: u'incl', NORM: u'include', LEMMA: u'include', POS: u'VERB'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c760575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy sentencizer\n",
    "# spacy_sentencized = []\n",
    "\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     for job_description in job_descriptions:\n",
    "#         for sentence in nlp(job_description).sents:\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0ea7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentencizer\n",
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    spacy_sentencized = [\n",
    "        sent \n",
    "        for job_description in job_descriptions \n",
    "        for sentence in nlp(job_description).sents \n",
    "        for sent in re.split(pattern, sentence.text) \n",
    "        if len(sent) != 0 \n",
    "    ]\n",
    "    \n",
    "    spacy_sentencized_lower = [\n",
    "        str(sent.strip().lower()) \n",
    "        for sent in spacy_sentencized\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "266c373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy sentencizer\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     spacy_sentencized = [\n",
    "#         re.split(pattern, sentence.text) \n",
    "#         for job_description in job_descriptions \n",
    "#         for sentence in nlp(job_description).sents \n",
    "# #         for sent in re.split(pattern, sentence.text) \n",
    "#         if len(sentence) != 0 \n",
    "#     ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d6eff4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drive change management activities for respective projects & programs and ensure changes are smoothly and successfully implemented to achieve lasting benefits.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentencized[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa78330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy tokenizer\n",
    "# spacy_tokenized = []\n",
    "\n",
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     for job_sentence in spacy_sentencized:\n",
    "#         for token in nlp.tokenizer(job_sentence):\n",
    "#             if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars:\n",
    "#                 spacy_tokenized.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78f574e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_sentencized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b715ec90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_sentencized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6ce53cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if str_fix_eg in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c222370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if str_fix_eg in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a454eeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "# str_fix = 'Apply appropriate and effective communication methods to senior management and important stakeholders \\(incl.'\n",
    "\n",
    "\n",
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if str_fix_incl.split('\\(')[0] in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd2a7d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if str_fix_incl.split('\\(')[0] in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c515eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Partner with Procurement in order to manage suppliers for the projects & programs in scope, for example interviews, staffing, supplier costs.',\n",
       " 'Communication   stakeholder management',\n",
       " 'Act as the key contact person for strategic stakeholders.',\n",
       " 'Apply appropriate and effective communication methods to senior management and important stakeholders (including vendor) throughout the project lifecycle.',\n",
       " 'As conflicts and escalations arise within projects, identify solutions and manage the resolution in a timely and appropriate manner.',\n",
       " 'Drive change management activities for respective projects & programs and ensure changes are smoothly and successfully implemented to achieve lasting benefits.',\n",
       " 'Project Controlling']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentencized[18:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d97282ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "for idx, sent in enumerate(spacy_sentencized):\n",
    "    if 'Power' in sent:\n",
    "        print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b2147d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(idx for idx, sent in enumerate(spacy_sentencized) if 'Power' in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b314b701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Strong MS-Office skills (Word, Excel, PowerPoint)'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentencized[63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5dcf1dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Partner with Procurement in order to manage suppliers for the projects & programs in scope, for example interviews, staffing, supplier costs.\n",
      "\n",
      "Sentence 2: Communication   stakeholder management\n",
      "\n",
      "Sentence 3: Act as the key contact person for strategic stakeholders.\n",
      "\n",
      "Sentence 4: Apply appropriate and effective communication methods to senior management and important stakeholders (including vendor) throughout the project lifecycle.\n",
      "\n",
      "Sentence 5: As conflicts and escalations arise within projects, identify solutions and manage the resolution in a timely and appropriate manner.\n",
      "\n",
      "Sentence 6: Drive change management activities for respective projects & programs and ensure changes are smoothly and successfully implemented to achieve lasting benefits.\n",
      "\n",
      "Sentence 7: Project Controlling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(spacy_sentencized[18:25]):\n",
    "    print(f'Sentence {idx+1}: {sentence}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "869b4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy tokenizer\n",
    "# spacy_tokenized = []\n",
    "\n",
    "# for job_sentence in spacy_sentencized:\n",
    "# #         doc = nlp.tokenizer(job_sentence)\n",
    "#     spacy_tokenized.extend(\n",
    "#         [\n",
    "#             token.text for token in nlp.tokenizer(job_sentence) \n",
    "#             if token.text not in custom_punct_chars\n",
    "#             and not token.is_stop \n",
    "\n",
    "#         ]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2eeddf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_sentencized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78d95615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Planning, executing and tracking projects & programs in cooperation with the respective functions and project team members are a few examples of what is expected from this role.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_sentencized[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8d84b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tokenizer\n",
    "\n",
    "spacy_tokenized = [\n",
    "    str(token.text.strip().lower()) \n",
    "    for job_sentence in spacy_sentencized \n",
    "    for token in nlp.tokenizer(job_sentence) \n",
    "    if len(token) != 0 \n",
    "    and not token.is_stop \n",
    "    and not token.is_punct \n",
    "    and not token.text in custom_punct_chars\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5f6eb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "910"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7c9a630b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purpose',\n",
       " 'studio',\n",
       " 'director',\n",
       " 'special',\n",
       " 'projects',\n",
       " 'adidas',\n",
       " 'digital',\n",
       " 'task',\n",
       " 'manage',\n",
       " 'deliver']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e6dcfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.get_pipe('attribute_ruler').add([[{\"TEXT\":\"Angeltown\"}]],{\"LEMMA\":\"San Fransisco\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91acafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Spacy pos tagging\n",
    "# nlp_stemmer = LancasterStemmer()\n",
    "# nlp_stemmer = PorterStemmer()\n",
    "\n",
    "nlp_token_tags = []\n",
    "nlp_lemmas = []\n",
    "nlp_stems = []\n",
    "\n",
    "for job_description in job_descriptions:\n",
    "    for token in nlp(job_description):\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars:\n",
    "            nlp_token_tags.append(tuple([token.text, token.tag_]))\n",
    "            nlp_lemmas.append(token.lemma_)\n",
    "            nlp_stems.append(stemmer.stem(token.text))\n",
    "#         for sentence in doc.sents:\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "d7002207",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_token_tags = [\n",
    "    tuple([token.text.strip().lower(), token.tag_])\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "56c2309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_token_tags'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        tuple([token.text.strip().lower(), token.tag_])\n",
    "        for token in nlp(job_sentence)\n",
    "        \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "ac855d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_lemmas = [\n",
    "    token.lemma_.strip().lower()\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "8553d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_lemmas'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        token.lemma_.strip().lower()\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "5fab24dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stems = [\n",
    "    stemmer.stem(token.text.strip().lower())\n",
    "    for job_description in job_descriptions\n",
    "    for token in nlp(job_description)\n",
    "    if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "dd9ccde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description nlp_stems'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda job_sentence: [\n",
    "        stemmer.stem(token.text.strip().lower())\n",
    "        for token in nlp(job_sentence)\n",
    "        if len(token) != 0 and not token.is_stop and not token.is_punct and token.text not in custom_punct_chars\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "cc9beae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               [purpos]\n",
       "1                                     [studio, director]\n",
       "2                                     [special, project]\n",
       "3      [adida, digit, task, manag, deliv, studio, pro...\n",
       "4      [program, scope, compris, multipl, countri, fu...\n",
       "                             ...                        \n",
       "130    [strong, ms, offic, skill, word, excel, powerp...\n",
       "131                   [basic, experi, broad, understand]\n",
       "132    [work, knowledg, agil, work, method, exampl, s...\n",
       "133         [work, knowledg, pmi, method, ideal, certif]\n",
       "134                   [fluent, english, verbal, written]\n",
       "Name: Job Description nlp_stems, Length: 135, dtype: object"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description nlp_stems']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84b610ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "#     nlp_token_tags = [\n",
    "#         tuple([token.text, token.tag_]) \n",
    "#         for job_description in job_descriptions \n",
    "#         for token in nlp(job_description) \n",
    "#         if len(token) != 0 \n",
    "#         and not token.is_stop \n",
    "#         and not token.is_punct \n",
    "#         and not token.text in custom_punct_chars\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "366d6b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_num_and_frequency(row, text_col):\n",
    "\n",
    "    row['num_words'] = len(str(row[f'{text_col}']).split())\n",
    "    row['num_unique_words'] = len(set(str(row[f'{text_col}']).split()))\n",
    "    row['num_chars'] = len(str(row[f'{text_col}']))\n",
    "    row['num_punctuations'] = len([c for c in str(row[f'{text_col}']) if c in string.punctuation])\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "5e640ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = df_jobs.apply(lambda row: get_word_num_and_frequency(row=row, text_col='Job Description spacy_sentencized'), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ab5cb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in spacy_sentencized:\n",
    "#     num_words = len(sent.split())\n",
    "#     num_unique_words = len(set(sent.split()))\n",
    "#     num_chars = len(sent)\n",
    "#     num_punctuations = len([c for c in sent if c in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "890a2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(df_jobs_to_be_processed, text_col, algo='vader', sentiment_range=(-1,1)):\n",
    "\n",
    "    ## calculate sentiment\n",
    "    if algo == 'vader':\n",
    "        df_jobs_to_be_processed['sentiment'] = df_jobs_to_be_processed[text_col].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x)['compound'] if isinstance(x, str) else np.nan)\n",
    "    elif algo == 'textblob':\n",
    "        df_jobs_to_be_processed['sentiment'] = df_jobs_to_be_processed[text_col].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    ## rescaled\n",
    "    if sentiment_range != (-1,1):\n",
    "        df_jobs_to_be_processed['sentiment'] = preprocessing.MinMaxScaler(feature_range=sentiment_range).fit_transform(df_jobs_to_be_processed[['sentiment']])\n",
    "    # print(df_jobs_to_be_processed[['sentiment']].describe().T)\n",
    "\n",
    "    return df_jobs_to_be_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "31a0f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_sentiment(df_jobs, text_col='Job Description spacy_sentencized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1cb854ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.4019\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4215\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.6369\n",
      "0.0\n",
      "0.0\n",
      "0.5719\n",
      "0.0\n",
      "0.4019\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5994\n",
      "-0.2263\n",
      "0.8807\n",
      "0.0\n",
      "0.5719\n",
      "0.4588\n",
      "0.34\n",
      "0.3818\n",
      "0.0\n",
      "0.0\n",
      "0.8225\n",
      "0.6369\n",
      "0.3818\n",
      "0.0\n",
      "0.4404\n",
      "0.0\n",
      "0.0\n",
      "0.3818\n",
      "0.4215\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4215\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.296\n",
      "0.0\n",
      "0.0\n",
      "0.4927\n",
      "0.3182\n",
      "0.3182\n",
      "0.5859\n",
      "0.0\n",
      "0.4404\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.743\n",
      "0.3182\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.2023\n",
      "0.0\n",
      "0.0\n",
      "0.4215\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.8442\n",
      "0.3818\n",
      "0.2023\n",
      "0.4588\n",
      "0.0\n",
      "0.4019\n",
      "0.0\n",
      "0.0\n",
      "0.5994\n",
      "-0.2263\n",
      "0.8126\n",
      "0.0\n",
      "0.5719\n",
      "0.4588\n",
      "0.34\n",
      "0.3612\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-0.128\n",
      "-0.128\n",
      "0.0\n",
      "0.8402\n",
      "0.4404\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.1779\n",
      "0.4215\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.296\n",
      "0.25\n",
      "0.0\n",
      "0.4404\n",
      "0.3182\n",
      "0.0\n",
      "0.4404\n",
      "0.0\n",
      "0.743\n",
      "0.0\n",
      "0.0\n",
      "0.4215\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "for sent in spacy_sentencized:\n",
    "    print(vader.polarity_scores(sent)['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2dd6c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment\n",
    "# from textblob import TextBlob, Word\n",
    "\n",
    "# for sent in spacy_sentencized:\n",
    "#     print(TextBlob(sent).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "2b8b9d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Bert Tokenizer\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertForPreTraining\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "df_jobs['bert_tokenized'] = df_jobs['Job Description spacy_sentencized'].apply(lambda sentence: bert_tokenizer.tokenize(str(sentence)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f80932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['bert_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.encode_plus(sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "525d00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_similarity(sentences, bert_model):\n",
    "\n",
    "    embeddings = bert_model.encode(sentences, show_progress_bar=True)\n",
    "    cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "    #Sort list by the highest cosine similarity score\n",
    "    all_sentence_combinations = sorted([[cos_sim[i][j], i, j] for i in range(len(cos_sim)-1) for j in range(i+1, len(cos_sim))], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    if args['print_enabled'] is True:\n",
    "        print('Top-5 most similar pairs:')\n",
    "        for score, i, j in all_sentence_combinations[:5]:\n",
    "            print(f'{sentences[i]} \\t {sentences[j]} \\t {cos_sim[i][j]:.4f}')\n",
    "\n",
    "    if args['save_enabled'] is True:\n",
    "        bert_model.to_json_file('bert_config.json')\n",
    "\n",
    "    return all_sentence_combinations, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c08abfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3800, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [101, 2004, 1037, 2996, 1037, 2472, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2569, 3934, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 27133, 8883, 3617, 2115, 4708, 2003, 2000, 6133, 1998, 8116, 2996, 1037, 3454, 2164, 2440, 3934, 2346, 2863, 2361, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1996, 2565, 9531, 2089, 15821, 3674, 3032, 1998, 4972, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2115, 7863, 2003, 2000, 6204, 1996, 3934, 2306, 1996, 7372, 1997, 2051, 1010, 3465, 1004, 3737, 2096, 7995, 2006, 10390, 1005, 10908, 1998, 5918, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4041, 1010, 23448, 1998, 9651, 3934, 1004, 3454, 1999, 6792, 2007, 1996, 7972, 4972, 1998, 2622, 2136, 2372, 2024, 1037, 2261, 4973, 1997, 2054, 2003, 3517, 2013, 2023, 2535, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 5678, 1010, 2017, 2097, 2022, 7995, 2006, 7692, 4041, 1010, 21431, 2968, 1998, 12016, 3450, 1010, 3772, 2004, 1996, 2430, 8278, 2000, 3145, 22859, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 9531, 1024, 6133, 1998, 8116, 2996, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1037, 3454, 2164, 2440, 3934, 2346, 2863, 2361, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 2565, 6959, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 1996, 2275, 2039, 1998, 4897, 2041, 1997, 1996, 2996, 1037, 9594, 2015, 1006, 6734, 1010, 8344, 1010, 2582, 5269, 1007, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 5676, 3154, 4082, 1998, 8290, 4275, 1010, 6194, 1998, 7316, 7705, 2015, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 1996, 6143, 5386, 2007, 6291, 16452, 1998, 1999, 3006, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2275, 2039, 8221, 2005, 2035, 2047, 2537, 7832, 29464, 7563, 1015, 8319, 4180, 1998, 6133, 2039, 2127, 1996, 2391, 2027, 2064, 2022, 2333, 2046, 3742, 2537, 2012, 4094, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 2035, 2327, 8543, 2537, 11186, 29464, 20773, 1010, 6887, 2906, 16230, 1010, 3802, 8292, 14621, 24805, 3351, 4637, 2291, 7248, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7692, 4041, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2433, 1010, 2599, 1010, 6133, 1998, 8080, 4800, 1011, 8360, 2136, 4219, 1006, 4722, 2015, 1998, 6327, 2015, 1007, 2000, 8116, 1998, 2490, 3934, 1004, 3454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4256, 2007, 21423, 1999, 2344, 2000, 6133, 20141, 2005, 1996, 3934, 1004, 3454, 1999, 9531, 1010, 2005, 2742, 7636, 1010, 3095, 2075, 1010, 17024, 5366, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4807, 8406, 14528, 2968, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2552, 2004, 1996, 3145, 3967, 2711, 2005, 6143, 22859, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6611, 6413, 1998, 4621, 4807, 4725, 2000, 3026, 2968, 1998, 2590, 22859, 1006, 2164, 21431, 1007, 2802, 1996, 2622, 2166, 23490, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2004, 9755, 1998, 9686, 25015, 9285, 13368, 2306, 3934, 1010, 6709, 7300, 1998, 6133, 1996, 5813, 1999, 1037, 23259, 1998, 6413, 5450, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3298, 2689, 2968, 3450, 2005, 7972, 3934, 1004, 3454, 1998, 5676, 3431, 2024, 15299, 1998, 5147, 7528, 2000, 6162, 9879, 6666, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 9756, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 2622, 9756, 2004, 2019, 2981, 5783, 2000, 5676, 2008, 5025, 2622, 5366, 2024, 1999, 2240, 1996, 5462, 5166, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2599, 2030, 6204, 8147, 4391, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 20410, 7375, 1997, 3737, 16375, 8853, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 5676, 2622, 1004, 2565, 6959, 2114, 2565, 1047, 18136, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 8080, 2622, 1004, 2565, 10857, 1006, 3465, 1010, 3947, 1010, 9531, 1010, 3802, 8292, 14621, 1007, 2114, 2933, 1999, 2344, 2000, 10408, 6149, 3512, 2030, 4652, 3512, 4506, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2111, 2968, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 5676, 6413, 4105, 4813, 2024, 2556, 2012, 2296, 2504, 2011, 4526, 1037, 14354, 2389, 1998, 16408, 2147, 4044, 1999, 2029, 5126, 2024, 8868, 1010, 4738, 1998, 3024, 2007, 2476, 6695, 2083, 2458, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 10843, 8080, 1998, 16157, 2136, 2147, 11066, 1998, 13296, 8122, 2007, 1996, 2490, 1997, 2009, 3001, 1010, 2951, 1998, 4106, 1998, 2136, 12247, 1998, 2191, 6413, 3431, 1999, 2344, 2000, 3113, 2449, 3791, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3073, 2136, 2372, 3622, 4311, 2007, 3154, 3257, 1998, 7889, 2008, 2024, 13115, 2007, 2449, 3791, 1998, 3617, 11100, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3145, 6550, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2996, 1037, 1024, 5541, 1010, 3136, 1004, 2537, 1010, 4180, 5656, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3617, 2780, 1006, 4041, 1010, 3325, 2640, 1010, 13791, 1012, 1012, 1012, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2350, 3795, 22859, 1006, 3795, 4012, 5244, 2780, 1010, 3795, 3136, 1010, 4435, 2640, 1012, 1012, 1012, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2350, 3617, 2780, 1006, 3617, 4041, 1010, 3930, 1010, 3617, 3688, 1010, 26718, 2080, 1012, 1012, 1012, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7972, 2449, 3853, 1006, 2175, 4523, 1010, 5446, 1010, 17850, 1010, 4435, 5821, 1010, 17264, 7027, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3795, 2009, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6327, 4835, 1004, 6736, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 21423, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [101, 5918, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [101, 2495, 1004, 3325, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 28946, 3040, 3014, 2007, 3579, 2006, 2449, 3447, 1010, 4807, 2030, 3141, 2752, 1010, 2030, 5662, 5257, 1997, 2495, 1998, 3325, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2184, 1009, 2086, 1997, 1999, 1011, 5995, 3325, 3141, 2000, 2622, 2968, 2030, 2714, 7832, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1020, 1011, 1022, 2086, 1997, 2658, 3325, 1999, 1996, 3617, 16927, 4044, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 1999, 1999, 1011, 2160, 2996, 4897, 1011, 2041, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4824, 1997, 17338, 2213, 3746, 2678, 6100, 3742, 2537, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 1999, 2551, 2007, 6736, 1998, 2030, 24853, 2003, 1037, 4606, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 2551, 2007, 22859, 2012, 2536, 3026, 3012, 3798, 2004, 2092, 2004, 3395, 3043, 8519, 2408, 2536, 4972, 1006, 9797, 1010, 2449, 22859, 1010, 4031, 1998, 5821, 2780, 1010, 3802, 8292, 14621, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1017, 1009, 2086, 1997, 3325, 1999, 2877, 1037, 2136, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3730, 1011, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2200, 2204, 4807, 4813, 1010, 2926, 2043, 21935, 2007, 2367, 3798, 1997, 2449, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3754, 2000, 2147, 1999, 1037, 3435, 1011, 13823, 4044, 2007, 2367, 2248, 8578, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3754, 2000, 5047, 27637, 1998, 4895, 23395, 3375, 8146, 2046, 2895, 3085, 3450, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 8200, 6143, 9273, 3388, 1998, 3754, 2000, 3188, 25090, 4371, 1998, 11849, 2152, 3616, 1997, 8518, 2007, 9671, 2147, 11066, 1998, 5197, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7300, 1011, 8048, 3921, 1998, 10670, 4818, 9273, 3388, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2204, 15973, 1998, 17826, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3811, 2764, 4105, 4813, 2024, 3223, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2524, 1011, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3716, 1997, 3617, 6786, 1998, 4806, 7248, 1998, 3226, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2844, 5796, 1011, 2436, 4813, 1006, 2773, 1010, 24970, 1010, 2373, 8400, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3754, 2000, 3604, 1010, 4968, 2030, 2248, 1010, 2004, 3223, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1999, 1011, 5995, 4824, 1997, 2009, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 19376, 2394, 2119, 12064, 2135, 1998, 2517, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3800, 1024, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2004, 1037, 3208, 3617, 5656, 1998, 3454, 1999, 27133, 8883, 3006, 17338, 2213, 3029, 2115, 4708, 2003, 2000, 6133, 1998, 8116, 2235, 3617, 3934, 1010, 3278, 14645, 2015, 2030, 4942, 1011, 3787, 1997, 3054, 1011, 4094, 3617, 3934, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1996, 2622, 9531, 2003, 2006, 1037, 2334, 1006, 3006, 1007, 2504, 2408, 9639, 1998, 4972, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2115, 7863, 2003, 2000, 6204, 1996, 3934, 2306, 1996, 7372, 1997, 2051, 1010, 3465, 1004, 3737, 2096, 7995, 2006, 10390, 1005, 10908, 1998, 5918, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4041, 1010, 23448, 1998, 9651, 3934, 1999, 6792, 2007, 1996, 7972, 4972, 1998, 2622, 2136, 2372, 2024, 1037, 2261, 4973, 1997, 2054, 2003, 3517, 2013, 2023, 2535, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 5678, 1010, 2017, 2097, 2022, 7995, 2006, 7692, 2968, 1998, 12016, 3450, 1010, 3772, 2004, 1996, 2430, 8278, 2000, 2334, 22859, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3145, 10198, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1024, 9531, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 1998, 8116, 2235, 3617, 3934, 2006, 1037, 2334, 1006, 3006, 1007, 2504, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 2565, 6959, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 8116, 3934, 2203, 1011, 2000, 1011, 2203, 5147, 1010, 28946, 2083, 4621, 4646, 1997, 7610, 2072, 29003, 16134, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4503, 6851, 3488, 2008, 3499, 4346, 16987, 2006, 5082, 1998, 12151, 10831, 2006, 2051, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6709, 6970, 3207, 11837, 4181, 9243, 2007, 2060, 3934, 1998, 9611, 3314, 4013, 19620, 2135, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3298, 7142, 7620, 1997, 3688, 1010, 6194, 2030, 3001, 2306, 1996, 2622, 9531, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7692, 4041, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2599, 1998, 6133, 2136, 4219, 1006, 4722, 2015, 1998, 6327, 2015, 1007, 2000, 8116, 1998, 2490, 3934, 1004, 3454, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 4807, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [101, 2552, 2004, 1996, 3145, 3967, 2711, 2005, 2334, 22859, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6611, 6413, 1998, 4621, 4807, 4725, 2000, 3026, 2968, 1998, 2590, 22859, 2802, 1996, 2622, 2166, 23490, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2004, 9755, 1998, 9686, 25015, 9285, 13368, 2306, 3934, 1010, 6709, 7300, 1998, 3298, 2037, 5813, 1999, 1037, 23259, 1998, 6413, 5450, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 2689, 2306, 3934, 1998, 5676, 3431, 2024, 15299, 1998, 5147, 7528, 2000, 6162, 9879, 6666, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 9756, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6133, 2622, 9756, 2004, 2019, 2981, 5783, 2000, 5676, 2008, 5025, 2622, 5366, 2024, 1999, 2240, 1996, 5462, 5166, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 6204, 8147, 4391, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 20410, 12646, 2007, 3737, 16375, 8853, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 9398, 3686, 2065, 2622, 2024, 12771, 2114, 2565, 1047, 18136, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 8080, 2622, 10857, 1006, 3465, 1010, 3947, 1010, 9531, 1010, 3802, 8292, 14621, 1007, 2114, 2933, 1999, 2344, 2000, 10408, 6149, 3512, 2030, 4652, 3512, 4506, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1000, 2065, 3223, 1000, 10198, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 29003, 8651, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3298, 1996, 7375, 1997, 3787, 1997, 1996, 8651, 2000, 2440, 29003, 2551, 5549, 1998, 10956, 29003, 9886, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 9002, 2000, 4975, 1998, 4032, 3436, 1996, 5157, 1004, 6959, 2832, 2306, 3795, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 10956, 3988, 5157, 10287, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3006, 2458, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3443, 2449, 3572, 2005, 2115, 3006, 2007, 2490, 2013, 2334, 3795, 25095, 2136, 2000, 16157, 3930, 6695, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 10408, 1996, 2918, 4132, 9859, 1010, 3857, 2011, 3795, 1010, 3223, 2000, 13883, 2449, 3572, 1010, 2362, 2007, 2009, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3145, 6550, 1024, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2877, 5034, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 3208, 2030, 1006, 5034, 1012, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 16101, 1012, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3617, 5656, 1004, 3454, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2622, 2136, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2334, 2622, 22859, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3795, 3617, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3795, 2009, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3617, 4341, 7300, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7972, 2449, 4972, 1006, 23092, 1010, 5446, 1010, 17850, 1010, 4435, 5821, 1010, 17264, 7027, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 17850, 2968, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}\n",
      "{'input_ids': [101, 9756, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [101, 2495, 1004, 3325, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2118, 3014, 2007, 3579, 2006, 2449, 3447, 1010, 4807, 2030, 2009, 2030, 3141, 2752, 1010, 2030, 5662, 5257, 1997, 2495, 1998, 3325, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1019, 1009, 2086, 1997, 1999, 1011, 5995, 2658, 3325, 3141, 2000, 2622, 2968, 2030, 2714, 7832, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 1015, 1011, 1017, 2086, 1997, 2658, 3325, 1999, 1996, 16927, 4044, 1997, 1996, 2622, 2181, 1999, 9531, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 10003, 3006, 2449, 9353, 27417, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 1999, 17338, 15810, 3401, 2003, 1037, 4606, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 1999, 5446, 2003, 1037, 4606, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3325, 2551, 2007, 22859, 2012, 2536, 3026, 3012, 3798, 2004, 2092, 2004, 3395, 3043, 8519, 2408, 2536, 4972, 1006, 9797, 1010, 2449, 22859, 1010, 4031, 1998, 5821, 2780, 1010, 3802, 8292, 14621, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3716, 4813, 1998, 7590, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3730, 1011, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2204, 4807, 4813, 1010, 2926, 2043, 21935, 2007, 2367, 3798, 1997, 2449, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3754, 2000, 2147, 1999, 1037, 3435, 1011, 13823, 4044, 2007, 2367, 2248, 8578, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 7300, 1011, 8048, 3921, 1998, 10670, 4818, 9273, 3388, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2204, 15973, 1998, 17826, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2524, 1011, 4813, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2844, 5796, 1011, 2436, 4813, 1006, 2773, 1010, 24970, 1010, 2373, 8400, 1007, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 3937, 3325, 1998, 1037, 5041, 4824, 1997, 2009, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2551, 3716, 1997, 29003, 2551, 4725, 2005, 2742, 8040, 6824, 22827, 8193, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 2551, 3716, 1997, 7610, 2072, 4725, 28946, 2007, 10618, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 19376, 2394, 2119, 12064, 2135, 1998, 2517, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for sentences in spacy_sentencized:\n",
    "    print(bert_tokenizer.encode_plus(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "c7390ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6b127cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "         ...,\n",
       "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "76ab2d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[540], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m(\n\u001b[1;32m      2\u001b[0m     BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m         bert_model_name,\n\u001b[1;32m      4\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m         output_additional_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     ),\n\u001b[1;32m      8\u001b[0m     BertAligner\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert_model_name),\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cls' is not defined"
     ]
    }
   ],
   "source": [
    "cls(\n",
    "    BertModel.from_pretrained(\n",
    "        bert_model_name,\n",
    "        output_attentions=True,\n",
    "        output_hidden_states=True,\n",
    "        output_additional_info=True,\n",
    "    ),\n",
    "    BertAligner.from_pretrained(bert_model_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy sentencizer\n",
    "# spacy_sentencized = []\n",
    "\n",
    "if all(custom_punct_char in sentencizer.punct_chars for custom_punct_char in custom_punct_chars):\n",
    "    for job_description in job_descriptions:\n",
    "        for token in nlp(job_description):\n",
    "            print(token.text, token.tag_)\n",
    "#             for sent in re.split(pattern, sentence.text):\n",
    "#                 if len(sent) != 0:\n",
    "#                     spacy_sentencized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526da0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "bigram_rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "rules = [\n",
    "    ['VERB', 'ADJ', 'NOUN'],\n",
    "    ['NOUN', 'VERB', 'ADV'],\n",
    "    ['NOUN', 'ADP', 'NOUN'],\n",
    "    # more rules here...\n",
    "]\n",
    "\n",
    "trigram_patterns = [[{\"POS\": i} for i in j] for j in rules]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa22c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2de96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "    'noun_verb': [{'POS': 'NOUN'}, {'POS': 'VERB'}],\n",
    "    'verb_noun': [{'POS': 'VERB'}, {'POS': 'NOUN'}],\n",
    "    'adj_noun': [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "    'adj_propn': [{'POS': 'ADJ'}, {'POS': 'PROPN'}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern_name, pattern in patterns.items():\n",
    "    matcher.add(pattern_name, [pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e002bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_description in job_descriptions:\n",
    "    doc = nlp(job_description)\n",
    "    matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    \n",
    "    # Get string representation\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "\n",
    "    # The matched span\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    print(repr(span.text))\n",
    "    print(match_id, string_id, start, end)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e73cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6595ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentence in spacy_sentencized:\n",
    "#     print(model.encode(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.electra.modeling_tf_electra import TFElectraMainLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pavanchhatpar/electra-base-sentence-splitter\")\n",
    "\n",
    "model = AutoModelForPreTraining.from_pretrained(\"pavanchhatpar/electra-base-sentence-splitter\", from_tf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ee3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline('sentence-splitter', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
