{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a818288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
      "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
      "[nltk_data]     Models/nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
    "\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import unicodedata
import contextlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict
random_state = 42
random.seed(random_state)\n",
    "random_state= 42
       random.seed(random_state)\n",
    "\n",
    "# Set up Spacy\n",
    "import spacy\n",
    "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up NLK\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from nltk.tag import pos_tag, pos_tag_sents\n",
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set up Gensim\n",
    "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "# Set up Bert\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
    "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487cf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_drop_scores(tag_list):\n",
    "\n",
    "    return [\n",
    "        [(tag_list[i][0], tag_list[i][1])]\n",
    "        for tag_tuple in tag_list\n",
    "        for i in range(len(tag_list))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099fb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dd11c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_jobs['Job Description nltk_token_tags'][2][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3edd566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inproceedings{sajjad-NAACL,\n",
    "#   title={Analyzing Encoded Concepts in Transformer Language Models},\n",
    "#   author={Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan and Jia Xu},\n",
    "#   booktitle={North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL)},\n",
    "#   series={NAACL~'22},\n",
    "#   year={2022},\n",
    "#   address={Seattle}\n",
    "# }\n",
    "\n",
    "# Hassan Sajjad, Nadir Durrani, F. D., Firoj Alam, Abdul Rafae Khan, & Xu, J. (2022). Analyzing encoded concepts in transformer language models. North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL).\n",
    "\n",
    "# model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "# bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "# bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "df_jobs['Job Description bert_token_tags_with_scores'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: [\n",
    "        (bert_pos_tag['word'], bert_pos_tag['entity'], bert_pos_tag['score'])\n",
    "        for i in range(len(sentence.split()))\n",
    "        for bert_pos_tag in bert_pos_tagger(sentence)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_jobs['Job Description bert_token_tags'] = df_jobs['Job Description bert_token_tags_with_scores'].apply(\n",
    "    lambda tag_list: [\n",
    "        [(tag_list[i][0], tag_list[i][1])]\n",
    "        for tag_tuple in tag_list\n",
    "        for i in range(len(tag_list))\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2223dc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(about, UH, 0.84219587), (our, UH, 0.58265716...\n",
       "1    [(the, NNP, 0.790051), (global, NNP, 0.8412097...\n",
       "2    [(its, FW, 0.29637802), (purpose, FW, 0.399035...\n",
       "Name: Job Description bert_token_tags_with_scores, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description bert_token_tags_with_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5eb897b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[(about, UH)], [(our, UH)], [(client, NN)], [...\n",
       "1    [[(the, NNP)], [(global, NNP)], [(ky, NNP)], [...\n",
       "2    [[(its, FW)], [(purpose, FW)], [(is, FW)], [(e...\n",
       "Name: Job Description bert_token_tags, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs['Job Description bert_token_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b328a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cebc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inproceedings{sajjad-NAACL,\n",
    "#   title={Analyzing Encoded Concepts in Transformer Language Models},\n",
    "#   author={Hassan Sajjad, Nadir Durrani, Fahim Dalvi, Firoj Alam, Abdul Rafae Khan and Jia Xu},\n",
    "#   booktitle={North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL)},\n",
    "#   series={NAACL~'22},\n",
    "#   year={2022},\n",
    "#   address={Seattle}\n",
    "# }\n",
    "\n",
    "# Hassan Sajjad, Nadir Durrani, F. D., Firoj Alam, Abdul Rafae Khan, & Xu, J. (2022). Analyzing encoded concepts in transformer language models. North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL).\n",
    "\n",
    "# model_name = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# pipeline = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "bert_pos_model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "bert_pos_tokenizer = AutoTokenizer.from_pretrained(bert_pos_model_name)\n",
    "bert_pos_model = AutoModelForTokenClassification.from_pretrained(bert_pos_model_name)\n",
    "bert_pos_tagger = TokenClassificationPipeline(model=bert_pos_model, tokenizer=bert_pos_tokenizer)\n",
    "\n",
    "df_jobs['Job Description bert_token_tags_with_scores'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
    "    lambda sentence: [\n",
    "        (\n",
    "            bert_pos_tagger(sentence)[i])\n",
    "            for i in range(len(sentence.split())\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# df_jobs['Job Description bert_token_tags'] = df_jobs['Job Description bert_token_tags_with_scores'].apply(\n",
    "#     lambda tag_list: tags_drop_scores(tag_list)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description bert_token_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description bert_token_tags_with_scores']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48edea5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
