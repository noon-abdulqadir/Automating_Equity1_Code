{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "5754ae73",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package words to\n",
                        "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
                        "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
                        "[nltk_data]     Models/nltk...\n",
                        "[nltk_data]   Package words is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
                        "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
                        "[nltk_data]     Models/nltk...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
                        "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
                        "[nltk_data]     Models/nltk...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     /Users/nyxinsane/Documents/Work - UvA/Automating\n",
                        "[nltk_data]     Equity/Study 1/Study1_Code/data/Language\n",
                        "[nltk_data]     Models/nltk...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
                        "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import importlib\n",
                "from pathlib import Path\n",
                "import numpy as np\n",
                "\n",
                "mod = sys.modules[__name__]\n",
                "\n",
                "code_dir = None\n",
                "code_dir_name = 'Code'\n",
                "unwanted_subdir_name = 'Analysis'\n",
                "\n",
                "for _ in range(5):\n",
                "\n",
                "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
                "\n",
                "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
                "\n",
                "        code_dir = str(Path.cwd().parents[_])\n",
                "\n",
                "        if code_dir is not None:\n",
                "            break\n",
                "\n",
                "# %load_ext autoreload\n",
                "# %autoreload 2\n",
                "\n",
                "# MAIN DIR\n",
                "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
                "\n",
                "# code_dir\n",
                "code_dir = f'{code_dir}/'\n",
                "sys.path.append(code_dir)\n",
                "\n",
                "# scraping dir\n",
                "scraped_data = f'{code_dir}scraped_data/'\n",
                "\n",
                "# data dir\n",
                "data_dir = f'{code_dir}data/'\n",
                "\n",
                "# lang models dir\n",
                "llm_path = f'{data_dir}Language Models'\n",
                "\n",
                "# sites\n",
                "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
                "\n",
                "# columns\n",
                "cols=['Sector', \n",
                "      'Sector Code', \n",
                "      'Gender', \n",
                "      'Age', \n",
                "      'Language', \n",
                "      'Dutch Requirement', \n",
                "      'English Requirement', \n",
                "      'Gender_Female', \n",
                "      'Gender_Mixed', \n",
                "      'Gender_Male', \n",
                "      'Age_Older', \n",
                "      'Age_Mixed', \n",
                "      'Age_Younger', \n",
                "      'Gender_Num', \n",
                "      'Age_Num', \n",
                "      '% Female', \n",
                "      '% Male', \n",
                "      '% Older', \n",
                "      '% Younger']\n",
                "\n",
                "int_variable: str = 'Job ID'\n",
                "str_variable: str = 'Job Description'\n",
                "gender: str = 'Gender'\n",
                "age: str = 'Age'\n",
                "language: str = 'en'\n",
                "languages = [\"en\", \"['nl', 'en']\", ['en', 'nl']]\n",
                "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
                "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
                "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n",
                "\n",
                "import string\n",
                "import re\n",
                "import time\n",
                "import json\n",
                "import csv\n",
                "import glob\n",
                "import pickle\n",
                "import random\n",
                "import itertools\n",
             "import unicodedata
                import contextlib\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import googletrans\n",
                "from googletrans import Translator\n",
             "from collections import defaultdict
                random_state = 42
                random.seed(random_state)\n",
             "random_state= 42
                random.seed(random_state)\n",
                "\n",
                "# Set up Spacy\n",
                "import spacy\n",
                "from spacy.symbols import NORM, ORTH, LEMMA, POS\n",
                "from spacy.matcher import Matcher\n",
                "\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "# Set up NLK\n",
                "import nltk\n",
                "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
                "from nltk.corpus import stopwords, wordnet\n",
                "from nltk.stem import WordNetLemmatizer, SnowballStemmer, PorterStemmer, LancasterStemmer\n",
                "from nltk.tag import pos_tag, pos_tag_sents\n",
                "from nltk.util import ngrams, bigrams, trigrams\n",
                "\n",
                "nltk_path = f'{llm_path}/nltk'\n",
                "nltk.data.path.append(nltk_path)\n",
                "\n",
                "nltk.download('words', download_dir = nltk_path)\n",
                "nltk.download('stopwords', download_dir = nltk_path)\n",
                "nltk.download('punkt', download_dir = nltk_path)\n",
                "nltk.download('averaged_perceptron_tagger', download_dir = nltk_path)\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "punctuations = list(string.punctuation)\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "# Set up Gensim\n",
                "from gensim.utils import save_as_line_sentence, simple_preprocess\n",
                "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, preprocess_documents\n",
                "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
                "\n",
                "# Set up Bert\n",
                "from transformers import AutoTokenizer, AutoModelForTokenClassification, TokenClassificationPipeline, BertTokenizer, BertForPreTraining, BertConfig, BertModel\n",
                "bert_model_name = 'bert-base-uncased'\n",
                "bert_tokenizer = BertTokenizerFast.from_pretrained(bert_model_name, strip_accents = True)\n",
                "bert_model = BertForSequenceClassification.from_pretrained(bert_model_name)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "89e0a774",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_tags_lemmas_stems_spacy_nltk.pkl').reset_index(drop=True)[:10]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "2367a07f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def spacy_make_ngrams(sentence, matcher, gram_type):\n",
                "\n",
                "    doc = nlp(sentence)\n",
                "    matches = matcher(doc)\n",
                "    matches_list = []\n",
                "\n",
                "    for idx in range(len(matches)):\n",
                "        for match_id, start, end in matches:\n",
                "            if nlp.vocab.strings[match_id].split('_')[0] == gram_type:\n",
                "                match = doc[matches[idx][1]: matches[idx][2]].text\n",
                "                matches_list.append(match.lower())\n",
                "    \n",
                "    return list(set(matches_list))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c9bb650c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description spacy_1grams_original_list'] = df_jobs['Job Description spacy_tokenized']\n",
                "df_jobs['Job Description spacy_1grams'] = df_jobs['Job Description spacy_tokenized'].apply(\n",
                "    lambda tokens: [\n",
                "        tuple(token.split())\n",
                "        for token in tokens\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "3e3a0aca",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spacy bi and trigrams\n",
                "matcher = Matcher(nlp.vocab)\n",
                "\n",
                "bigram_rules = [\n",
                "    ['NOUN', 'VERB'],\n",
                "    ['VERB', 'NOUN'],\n",
                "    ['ADJ', 'NOUN'],\n",
                "    ['ADJ', 'PROPN'],\n",
                "    # more rules here...\n",
                "]\n",
                "\n",
                "trigram_rules = [\n",
                "    ['VERB', 'ADJ', 'NOUN'],\n",
                "    ['NOUN', 'VERB', 'ADV'],\n",
                "    ['NOUN', 'ADP', 'NOUN'],\n",
                "    # more rules here...\n",
                "]\n",
                "\n",
                "patters_dict = {\n",
                "    'bigram_patterns': [[{'POS': i} for i in j] for j in bigram_rules],\n",
                "    'trigram_patterns': [[{'POS': i} for i in j] for j in trigram_rules],\n",
                "}\n",
                "\n",
                "ngram_dict = {\n",
                "    'bigram': 2,\n",
                "    'trigram': 3,\n",
                "}\n",
                "\n",
                "for ngram_name, ngram_num in ngram_dict.items():\n",
                "    \n",
                "    \n",
                "#     matcher.add(f'{ngram_name}_patterns', patters_dict[f'{ngram_name}_patterns'])\n",
                "\n",
                "#     df_jobs[f'Job Description spacy_{str(ngram_num)}grams_original_list'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
                "#         lambda sentence: \n",
                "#             [\n",
                "#                 '_'.join(ngram_.split())\n",
                "#                 for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
                "#             ]\n",
                "#     )\n",
                "    \n",
                "#     df_jobs[f'Job Description spacy_{str(ngram_num)}grams'] = df_jobs['Job Description spacy_sentencized'].apply(\n",
                "#         lambda sentence: \n",
                "#             [\n",
                "#                 tuple(ngram_.split())\n",
                "#                 for ngram_ in spacy_make_ngrams(sentence, matcher, ngram_name)\n",
                "#             ]\n",
                "#     )\n",
                "\n",
                "    df_jobs[f'Job Description spacy_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
                "        to_replace = {\n",
                "            ' '.join(ngram_.split('_')): ngram_\n",
                "            for ngrams_list in df_jobs[f'Job Description spacy_{str(ngram_num)}grams_original_list']\n",
                "            for ngram_ in ngrams_list\n",
                "            if '_' in ngram_\n",
                "        }\n",
                "    )\n",
                "    \n",
                "    if f'{ngram_name}_patterns' in matcher:\n",
                "        matcher.remove(f'{ngram_name}_patterns')\n",
                "    assert f'{ngram_name}_patterns' not in matcher\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "72fb5c5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Spacy Allgrams\n",
                "df_jobs['Job Description spacy_123grams_original_list'] = df_jobs['Job Description spacy_tokenized'] + df_jobs['Job Description spacy_2grams_original_list'] + df_jobs['Job Description spacy_3grams_original_list']\n",
                "df_jobs['Job Description spacy_123grams'] = df_jobs['Job Description spacy_1grams'] + df_jobs['Job Description spacy_2grams'] + df_jobs['Job Description spacy_3grams']\n",
                "df_jobs['Job Description spacy_123grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
                "    regex = {\n",
                "        ' '.join(ngram_.split('_')): ngram_\n",
                "        for ngrams_list in df_jobs[f'Job Description spacy_123grams_original_list']\n",
                "        for ngram_ in ngrams_list\n",
                "        if '_' in ngram_\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "c0536251",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description nltk_1grams_original_list'] = df_jobs['Job Description nltk_tokenized']\n",
                "df_jobs['Job Description nltk_1grams'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
                "    lambda tokens: [\n",
                "        tuple(token.split())\n",
                "        for token in tokens\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "49954d48",
            "metadata": {},
            "outputs": [],
            "source": [
                "# NLTK bi and trigrams\n",
                "ngram_dict = {\n",
                "    'bigram': 2,\n",
                "    'trigram': 3\n",
                "}\n",
                "\n",
                "for ngram_name, ngram_num in ngram_dict.items():\n",
                "\n",
                "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams_original_list'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
                "        lambda tokens:\n",
                "            list(\n",
                "                '_'.join(ngram_list)\n",
                "                for ngram_list in ngrams(tokens, ngram_num)\n",
                "            )\n",
                "    )\n",
                "\n",
                "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams'] = df_jobs['Job Description nltk_tokenized'].apply(\n",
                "        lambda tokens: list(ngrams(tokens, ngram_num))\n",
                "    )\n",
                "\n",
                "    df_jobs[f'Job Description nltk_{str(ngram_num)}grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
                "        regex = {\n",
                "            ' '.join(ngram_.split('_')): ngram_\n",
                "            for ngrams_list in df_jobs[f'Job Description nltk_{str(ngram_num)}grams_original_list']\n",
                "            for ngram_ in ngrams_list\n",
                "            if '_' in ngram_\n",
                "        }\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "04dfe7d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# NLTK Allgrams\n",
                "df_jobs[f'Job Description nltk_123grams_original_list'] = df_jobs['Job Description nltk_tokenized'] + df_jobs['Job Description nltk_2grams_original_list'] + df_jobs['Job Description nltk_3grams_original_list']\n",
                "df_jobs[f'Job Description nltk_123grams'] = df_jobs['Job Description nltk_1grams'] + df_jobs['Job Description nltk_2grams'] + df_jobs['Job Description nltk_3grams']\n",
                "df_jobs['Job Description nltk_123grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().replace(\n",
                "    regex = {\n",
                "        ' '.join(ngram_.split('_')): ngram_\n",
                "        for ngrams_list in df_jobs[f'Job Description nltk_123grams_original_list']\n",
                "        for ngram_ in ngrams_list\n",
                "        if '_' in ngram_\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "6cd9a6c1",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_original_list'] = df_jobs['Job Description gensim_tokenized']\n",
                "df_jobs['Job Description gensim_1grams'] = df_jobs['Job Description gensim_tokenized'].apply(\n",
                "    lambda tokens: [\n",
                "        tuple(token.split())\n",
                "        for token in tokens\n",
                "    ]\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "8c87f492",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gensim bi and trigrams\n",
                "# Gensim Bigrams\n",
                "bigram = Phraser(Phrases(df_jobs['Job Description gensim_tokenized'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
                "df_jobs['Job Description gensim_2grams_original_list_all'] = bigram[df_jobs['Job Description gensim_tokenized']]\n",
                "df_jobs['Job Description gensim_2grams_original_list'] = df_jobs['Job Description gensim_2grams_original_list_all'].apply(\n",
                "    lambda ngrams_list: [\n",
                "        ngram_\n",
                "        for ngram_ in ngrams_list\n",
                "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
                "    ]\n",
                ")\n",
                "df_jobs['Job Description gensim_2grams'] = df_jobs['Job Description gensim_2grams_original_list'].apply(\n",
                "    lambda ngrams: [\n",
                "        tuple(ngram.split('_'))\n",
                "        for ngram in ngrams\n",
                "        if '_' in ngram\n",
                "    ]\n",
                ")\n",
                "df_jobs[f'Job Description gensim_2grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().apply(\n",
                "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
                ").replace(\n",
                "    regex = {\n",
                "        ' '.join(ngram_.split('_')): ngram_\n",
                "        for ngrams_list in df_jobs[f'Job Description gensim_2grams_original_list']\n",
                "        for ngram_ in ngrams_list\n",
                "        if '_' in ngram_\n",
                "    }\n",
                ")\n",
                "\n",
                "# Gensim Trigrams\n",
                "trigram = Phraser(Phrases(df_jobs['Job Description gensim_2grams_original_list_all'], connector_words=ENGLISH_CONNECTOR_WORDS, min_count=1, threshold=1))\n",
                "df_jobs['Job Description gensim_3grams_original_list_all'] = trigram[df_jobs['Job Description gensim_2grams_original_list_all']]\n",
                "df_jobs['Job Description gensim_3grams_original_list'] = df_jobs['Job Description gensim_3grams_original_list_all'].apply(\n",
                "    lambda ngrams_list: [\n",
                "        ngram_\n",
                "        for ngram_ in ngrams_list\n",
                "        if len(re.findall('[a-zA-Z]*\\_[a-zA-Z]*\\_[a-zA-Z]*', ngram_)) != 0\n",
                "    ]\n",
                ")\n",
                "df_jobs['Job Description gensim_3grams'] = df_jobs['Job Description gensim_3grams_original_list'].apply(\n",
                "    lambda ngrams: [\n",
                "        tuple(ngram.split('_'))\n",
                "        for ngram in ngrams\n",
                "        if '_' in ngram\n",
                "    ]\n",
                ")\n",
                "df_jobs[f'Job Description gensim_3grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().apply(\n",
                "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
                ").replace(\n",
                "    regex = {\n",
                "        ' '.join(ngram_.split('_')): ngram_\n",
                "        for ngrams_list in df_jobs[f'Job Description gensim_3grams_original_list']\n",
                "        for ngram_ in ngrams_list\n",
                "        if '_' in ngram_\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "d26d0ff6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gensim Allgrams\n",
                "df_jobs[f'Job Description gensim_123grams_original_list'] = df_jobs['Job Description gensim_tokenized'] + df_jobs['Job Description gensim_2grams_original_list'] + df_jobs['Job Description gensim_3grams_original_list']\n",
                "df_jobs[f'Job Description gensim_123grams'] = df_jobs['Job Description gensim_1grams'] + df_jobs['Job Description gensim_2grams'] + df_jobs['Job Description gensim_3grams']\n",
                "df_jobs['Job Description gensim_123grams_in_sent'] = df_jobs['Job Description spacy_sentencized'].str.lower().apply(\n",
                "    lambda sentence: ' '.join(preprocess_string(re.sub(pattern, ' ', sentence.strip().lower())))\n",
                ").replace(\n",
                "    regex = {\n",
                "        ' '.join(ngram_.split('_')): ngram_\n",
                "        for ngrams_list in df_jobs[f'Job Description gensim_123grams_original_list']\n",
                "        for ngram_ in ngrams_list\n",
                "        if '_' in ngram_\n",
                "    }\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "e7c2b15d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_abs_frequency(row, ngram_num, embedding_library):\n",
                "\n",
                "    abs_word_freq = defaultdict(int)\n",
                "\n",
                "    for word in row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']:\n",
                "        abs_word_freq[word] += 1\n",
                "\n",
                "        abs_wtd_df = (\n",
                "            pd.DataFrame.from_dict(abs_word_freq, orient='index')\n",
                "            .rename(columns={0: 'abs_word_freq'})\n",
                "            .sort_values(by=['abs_word_freq'], ascending=False)\n",
                "            )\n",
                "        abs_wtd_df.insert(1, 'abs_word_perc', value=abs_wtd_df['abs_word_freq'] / abs_wtd_df['abs_word_freq'].sum())\n",
                "        abs_wtd_df.insert(2, 'abs_word_perc_cum', abs_wtd_df['abs_word_perc'].cumsum())\n",
                "\n",
                "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_freq'] = str(abs_wtd_df['abs_word_freq'].to_dict())\n",
                "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc'] = str(abs_wtd_df['abs_word_perc'].to_dict())\n",
                "        row[f'Job Description {embedding_library}_{ngram_num}grams_abs_word_perc_cum'] = str(abs_wtd_df['abs_word_perc_cum'].to_dict())\n",
                "\n",
                "    return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "b7b216fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "ngrams_list=[1, 2, 3, 123]\n",
                "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
                "\n",
                "for embedding_library, ngram_num in tqdm_product(embedding_libraries_list, ngrams_list):\n",
                "    df_jobs = df_jobs.apply(lambda row: get_abs_frequency(row=row, ngram_num=ngram_num, embedding_library=embedding_library), axis='columns')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "264f8f91",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'corpus' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcorpus\u001b[49m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
                    ]
                }
            ],
            "source": [
                "corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "902ba2df",
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx, row in df_jobs.iterrows():\n",
                "    print([row['Job Description gensim_1grams_corpus'][0][0]])\n",
                "#     print(type(row['Job Description spacy_2grams']))\n",
                "#     print(corpora.Dictionary([row['Job Description spacy_1grams']]))\n",
                "#     print(corpora.Dictionary(list(row['Job Description spacy_2grams'])))\n",
                "#     print(corpora.Dictionary(row['Job Description spacy_3grams']))\n",
                "#     print(corpora.Dictionary(row['Job Description spacy_123grams']))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "883990e7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim import corpora, models\n",
                "from gensim.corpora import Dictionary\n",
                "from gensim.models import CoherenceModel, FastText, KeyedVectors, TfidfModel, Word2Vec\n",
                "\n",
                "ngrams_list=[1, 2, 3, 123]\n",
                "embedding_libraries_list = ['spacy', 'nltk', 'gensim']\n",
                "\n",
                "def get_corpus_and_dictionary(row, ngram_num, embedding_library):\n",
                "    \n",
                "    ngrams_original_list = row[f'Job Description {embedding_library}_{ngram_num}grams_original_list']\n",
                "    dictionary = corpora.Dictionary([ngrams_original_list])\n",
                "    BoW_corpus = [dictionary.doc2bow(ngrams_original_list)]\n",
                "    tfidf = TfidfModel(BoW_corpus, smartirs='ntc')\n",
                "    tfidf_matrix = [tfidf[doc] for doc in BoW_corpus]\n",
                "\n",
                "    row[f'Job Description {embedding_library}_{ngram_num}grams_dictionary'] = dictionary\n",
                "    row[f'Job Description {embedding_library}_{ngram_num}grams_BoW_corpus'] = BoW_corpus\n",
                "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf'] = tfidf\n",
                "    row[f'Job Description {embedding_library}_{ngram_num}grams_tfidf_matrix'] = tfidf_matrix\n",
                "    \n",
                "    return row\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "9c8b96ce",
            "metadata": {},
            "outputs": [],
            "source": [
                "for embedding_library, ngram_num in tqdm_product(embedding_libraries_list, ngrams_list):\n",
                "    df_jobs = df_jobs.apply(\n",
                "        lambda row: get_corpus_and_dictionary(\n",
                "            row=row, ngram_num=ngram_num, embedding_library=embedding_library\n",
                "        ),\n",
                "        axis='columns'\n",
                "    )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "ba7ad441",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0                                         [[(0, 1.0)]]\n",
                            "1    [[(0, 0.4082482904638631), (1, 0.4082482904638...\n",
                            "2    [[(0, 0.2886751345948129), (1, 0.2886751345948...\n",
                            "3    [[(0, 0.20851441405707477), (1, 0.208514414057...\n",
                            "4    [[(0, 0.3333333333333333), (1, 0.3333333333333...\n",
                            "5    [[(0, 0.2773500981126146), (1, 0.2773500981126...\n",
                            "6    [[(0, 0.5773502691896258), (1, 0.2886751345948...\n",
                            "7    [[(0, 0.1889822365046136), (1, 0.1889822365046...\n",
                            "8    [[(0, 0.24253562503633297), (1, 0.242535625036...\n",
                            "9    [[(0, 0.23570226039551587), (1, 0.235702260395...\n",
                            "Name: Job Description gensim_1grams_tfidf_matrix, dtype: object"
                        ]
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "df_jobs['Job Description gensim_1grams_tfidf_matrix']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6938f33",
            "metadata": {},
            "outputs": [],
            "source": [
                "for idx, row in df_jobs.iterrows():\n",
                "    BoW_corpus = row['Job Description gensim_1grams_BoW_corpus']\n",
                "    dictionary = row['Job Description gensim_1grams_dictionary']\n",
                "#     for doc in BoW_corpus:\n",
                "#         print([[dictionary[id], freq] for id, freq in doc])\n",
                "    \n",
                "    tfidf = models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
                "    print(idx, tfidf)\n",
                "    \n",
                "#     for doc in tfidf[BoW_corpus]:\n",
                "#         print([[dictionary[id], np.around(freq,decimals=2)] for id, freq in doc])\n",
                "    \n",
                "#     for doc in BoW_corpus:\n",
                "#         x = tfidf[doc]\n",
                "    \n",
                "#     u = [\n",
                "#         tfidf[doc]\n",
                "#         for doc in BoW_corpus\n",
                "#     ]\n",
                "\n",
                "#     print(x)\n",
                "#     print('-'*20)\n",
                "#     print(u)\n",
                "#     print('+'*20)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f42fed80",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2bcf4524",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_tfidf']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "44f8be92",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_123grams_tfidf_matrix']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2628704",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_corpus'].apply(lambda x: print(type(x[0][0])))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fddb3734",
            "metadata": {},
            "outputs": [],
            "source": [
                "type(corpus[0][0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8bc19be8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# import gensim.downloader as api\n",
                "from gensim.models import TfidfModel\n",
                "from gensim.corpora import Dictionary\n",
                "\n",
                "# dataset = api.load(\"text8\")\n",
                "dataset = [df_jobs['Job Description gensim_1grams_original_list'][3]]\n",
                "dct = Dictionary(dataset)  # fit dictionary\n",
                "corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
                "\n",
                "model = TfidfModel(corpus)  # fit model\n",
                "# vector = model[corpus[0]]  # apply model to the first corpus document"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1454989b",
            "metadata": {},
            "outputs": [],
            "source": [
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "09449149",
            "metadata": {},
            "outputs": [],
            "source": [
                "[line for line in dataset]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9d5a256b",
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29392051",
            "metadata": {},
            "outputs": [],
            "source": [
                "list(dct)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "75b39591",
            "metadata": {},
            "outputs": [],
            "source": [
                "list(df_jobs['Job Description gensim_1grams_dictionary'][3])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "937e3250",
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cf69a4c6",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_2grams_corpus'][3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "be3f2220",
            "metadata": {},
            "outputs": [],
            "source": [
                "corpus[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cf6e4524",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_corpus'][3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dff7009c",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_original_list'][3]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f12dbe7",
            "metadata": {},
            "outputs": [],
            "source": [
                "df_jobs['Job Description gensim_1grams_corpus'].apply(lambda x: print(x[0]))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "study1_3.10",
            "language": "python",
            "name": "study1_3.10"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.8"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
