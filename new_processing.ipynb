{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = '/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/'\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{main_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{main_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manually collected dictionary of incorrect/faulty keywords in scraped site data\n",
    "with open(f'{scraped_data}CBS/Data/keyword_trans_dict.txt') as f:\n",
    "    keyword_trans_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 111 words to fix\n",
    "len(keyword_trans_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_broken_linkedin_files(glob_path):\n",
    "    fix_list = []\n",
    "    data_dict = {}\n",
    "    data_list = []\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "\n",
    "        with open(glob_path, encoding = 'utf-8') as csv_file_handler:\n",
    "            csv_reader = csv.DictReader(csv_file_handler)\n",
    "\n",
    "            for rows in csv_reader:\n",
    "                first_key = str(list(rows.keys())[0])\n",
    "                key = rows[first_key]\n",
    "                data_dict[key] = rows\n",
    "\n",
    "        for num in data_dict:\n",
    "            data_list.append(data_dict[num])\n",
    "\n",
    "        with open(glob_path, 'w', encoding = 'utf-8') as json_file_handler:\n",
    "            json_file_handler.write(json.dumps(data_list, indent = 4))\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keywords(df_temp):\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        for key, value in keyword_trans_dict.items():\n",
    "            df_temp.loc[\n",
    "                df_temp['Search Keyword'].astype(str).apply(\n",
    "                lambda x: x.lower().strip()\n",
    "                ) == str(key).lower().strip(), 'Search Keyword'\n",
    "            ] = str(value).lower().strip()\n",
    "\n",
    "        unfixed = df_temp.loc[\n",
    "            df_temp['Search Keyword'].astype(str).apply(lambda x: x.lower().strip()).isin([x.lower().strip() for x in list(keyword_trans_dict.keys())])\n",
    "        ]\n",
    "\n",
    "        if len(unfixed) != 0:\n",
    "            for key, value in keyword_trans_dict.items():\n",
    "                for idx, row in df_temp.iterrows():\n",
    "                    if row['Search Keyword'].astype(str).lower().strip() == str(key).lower().strip():\n",
    "                        df_temp.loc[idx, 'Search Keyword'] = str(value).lower().strip()\n",
    "    \n",
    "\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_paths = []\n",
    "\n",
    "for site in site_list:\n",
    "    glob_paths.extend(glob.glob(f'{scraped_data}/{site}/Data/*.json')+glob.glob(f'{scraped_data}/{site}/Data/*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 955 json and csv files\n",
    "len(glob_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix list catches all incorrect/faculty keyword search terms\n",
    "\n",
    "fix_list = []\n",
    "\n",
    "# Appended data catches all the fixed and cleaned dfs\n",
    "appended_data = []\n",
    "\n",
    "for glob_path in glob_paths:\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "        try:\n",
    "            df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "        except ValueError:\n",
    "            fix_list.append(glob_path)\n",
    "            if 'scraped_data/LinkedIn/Data/linkedin_jobs_df_' in glob_path:\n",
    "                data_json = fix_broken_linkedin_files(glob_path)\n",
    "                try:\n",
    "                    df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "                except ValueError:\n",
    "                    fix_list.append(glob_path)\n",
    "    elif glob_path.endswith('.csv'):\n",
    "        df_temp = pd.read_csv(glob_path).reset_index(drop=True)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        df_temp = fix_keywords(df_temp)\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
    "        df_temp.drop(\n",
    "        df_temp.columns[\n",
    "                df_temp.columns.str.contains(\n",
    "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
    "                )\n",
    "            ],\n",
    "            axis='columns',\n",
    "            inplace=True,\n",
    "            errors='ignore',\n",
    "        )\n",
    "#         df_temp.drop(columns=df_temp.filter(regex=re.compile(r\"^unnamed\", re.IGNORECASE)).columns, axis='columns', inplace=True, errors='ignore')\n",
    "    \n",
    "        if glob_path.endswith('.json'):\n",
    "            df_temp.to_json(glob_path, orient='records')\n",
    "        elif glob_path.endswith('.csv'):\n",
    "            df_temp.to_csv(glob_path, index=False)\n",
    "\n",
    "        appended_data.append(df_temp.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 527\n",
    "len(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(fix_list) != 0:\n",
    "    print('Some keywords to fix!')\n",
    "    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
    "        json.dump(fix_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.concat(appended_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw.pickle')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START HERE FROM FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = '/Users/nyxinsane/Documents/Work - UvA/Automating Equity/Study 1/Study1_Code/'\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{main_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{main_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204394"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len = 204394\n",
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204394 entries, 0 to 204393\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Search Keyword     204394 non-null  object \n",
      " 1   Platform           204394 non-null  object \n",
      " 2   Job ID             204394 non-null  object \n",
      " 3   Job Title          204394 non-null  object \n",
      " 4   Company Name       204384 non-null  object \n",
      " 5   Location           204394 non-null  object \n",
      " 6   Job Description    204379 non-null  object \n",
      " 7   Rating             51158 non-null   float64\n",
      " 8   Employment Type    203334 non-null  object \n",
      " 9   Company URL        193929 non-null  object \n",
      " 10  Job URL            204394 non-null  object \n",
      " 11  Job Age            204394 non-null  object \n",
      " 12  Job Age Number     204394 non-null  object \n",
      " 13  Collection Date    204394 non-null  object \n",
      " 14  Data Row           153230 non-null  float64\n",
      " 15  Tracking ID        153230 non-null  object \n",
      " 16  Industry           154296 non-null  object \n",
      " 17  Job Date           153236 non-null  object \n",
      " 18  Type of ownership  1060 non-null    object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 29.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name',\n",
       "       'Location', 'Job Description', 'Rating', 'Employment Type',\n",
       "       'Company URL', 'Job URL', 'Job Age', 'Job Age Number',\n",
       "       'Collection Date', 'Data Row', 'Tracking ID', 'Industry', 'Job Date',\n",
       "       'Type of ownership'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "\n",
    "for idx, row in df_jobs.iterrows():\n",
    "    if len(str(row['Job Description'])) != 0:\n",
    "        try:\n",
    "            df_jobs.loc[idx, 'Language'] = translator.detect(str(row['Job Description']).lower().strip()).lang\n",
    "        except:\n",
    "            time.sleep(3600)\n",
    "            df_jobs.loc[idx, 'Language'] = translator.detect(str(row['Job Description']).lower().strip()).lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = Translator()\n",
    "# try:\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "# except:\n",
    "#     time.sleep(3600)\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_translated.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_translated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns\n",
    "df_jobs.columns = df_jobs.columns.to_series().apply(lambda x: str(x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df_jobs.dropna(axis='index', how='all', inplace=True)\n",
    "df_jobs.dropna(axis='columns', how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in general and on subset of 'Job Description'\n",
    "subset_list=[int_variable, str_variable]\n",
    "\n",
    "# df_jobs.drop_duplicates(keep='first', ignore_index=True, inplace=True)\n",
    "df_jobs.drop_duplicates(subset=['Platform', 'Job ID', 'Search Keyword', 'Job Description'], keep='first', ignore_index=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "\n",
    "for variable in subset_list:\n",
    "    df_jobs = df_jobs.loc[\n",
    "        (\n",
    "            df_jobs[variable].apply(lambda x: isinstance(x, str))\n",
    "        )\n",
    "        & (~df_jobs[variable].isin(nan_list))\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Job Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-english job descriptions\n",
    "# df_jobs.drop(df_jobs.index[df_jobs['Language'] == str(language)], axis='index', inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44\n",
    "len(df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{data_dir}df_jobs_dropped.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and count unique search keywords\n",
    "search_keywords = list(set(df_jobs['Search Keyword'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(search_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = list(set(df_jobs['Job Description'].to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(job_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLK\n",
    "nltk_path = f'{llm_path}/nltk'\n",
    "nltk.data.path.append(nltk_path)\n",
    "\n",
    "nltk.download('words', download_dir = nltk_path)\n",
    "nltk.download('punkt', download_dir = nltk_path)\n",
    "nltk.download('stopwords', download_dir = nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCY\n",
    "sentencizer = Sentencizer(punct_chars=[',,', ',,,', ',,,,'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_description in job_descriptions:\n",
    "    print([doc for doc in sentencizer()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sentences = []\n",
    "for job_description in job_descriptions:\n",
    "    job_sentences.extend(sent_tokenize(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_sentences[0].split('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy with nlp to sent tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e64b55c31e662d3b8ca165241f15a246a93354fe580fc7a1249b2f351dbc5a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
