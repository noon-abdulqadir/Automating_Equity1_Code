{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a manually collected dictionary of incorrect/faulty keywords in scraped site data\n",
    "with open(f'{scraped_data}CBS/Data/keyword_trans_dict.txt') as f:\n",
    "    keyword_trans_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 111 words to fix\n",
    "len(keyword_trans_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_broken_linkedin_files(glob_path):\n",
    "    fix_list = []\n",
    "    data_dict = {}\n",
    "    data_list = []\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "\n",
    "        with open(glob_path, encoding = 'utf-8') as csv_file_handler:\n",
    "            csv_reader = csv.DictReader(csv_file_handler)\n",
    "\n",
    "            for rows in csv_reader:\n",
    "                first_key = str(list(rows.keys())[0])\n",
    "                key = rows[first_key]\n",
    "                data_dict[key] = rows\n",
    "\n",
    "        for num in data_dict:\n",
    "            data_list.append(data_dict[num])\n",
    "\n",
    "        with open(glob_path, 'w', encoding = 'utf-8') as json_file_handler:\n",
    "            json_file_handler.write(json.dumps(data_list, indent = 4))\n",
    "    \n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_keywords(df_temp):\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        for key, value in keyword_trans_dict.items():\n",
    "            df_temp.loc[\n",
    "                df_temp[df_temp['Search Keyword'].notnull()]['Search Keyword'].astype(str).apply(\n",
    "                lambda x: x.lower().strip()\n",
    "                ) == str(key).lower().strip(), 'Search Keyword'\n",
    "            ] = str(value).lower().strip()\n",
    "\n",
    "        unfixed = df_temp.loc[\n",
    "            df_temp[df_temp['Search Keyword'].notnull()]['Search Keyword'].astype(str).apply(lambda x: x.lower().strip()).isin([x.lower().strip() for x in list(keyword_trans_dict.keys())])\n",
    "        ]\n",
    "\n",
    "        if len(unfixed) != 0:\n",
    "            for key, value in keyword_trans_dict.items():\n",
    "                for idx, row in df_temp.iterrows():\n",
    "                    if row['Search Keyword'].astype(str).lower().strip() == str(key).lower().strip():\n",
    "                        df_temp.loc[idx, 'Search Keyword'] = str(value).lower().strip()\n",
    "    \n",
    "\n",
    "    return df_temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob_paths = []\n",
    "\n",
    "for site in site_list:\n",
    "    glob_paths.extend(glob.glob(f'{scraped_data}/{site}/Data/*.json')+glob.glob(f'{scraped_data}/{site}/Data/*.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 955 json and csv files\n",
    "len(glob_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use paths to open files, fix keywords, and drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix list catches all incorrect/faculty keyword search terms\n",
    "\n",
    "fix_list = []\n",
    "\n",
    "# Appended data catches all the fixed and cleaned dfs\n",
    "appended_data = []\n",
    "\n",
    "for glob_path in glob_paths:\n",
    "\n",
    "    if glob_path.endswith('.json'):\n",
    "        try:\n",
    "            df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "        except ValueError:\n",
    "            fix_list.append(glob_path)\n",
    "            if 'scraped_data/LinkedIn/Data/linkedin_jobs_df_' in glob_path:\n",
    "                data_json = fix_broken_linkedin_files(glob_path)\n",
    "                try:\n",
    "                    df_temp = pd.read_json(glob_path).reset_index(drop=True)\n",
    "                except ValueError:\n",
    "                    fix_list.append(glob_path)\n",
    "    elif glob_path.endswith('.csv'):\n",
    "        df_temp = pd.read_csv(glob_path).reset_index(drop=True)\n",
    "\n",
    "    if len(df_temp) > 0 and isinstance(df_temp, pd.DataFrame):\n",
    "        df_temp = fix_keywords(df_temp)\n",
    "        df_temp.reset_index(drop=True, inplace=True)\n",
    "        df_temp.drop(columns=cols, axis='columns', inplace=True, errors='ignore')\n",
    "        df_temp.drop(\n",
    "        df_temp.columns[\n",
    "                df_temp.columns.str.contains(\n",
    "                    'unnamed|index|level', regex=True, case=False, flags=re.I\n",
    "                )\n",
    "            ],\n",
    "            axis='columns',\n",
    "            inplace=True,\n",
    "            errors='ignore',\n",
    "        )\n",
    "    \n",
    "        if glob_path.endswith('.json'):\n",
    "            df_temp.to_json(glob_path, orient='records')\n",
    "        elif glob_path.endswith('.csv'):\n",
    "            df_temp.to_csv(glob_path, index=False)\n",
    "\n",
    "        appended_data.append(df_temp.reset_index(drop=True))\n",
    "\n",
    "# Concatonate list of dfs into one large df_jobs\n",
    "df_jobs = pd.concat(appended_data).reset_index(drop=True)\n",
    "\n",
    "# Save df_jobs to file\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dfs, len = 527\n",
    "len(appended_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204394\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we couldn't fix some keywords, we add them to list fix_list and write to file\n",
    "if len(fix_list) != 0:\n",
    "    print('Some keywords to fix!')\n",
    "    with open(f'{data_dir}fix_list.txt', 'w') as f:\n",
    "        json.dump(fix_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_RAW\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204394\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean columns\n",
    "df_jobs.columns = df_jobs.columns.to_series().apply(lambda x: str(x).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "df_jobs.dropna(axis='index', how='all', inplace=True)\n",
    "df_jobs.dropna(axis='columns', how='all', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 204394\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates on subset of 'Job Description'\n",
    "df_jobs.drop_duplicates(subset=['Job Description'], keep='first', ignore_index=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 62579\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows with missing 'Job Description'\n",
    "df_jobs.drop(\n",
    "    df_jobs.index[\n",
    "        df_jobs['Job Description'].isin(nan_list) | \n",
    "        df_jobs.loc[df_jobs['Job Description'].isnull(), 'Job Description'] | \n",
    "        df_jobs.loc[df_jobs['Job Description'].isna(), 'Job Description']\n",
    "    ], \n",
    "    axis='index',\n",
    "    inplace=True,\n",
    "    errors='ignore'\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 62577\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_jobs to file\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_dropped.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_dropped.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START HERE IF SOURCING FROM DF_RAW_DROPPED\n",
    "### PLEASE SET CORRECT DIRECTORY PATHS BELOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "mod = sys.modules[__name__]\n",
    "\n",
    "code_dir = None\n",
    "code_dir_name = 'Code'\n",
    "unwanted_subdir_name = 'Analysis'\n",
    "\n",
    "for _ in range(5):\n",
    "\n",
    "    parent_path = str(Path.cwd().parents[_]).split('/')[-1]\n",
    "\n",
    "    if (code_dir_name in parent_path) and (unwanted_subdir_name not in parent_path):\n",
    "\n",
    "        code_dir = str(Path.cwd().parents[_])\n",
    "\n",
    "        if code_dir is not None:\n",
    "            break\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN DIR\n",
    "main_dir = f'{str(Path(code_dir).parents[0])}/'\n",
    "\n",
    "# code_dir\n",
    "code_dir = f'{code_dir}/'\n",
    "sys.path.append(code_dir)\n",
    "\n",
    "# scraping dir\n",
    "scraped_data = f'{code_dir}scraped_data/'\n",
    "\n",
    "# data dir\n",
    "data_dir = f'{code_dir}data/'\n",
    "\n",
    "# lang models dir\n",
    "llm_path = f'{data_dir}Language Models'\n",
    "\n",
    "# sites\n",
    "site_list=['Indeed', 'Glassdoor', 'LinkedIn']\n",
    "\n",
    "# columns\n",
    "cols=['Sector', \n",
    "      'Sector Code', \n",
    "      'Gender', \n",
    "      'Age', \n",
    "      'Language', \n",
    "      'Dutch Requirement', \n",
    "      'English Requirement', \n",
    "      'Gender_Female', \n",
    "      'Gender_Mixed', \n",
    "      'Gender_Male', \n",
    "      'Age_Older', \n",
    "      'Age_Mixed', \n",
    "      'Age_Younger', \n",
    "      'Gender_Num', \n",
    "      'Age_Num', \n",
    "      '% Female', \n",
    "      '% Male', \n",
    "      '% Older', \n",
    "      '% Younger']\n",
    "\n",
    "int_variable: str = 'Job ID'\n",
    "str_variable: str = 'Job Description'\n",
    "gender: str = 'Gender'\n",
    "age: str = 'Age'\n",
    "language: str = 'en'\n",
    "str_cols = ['Search Keyword', 'Platform', 'Job ID', 'Job Title', 'Company Name', 'Location', 'Job Description', 'Company URL', 'Job URL', 'Tracking ID']\n",
    "nan_list = [None, 'None', '', ' ', [], -1, '-1', 0, '0', 'nan', np.nan, 'Nan']\n",
    "pattern = r'[\\n]+|[,]{2,}|[|]{2,}|[\\n\\r]+|(?<=[a-z]\\.)(?=\\s*[A-Z])|(?=\\:+[A-Z])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import googletrans\n",
    "from pathlib import Path\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.pipeline import Sentencizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_dropped.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62577"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 62577\n",
    "len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62577 entries, 0 to 62576\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Search Keyword     62577 non-null  object \n",
      " 1   Platform           62577 non-null  object \n",
      " 2   Job ID             62577 non-null  object \n",
      " 3   Job Title          62577 non-null  object \n",
      " 4   Company Name       62574 non-null  object \n",
      " 5   Location           62577 non-null  object \n",
      " 6   Job Description    62577 non-null  object \n",
      " 7   Rating             3975 non-null   float64\n",
      " 8   Employment Type    61995 non-null  object \n",
      " 9   Company URL        59263 non-null  object \n",
      " 10  Job URL            62577 non-null  object \n",
      " 11  Job Age            62577 non-null  object \n",
      " 12  Job Age Number     62577 non-null  object \n",
      " 13  Collection Date    62577 non-null  object \n",
      " 14  Data Row           58599 non-null  float64\n",
      " 15  Tracking ID        58599 non-null  object \n",
      " 16  Industry           59184 non-null  object \n",
      " 17  Job Date           58602 non-null  object \n",
      " 18  Type of ownership  582 non-null    object \n",
      "dtypes: float64(2), object(17)\n",
      "memory usage: 9.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_jobs.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 64684\n",
    "# len(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(idx, row, df_jobs):\n",
    "\n",
    "    translator = Translator()\n",
    "    googletrans_readtime_error = googletrans.client.httpx._client.httpcore._exceptions.ReadTimeout\n",
    "\n",
    "    try:\n",
    "        print(f'Row {idx}: Translation in progress.')\n",
    "        time.sleep(60)\n",
    "        df_jobs.loc[idx, 'Language'] = str(translator.detect(str(row['Job Description']).lower().strip()).lang)\n",
    "        print(f'Row {idx}: Translation done.')\n",
    "    except:# googletrans_readtime_error:\n",
    "        print(f'Row {idx}: Sleeping for an hour.')\n",
    "        print('-'*30)\n",
    "        time.sleep(3600)\n",
    "        print(f'Row {idx}: Done sleeping.')\n",
    "        print('-'*30)\n",
    "        df_jobs.loc[idx, 'Language'] = str(translator.detect(str(row['Job Description']).lower().strip()).lang)\n",
    "\n",
    "    return df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: Translation in progress.\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "googletrans_readtime_error = googletrans.client.httpx._client.httpcore._exceptions.ReadTimeout\n",
    "\n",
    "# try:\n",
    "#     time.sleep(60)\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "# except:\n",
    "#     time.sleep(3600)\n",
    "#     df_jobs['Language'] = df_jobs['Job Description'].apply(lambda x: translator.detect(str(x).lower().strip()).lang)\n",
    "\n",
    "for idx, row in df_jobs.iterrows():\n",
    "    # This part ensures we don't start lang detection from index 0 if some lang detection was already done\n",
    "    if len(str(row['Job Description'])) != 0:\n",
    "        if 'Language' in df_jobs.columns: # if language col exists, check for nan in lang\n",
    "            if type(row['Language']) == float and np.isnan(row['Language']): #if lang is nan, detect language\n",
    "                df_jobs = language_detection(idx, row, df_jobs)\n",
    "            else: # elif lang is not nan, skip and go to next idx\n",
    "                continue\n",
    "        else: # if language col does not exist, detect lang\n",
    "            df_jobs = language_detection(idx, row, df_jobs)\n",
    "\n",
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_language_detected.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_jobs) > 0 and isinstance(df_jobs, pd.DataFrame):\n",
    "    df_jobs.to_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl')\n",
    "    \n",
    "    df_jobs.to_csv(f'{data_dir}df_jobs_raw_language_detected.csv', index=False)\n",
    "else:\n",
    "    print(f'ERORR: LENGTH OF DF = {len(df_jobs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-english job descriptions\n",
    "# df_jobs.drop(df_jobs.index[df_jobs['Language'] == str(language)], axis='index', inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jobs = pd.read_pickle(f'{data_dir}df_jobs_raw_language_detected.pkl').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and count unique search keywords\n",
    "search_keywords = list(set(df_jobs['Search Keyword'].to_list()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(search_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keywords\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study1_3.10",
   "language": "python",
   "name": "study1_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e64b55c31e662d3b8ca165241f15a246a93354fe580fc7a1249b2f351dbc5a06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
